{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd882766-c479-48ce-a597-cdccea152c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.func import vmap, grad\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as spstats\n",
    "import matplotlib.pyplot as plt\n",
    "import DGM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f739b77-1537-4ad3-9f2f-f7e5ff45b8de",
   "metadata": {},
   "source": [
    "# Deep Galerkin Method for the High-dimensional Black-Scholes Equation\n",
    "\n",
    "An extension of **DGM: A deep learning algorithm for solving partial diï¬€erential equations** \n",
    "\n",
    "<br>\n",
    "\n",
    "Bilal Saleh Husain, 2024-25\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd1290b-e305-41db-aace-b6509e2fbe90",
   "metadata": {},
   "source": [
    "### Equation, Simulation, & DGM Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1464df8-6a40-4779-8bf0-e71122e6190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the problem\n",
    "\n",
    "T = 1.0             # Maturity time\n",
    "delta = 2 / 3       # Recovery rate\n",
    "R = 0.02            # Risk-free interest rate\n",
    "mu_bar = 0.02       # Drift coefficient\n",
    "sigma_bar = 0.2     # Volatility coefficient\n",
    "vh = 50             # Threshold for intensity function\n",
    "vl = 70             # Threshold for intensity function\n",
    "gamma_h = 0.2       # High intensity\n",
    "gamma_l = 0.02      # Low intensity\n",
    "\n",
    "S0 = 100\n",
    "d=100               # Spatial dimension of problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df2416f0-5dca-4020-9dfa-6bdbf3395ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network parameters\n",
    "\n",
    "layer_width = 50         # Number of units per layer\n",
    "n_layers = 3              # Number of LSTM layers\n",
    "input_dim = 100           # Dimension of the underlying assets (x1, x2, ..., x100)\n",
    "learning_rate = 0.001    # Learning rate for the optimizer\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "\n",
    "sampling_stages = 15000     # Number of sampling stages\n",
    "steps_per_sample = 1     # Number of optimization steps per sample\n",
    "nSim_interior = 5000      # Number of interior samples\n",
    "nSim_terminal = 5000      # Number of terminal samples\n",
    "\n",
    "\n",
    "# Domain bounds\n",
    "\n",
    "t_low = 0    # Lower bound for time\n",
    "S_low = 50.0      # Lower bound for asset prices\n",
    "S_high = 180.0   # Upper bound for asset prices "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c554d01c-e36d-41ed-86ed-c5ec36356682",
   "metadata": {},
   "source": [
    "### Terminal Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "762f40ce-7af0-4509-bb8f-058adfe4edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminal condition g(x)\n",
    "def terminal_condition(x):\n",
    "    \"\"\"\n",
    "    Terminal condition g(x) = min{x1, x2, ..., x100}.\n",
    "    \"\"\"\n",
    "    return torch.min(x, dim=1, keepdim=True).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81b08ef-f01a-44bc-b24f-56a8ceb641c1",
   "metadata": {},
   "source": [
    "### Samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87f586c8-f6af-452b-a399-d091e6fd9bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampler(nSim_interior, nSim_terminal, S0=100.0):\n",
    "    \"\"\"\n",
    "    Generate training samples for the DGM network.\n",
    "    Args:\n",
    "        nSim_interior: Number of interior samples\n",
    "        nSim_terminal: Number of terminal samples\n",
    "        S0: Initial asset price (not used in uniform sampling)\n",
    "    Returns:\n",
    "        Tuple of torch tensors (t_interior, S_interior, t_terminal, S_terminal)\n",
    "    \"\"\"\n",
    "    # Interior samples - time uniformly in [0,T], assets in [50,150] around thresholds\n",
    "    t_interior = np.random.uniform(low=0, high=T, size=[nSim_interior, 1])\n",
    "    S_interior = np.random.uniform(low=S_low, high=S_high, size=[nSim_interior, 100])\n",
    "    \n",
    "    # Terminal samples - fixed time T, assets in same range\n",
    "    t_terminal = np.ones((nSim_terminal, 1)) * T\n",
    "    S_terminal = np.random.uniform(low=S_low, high=S_high, size=[nSim_terminal, 100])\n",
    "    \n",
    "    # Convert to torch tensors\n",
    "    t_interior = torch.tensor(t_interior, dtype=torch.float32, requires_grad=True)\n",
    "    S_interior = torch.tensor(S_interior, dtype=torch.float32, requires_grad=True)\n",
    "    t_terminal = torch.tensor(t_terminal, dtype=torch.float32)\n",
    "    S_terminal = torch.tensor(S_terminal, dtype=torch.float32)\n",
    "    \n",
    "    return t_interior, S_interior, t_terminal, S_terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60079290-7945-49e9-b710-a9a865e88b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampler(nSim_interior, nSim_terminal, S0=100.0):\n",
    "    # Interior samples - time uniformly in [0,T], assets lognormally distributed\n",
    "    t_interior = np.random.uniform(low=0.0, high=T, size=[nSim_interior, 1])\n",
    "    \n",
    "    # Generate correlated Brownian motions (if needed)\n",
    "    Z = np.random.normal(0, 1, size=[nSim_interior, 100])  # Standard Gaussian\n",
    "    \n",
    "    # Scale volatility by sqrt(t) for interior samples\n",
    "    S_interior = S0 * np.exp(\n",
    "        (mu_bar - 0.5 * sigma_bar**2) * t_interior + \n",
    "        sigma_bar * np.sqrt(t_interior) * Z\n",
    "    )\n",
    "    \n",
    "    # Terminal samples - fixed time T\n",
    "    t_terminal = np.ones((nSim_terminal, 1)) * T\n",
    "    Z_terminal = np.random.normal(0, 1, size=[nSim_terminal, 100])\n",
    "    S_terminal = S0 * np.exp(\n",
    "        (mu_bar - 0.5 * sigma_bar**2) * T + \n",
    "        sigma_bar * np.sqrt(T) * Z_terminal\n",
    "    )\n",
    "    \n",
    "    # Convert to torch tensors\n",
    "    t_interior = torch.tensor(t_interior, dtype=torch.float32, requires_grad=True)\n",
    "    S_interior = torch.tensor(S_interior, dtype=torch.float32, requires_grad=True)\n",
    "    t_terminal = torch.tensor(t_terminal, dtype=torch.float32)\n",
    "    S_terminal = torch.tensor(S_terminal, dtype=torch.float32)\n",
    "    \n",
    "    return t_interior, S_interior, t_terminal, S_terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc193d39-5cc4-4f4e-ac07-8385def519e0",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d4a2266-b198-4f0b-8486-a46143319caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, t_interior, S_interior, t_terminal, S_terminal):\n",
    "\n",
    "    # PDE Loss Calculation\n",
    "    t_interior.requires_grad_(True)\n",
    "    S_interior.requires_grad_(True)\n",
    "    \n",
    "    V = model(t_interior, S_interior)\n",
    "    \n",
    "    # Compute first derivatives\n",
    "    V_t = torch.autograd.grad(V.sum(), t_interior, create_graph=True)[0]\n",
    "    V_x = torch.autograd.grad(V.sum(), S_interior, create_graph=True)[0]\n",
    "\n",
    "    \n",
    "    # ---- torch.func Hessian diagonal ----\n",
    "    def get_second_derivative(S_batch):\n",
    "        \"\"\"Helper function for vmap\"\"\"\n",
    "        S_batch = S_batch.unsqueeze(0)  # Add batch dim\n",
    "        t_batch = t_interior[0:1]  # Reuse time from first sample\n",
    "        \n",
    "        def first_deriv(S):\n",
    "            V = model(t_batch, S)\n",
    "            return torch.autograd.grad(V.sum(), S, create_graph=True)[0]\n",
    "        \n",
    "        # Compute diagonal Hessian elements\n",
    "        hessian_diag = grad(lambda S: first_deriv(S).sum())(S_batch)[0]\n",
    "        return hessian_diag.squeeze(0)\n",
    "    \n",
    "    # Vectorize over batch dimension\n",
    "    V_xx = vmap(get_second_derivative)(S_interior)\n",
    "    # ---- end torch.func ----\n",
    "\n",
    "    # Vectorized Q(V) calculation\n",
    "    Q_V = torch.where(V < vh, gamma_h,\n",
    "                     torch.where(V >= vl, gamma_l,\n",
    "                               ((gamma_h - gamma_l)/(vh - vl)) * (V - vh) + gamma_h))\n",
    "    \n",
    "    # PDE residual\n",
    "    pde_residual = (\n",
    "        V_t\n",
    "        + mu_bar * torch.sum(S_interior * V_x, dim=1, keepdim=True)\n",
    "        + 0.5 * sigma_bar**2 * torch.sum(S_interior**2 * V_xx, dim=1, keepdim=True)\n",
    "        - (1 - delta) * Q_V * V\n",
    "        - R * V\n",
    "    )\n",
    "\n",
    "    L_pde = torch.mean(pde_residual**2)\n",
    "    \n",
    "    # Terminal Condition Loss\n",
    "    target_payoff = torch.min(S_terminal, dim=1, keepdim=True)[0]  # min payoff\n",
    "    fitted_payoff = model(t_terminal, S_terminal)\n",
    "    L_terminal = torch.mean((fitted_payoff - target_payoff)**2)\n",
    "    \n",
    "    return L_pde, L_terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0568cd-0a94-4893-94cb-976fc2231514",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3f6017-5854-4897-9e41-5a9e12c6558c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 3595.395263671875, L1: 15.67109489440918, L3: 3579.72412109375\n",
      "Current prediction:  8.875784873962402 \n",
      "\n",
      "Iteration 1, Loss: 3012.50634765625, L1: 15.328348159790039, L3: 2997.177978515625\n",
      "Current prediction:  12.202801704406738 \n",
      "\n",
      "Iteration 2, Loss: 2551.241455078125, L1: 18.737104415893555, L3: 2532.50439453125\n",
      "Current prediction:  17.002155303955078 \n",
      "\n",
      "Iteration 3, Loss: 2179.649169921875, L1: 19.707265853881836, L3: 2159.94189453125\n",
      "Current prediction:  19.35318946838379 \n",
      "\n",
      "Iteration 4, Loss: 1878.2037353515625, L1: 19.64272117614746, L3: 1858.56103515625\n",
      "Current prediction:  22.084819793701172 \n",
      "\n",
      "Iteration 5, Loss: 1650.39599609375, L1: 18.334640502929688, L3: 1632.0614013671875\n",
      "Current prediction:  26.287307739257812 \n",
      "\n",
      "Iteration 6, Loss: 1472.5941162109375, L1: 26.435155868530273, L3: 1446.158935546875\n",
      "Current prediction:  28.83115577697754 \n",
      "\n",
      "Iteration 7, Loss: 1316.9522705078125, L1: 19.610191345214844, L3: 1297.342041015625\n",
      "Current prediction:  29.485971450805664 \n",
      "\n",
      "Iteration 8, Loss: 1225.717529296875, L1: 31.419721603393555, L3: 1194.2978515625\n",
      "Current prediction:  30.730009078979492 \n",
      "\n",
      "Iteration 9, Loss: 1127.7008056640625, L1: 24.115785598754883, L3: 1103.5849609375\n",
      "Current prediction:  31.009429931640625 \n",
      "\n",
      "Iteration 10, Loss: 1062.1531982421875, L1: 23.152530670166016, L3: 1039.0006103515625\n",
      "Current prediction:  31.457435607910156 \n",
      "\n",
      "Iteration 11, Loss: 996.44921875, L1: 22.73270606994629, L3: 973.7164916992188\n",
      "Current prediction:  32.04115676879883 \n",
      "\n",
      "Iteration 12, Loss: 938.9838256835938, L1: 22.22179412841797, L3: 916.7620239257812\n",
      "Current prediction:  32.99720764160156 \n",
      "\n",
      "Iteration 13, Loss: 906.6508178710938, L1: 21.27288055419922, L3: 885.3779296875\n",
      "Current prediction:  33.28644943237305 \n",
      "\n",
      "Iteration 14, Loss: 867.2256469726562, L1: 19.96689224243164, L3: 847.2587280273438\n",
      "Current prediction:  33.47545623779297 \n",
      "\n",
      "Iteration 15, Loss: 833.494140625, L1: 17.91097068786621, L3: 815.5831909179688\n",
      "Current prediction:  33.67148971557617 \n",
      "\n",
      "Iteration 16, Loss: 805.7759399414062, L1: 18.403200149536133, L3: 787.3727416992188\n",
      "Current prediction:  33.968177795410156 \n",
      "\n",
      "Iteration 17, Loss: 793.6220703125, L1: 17.82967185974121, L3: 775.7924194335938\n",
      "Current prediction:  34.160743713378906 \n",
      "\n",
      "Iteration 18, Loss: 778.6421508789062, L1: 19.41363525390625, L3: 759.228515625\n",
      "Current prediction:  34.353553771972656 \n",
      "\n",
      "Iteration 19, Loss: 759.3514404296875, L1: 17.887466430664062, L3: 741.4639892578125\n",
      "Current prediction:  34.54646682739258 \n",
      "\n",
      "Iteration 20, Loss: 749.9373168945312, L1: 17.274181365966797, L3: 732.6631469726562\n",
      "Current prediction:  34.73942184448242 \n",
      "\n",
      "Iteration 21, Loss: 728.6055297851562, L1: 15.445322036743164, L3: 713.1602172851562\n",
      "Current prediction:  34.93297576904297 \n",
      "\n",
      "Iteration 22, Loss: 729.0232543945312, L1: 16.773401260375977, L3: 712.2498779296875\n",
      "Current prediction:  35.165626525878906 \n",
      "\n",
      "Iteration 23, Loss: 702.5887451171875, L1: 17.450725555419922, L3: 685.1380004882812\n",
      "Current prediction:  35.68452072143555 \n",
      "\n",
      "Iteration 24, Loss: 693.1240234375, L1: 15.87135124206543, L3: 677.252685546875\n",
      "Current prediction:  35.9527587890625 \n",
      "\n",
      "Iteration 25, Loss: 680.6768798828125, L1: 17.089567184448242, L3: 663.5873413085938\n",
      "Current prediction:  36.14958953857422 \n",
      "\n",
      "Iteration 26, Loss: 665.8634643554688, L1: 18.077360153198242, L3: 647.7861328125\n",
      "Current prediction:  36.34612274169922 \n",
      "\n",
      "Iteration 27, Loss: 645.8233642578125, L1: 15.600872039794922, L3: 630.2224731445312\n",
      "Current prediction:  36.542293548583984 \n",
      "\n",
      "Iteration 28, Loss: 633.4908447265625, L1: 16.347490310668945, L3: 617.1433715820312\n",
      "Current prediction:  36.73801040649414 \n",
      "\n",
      "Iteration 29, Loss: 622.6712036132812, L1: 14.095250129699707, L3: 608.575927734375\n",
      "Current prediction:  36.93327331542969 \n",
      "\n",
      "Iteration 30, Loss: 615.1640625, L1: 16.086849212646484, L3: 599.0772094726562\n",
      "Current prediction:  37.15915298461914 \n",
      "\n",
      "Iteration 31, Loss: 598.5616455078125, L1: 13.724227905273438, L3: 584.83740234375\n",
      "Current prediction:  37.448509216308594 \n",
      "\n",
      "Iteration 32, Loss: 593.8131103515625, L1: 17.044971466064453, L3: 576.7681274414062\n",
      "Current prediction:  37.64438247680664 \n",
      "\n",
      "Iteration 33, Loss: 585.493896484375, L1: 15.997364044189453, L3: 569.4965209960938\n",
      "Current prediction:  37.83967971801758 \n",
      "\n",
      "Iteration 34, Loss: 577.4052124023438, L1: 16.21048927307129, L3: 561.1947021484375\n",
      "Current prediction:  38.03440856933594 \n",
      "\n",
      "Iteration 35, Loss: 570.9462280273438, L1: 14.981339454650879, L3: 555.9649047851562\n",
      "Current prediction:  38.228580474853516 \n",
      "\n",
      "Iteration 36, Loss: 561.6732788085938, L1: 15.342859268188477, L3: 546.3304443359375\n",
      "Current prediction:  38.42216873168945 \n",
      "\n",
      "Iteration 37, Loss: 544.6190185546875, L1: 14.432720184326172, L3: 530.186279296875\n",
      "Current prediction:  38.97591018676758 \n",
      "\n",
      "Iteration 38, Loss: 539.4595336914062, L1: 17.150611877441406, L3: 522.3088989257812\n",
      "Current prediction:  39.17005920410156 \n",
      "\n",
      "Iteration 39, Loss: 525.16455078125, L1: 17.151153564453125, L3: 508.0133972167969\n",
      "Current prediction:  39.36341857910156 \n",
      "\n",
      "Iteration 40, Loss: 517.7595825195312, L1: 16.54387855529785, L3: 501.2156982421875\n",
      "Current prediction:  39.556034088134766 \n",
      "\n",
      "Iteration 41, Loss: 502.44091796875, L1: 15.595035552978516, L3: 486.84588623046875\n",
      "Current prediction:  39.747825622558594 \n",
      "\n",
      "Iteration 42, Loss: 491.40557861328125, L1: 14.266884803771973, L3: 477.1387023925781\n",
      "Current prediction:  39.93876266479492 \n",
      "\n",
      "Iteration 43, Loss: 479.8387756347656, L1: 15.46455192565918, L3: 464.3742370605469\n",
      "Current prediction:  40.128719329833984 \n",
      "\n",
      "Iteration 44, Loss: 471.42559814453125, L1: 15.667445182800293, L3: 455.7581481933594\n",
      "Current prediction:  40.31769943237305 \n",
      "\n",
      "Iteration 45, Loss: 467.2119445800781, L1: 15.74593448638916, L3: 451.46600341796875\n",
      "Current prediction:  40.505767822265625 \n",
      "\n",
      "Iteration 46, Loss: 455.96441650390625, L1: 16.009450912475586, L3: 439.9549560546875\n",
      "Current prediction:  40.692840576171875 \n",
      "\n",
      "Iteration 47, Loss: 449.47808837890625, L1: 15.485349655151367, L3: 433.99273681640625\n",
      "Current prediction:  40.87897872924805 \n",
      "\n",
      "Iteration 48, Loss: 436.9101257324219, L1: 13.82557201385498, L3: 423.0845642089844\n",
      "Current prediction:  41.064117431640625 \n",
      "\n",
      "Iteration 49, Loss: 431.9458923339844, L1: 15.160326957702637, L3: 416.7855529785156\n",
      "Current prediction:  41.24827194213867 \n",
      "\n",
      "Iteration 50, Loss: 429.72314453125, L1: 15.506696701049805, L3: 414.2164611816406\n",
      "Current prediction:  41.431522369384766 \n",
      "\n",
      "Iteration 51, Loss: 414.29534912109375, L1: 13.849717140197754, L3: 400.44561767578125\n",
      "Current prediction:  41.61375045776367 \n",
      "\n",
      "Iteration 52, Loss: 407.130859375, L1: 14.59733772277832, L3: 392.53350830078125\n",
      "Current prediction:  41.7949104309082 \n",
      "\n",
      "Iteration 53, Loss: 402.4738464355469, L1: 14.463587760925293, L3: 388.01025390625\n",
      "Current prediction:  41.97505569458008 \n",
      "\n",
      "Iteration 54, Loss: 393.22052001953125, L1: 14.918771743774414, L3: 378.3017578125\n",
      "Current prediction:  42.15412139892578 \n",
      "\n",
      "Iteration 55, Loss: 385.2303771972656, L1: 13.594215393066406, L3: 371.63616943359375\n",
      "Current prediction:  42.3321418762207 \n",
      "\n",
      "Iteration 56, Loss: 383.662353515625, L1: 15.042670249938965, L3: 368.61968994140625\n",
      "Current prediction:  42.5091552734375 \n",
      "\n",
      "Iteration 57, Loss: 372.966552734375, L1: 15.379912376403809, L3: 357.5866394042969\n",
      "Current prediction:  42.68507766723633 \n",
      "\n",
      "Iteration 58, Loss: 367.7085266113281, L1: 14.54645824432373, L3: 353.1620788574219\n",
      "Current prediction:  42.859981536865234 \n",
      "\n",
      "Iteration 59, Loss: 366.6398620605469, L1: 15.423847198486328, L3: 351.21600341796875\n",
      "Current prediction:  43.0339469909668 \n",
      "\n",
      "Iteration 60, Loss: 352.8506774902344, L1: 14.224526405334473, L3: 338.62615966796875\n",
      "Current prediction:  43.2067985534668 \n",
      "\n",
      "Iteration 61, Loss: 348.6734619140625, L1: 14.611218452453613, L3: 334.062255859375\n",
      "Current prediction:  43.378623962402344 \n",
      "\n",
      "Iteration 62, Loss: 340.38641357421875, L1: 14.78113079071045, L3: 325.60528564453125\n",
      "Current prediction:  43.54936981201172 \n",
      "\n",
      "Iteration 63, Loss: 335.1131591796875, L1: 15.046331405639648, L3: 320.06683349609375\n",
      "Current prediction:  43.71903610229492 \n",
      "\n",
      "Iteration 64, Loss: 328.0216369628906, L1: 14.5659761428833, L3: 313.4556579589844\n",
      "Current prediction:  44.3184700012207 \n",
      "\n",
      "Iteration 65, Loss: 316.8127746582031, L1: 14.733329772949219, L3: 302.0794372558594\n",
      "Current prediction:  44.512874603271484 \n",
      "\n",
      "Iteration 66, Loss: 309.8728942871094, L1: 14.983860969543457, L3: 294.8890380859375\n",
      "Current prediction:  44.680938720703125 \n",
      "\n",
      "Iteration 67, Loss: 301.1976013183594, L1: 15.008795738220215, L3: 286.1888122558594\n",
      "Current prediction:  44.84779357910156 \n",
      "\n",
      "Iteration 68, Loss: 297.3834533691406, L1: 15.261659622192383, L3: 282.1217956542969\n",
      "Current prediction:  45.013484954833984 \n",
      "\n",
      "Iteration 69, Loss: 290.5645751953125, L1: 15.484046936035156, L3: 275.0805358886719\n",
      "Current prediction:  45.178016662597656 \n",
      "\n",
      "Iteration 70, Loss: 285.08984375, L1: 15.497858047485352, L3: 269.59197998046875\n",
      "Current prediction:  45.341346740722656 \n",
      "\n",
      "Iteration 71, Loss: 276.2184143066406, L1: 15.564598083496094, L3: 260.65380859375\n",
      "Current prediction:  45.503395080566406 \n",
      "\n",
      "Iteration 72, Loss: 271.55157470703125, L1: 15.57889175415039, L3: 255.97267150878906\n",
      "Current prediction:  45.6641960144043 \n",
      "\n",
      "Iteration 73, Loss: 270.8100891113281, L1: 15.921150207519531, L3: 254.88893127441406\n",
      "Current prediction:  45.82383346557617 \n",
      "\n",
      "Iteration 74, Loss: 265.9460144042969, L1: 15.847110748291016, L3: 250.09890747070312\n",
      "Current prediction:  45.98237991333008 \n",
      "\n",
      "Iteration 75, Loss: 267.00262451171875, L1: 16.37396240234375, L3: 250.62864685058594\n",
      "Current prediction:  46.13996124267578 \n",
      "\n",
      "Iteration 76, Loss: 258.9959716796875, L1: 16.622879028320312, L3: 242.3730926513672\n",
      "Current prediction:  46.29644775390625 \n",
      "\n",
      "Iteration 77, Loss: 252.0487823486328, L1: 16.41133689880371, L3: 235.637451171875\n",
      "Current prediction:  46.4517936706543 \n",
      "\n",
      "Iteration 78, Loss: 250.1382598876953, L1: 17.388853073120117, L3: 232.74940490722656\n",
      "Current prediction:  46.60599899291992 \n",
      "\n",
      "Iteration 79, Loss: 244.43862915039062, L1: 16.464309692382812, L3: 227.9743194580078\n",
      "Current prediction:  46.75911331176758 \n",
      "\n",
      "Iteration 80, Loss: 240.3094482421875, L1: 16.54384994506836, L3: 223.76559448242188\n",
      "Current prediction:  46.911155700683594 \n",
      "\n",
      "Iteration 81, Loss: 235.4365234375, L1: 17.578357696533203, L3: 217.85816955566406\n",
      "Current prediction:  47.06205749511719 \n",
      "\n",
      "Iteration 82, Loss: 233.61585998535156, L1: 17.687864303588867, L3: 215.92799377441406\n",
      "Current prediction:  47.21187973022461 \n",
      "\n",
      "Iteration 83, Loss: 227.1272430419922, L1: 17.10264778137207, L3: 210.02459716796875\n",
      "Current prediction:  47.360572814941406 \n",
      "\n",
      "Iteration 84, Loss: 228.4358367919922, L1: 17.9764461517334, L3: 210.4593963623047\n",
      "Current prediction:  47.508296966552734 \n",
      "\n",
      "Iteration 85, Loss: 219.61181640625, L1: 17.670188903808594, L3: 201.94161987304688\n",
      "Current prediction:  47.654903411865234 \n",
      "\n",
      "Iteration 86, Loss: 214.98287963867188, L1: 17.602136611938477, L3: 197.3807373046875\n",
      "Current prediction:  47.80038833618164 \n",
      "\n",
      "Iteration 87, Loss: 209.15554809570312, L1: 17.72503662109375, L3: 191.43051147460938\n",
      "Current prediction:  47.94464111328125 \n",
      "\n",
      "Iteration 88, Loss: 208.32958984375, L1: 18.19773292541504, L3: 190.13185119628906\n",
      "Current prediction:  48.08777618408203 \n",
      "\n",
      "Iteration 89, Loss: 201.4331512451172, L1: 18.288097381591797, L3: 183.14505004882812\n",
      "Current prediction:  48.229644775390625 \n",
      "\n",
      "Iteration 90, Loss: 199.7825927734375, L1: 18.430782318115234, L3: 181.351806640625\n",
      "Current prediction:  48.37039566040039 \n",
      "\n",
      "Iteration 91, Loss: 192.93162536621094, L1: 18.643957138061523, L3: 174.2876739501953\n",
      "Current prediction:  48.50984573364258 \n",
      "\n",
      "Iteration 92, Loss: 185.25384521484375, L1: 18.135774612426758, L3: 167.11807250976562\n",
      "Current prediction:  49.404579162597656 \n",
      "\n",
      "Iteration 93, Loss: 182.29586791992188, L1: 18.255704879760742, L3: 164.0401611328125\n",
      "Current prediction:  49.58781051635742 \n",
      "\n",
      "Iteration 94, Loss: 173.2776641845703, L1: 18.50619125366211, L3: 154.77146911621094\n",
      "Current prediction:  49.724517822265625 \n",
      "\n",
      "Iteration 95, Loss: 166.82923889160156, L1: 18.725114822387695, L3: 148.1041259765625\n",
      "Current prediction:  50.3343505859375 \n",
      "\n",
      "Iteration 96, Loss: 160.47567749023438, L1: 18.869361877441406, L3: 141.60630798339844\n",
      "Current prediction:  50.596160888671875 \n",
      "\n",
      "Iteration 97, Loss: 154.73150634765625, L1: 18.722618103027344, L3: 136.00888061523438\n",
      "Current prediction:  50.730125427246094 \n",
      "\n",
      "Iteration 98, Loss: 148.26446533203125, L1: 19.19210433959961, L3: 129.07235717773438\n",
      "Current prediction:  50.86281967163086 \n",
      "\n",
      "Iteration 99, Loss: 143.6356201171875, L1: 18.722881317138672, L3: 124.91273498535156\n",
      "Current prediction:  50.994163513183594 \n",
      "\n",
      "Iteration 100, Loss: 142.231689453125, L1: 18.36896324157715, L3: 123.86272430419922\n",
      "Current prediction:  51.12432098388672 \n",
      "\n",
      "Iteration 101, Loss: 137.52911376953125, L1: 18.3693790435791, L3: 119.15973663330078\n",
      "Current prediction:  51.25318908691406 \n",
      "\n",
      "Iteration 102, Loss: 135.54759216308594, L1: 18.286542892456055, L3: 117.26104736328125\n",
      "Current prediction:  51.38075256347656 \n",
      "\n",
      "Iteration 103, Loss: 135.27818298339844, L1: 18.271982192993164, L3: 117.0062026977539\n",
      "Current prediction:  51.50722885131836 \n",
      "\n",
      "Iteration 104, Loss: 129.58599853515625, L1: 18.007484436035156, L3: 111.5785140991211\n",
      "Current prediction:  51.6324462890625 \n",
      "\n",
      "Iteration 105, Loss: 128.28712463378906, L1: 17.88020896911621, L3: 110.40692138671875\n",
      "Current prediction:  51.75643539428711 \n",
      "\n",
      "Iteration 106, Loss: 127.10822296142578, L1: 17.897369384765625, L3: 109.21085357666016\n",
      "Current prediction:  51.87931442260742 \n",
      "\n",
      "Iteration 107, Loss: 121.38545989990234, L1: 17.91880989074707, L3: 103.4666519165039\n",
      "Current prediction:  52.000823974609375 \n",
      "\n",
      "Iteration 108, Loss: 120.2545166015625, L1: 17.736108779907227, L3: 102.5184097290039\n",
      "Current prediction:  52.1210823059082 \n",
      "\n",
      "Iteration 109, Loss: 116.94354248046875, L1: 17.522653579711914, L3: 99.42089080810547\n",
      "Current prediction:  52.24016189575195 \n",
      "\n",
      "Iteration 110, Loss: 116.61943054199219, L1: 17.508516311645508, L3: 99.11091613769531\n",
      "Current prediction:  52.358097076416016 \n",
      "\n",
      "Iteration 111, Loss: 114.7549819946289, L1: 17.487581253051758, L3: 97.26740264892578\n",
      "Current prediction:  52.47489929199219 \n",
      "\n",
      "Iteration 112, Loss: 115.93563079833984, L1: 17.362092971801758, L3: 98.57353973388672\n",
      "Current prediction:  52.59077072143555 \n",
      "\n",
      "Iteration 113, Loss: 108.99962615966797, L1: 17.273191452026367, L3: 91.72643280029297\n",
      "Current prediction:  52.705501556396484 \n",
      "\n",
      "Iteration 114, Loss: 107.8537826538086, L1: 17.620389938354492, L3: 90.23339080810547\n",
      "Current prediction:  52.81902313232422 \n",
      "\n",
      "Iteration 115, Loss: 106.66062927246094, L1: 17.099016189575195, L3: 89.56161499023438\n",
      "Current prediction:  52.93143081665039 \n",
      "\n",
      "Iteration 116, Loss: 104.45979309082031, L1: 17.093088150024414, L3: 87.36670684814453\n",
      "Current prediction:  53.04281234741211 \n",
      "\n",
      "Iteration 117, Loss: 103.27165222167969, L1: 17.503080368041992, L3: 85.76856994628906\n",
      "Current prediction:  53.15306091308594 \n",
      "\n",
      "Iteration 118, Loss: 99.95103454589844, L1: 16.907123565673828, L3: 83.04391479492188\n",
      "Current prediction:  53.26209259033203 \n",
      "\n",
      "Iteration 119, Loss: 100.2059326171875, L1: 17.05805206298828, L3: 83.14788055419922\n",
      "Current prediction:  53.37010955810547 \n",
      "\n",
      "Iteration 120, Loss: 94.43211364746094, L1: 16.758399963378906, L3: 77.67371368408203\n",
      "Current prediction:  53.476783752441406 \n",
      "\n",
      "Iteration 121, Loss: 95.22843933105469, L1: 16.724567413330078, L3: 78.50386810302734\n",
      "Current prediction:  53.58230209350586 \n",
      "\n",
      "Iteration 122, Loss: 95.35067749023438, L1: 17.40884780883789, L3: 77.94182586669922\n",
      "Current prediction:  53.68681716918945 \n",
      "\n",
      "Iteration 123, Loss: 91.39100646972656, L1: 16.660432815551758, L3: 74.73057556152344\n",
      "Current prediction:  53.79020309448242 \n",
      "\n",
      "Iteration 124, Loss: 95.02469635009766, L1: 16.900300979614258, L3: 78.12439727783203\n",
      "Current prediction:  53.89288330078125 \n",
      "\n",
      "Iteration 125, Loss: 90.8857650756836, L1: 16.733789443969727, L3: 74.1519775390625\n",
      "Current prediction:  53.99457931518555 \n",
      "\n",
      "Iteration 126, Loss: 90.20929718017578, L1: 16.720184326171875, L3: 73.4891128540039\n",
      "Current prediction:  54.09538269042969 \n",
      "\n",
      "Iteration 127, Loss: 88.12738037109375, L1: 16.752296447753906, L3: 71.37508392333984\n",
      "Current prediction:  54.195213317871094 \n",
      "\n",
      "Iteration 128, Loss: 87.34930419921875, L1: 16.958717346191406, L3: 70.39058685302734\n",
      "Current prediction:  54.2940559387207 \n",
      "\n",
      "Iteration 129, Loss: 85.62478637695312, L1: 16.660024642944336, L3: 68.96475982666016\n",
      "Current prediction:  54.39204025268555 \n",
      "\n",
      "Iteration 130, Loss: 84.49880981445312, L1: 17.169296264648438, L3: 67.32951354980469\n",
      "Current prediction:  54.4891242980957 \n",
      "\n",
      "Iteration 131, Loss: 82.65255737304688, L1: 16.45903205871582, L3: 66.19352722167969\n",
      "Current prediction:  54.585201263427734 \n",
      "\n",
      "Iteration 132, Loss: 84.2274169921875, L1: 16.892099380493164, L3: 67.33531951904297\n",
      "Current prediction:  54.68050003051758 \n",
      "\n",
      "Iteration 133, Loss: 83.95368957519531, L1: 16.619487762451172, L3: 67.3342056274414\n",
      "Current prediction:  54.775123596191406 \n",
      "\n",
      "Iteration 134, Loss: 80.84716796875, L1: 16.34178924560547, L3: 64.50537872314453\n",
      "Current prediction:  54.86885452270508 \n",
      "\n",
      "Iteration 135, Loss: 79.29261016845703, L1: 16.079919815063477, L3: 63.21269226074219\n",
      "Current prediction:  54.961639404296875 \n",
      "\n",
      "Iteration 136, Loss: 79.8486328125, L1: 16.352460861206055, L3: 63.49617385864258\n",
      "Current prediction:  55.053680419921875 \n",
      "\n",
      "Iteration 137, Loss: 78.59832763671875, L1: 16.061037063598633, L3: 62.53729248046875\n",
      "Current prediction:  55.144935607910156 \n",
      "\n",
      "Iteration 138, Loss: 80.6263427734375, L1: 16.09531021118164, L3: 64.5310287475586\n",
      "Current prediction:  55.23565673828125 \n",
      "\n",
      "Iteration 139, Loss: 78.0380859375, L1: 16.053356170654297, L3: 61.98472595214844\n",
      "Current prediction:  55.325782775878906 \n",
      "\n",
      "Iteration 140, Loss: 76.01741027832031, L1: 16.13275718688965, L3: 59.88465118408203\n",
      "Current prediction:  55.41518783569336 \n",
      "\n",
      "Iteration 141, Loss: 76.64405059814453, L1: 15.926107406616211, L3: 60.71794509887695\n",
      "Current prediction:  55.50389862060547 \n",
      "\n",
      "Iteration 142, Loss: 74.14268493652344, L1: 15.74738597869873, L3: 58.395301818847656\n",
      "Current prediction:  55.59181594848633 \n",
      "\n",
      "Iteration 143, Loss: 71.81523895263672, L1: 15.72461223602295, L3: 56.09062576293945\n",
      "Current prediction:  55.67877197265625 \n",
      "\n",
      "Iteration 144, Loss: 71.86277770996094, L1: 15.703917503356934, L3: 56.15886306762695\n",
      "Current prediction:  55.76491928100586 \n",
      "\n",
      "Iteration 145, Loss: 68.75972747802734, L1: 15.541852951049805, L3: 53.21787643432617\n",
      "Current prediction:  55.85007095336914 \n",
      "\n",
      "Iteration 146, Loss: 68.29292297363281, L1: 15.830787658691406, L3: 52.46213150024414\n",
      "Current prediction:  55.93427276611328 \n",
      "\n",
      "Iteration 147, Loss: 68.00895690917969, L1: 15.704704284667969, L3: 52.30424880981445\n",
      "Current prediction:  56.017478942871094 \n",
      "\n",
      "Iteration 148, Loss: 65.39261627197266, L1: 15.335273742675781, L3: 50.057342529296875\n",
      "Current prediction:  56.09965133666992 \n",
      "\n",
      "Iteration 149, Loss: 64.79075622558594, L1: 16.011634826660156, L3: 48.77912139892578\n",
      "Current prediction:  56.18075942993164 \n",
      "\n",
      "Iteration 150, Loss: 64.19933319091797, L1: 15.536437034606934, L3: 48.66289520263672\n",
      "Current prediction:  56.26081848144531 \n",
      "\n",
      "Iteration 151, Loss: 61.96497344970703, L1: 15.68623161315918, L3: 46.27873992919922\n",
      "Current prediction:  56.33970260620117 \n",
      "\n",
      "Iteration 152, Loss: 62.91901397705078, L1: 15.474974632263184, L3: 47.44403839111328\n",
      "Current prediction:  56.41757583618164 \n",
      "\n",
      "Iteration 153, Loss: 61.25279235839844, L1: 15.576972007751465, L3: 45.675819396972656\n",
      "Current prediction:  56.49427795410156 \n",
      "\n",
      "Iteration 154, Loss: 58.825714111328125, L1: 14.934907913208008, L3: 43.89080810546875\n",
      "Current prediction:  56.56988525390625 \n",
      "\n",
      "Iteration 155, Loss: 59.598228454589844, L1: 15.098718643188477, L3: 44.49951171875\n",
      "Current prediction:  56.6445198059082 \n",
      "\n",
      "Iteration 156, Loss: 57.549217224121094, L1: 15.191425323486328, L3: 42.357791900634766\n",
      "Current prediction:  56.71818161010742 \n",
      "\n",
      "Iteration 157, Loss: 58.83806610107422, L1: 14.976503372192383, L3: 43.86156463623047\n",
      "Current prediction:  56.79084777832031 \n",
      "\n",
      "Iteration 158, Loss: 57.33598709106445, L1: 14.657525062561035, L3: 42.678462982177734\n",
      "Current prediction:  56.86261749267578 \n",
      "\n",
      "Iteration 159, Loss: 57.33362579345703, L1: 15.046246528625488, L3: 42.28738021850586\n",
      "Current prediction:  56.93350601196289 \n",
      "\n",
      "Iteration 160, Loss: 55.33739471435547, L1: 14.657140731811523, L3: 40.68025207519531\n",
      "Current prediction:  57.00347900390625 \n",
      "\n",
      "Iteration 161, Loss: 55.61656951904297, L1: 14.80320930480957, L3: 40.81336212158203\n",
      "Current prediction:  57.072601318359375 \n",
      "\n",
      "Iteration 162, Loss: 55.24485778808594, L1: 14.649981498718262, L3: 40.59487533569336\n",
      "Current prediction:  57.140777587890625 \n",
      "\n",
      "Iteration 163, Loss: 54.789756774902344, L1: 14.829862594604492, L3: 39.95989227294922\n",
      "Current prediction:  57.20791244506836 \n",
      "\n",
      "Iteration 164, Loss: 54.44924545288086, L1: 14.342991828918457, L3: 40.10625457763672\n",
      "Current prediction:  57.27416229248047 \n",
      "\n",
      "Iteration 165, Loss: 53.54829406738281, L1: 14.54010009765625, L3: 39.00819396972656\n",
      "Current prediction:  57.33942413330078 \n",
      "\n",
      "Iteration 166, Loss: 52.857444763183594, L1: 14.53764533996582, L3: 38.319801330566406\n",
      "Current prediction:  57.40403747558594 \n",
      "\n",
      "Iteration 167, Loss: 52.69976043701172, L1: 13.903379440307617, L3: 38.796382904052734\n",
      "Current prediction:  57.46779251098633 \n",
      "\n",
      "Iteration 168, Loss: 52.194358825683594, L1: 14.212568283081055, L3: 37.98179244995117\n",
      "Current prediction:  57.530845642089844 \n",
      "\n",
      "Iteration 169, Loss: 50.86607360839844, L1: 14.2618408203125, L3: 36.60423278808594\n",
      "Current prediction:  57.593143463134766 \n",
      "\n",
      "Iteration 170, Loss: 50.68292999267578, L1: 14.029539108276367, L3: 36.65339279174805\n",
      "Current prediction:  57.65462875366211 \n",
      "\n",
      "Iteration 171, Loss: 50.17449951171875, L1: 14.334527969360352, L3: 35.83997344970703\n",
      "Current prediction:  57.715145111083984 \n",
      "\n",
      "Iteration 172, Loss: 49.8004150390625, L1: 13.814409255981445, L3: 35.98600769042969\n",
      "Current prediction:  57.77482604980469 \n",
      "\n",
      "Iteration 173, Loss: 50.36578369140625, L1: 15.50063419342041, L3: 34.865150451660156\n",
      "Current prediction:  57.83369064331055 \n",
      "\n",
      "Iteration 174, Loss: 48.76399230957031, L1: 13.88606071472168, L3: 34.877933502197266\n",
      "Current prediction:  57.891761779785156 \n",
      "\n",
      "Iteration 175, Loss: 49.04468536376953, L1: 14.014641761779785, L3: 35.03004455566406\n",
      "Current prediction:  57.94904327392578 \n",
      "\n",
      "Iteration 176, Loss: 48.44791793823242, L1: 13.757817268371582, L3: 34.690101623535156\n",
      "Current prediction:  58.0054817199707 \n",
      "\n",
      "Iteration 177, Loss: 47.26011657714844, L1: 13.599546432495117, L3: 33.66057205200195\n",
      "Current prediction:  58.06114959716797 \n",
      "\n",
      "Iteration 178, Loss: 47.076011657714844, L1: 13.696320533752441, L3: 33.37969207763672\n",
      "Current prediction:  58.11590576171875 \n",
      "\n",
      "Iteration 179, Loss: 46.52500534057617, L1: 13.831974983215332, L3: 32.693031311035156\n",
      "Current prediction:  58.1699104309082 \n",
      "\n",
      "Iteration 180, Loss: 45.884727478027344, L1: 13.849843978881836, L3: 32.03488540649414\n",
      "Current prediction:  58.22296142578125 \n",
      "\n",
      "Iteration 181, Loss: 46.47792434692383, L1: 13.348052024841309, L3: 33.1298713684082\n",
      "Current prediction:  58.27500534057617 \n",
      "\n",
      "Iteration 182, Loss: 46.84424591064453, L1: 13.707797050476074, L3: 33.13644790649414\n",
      "Current prediction:  58.32648849487305 \n",
      "\n",
      "Iteration 183, Loss: 48.42138671875, L1: 15.253057479858398, L3: 33.168331146240234\n",
      "Current prediction:  58.37725830078125 \n",
      "\n",
      "Iteration 184, Loss: 45.973167419433594, L1: 13.892926216125488, L3: 32.08024215698242\n",
      "Current prediction:  58.427310943603516 \n",
      "\n",
      "Iteration 185, Loss: 44.5430908203125, L1: 13.818603515625, L3: 30.7244873046875\n",
      "Current prediction:  58.47669982910156 \n",
      "\n",
      "Iteration 186, Loss: 44.8792610168457, L1: 13.67099666595459, L3: 31.208263397216797\n",
      "Current prediction:  58.52546691894531 \n",
      "\n",
      "Iteration 187, Loss: 45.079551696777344, L1: 13.727487564086914, L3: 31.352062225341797\n",
      "Current prediction:  58.57373046875 \n",
      "\n",
      "Iteration 188, Loss: 44.37772750854492, L1: 14.072003364562988, L3: 30.30572509765625\n",
      "Current prediction:  58.62128448486328 \n",
      "\n",
      "Iteration 189, Loss: 44.15351104736328, L1: 13.511466026306152, L3: 30.642045974731445\n",
      "Current prediction:  58.66816329956055 \n",
      "\n",
      "Iteration 190, Loss: 43.83087921142578, L1: 13.647817611694336, L3: 30.183059692382812\n",
      "Current prediction:  58.71450424194336 \n",
      "\n",
      "Iteration 191, Loss: 44.66323471069336, L1: 13.426884651184082, L3: 31.23634910583496\n",
      "Current prediction:  58.760372161865234 \n",
      "\n",
      "Iteration 192, Loss: 42.98960876464844, L1: 13.140229225158691, L3: 29.849380493164062\n",
      "Current prediction:  58.80559539794922 \n",
      "\n",
      "Iteration 193, Loss: 45.12779998779297, L1: 14.067790985107422, L3: 31.060007095336914\n",
      "Current prediction:  58.85035705566406 \n",
      "\n",
      "Iteration 194, Loss: 43.71947479248047, L1: 13.178752899169922, L3: 30.540721893310547\n",
      "Current prediction:  58.89476013183594 \n",
      "\n",
      "Iteration 195, Loss: 43.2435302734375, L1: 13.018935203552246, L3: 30.22459602355957\n",
      "Current prediction:  58.938720703125 \n",
      "\n",
      "Iteration 196, Loss: 42.50779342651367, L1: 13.177765846252441, L3: 29.330028533935547\n",
      "Current prediction:  58.98200607299805 \n",
      "\n",
      "Iteration 197, Loss: 43.13638687133789, L1: 12.918416023254395, L3: 30.217971801757812\n",
      "Current prediction:  59.025001525878906 \n",
      "\n",
      "Iteration 198, Loss: 42.044715881347656, L1: 12.794325828552246, L3: 29.250391006469727\n",
      "Current prediction:  59.06760787963867 \n",
      "\n",
      "Iteration 199, Loss: 42.143348693847656, L1: 12.62695598602295, L3: 29.516393661499023\n",
      "Current prediction:  59.109676361083984 \n",
      "\n",
      "Iteration 200, Loss: 41.79737091064453, L1: 12.623661041259766, L3: 29.173709869384766\n",
      "Current prediction:  59.15118408203125 \n",
      "\n",
      "Iteration 201, Loss: 42.03289794921875, L1: 13.193751335144043, L3: 28.839147567749023\n",
      "Current prediction:  59.19218826293945 \n",
      "\n",
      "Iteration 202, Loss: 41.512115478515625, L1: 12.628087997436523, L3: 28.8840274810791\n",
      "Current prediction:  59.23252868652344 \n",
      "\n",
      "Iteration 203, Loss: 41.32680130004883, L1: 12.49623966217041, L3: 28.830562591552734\n",
      "Current prediction:  59.27212905883789 \n",
      "\n",
      "Iteration 204, Loss: 40.959205627441406, L1: 12.493925094604492, L3: 28.465280532836914\n",
      "Current prediction:  59.311031341552734 \n",
      "\n",
      "Iteration 205, Loss: 41.35700225830078, L1: 12.643250465393066, L3: 28.71375274658203\n",
      "Current prediction:  59.3492546081543 \n",
      "\n",
      "Iteration 206, Loss: 40.328975677490234, L1: 12.543878555297852, L3: 27.785097122192383\n",
      "Current prediction:  59.386817932128906 \n",
      "\n",
      "Iteration 207, Loss: 41.4511604309082, L1: 12.882975578308105, L3: 28.56818389892578\n",
      "Current prediction:  59.42356872558594 \n",
      "\n",
      "Iteration 208, Loss: 41.777164459228516, L1: 12.790210723876953, L3: 28.986953735351562\n",
      "Current prediction:  59.45998001098633 \n",
      "\n",
      "Iteration 209, Loss: 41.0730094909668, L1: 12.549117088317871, L3: 28.523893356323242\n",
      "Current prediction:  59.492767333984375 \n",
      "\n",
      "Iteration 210, Loss: 41.54780197143555, L1: 12.675788879394531, L3: 28.872013092041016\n",
      "Current prediction:  58.67356491088867 \n",
      "\n",
      "Iteration 211, Loss: 44.07415008544922, L1: 13.051651954650879, L3: 31.022497177124023\n",
      "Current prediction:  58.70930862426758 \n",
      "\n",
      "Iteration 212, Loss: 43.01829147338867, L1: 12.857865333557129, L3: 30.160425186157227\n",
      "Current prediction:  58.7453498840332 \n",
      "\n",
      "Iteration 213, Loss: 42.459312438964844, L1: 12.882506370544434, L3: 29.576807022094727\n",
      "Current prediction:  58.78166198730469 \n",
      "\n",
      "Iteration 214, Loss: 42.618690490722656, L1: 12.891791343688965, L3: 29.726900100708008\n",
      "Current prediction:  58.817874908447266 \n",
      "\n",
      "Iteration 215, Loss: 43.09308624267578, L1: 12.96168327331543, L3: 30.13140296936035\n",
      "Current prediction:  58.85410690307617 \n",
      "\n",
      "Iteration 216, Loss: 43.94514846801758, L1: 12.768120765686035, L3: 31.17702865600586\n",
      "Current prediction:  58.89082717895508 \n",
      "\n",
      "Iteration 217, Loss: 42.13381576538086, L1: 12.718733787536621, L3: 29.415081024169922\n",
      "Current prediction:  58.92741775512695 \n",
      "\n",
      "Iteration 218, Loss: 41.616004943847656, L1: 12.878938674926758, L3: 28.7370662689209\n",
      "Current prediction:  58.963829040527344 \n",
      "\n",
      "Iteration 219, Loss: 43.39527130126953, L1: 15.018045425415039, L3: 28.377227783203125\n",
      "Current prediction:  58.99997329711914 \n",
      "\n",
      "Iteration 220, Loss: 41.863285064697266, L1: 12.631823539733887, L3: 29.231462478637695\n",
      "Current prediction:  59.03577423095703 \n",
      "\n",
      "Iteration 221, Loss: 42.35187530517578, L1: 13.07343864440918, L3: 29.2784366607666\n",
      "Current prediction:  59.07562255859375 \n",
      "\n",
      "Iteration 222, Loss: 41.888023376464844, L1: 13.061979293823242, L3: 28.826045989990234\n",
      "Current prediction:  59.92268753051758 \n",
      "\n",
      "Iteration 223, Loss: 41.025543212890625, L1: 12.707625389099121, L3: 28.31791877746582\n",
      "Current prediction:  60.003360748291016 \n",
      "\n",
      "Iteration 224, Loss: 40.6907844543457, L1: 13.651440620422363, L3: 27.039342880249023\n",
      "Current prediction:  60.03770065307617 \n",
      "\n",
      "Iteration 225, Loss: 41.32495880126953, L1: 13.429814338684082, L3: 27.895143508911133\n",
      "Current prediction:  60.071563720703125 \n",
      "\n",
      "Iteration 226, Loss: 41.20569610595703, L1: 13.491546630859375, L3: 27.714149475097656\n",
      "Current prediction:  60.1048469543457 \n",
      "\n",
      "Iteration 227, Loss: 40.97693634033203, L1: 13.625873565673828, L3: 27.351064682006836\n",
      "Current prediction:  60.13766860961914 \n",
      "\n",
      "Iteration 228, Loss: 40.640316009521484, L1: 12.785717964172363, L3: 27.854597091674805\n",
      "Current prediction:  60.17002868652344 \n",
      "\n",
      "Iteration 229, Loss: 40.55487060546875, L1: 13.11507797241211, L3: 27.439790725708008\n",
      "Current prediction:  60.20209503173828 \n",
      "\n",
      "Iteration 230, Loss: 40.95747375488281, L1: 12.663859367370605, L3: 28.293615341186523\n",
      "Current prediction:  60.23401641845703 \n",
      "\n",
      "Iteration 231, Loss: 40.41759490966797, L1: 12.386445999145508, L3: 28.031150817871094\n",
      "Current prediction:  60.265743255615234 \n",
      "\n",
      "Iteration 232, Loss: 41.436363220214844, L1: 12.486180305480957, L3: 28.950183868408203\n",
      "Current prediction:  60.297298431396484 \n",
      "\n",
      "Iteration 233, Loss: 40.85108184814453, L1: 12.408113479614258, L3: 28.442968368530273\n",
      "Current prediction:  60.32868194580078 \n",
      "\n",
      "Iteration 234, Loss: 41.397552490234375, L1: 12.367242813110352, L3: 29.030311584472656\n",
      "Current prediction:  60.358978271484375 \n",
      "\n",
      "Iteration 235, Loss: 40.336692810058594, L1: 12.434621810913086, L3: 27.902069091796875\n",
      "Current prediction:  60.336341857910156 \n",
      "\n",
      "Iteration 236, Loss: 41.7039794921875, L1: 12.671378135681152, L3: 29.03260040283203\n",
      "Current prediction:  59.615291595458984 \n",
      "\n",
      "Iteration 237, Loss: 42.653709411621094, L1: 12.411613464355469, L3: 30.242094039916992\n",
      "Current prediction:  58.7419548034668 \n",
      "\n",
      "Iteration 238, Loss: 42.15080261230469, L1: 12.404513359069824, L3: 29.74629020690918\n",
      "Current prediction:  58.763710021972656 \n",
      "\n",
      "Iteration 239, Loss: 41.80194091796875, L1: 12.530252456665039, L3: 29.271690368652344\n",
      "Current prediction:  58.80042266845703 \n",
      "\n",
      "Iteration 240, Loss: 42.15327072143555, L1: 12.601110458374023, L3: 29.552160263061523\n",
      "Current prediction:  58.83037567138672 \n",
      "\n",
      "Iteration 241, Loss: 41.413421630859375, L1: 12.34564208984375, L3: 29.067781448364258\n",
      "Current prediction:  58.865089416503906 \n",
      "\n",
      "Iteration 242, Loss: 41.514793395996094, L1: 12.59310531616211, L3: 28.921688079833984\n",
      "Current prediction:  58.90949249267578 \n",
      "\n",
      "Iteration 243, Loss: 40.64027404785156, L1: 12.476354598999023, L3: 28.16391944885254\n",
      "Current prediction:  58.99639129638672 \n",
      "\n",
      "Iteration 244, Loss: 40.88631057739258, L1: 12.602211952209473, L3: 28.284099578857422\n",
      "Current prediction:  59.28168869018555 \n",
      "\n",
      "Iteration 245, Loss: 40.44490432739258, L1: 12.25194263458252, L3: 28.192962646484375\n",
      "Current prediction:  60.142799377441406 \n",
      "\n",
      "Iteration 246, Loss: 40.357208251953125, L1: 12.211012840270996, L3: 28.146196365356445\n",
      "Current prediction:  60.67390823364258 \n",
      "\n",
      "Iteration 247, Loss: 39.864158630371094, L1: 12.14128303527832, L3: 27.722875595092773\n",
      "Current prediction:  60.77109909057617 \n",
      "\n",
      "Iteration 248, Loss: 39.36942672729492, L1: 11.937532424926758, L3: 27.431894302368164\n",
      "Current prediction:  60.80618667602539 \n",
      "\n",
      "Iteration 249, Loss: 38.459842681884766, L1: 12.080801963806152, L3: 26.379039764404297\n",
      "Current prediction:  60.8361930847168 \n",
      "\n",
      "Iteration 250, Loss: 38.55609130859375, L1: 11.944705963134766, L3: 26.611387252807617\n",
      "Current prediction:  60.86526107788086 \n",
      "\n",
      "Iteration 251, Loss: 39.39269256591797, L1: 11.95599365234375, L3: 27.436697006225586\n",
      "Current prediction:  60.89372634887695 \n",
      "\n",
      "Iteration 252, Loss: 38.969459533691406, L1: 11.921810150146484, L3: 27.04764747619629\n",
      "Current prediction:  60.92131423950195 \n",
      "\n",
      "Iteration 253, Loss: 37.924617767333984, L1: 11.818902969360352, L3: 26.105714797973633\n",
      "Current prediction:  60.94822311401367 \n",
      "\n",
      "Iteration 254, Loss: 39.17764663696289, L1: 11.86921215057373, L3: 27.308433532714844\n",
      "Current prediction:  60.974117279052734 \n",
      "\n",
      "Iteration 255, Loss: 39.228294372558594, L1: 11.957806587219238, L3: 27.27048683166504\n",
      "Current prediction:  60.99919509887695 \n",
      "\n",
      "Iteration 256, Loss: 38.989200592041016, L1: 12.032164573669434, L3: 26.9570369720459\n",
      "Current prediction:  61.02323913574219 \n",
      "\n",
      "Iteration 257, Loss: 39.51362228393555, L1: 12.141608238220215, L3: 27.372013092041016\n",
      "Current prediction:  61.0463752746582 \n",
      "\n",
      "Iteration 258, Loss: 39.070899963378906, L1: 11.987054824829102, L3: 27.083843231201172\n",
      "Current prediction:  61.068763732910156 \n",
      "\n",
      "Iteration 259, Loss: 38.881080627441406, L1: 11.869057655334473, L3: 27.012022018432617\n",
      "Current prediction:  61.09029006958008 \n",
      "\n",
      "Iteration 260, Loss: 38.28804016113281, L1: 11.709976196289062, L3: 26.578062057495117\n",
      "Current prediction:  61.11077880859375 \n",
      "\n",
      "Iteration 261, Loss: 38.215309143066406, L1: 11.486525535583496, L3: 26.728784561157227\n",
      "Current prediction:  61.13046646118164 \n",
      "\n",
      "Iteration 262, Loss: 38.43388366699219, L1: 11.681137084960938, L3: 26.752744674682617\n",
      "Current prediction:  61.14942169189453 \n",
      "\n",
      "Iteration 263, Loss: 38.38201141357422, L1: 11.617758750915527, L3: 26.764253616333008\n",
      "Current prediction:  61.16759490966797 \n",
      "\n",
      "Iteration 264, Loss: 38.62996292114258, L1: 11.67070484161377, L3: 26.959259033203125\n",
      "Current prediction:  61.18495178222656 \n",
      "\n",
      "Iteration 265, Loss: 38.39458084106445, L1: 11.696236610412598, L3: 26.69834327697754\n",
      "Current prediction:  61.20172119140625 \n",
      "\n",
      "Iteration 266, Loss: 37.59808349609375, L1: 11.341545104980469, L3: 26.25653839111328\n",
      "Current prediction:  61.21775436401367 \n",
      "\n",
      "Iteration 267, Loss: 37.814369201660156, L1: 11.418769836425781, L3: 26.395597457885742\n",
      "Current prediction:  61.23296356201172 \n",
      "\n",
      "Iteration 268, Loss: 39.765323638916016, L1: 11.419283866882324, L3: 28.346040725708008\n",
      "Current prediction:  61.248008728027344 \n",
      "\n",
      "Iteration 269, Loss: 37.146785736083984, L1: 11.382221221923828, L3: 25.764564514160156\n",
      "Current prediction:  61.26244354248047 \n",
      "\n",
      "Iteration 270, Loss: 37.982906341552734, L1: 11.555049896240234, L3: 26.4278564453125\n",
      "Current prediction:  61.27678298950195 \n",
      "\n",
      "Iteration 271, Loss: 37.0290412902832, L1: 11.294753074645996, L3: 25.734289169311523\n",
      "Current prediction:  61.29106140136719 \n",
      "\n",
      "Iteration 272, Loss: 37.68586730957031, L1: 11.456303596496582, L3: 26.229562759399414\n",
      "Current prediction:  61.3053092956543 \n",
      "\n",
      "Iteration 273, Loss: 35.92003631591797, L1: 11.40496826171875, L3: 24.51506805419922\n",
      "Current prediction:  61.31938552856445 \n",
      "\n",
      "Iteration 274, Loss: 38.50959014892578, L1: 11.333081245422363, L3: 27.176509857177734\n",
      "Current prediction:  61.333229064941406 \n",
      "\n",
      "Iteration 275, Loss: 38.565826416015625, L1: 11.337190628051758, L3: 27.228633880615234\n",
      "Current prediction:  61.34689712524414 \n",
      "\n",
      "Iteration 276, Loss: 39.37479782104492, L1: 11.363960266113281, L3: 28.01083755493164\n",
      "Current prediction:  61.36016082763672 \n",
      "\n",
      "Iteration 277, Loss: 38.74993133544922, L1: 12.37269401550293, L3: 26.37723731994629\n",
      "Current prediction:  61.373172760009766 \n",
      "\n",
      "Iteration 278, Loss: 38.418033599853516, L1: 11.43547534942627, L3: 26.982559204101562\n",
      "Current prediction:  61.386260986328125 \n",
      "\n",
      "Iteration 279, Loss: 37.32976150512695, L1: 11.214742660522461, L3: 26.115018844604492\n",
      "Current prediction:  61.3992919921875 \n",
      "\n",
      "Iteration 280, Loss: 37.744468688964844, L1: 11.152103424072266, L3: 26.592365264892578\n",
      "Current prediction:  61.412132263183594 \n",
      "\n",
      "Iteration 281, Loss: 38.42237091064453, L1: 11.21353816986084, L3: 27.208831787109375\n",
      "Current prediction:  61.424556732177734 \n",
      "\n",
      "Iteration 282, Loss: 37.43084716796875, L1: 11.28286361694336, L3: 26.147981643676758\n",
      "Current prediction:  61.436744689941406 \n",
      "\n",
      "Iteration 283, Loss: 37.64765167236328, L1: 11.217978477478027, L3: 26.42967414855957\n",
      "Current prediction:  61.44889450073242 \n",
      "\n",
      "Iteration 284, Loss: 39.160404205322266, L1: 12.328211784362793, L3: 26.832191467285156\n",
      "Current prediction:  61.460941314697266 \n",
      "\n",
      "Iteration 285, Loss: 37.65428924560547, L1: 11.3158597946167, L3: 26.338428497314453\n",
      "Current prediction:  61.472740173339844 \n",
      "\n",
      "Iteration 286, Loss: 38.628501892089844, L1: 11.20219898223877, L3: 27.42630386352539\n",
      "Current prediction:  61.48426818847656 \n",
      "\n",
      "Iteration 287, Loss: 38.37975311279297, L1: 11.265625953674316, L3: 27.11412811279297\n",
      "Current prediction:  61.495338439941406 \n",
      "\n",
      "Iteration 288, Loss: 37.799591064453125, L1: 11.241165161132812, L3: 26.558425903320312\n",
      "Current prediction:  61.50617599487305 \n",
      "\n",
      "Iteration 289, Loss: 38.38805389404297, L1: 11.186868667602539, L3: 27.201187133789062\n",
      "Current prediction:  61.51690673828125 \n",
      "\n",
      "Iteration 290, Loss: 38.38063049316406, L1: 11.124971389770508, L3: 27.255657196044922\n",
      "Current prediction:  61.52730178833008 \n",
      "\n",
      "Iteration 291, Loss: 37.63228988647461, L1: 11.109258651733398, L3: 26.52303123474121\n",
      "Current prediction:  61.537445068359375 \n",
      "\n",
      "Iteration 292, Loss: 36.35820388793945, L1: 11.142431259155273, L3: 25.21577262878418\n",
      "Current prediction:  61.54730224609375 \n",
      "\n",
      "Iteration 293, Loss: 37.6818733215332, L1: 11.047994613647461, L3: 26.633878707885742\n",
      "Current prediction:  61.55664825439453 \n",
      "\n",
      "Iteration 294, Loss: 37.77155303955078, L1: 11.084904670715332, L3: 26.686649322509766\n",
      "Current prediction:  61.56565475463867 \n",
      "\n",
      "Iteration 295, Loss: 36.12943649291992, L1: 11.12529182434082, L3: 25.0041446685791\n",
      "Current prediction:  61.574283599853516 \n",
      "\n",
      "Iteration 296, Loss: 37.69399642944336, L1: 11.010734558105469, L3: 26.68326187133789\n",
      "Current prediction:  61.58250427246094 \n",
      "\n",
      "Iteration 297, Loss: 38.39460754394531, L1: 11.159308433532715, L3: 27.235300064086914\n",
      "Current prediction:  61.59034729003906 \n",
      "\n",
      "Iteration 298, Loss: 38.123355865478516, L1: 11.066662788391113, L3: 27.05669403076172\n",
      "Current prediction:  61.597774505615234 \n",
      "\n",
      "Iteration 299, Loss: 37.931175231933594, L1: 11.342501640319824, L3: 26.588674545288086\n",
      "Current prediction:  61.60474395751953 \n",
      "\n",
      "Iteration 300, Loss: 39.076927185058594, L1: 11.828350067138672, L3: 27.248579025268555\n",
      "Current prediction:  61.61143493652344 \n",
      "\n",
      "Iteration 301, Loss: 37.96064376831055, L1: 11.149709701538086, L3: 26.81093406677246\n",
      "Current prediction:  61.61772918701172 \n",
      "\n",
      "Iteration 302, Loss: 36.940826416015625, L1: 11.196747779846191, L3: 25.744077682495117\n",
      "Current prediction:  61.623809814453125 \n",
      "\n",
      "Iteration 303, Loss: 37.885650634765625, L1: 11.043572425842285, L3: 26.842077255249023\n",
      "Current prediction:  61.629417419433594 \n",
      "\n",
      "Iteration 304, Loss: 37.37542724609375, L1: 11.022621154785156, L3: 26.352806091308594\n",
      "Current prediction:  61.634674072265625 \n",
      "\n",
      "Iteration 305, Loss: 37.47484588623047, L1: 11.057732582092285, L3: 26.417112350463867\n",
      "Current prediction:  61.639705657958984 \n",
      "\n",
      "Iteration 306, Loss: 37.305233001708984, L1: 11.047938346862793, L3: 26.257293701171875\n",
      "Current prediction:  61.64461135864258 \n",
      "\n",
      "Iteration 307, Loss: 38.06501770019531, L1: 11.078911781311035, L3: 26.986106872558594\n",
      "Current prediction:  61.64921569824219 \n",
      "\n",
      "Iteration 308, Loss: 36.6014518737793, L1: 11.052139282226562, L3: 25.549312591552734\n",
      "Current prediction:  61.65394592285156 \n",
      "\n",
      "Iteration 309, Loss: 36.6668701171875, L1: 11.553704261779785, L3: 25.1131649017334\n",
      "Current prediction:  61.65861129760742 \n",
      "\n",
      "Iteration 310, Loss: 37.609798431396484, L1: 11.011458396911621, L3: 26.59834098815918\n",
      "Current prediction:  61.663116455078125 \n",
      "\n",
      "Iteration 311, Loss: 37.20738220214844, L1: 10.969801902770996, L3: 26.237581253051758\n",
      "Current prediction:  61.66770553588867 \n",
      "\n",
      "Iteration 312, Loss: 38.32435607910156, L1: 11.280109405517578, L3: 27.044246673583984\n",
      "Current prediction:  61.67240905761719 \n",
      "\n",
      "Iteration 313, Loss: 37.795875549316406, L1: 10.795671463012695, L3: 27.000202178955078\n",
      "Current prediction:  61.67700958251953 \n",
      "\n",
      "Iteration 314, Loss: 37.46039581298828, L1: 11.072150230407715, L3: 26.388246536254883\n",
      "Current prediction:  61.68150329589844 \n",
      "\n",
      "Iteration 315, Loss: 39.30183029174805, L1: 11.062140464782715, L3: 28.23969078063965\n",
      "Current prediction:  61.68601608276367 \n",
      "\n",
      "Iteration 316, Loss: 38.311370849609375, L1: 12.10549545288086, L3: 26.205873489379883\n",
      "Current prediction:  61.690696716308594 \n",
      "\n",
      "Iteration 317, Loss: 37.3172492980957, L1: 11.140820503234863, L3: 26.176427841186523\n",
      "Current prediction:  61.69584655761719 \n",
      "\n",
      "Iteration 318, Loss: 37.62778854370117, L1: 11.069576263427734, L3: 26.558212280273438\n",
      "Current prediction:  61.70153045654297 \n",
      "\n",
      "Iteration 319, Loss: 37.55216598510742, L1: 11.254257202148438, L3: 26.297908782958984\n",
      "Current prediction:  61.14147186279297 \n",
      "\n",
      "Iteration 320, Loss: 37.83492660522461, L1: 11.204596519470215, L3: 26.63033103942871\n",
      "Current prediction:  61.02348709106445 \n",
      "\n",
      "Iteration 321, Loss: 38.835453033447266, L1: 11.5645170211792, L3: 27.27093505859375\n",
      "Current prediction:  61.03105926513672 \n",
      "\n",
      "Iteration 322, Loss: 39.47365188598633, L1: 12.06844425201416, L3: 27.405208587646484\n",
      "Current prediction:  61.039764404296875 \n",
      "\n",
      "Iteration 323, Loss: 38.42455291748047, L1: 12.153019905090332, L3: 26.27153205871582\n",
      "Current prediction:  61.0495491027832 \n",
      "\n",
      "Iteration 324, Loss: 39.475032806396484, L1: 12.120549201965332, L3: 27.35448455810547\n",
      "Current prediction:  61.060367584228516 \n",
      "\n",
      "Iteration 325, Loss: 38.0583381652832, L1: 12.024969100952148, L3: 26.033369064331055\n",
      "Current prediction:  61.072296142578125 \n",
      "\n",
      "Iteration 326, Loss: 39.624847412109375, L1: 12.253422737121582, L3: 27.37142562866211\n",
      "Current prediction:  61.085205078125 \n",
      "\n",
      "Iteration 327, Loss: 39.883331298828125, L1: 12.279319763183594, L3: 27.6040096282959\n",
      "Current prediction:  61.09891891479492 \n",
      "\n",
      "Iteration 328, Loss: 38.783023834228516, L1: 11.921976089477539, L3: 26.861047744750977\n",
      "Current prediction:  61.113075256347656 \n",
      "\n",
      "Iteration 329, Loss: 38.85558319091797, L1: 12.06847858428955, L3: 26.7871036529541\n",
      "Current prediction:  61.12773132324219 \n",
      "\n",
      "Iteration 330, Loss: 39.21064758300781, L1: 11.798563957214355, L3: 27.412084579467773\n",
      "Current prediction:  61.14259719848633 \n",
      "\n",
      "Iteration 331, Loss: 40.179229736328125, L1: 11.990917205810547, L3: 28.188312530517578\n",
      "Current prediction:  61.15779113769531 \n",
      "\n",
      "Iteration 332, Loss: 38.56993103027344, L1: 11.668110847473145, L3: 26.901819229125977\n",
      "Current prediction:  61.17278289794922 \n",
      "\n",
      "Iteration 333, Loss: 37.629764556884766, L1: 11.841830253601074, L3: 25.787935256958008\n",
      "Current prediction:  61.18800735473633 \n",
      "\n",
      "Iteration 334, Loss: 38.56846618652344, L1: 11.605096817016602, L3: 26.963369369506836\n",
      "Current prediction:  61.203121185302734 \n",
      "\n",
      "Iteration 335, Loss: 38.62171173095703, L1: 11.61290168762207, L3: 27.008811950683594\n",
      "Current prediction:  61.21807098388672 \n",
      "\n",
      "Iteration 336, Loss: 38.69990158081055, L1: 11.896475791931152, L3: 26.803424835205078\n",
      "Current prediction:  61.232913970947266 \n",
      "\n",
      "Iteration 337, Loss: 38.02202606201172, L1: 11.93303394317627, L3: 26.088993072509766\n",
      "Current prediction:  61.24760055541992 \n",
      "\n",
      "Iteration 338, Loss: 37.59104537963867, L1: 11.782997131347656, L3: 25.808048248291016\n",
      "Current prediction:  61.26176834106445 \n",
      "\n",
      "Iteration 339, Loss: 38.71831130981445, L1: 11.648430824279785, L3: 27.069881439208984\n",
      "Current prediction:  61.27549743652344 \n",
      "\n",
      "Iteration 340, Loss: 37.04716491699219, L1: 11.438592910766602, L3: 25.60857391357422\n",
      "Current prediction:  61.289161682128906 \n",
      "\n",
      "Iteration 341, Loss: 38.042579650878906, L1: 11.637462615966797, L3: 26.405118942260742\n",
      "Current prediction:  61.30289077758789 \n",
      "\n",
      "Iteration 342, Loss: 38.82067108154297, L1: 11.6224365234375, L3: 27.19823455810547\n",
      "Current prediction:  61.3162841796875 \n",
      "\n",
      "Iteration 343, Loss: 37.97031784057617, L1: 11.92225456237793, L3: 26.048063278198242\n",
      "Current prediction:  61.32957077026367 \n",
      "\n",
      "Iteration 344, Loss: 38.5992431640625, L1: 11.769399642944336, L3: 26.829843521118164\n",
      "Current prediction:  61.34257125854492 \n",
      "\n",
      "Iteration 345, Loss: 37.3858528137207, L1: 11.622076034545898, L3: 25.763776779174805\n",
      "Current prediction:  61.3554801940918 \n",
      "\n",
      "Iteration 346, Loss: 38.388397216796875, L1: 11.938403129577637, L3: 26.449993133544922\n",
      "Current prediction:  61.36793899536133 \n",
      "\n",
      "Iteration 347, Loss: 37.96490478515625, L1: 11.841750144958496, L3: 26.12315559387207\n",
      "Current prediction:  61.380516052246094 \n",
      "\n",
      "Iteration 348, Loss: 38.399105072021484, L1: 12.412400245666504, L3: 25.986703872680664\n",
      "Current prediction:  61.393104553222656 \n",
      "\n",
      "Iteration 349, Loss: 37.92551040649414, L1: 11.544737815856934, L3: 26.38077163696289\n",
      "Current prediction:  61.40584182739258 \n",
      "\n",
      "Iteration 350, Loss: 38.62834167480469, L1: 12.082551956176758, L3: 26.545787811279297\n",
      "Current prediction:  60.81557083129883 \n",
      "\n",
      "Iteration 351, Loss: 38.129112243652344, L1: 12.034656524658203, L3: 26.094457626342773\n",
      "Current prediction:  60.828887939453125 \n",
      "\n",
      "Iteration 352, Loss: 39.103675842285156, L1: 12.154382705688477, L3: 26.94929313659668\n",
      "Current prediction:  60.842811584472656 \n",
      "\n",
      "Iteration 353, Loss: 38.96631622314453, L1: 12.38011646270752, L3: 26.586200714111328\n",
      "Current prediction:  60.857383728027344 \n",
      "\n",
      "Iteration 354, Loss: 40.04225540161133, L1: 12.632523536682129, L3: 27.409730911254883\n",
      "Current prediction:  60.87260055541992 \n",
      "\n",
      "Iteration 355, Loss: 38.42415237426758, L1: 12.206563949584961, L3: 26.217588424682617\n",
      "Current prediction:  60.88844680786133 \n",
      "\n",
      "Iteration 356, Loss: 38.79941177368164, L1: 12.366328239440918, L3: 26.43308448791504\n",
      "Current prediction:  60.904781341552734 \n",
      "\n",
      "Iteration 357, Loss: 39.934932708740234, L1: 12.46884822845459, L3: 27.466083526611328\n",
      "Current prediction:  60.92172622680664 \n",
      "\n",
      "Iteration 358, Loss: 38.71759033203125, L1: 12.19482135772705, L3: 26.522768020629883\n",
      "Current prediction:  60.908973693847656 \n",
      "\n",
      "Iteration 359, Loss: 38.97002410888672, L1: 12.27066421508789, L3: 26.699359893798828\n",
      "Current prediction:  60.40349578857422 \n",
      "\n",
      "Iteration 360, Loss: 38.84748458862305, L1: 12.740729331970215, L3: 26.106754302978516\n",
      "Current prediction:  60.28322982788086 \n",
      "\n",
      "Iteration 361, Loss: 38.59795379638672, L1: 12.427756309509277, L3: 26.170198440551758\n",
      "Current prediction:  60.29612350463867 \n",
      "\n",
      "Iteration 362, Loss: 38.6804313659668, L1: 12.47457504272461, L3: 26.205856323242188\n",
      "Current prediction:  60.31417465209961 \n",
      "\n",
      "Iteration 363, Loss: 38.4428596496582, L1: 12.26207447052002, L3: 26.180784225463867\n",
      "Current prediction:  60.33287811279297 \n",
      "\n",
      "Iteration 364, Loss: 38.49064254760742, L1: 12.508742332458496, L3: 25.98189926147461\n",
      "Current prediction:  60.351478576660156 \n",
      "\n",
      "Iteration 365, Loss: 38.968990325927734, L1: 12.645241737365723, L3: 26.323747634887695\n",
      "Current prediction:  60.36988830566406 \n",
      "\n",
      "Iteration 366, Loss: 39.513671875, L1: 12.826593399047852, L3: 26.68707847595215\n",
      "Current prediction:  60.38812255859375 \n",
      "\n",
      "Iteration 367, Loss: 38.415775299072266, L1: 13.39271068572998, L3: 25.0230655670166\n",
      "Current prediction:  60.40972137451172 \n",
      "\n",
      "Iteration 368, Loss: 37.12391662597656, L1: 11.933046340942383, L3: 25.190868377685547\n",
      "Current prediction:  60.49125671386719 \n",
      "\n",
      "Iteration 369, Loss: 38.79100036621094, L1: 12.25806999206543, L3: 26.532930374145508\n",
      "Current prediction:  60.92947006225586 \n",
      "\n",
      "Iteration 370, Loss: 38.702301025390625, L1: 12.340049743652344, L3: 26.362253189086914\n",
      "Current prediction:  61.15399932861328 \n",
      "\n",
      "Iteration 371, Loss: 38.20396423339844, L1: 12.051267623901367, L3: 26.15269660949707\n",
      "Current prediction:  61.17200469970703 \n",
      "\n",
      "Iteration 372, Loss: 38.641448974609375, L1: 12.368387222290039, L3: 26.273061752319336\n",
      "Current prediction:  61.187503814697266 \n",
      "\n",
      "Iteration 373, Loss: 37.71366500854492, L1: 11.334015846252441, L3: 26.379650115966797\n",
      "Current prediction:  61.201900482177734 \n",
      "\n",
      "Iteration 374, Loss: 37.27857208251953, L1: 11.520816802978516, L3: 25.75775718688965\n",
      "Current prediction:  61.215702056884766 \n",
      "\n",
      "Iteration 375, Loss: 35.9818115234375, L1: 10.996932029724121, L3: 24.984878540039062\n",
      "Current prediction:  61.228736877441406 \n",
      "\n",
      "Iteration 376, Loss: 38.274173736572266, L1: 12.079432487487793, L3: 26.19474220275879\n",
      "Current prediction:  61.24116134643555 \n",
      "\n",
      "Iteration 377, Loss: 37.62548828125, L1: 12.367810249328613, L3: 25.25767707824707\n",
      "Current prediction:  61.25288772583008 \n",
      "\n",
      "Iteration 378, Loss: 37.16765213012695, L1: 11.463889122009277, L3: 25.70376205444336\n",
      "Current prediction:  61.26399612426758 \n",
      "\n",
      "Iteration 379, Loss: 37.69514083862305, L1: 11.926675796508789, L3: 25.768465042114258\n",
      "Current prediction:  61.27416229248047 \n",
      "\n",
      "Iteration 380, Loss: 36.89091491699219, L1: 11.609745979309082, L3: 25.281169891357422\n",
      "Current prediction:  61.283538818359375 \n",
      "\n",
      "Iteration 381, Loss: 37.51286315917969, L1: 12.237077713012695, L3: 25.275785446166992\n",
      "Current prediction:  61.29188537597656 \n",
      "\n",
      "Iteration 382, Loss: 37.74348831176758, L1: 11.730112075805664, L3: 26.013376235961914\n",
      "Current prediction:  61.299522399902344 \n",
      "\n",
      "Iteration 383, Loss: 38.7134895324707, L1: 13.374204635620117, L3: 25.339284896850586\n",
      "Current prediction:  61.30650329589844 \n",
      "\n",
      "Iteration 384, Loss: 38.36237335205078, L1: 12.897488594055176, L3: 25.46488380432129\n",
      "Current prediction:  61.3128547668457 \n",
      "\n",
      "Iteration 385, Loss: 38.07102966308594, L1: 12.517895698547363, L3: 25.55313491821289\n",
      "Current prediction:  61.31875228881836 \n",
      "\n",
      "Iteration 386, Loss: 38.36296844482422, L1: 12.14454460144043, L3: 26.218425750732422\n",
      "Current prediction:  61.324012756347656 \n",
      "\n",
      "Iteration 387, Loss: 38.86117172241211, L1: 12.3182954788208, L3: 26.542875289916992\n",
      "Current prediction:  60.45995330810547 \n",
      "\n",
      "Iteration 388, Loss: 38.217079162597656, L1: 12.194856643676758, L3: 26.0222225189209\n",
      "Current prediction:  60.46735382080078 \n",
      "\n",
      "Iteration 389, Loss: 38.270328521728516, L1: 12.231322288513184, L3: 26.03900718688965\n",
      "Current prediction:  60.47610092163086 \n",
      "\n",
      "Iteration 390, Loss: 39.38261032104492, L1: 12.198969841003418, L3: 27.18364143371582\n",
      "Current prediction:  60.486114501953125 \n",
      "\n",
      "Iteration 391, Loss: 40.01948547363281, L1: 12.478069305419922, L3: 27.54141616821289\n",
      "Current prediction:  60.49753952026367 \n",
      "\n",
      "Iteration 392, Loss: 40.34417724609375, L1: 12.414406776428223, L3: 27.929771423339844\n",
      "Current prediction:  60.51042175292969 \n",
      "\n",
      "Iteration 393, Loss: 39.97246551513672, L1: 12.22862434387207, L3: 27.74384307861328\n",
      "Current prediction:  60.524513244628906 \n",
      "\n",
      "Iteration 394, Loss: 39.87221908569336, L1: 12.186676979064941, L3: 27.6855411529541\n",
      "Current prediction:  60.5400276184082 \n",
      "\n",
      "Iteration 395, Loss: 39.39582443237305, L1: 12.225602149963379, L3: 27.17022132873535\n",
      "Current prediction:  60.55683135986328 \n",
      "\n",
      "Iteration 396, Loss: 39.20533752441406, L1: 12.179844856262207, L3: 27.025493621826172\n",
      "Current prediction:  60.574798583984375 \n",
      "\n",
      "Iteration 397, Loss: 38.75494384765625, L1: 12.021629333496094, L3: 26.733312606811523\n",
      "Current prediction:  60.59381103515625 \n",
      "\n",
      "Iteration 398, Loss: 39.2166633605957, L1: 11.899649620056152, L3: 27.317012786865234\n",
      "Current prediction:  60.613624572753906 \n",
      "\n",
      "Iteration 399, Loss: 38.93477249145508, L1: 11.623960494995117, L3: 27.31081199645996\n",
      "Current prediction:  60.634159088134766 \n",
      "\n",
      "Iteration 400, Loss: 40.057430267333984, L1: 11.6225004196167, L3: 28.4349308013916\n",
      "Current prediction:  60.654945373535156 \n",
      "\n",
      "Iteration 401, Loss: 38.880008697509766, L1: 11.422870635986328, L3: 27.457138061523438\n",
      "Current prediction:  60.70908737182617 \n",
      "\n",
      "Iteration 402, Loss: 38.69972610473633, L1: 11.299384117126465, L3: 27.400341033935547\n",
      "Current prediction:  61.56524658203125 \n",
      "\n",
      "Iteration 403, Loss: 38.782615661621094, L1: 11.2188138961792, L3: 27.56380271911621\n",
      "Current prediction:  61.58987045288086 \n",
      "\n",
      "Iteration 404, Loss: 38.915321350097656, L1: 11.257094383239746, L3: 27.658227920532227\n",
      "Current prediction:  61.6104850769043 \n",
      "\n",
      "Iteration 405, Loss: 38.92474365234375, L1: 11.220964431762695, L3: 27.703777313232422\n",
      "Current prediction:  61.63035583496094 \n",
      "\n",
      "Iteration 406, Loss: 38.333988189697266, L1: 11.095690727233887, L3: 27.238296508789062\n",
      "Current prediction:  61.649757385253906 \n",
      "\n",
      "Iteration 407, Loss: 36.46986770629883, L1: 11.235677719116211, L3: 25.234189987182617\n",
      "Current prediction:  61.668556213378906 \n",
      "\n",
      "Iteration 408, Loss: 38.240264892578125, L1: 11.205208778381348, L3: 27.035057067871094\n",
      "Current prediction:  61.686309814453125 \n",
      "\n",
      "Iteration 409, Loss: 37.71343231201172, L1: 11.290653228759766, L3: 26.422780990600586\n",
      "Current prediction:  61.703224182128906 \n",
      "\n",
      "Iteration 410, Loss: 37.853240966796875, L1: 11.40008544921875, L3: 26.453155517578125\n",
      "Current prediction:  61.71919631958008 \n",
      "\n",
      "Iteration 411, Loss: 38.20006561279297, L1: 11.256595611572266, L3: 26.94346809387207\n",
      "Current prediction:  61.73460388183594 \n",
      "\n",
      "Iteration 412, Loss: 37.38951110839844, L1: 11.096399307250977, L3: 26.29311180114746\n",
      "Current prediction:  61.749454498291016 \n",
      "\n",
      "Iteration 413, Loss: 38.27396774291992, L1: 11.136075019836426, L3: 27.137893676757812\n",
      "Current prediction:  61.763275146484375 \n",
      "\n",
      "Iteration 414, Loss: 38.02568817138672, L1: 11.149169921875, L3: 26.876516342163086\n",
      "Current prediction:  61.776737213134766 \n",
      "\n",
      "Iteration 415, Loss: 38.12067413330078, L1: 11.183536529541016, L3: 26.937137603759766\n",
      "Current prediction:  61.78959274291992 \n",
      "\n",
      "Iteration 416, Loss: 39.28848648071289, L1: 11.138092041015625, L3: 28.150394439697266\n",
      "Current prediction:  61.801734924316406 \n",
      "\n",
      "Iteration 417, Loss: 37.88019943237305, L1: 11.249812126159668, L3: 26.630388259887695\n",
      "Current prediction:  61.81327819824219 \n",
      "\n",
      "Iteration 418, Loss: 38.15576934814453, L1: 10.888884544372559, L3: 27.266883850097656\n",
      "Current prediction:  61.82415008544922 \n",
      "\n",
      "Iteration 419, Loss: 37.88338088989258, L1: 11.283404350280762, L3: 26.5999755859375\n",
      "Current prediction:  61.83507537841797 \n",
      "\n",
      "Iteration 420, Loss: 38.324676513671875, L1: 11.02220344543457, L3: 27.302474975585938\n",
      "Current prediction:  61.84550857543945 \n",
      "\n",
      "Iteration 421, Loss: 38.47922897338867, L1: 10.723601341247559, L3: 27.75562858581543\n",
      "Current prediction:  61.855369567871094 \n",
      "\n",
      "Iteration 422, Loss: 36.66236114501953, L1: 10.77303695678711, L3: 25.88932228088379\n",
      "Current prediction:  61.86476516723633 \n",
      "\n",
      "Iteration 423, Loss: 38.751609802246094, L1: 11.760080337524414, L3: 26.991531372070312\n",
      "Current prediction:  61.87368392944336 \n",
      "\n",
      "Iteration 424, Loss: 37.04595184326172, L1: 10.921364784240723, L3: 26.12458610534668\n",
      "Current prediction:  61.882415771484375 \n",
      "\n",
      "Iteration 425, Loss: 37.52863311767578, L1: 11.039427757263184, L3: 26.489206314086914\n",
      "Current prediction:  61.89097213745117 \n",
      "\n",
      "Iteration 426, Loss: 37.814605712890625, L1: 10.933423042297363, L3: 26.881183624267578\n",
      "Current prediction:  61.899444580078125 \n",
      "\n",
      "Iteration 427, Loss: 37.18413543701172, L1: 10.845065116882324, L3: 26.339069366455078\n",
      "Current prediction:  61.90771484375 \n",
      "\n",
      "Iteration 428, Loss: 38.20858383178711, L1: 10.829663276672363, L3: 27.37891960144043\n",
      "Current prediction:  61.91568374633789 \n",
      "\n",
      "Iteration 429, Loss: 37.465267181396484, L1: 10.676718711853027, L3: 26.78854751586914\n",
      "Current prediction:  61.92338943481445 \n",
      "\n",
      "Iteration 430, Loss: 37.02385330200195, L1: 10.879395484924316, L3: 26.14445686340332\n",
      "Current prediction:  61.931026458740234 \n",
      "\n",
      "Iteration 431, Loss: 37.42822265625, L1: 11.059402465820312, L3: 26.368818283081055\n",
      "Current prediction:  61.93837356567383 \n",
      "\n",
      "Iteration 432, Loss: 36.54127883911133, L1: 10.770405769348145, L3: 25.770872116088867\n",
      "Current prediction:  61.94549560546875 \n",
      "\n",
      "Iteration 433, Loss: 37.51856231689453, L1: 10.573768615722656, L3: 26.944793701171875\n",
      "Current prediction:  61.951873779296875 \n",
      "\n",
      "Iteration 434, Loss: 37.47503662109375, L1: 10.672374725341797, L3: 26.80265998840332\n",
      "Current prediction:  61.95819091796875 \n",
      "\n",
      "Iteration 435, Loss: 38.400962829589844, L1: 10.54952621459961, L3: 27.8514347076416\n",
      "Current prediction:  61.96431350708008 \n",
      "\n",
      "Iteration 436, Loss: 37.142215728759766, L1: 10.62606143951416, L3: 26.51615333557129\n",
      "Current prediction:  61.97003936767578 \n",
      "\n",
      "Iteration 437, Loss: 38.031272888183594, L1: 10.556392669677734, L3: 27.474882125854492\n",
      "Current prediction:  61.975616455078125 \n",
      "\n",
      "Iteration 438, Loss: 38.02278518676758, L1: 10.642338752746582, L3: 27.380447387695312\n",
      "Current prediction:  61.981292724609375 \n",
      "\n",
      "Iteration 439, Loss: 37.44794464111328, L1: 10.573952674865723, L3: 26.873992919921875\n",
      "Current prediction:  61.98695373535156 \n",
      "\n",
      "Iteration 440, Loss: 37.48822021484375, L1: 10.613443374633789, L3: 26.874774932861328\n",
      "Current prediction:  61.99237060546875 \n",
      "\n",
      "Iteration 441, Loss: 37.63493347167969, L1: 10.632658958435059, L3: 27.002275466918945\n",
      "Current prediction:  61.99751281738281 \n",
      "\n",
      "Iteration 442, Loss: 37.4426155090332, L1: 10.573787689208984, L3: 26.86882781982422\n",
      "Current prediction:  62.00278854370117 \n",
      "\n",
      "Iteration 443, Loss: 37.470001220703125, L1: 10.567880630493164, L3: 26.902122497558594\n",
      "Current prediction:  62.00786209106445 \n",
      "\n",
      "Iteration 444, Loss: 37.29228591918945, L1: 10.538779258728027, L3: 26.75350570678711\n",
      "Current prediction:  62.01286315917969 \n",
      "\n",
      "Iteration 445, Loss: 36.37450408935547, L1: 10.54813289642334, L3: 25.826370239257812\n",
      "Current prediction:  62.01793670654297 \n",
      "\n",
      "Iteration 446, Loss: 36.86601257324219, L1: 10.58768081665039, L3: 26.27833366394043\n",
      "Current prediction:  62.022682189941406 \n",
      "\n",
      "Iteration 447, Loss: 37.908660888671875, L1: 10.579141616821289, L3: 27.329519271850586\n",
      "Current prediction:  62.02683639526367 \n",
      "\n",
      "Iteration 448, Loss: 36.778175354003906, L1: 10.561285018920898, L3: 26.216888427734375\n",
      "Current prediction:  62.03067398071289 \n",
      "\n",
      "Iteration 449, Loss: 37.825889587402344, L1: 10.564441680908203, L3: 27.261449813842773\n",
      "Current prediction:  62.03429412841797 \n",
      "\n",
      "Iteration 450, Loss: 38.47420120239258, L1: 10.495676040649414, L3: 27.978525161743164\n",
      "Current prediction:  62.03739547729492 \n",
      "\n",
      "Iteration 451, Loss: 36.6480712890625, L1: 10.502220153808594, L3: 26.145849227905273\n",
      "Current prediction:  62.04035568237305 \n",
      "\n",
      "Iteration 452, Loss: 36.29924774169922, L1: 10.493703842163086, L3: 25.8055419921875\n",
      "Current prediction:  62.043243408203125 \n",
      "\n",
      "Iteration 453, Loss: 36.584224700927734, L1: 10.477818489074707, L3: 26.106407165527344\n",
      "Current prediction:  62.0457649230957 \n",
      "\n",
      "Iteration 454, Loss: 37.51099395751953, L1: 10.570642471313477, L3: 26.940349578857422\n",
      "Current prediction:  62.0478515625 \n",
      "\n",
      "Iteration 455, Loss: 38.05804443359375, L1: 10.629073143005371, L3: 27.428972244262695\n",
      "Current prediction:  62.0495491027832 \n",
      "\n",
      "Iteration 456, Loss: 37.50938415527344, L1: 10.670270919799805, L3: 26.839113235473633\n",
      "Current prediction:  61.28653335571289 \n",
      "\n",
      "Iteration 457, Loss: 38.97944641113281, L1: 10.929952621459961, L3: 28.04949378967285\n",
      "Current prediction:  61.2891845703125 \n",
      "\n",
      "Iteration 458, Loss: 37.43498229980469, L1: 11.518816947937012, L3: 25.916166305541992\n",
      "Current prediction:  61.29307556152344 \n",
      "\n",
      "Iteration 459, Loss: 37.87357711791992, L1: 11.401546478271484, L3: 26.472030639648438\n",
      "Current prediction:  61.297969818115234 \n",
      "\n",
      "Iteration 460, Loss: 37.229225158691406, L1: 11.2828369140625, L3: 25.94639015197754\n",
      "Current prediction:  61.30393600463867 \n",
      "\n",
      "Iteration 461, Loss: 37.75014877319336, L1: 11.31140422821045, L3: 26.438743591308594\n",
      "Current prediction:  61.311092376708984 \n",
      "\n",
      "Iteration 462, Loss: 38.36908721923828, L1: 11.438092231750488, L3: 26.930994033813477\n",
      "Current prediction:  61.31903076171875 \n",
      "\n",
      "Iteration 463, Loss: 38.01057434082031, L1: 11.473479270935059, L3: 26.537094116210938\n",
      "Current prediction:  61.32738494873047 \n",
      "\n",
      "Iteration 464, Loss: 38.34059524536133, L1: 11.518206596374512, L3: 26.8223876953125\n",
      "Current prediction:  61.336151123046875 \n",
      "\n",
      "Iteration 465, Loss: 37.868343353271484, L1: 11.347020149230957, L3: 26.52132225036621\n",
      "Current prediction:  61.34502029418945 \n",
      "\n",
      "Iteration 466, Loss: 37.793670654296875, L1: 11.395404815673828, L3: 26.398265838623047\n",
      "Current prediction:  61.354270935058594 \n",
      "\n",
      "Iteration 467, Loss: 38.88066864013672, L1: 11.808120727539062, L3: 27.072546005249023\n",
      "Current prediction:  61.363765716552734 \n",
      "\n",
      "Iteration 468, Loss: 37.511531829833984, L1: 11.438658714294434, L3: 26.072872161865234\n",
      "Current prediction:  61.37358093261719 \n",
      "\n",
      "Iteration 469, Loss: 38.58775329589844, L1: 11.554283142089844, L3: 27.03346824645996\n",
      "Current prediction:  61.38338851928711 \n",
      "\n",
      "Iteration 470, Loss: 38.19046401977539, L1: 11.544997215270996, L3: 26.645465850830078\n",
      "Current prediction:  61.39364242553711 \n",
      "\n",
      "Iteration 471, Loss: 38.279884338378906, L1: 11.401878356933594, L3: 26.878005981445312\n",
      "Current prediction:  61.40398025512695 \n",
      "\n",
      "Iteration 472, Loss: 38.091697692871094, L1: 11.48499870300293, L3: 26.606700897216797\n",
      "Current prediction:  61.41420364379883 \n",
      "\n",
      "Iteration 473, Loss: 37.51779556274414, L1: 11.231796264648438, L3: 26.285999298095703\n",
      "Current prediction:  61.42466354370117 \n",
      "\n",
      "Iteration 474, Loss: 37.754512786865234, L1: 11.234509468078613, L3: 26.520002365112305\n",
      "Current prediction:  61.435150146484375 \n",
      "\n",
      "Iteration 475, Loss: 38.28334426879883, L1: 11.005158424377441, L3: 27.278186798095703\n",
      "Current prediction:  61.44548034667969 \n",
      "\n",
      "Iteration 476, Loss: 38.60106658935547, L1: 10.816727638244629, L3: 27.784339904785156\n",
      "Current prediction:  61.88485336303711 \n",
      "\n",
      "Iteration 477, Loss: 37.53369140625, L1: 10.697393417358398, L3: 26.83629608154297\n",
      "Current prediction:  62.23076248168945 \n",
      "\n",
      "Iteration 478, Loss: 36.59075927734375, L1: 10.58091926574707, L3: 26.00984001159668\n",
      "Current prediction:  62.239803314208984 \n",
      "\n",
      "Iteration 479, Loss: 37.825538635253906, L1: 10.552807807922363, L3: 27.27273178100586\n",
      "Current prediction:  62.248355865478516 \n",
      "\n",
      "Iteration 480, Loss: 37.251708984375, L1: 10.594345092773438, L3: 26.657365798950195\n",
      "Current prediction:  62.25636291503906 \n",
      "\n",
      "Iteration 481, Loss: 36.51197052001953, L1: 10.538771629333496, L3: 25.97319984436035\n",
      "Current prediction:  62.263519287109375 \n",
      "\n",
      "Iteration 482, Loss: 38.10614776611328, L1: 10.602401733398438, L3: 27.50374412536621\n",
      "Current prediction:  62.26968765258789 \n",
      "\n",
      "Iteration 483, Loss: 37.033050537109375, L1: 10.593592643737793, L3: 26.439456939697266\n",
      "Current prediction:  62.275108337402344 \n",
      "\n",
      "Iteration 484, Loss: 36.96867370605469, L1: 10.564480781555176, L3: 26.404193878173828\n",
      "Current prediction:  62.279903411865234 \n",
      "\n",
      "Iteration 485, Loss: 37.401832580566406, L1: 10.754971504211426, L3: 26.646862030029297\n",
      "Current prediction:  62.28402328491211 \n",
      "\n",
      "Iteration 486, Loss: 37.59845733642578, L1: 10.59115219116211, L3: 27.00730323791504\n",
      "Current prediction:  62.28757858276367 \n",
      "\n",
      "Iteration 487, Loss: 37.16758346557617, L1: 10.474124908447266, L3: 26.693458557128906\n",
      "Current prediction:  62.29060363769531 \n",
      "\n",
      "Iteration 488, Loss: 37.649635314941406, L1: 10.345226287841797, L3: 27.304407119750977\n",
      "Current prediction:  62.29311752319336 \n",
      "\n",
      "Iteration 489, Loss: 37.46360778808594, L1: 10.359495162963867, L3: 27.10411262512207\n",
      "Current prediction:  62.295143127441406 \n",
      "\n",
      "Iteration 490, Loss: 38.25390625, L1: 10.363499641418457, L3: 27.890405654907227\n",
      "Current prediction:  62.296478271484375 \n",
      "\n",
      "Iteration 491, Loss: 37.88884735107422, L1: 10.365391731262207, L3: 27.523456573486328\n",
      "Current prediction:  62.29745101928711 \n",
      "\n",
      "Iteration 492, Loss: 37.81328201293945, L1: 10.3994779586792, L3: 27.413803100585938\n",
      "Current prediction:  62.298622131347656 \n",
      "\n",
      "Iteration 493, Loss: 36.970985412597656, L1: 10.612920761108398, L3: 26.358062744140625\n",
      "Current prediction:  62.299720764160156 \n",
      "\n",
      "Iteration 494, Loss: 37.384849548339844, L1: 10.40146255493164, L3: 26.983386993408203\n",
      "Current prediction:  62.30033493041992 \n",
      "\n",
      "Iteration 495, Loss: 38.104400634765625, L1: 10.38191032409668, L3: 27.722490310668945\n",
      "Current prediction:  62.300594329833984 \n",
      "\n",
      "Iteration 496, Loss: 37.36656188964844, L1: 10.358504295349121, L3: 27.008056640625\n",
      "Current prediction:  62.300819396972656 \n",
      "\n",
      "Iteration 497, Loss: 37.67110824584961, L1: 10.33391284942627, L3: 27.337196350097656\n",
      "Current prediction:  62.301002502441406 \n",
      "\n",
      "Iteration 498, Loss: 36.48270797729492, L1: 10.375312805175781, L3: 26.10739517211914\n",
      "Current prediction:  62.301273345947266 \n",
      "\n",
      "Iteration 499, Loss: 36.464599609375, L1: 10.32435417175293, L3: 26.140247344970703\n",
      "Current prediction:  62.301143646240234 \n",
      "\n",
      "Iteration 500, Loss: 37.838802337646484, L1: 10.317654609680176, L3: 27.521146774291992\n",
      "Current prediction:  62.300689697265625 \n",
      "\n",
      "Iteration 501, Loss: 37.44061279296875, L1: 10.359305381774902, L3: 27.08130645751953\n",
      "Current prediction:  62.30009841918945 \n",
      "\n",
      "Iteration 502, Loss: 36.89116668701172, L1: 10.47260856628418, L3: 26.418556213378906\n",
      "Current prediction:  62.29933166503906 \n",
      "\n",
      "Iteration 503, Loss: 36.88986587524414, L1: 10.264528274536133, L3: 26.625337600708008\n",
      "Current prediction:  62.298126220703125 \n",
      "\n",
      "Iteration 504, Loss: 36.93373489379883, L1: 10.293234825134277, L3: 26.640499114990234\n",
      "Current prediction:  62.29680633544922 \n",
      "\n",
      "Iteration 505, Loss: 36.849185943603516, L1: 10.334829330444336, L3: 26.51435661315918\n",
      "Current prediction:  62.29506301879883 \n",
      "\n",
      "Iteration 506, Loss: 37.312843322753906, L1: 10.288628578186035, L3: 27.024215698242188\n",
      "Current prediction:  62.29303741455078 \n",
      "\n",
      "Iteration 507, Loss: 38.45633316040039, L1: 10.334146499633789, L3: 28.1221866607666\n",
      "Current prediction:  62.29044723510742 \n",
      "\n",
      "Iteration 508, Loss: 36.805931091308594, L1: 10.29697322845459, L3: 26.50895881652832\n",
      "Current prediction:  62.2873649597168 \n",
      "\n",
      "Iteration 509, Loss: 36.56084060668945, L1: 10.246441841125488, L3: 26.31439971923828\n",
      "Current prediction:  62.283973693847656 \n",
      "\n",
      "Iteration 510, Loss: 37.794708251953125, L1: 10.259662628173828, L3: 27.53504753112793\n",
      "Current prediction:  62.28040313720703 \n",
      "\n",
      "Iteration 511, Loss: 38.14494323730469, L1: 10.241706848144531, L3: 27.903234481811523\n",
      "Current prediction:  62.2767333984375 \n",
      "\n",
      "Iteration 512, Loss: 37.40266799926758, L1: 10.296385765075684, L3: 27.106281280517578\n",
      "Current prediction:  62.272857666015625 \n",
      "\n",
      "Iteration 513, Loss: 37.53644561767578, L1: 10.340248107910156, L3: 27.196197509765625\n",
      "Current prediction:  62.26873016357422 \n",
      "\n",
      "Iteration 514, Loss: 37.83622741699219, L1: 10.347016334533691, L3: 27.48921012878418\n",
      "Current prediction:  62.26442337036133 \n",
      "\n",
      "Iteration 515, Loss: 35.690826416015625, L1: 10.281635284423828, L3: 25.40919303894043\n",
      "Current prediction:  62.26059341430664 \n",
      "\n",
      "Iteration 516, Loss: 38.05636978149414, L1: 10.287735939025879, L3: 27.768634796142578\n",
      "Current prediction:  62.25661849975586 \n",
      "\n",
      "Iteration 517, Loss: 37.085838317871094, L1: 10.304269790649414, L3: 26.78156852722168\n",
      "Current prediction:  62.252357482910156 \n",
      "\n",
      "Iteration 518, Loss: 36.28978729248047, L1: 10.28711986541748, L3: 26.002666473388672\n",
      "Current prediction:  62.24827575683594 \n",
      "\n",
      "Iteration 519, Loss: 36.501548767089844, L1: 10.341968536376953, L3: 26.159578323364258\n",
      "Current prediction:  62.2442626953125 \n",
      "\n",
      "Iteration 520, Loss: 37.31669235229492, L1: 10.349881172180176, L3: 26.966812133789062\n",
      "Current prediction:  62.240116119384766 \n",
      "\n",
      "Iteration 521, Loss: 37.057987213134766, L1: 10.298548698425293, L3: 26.759437561035156\n",
      "Current prediction:  62.236080169677734 \n",
      "\n",
      "Iteration 522, Loss: 36.36479187011719, L1: 10.409974098205566, L3: 25.954818725585938\n",
      "Current prediction:  62.23206329345703 \n",
      "\n",
      "Iteration 523, Loss: 36.97705078125, L1: 10.350170135498047, L3: 26.62687873840332\n",
      "Current prediction:  62.22793960571289 \n",
      "\n",
      "Iteration 524, Loss: 38.10194396972656, L1: 10.368669509887695, L3: 27.733274459838867\n",
      "Current prediction:  62.22372817993164 \n",
      "\n",
      "Iteration 525, Loss: 36.088661193847656, L1: 10.346435546875, L3: 25.742225646972656\n",
      "Current prediction:  62.21992874145508 \n",
      "\n",
      "Iteration 526, Loss: 37.48914337158203, L1: 10.421404838562012, L3: 27.067737579345703\n",
      "Current prediction:  62.2161979675293 \n",
      "\n",
      "Iteration 527, Loss: 37.098594665527344, L1: 10.3568115234375, L3: 26.741785049438477\n",
      "Current prediction:  62.2123908996582 \n",
      "\n",
      "Iteration 528, Loss: 37.4710693359375, L1: 10.344948768615723, L3: 27.126121520996094\n",
      "Current prediction:  62.20862579345703 \n",
      "\n",
      "Iteration 529, Loss: 37.79777526855469, L1: 10.440462112426758, L3: 27.357315063476562\n",
      "Current prediction:  62.20496368408203 \n",
      "\n",
      "Iteration 530, Loss: 37.82775115966797, L1: 10.42147445678711, L3: 27.406274795532227\n",
      "Current prediction:  62.201412200927734 \n",
      "\n",
      "Iteration 531, Loss: 36.653297424316406, L1: 10.60061264038086, L3: 26.05268669128418\n",
      "Current prediction:  62.19815444946289 \n",
      "\n",
      "Iteration 532, Loss: 36.43424606323242, L1: 10.385698318481445, L3: 26.048547744750977\n",
      "Current prediction:  62.195396423339844 \n",
      "\n",
      "Iteration 533, Loss: 37.0318603515625, L1: 10.429465293884277, L3: 26.60239601135254\n",
      "Current prediction:  62.19293212890625 \n",
      "\n",
      "Iteration 534, Loss: 37.319576263427734, L1: 10.467543601989746, L3: 26.852031707763672\n",
      "Current prediction:  62.19059371948242 \n",
      "\n",
      "Iteration 535, Loss: 37.821414947509766, L1: 10.404546737670898, L3: 27.416868209838867\n",
      "Current prediction:  62.188194274902344 \n",
      "\n",
      "Iteration 536, Loss: 37.74629592895508, L1: 10.392498970031738, L3: 27.353796005249023\n",
      "Current prediction:  62.185794830322266 \n",
      "\n",
      "Iteration 537, Loss: 36.50281524658203, L1: 10.416423797607422, L3: 26.086393356323242\n",
      "Current prediction:  62.18330764770508 \n",
      "\n",
      "Iteration 538, Loss: 36.01693344116211, L1: 10.400259017944336, L3: 25.616674423217773\n",
      "Current prediction:  62.18108367919922 \n",
      "\n",
      "Iteration 539, Loss: 37.58799362182617, L1: 10.41498851776123, L3: 27.173006057739258\n",
      "Current prediction:  62.178611755371094 \n",
      "\n",
      "Iteration 540, Loss: 38.138309478759766, L1: 10.390597343444824, L3: 27.747713088989258\n",
      "Current prediction:  62.17623519897461 \n",
      "\n",
      "Iteration 541, Loss: 39.05048751831055, L1: 10.514174461364746, L3: 28.536312103271484\n",
      "Current prediction:  62.17384338378906 \n",
      "\n",
      "Iteration 542, Loss: 37.69813537597656, L1: 10.445162773132324, L3: 27.252971649169922\n",
      "Current prediction:  62.171417236328125 \n",
      "\n",
      "Iteration 543, Loss: 36.7972412109375, L1: 10.453965187072754, L3: 26.34327507019043\n",
      "Current prediction:  62.169464111328125 \n",
      "\n",
      "Iteration 544, Loss: 37.22211456298828, L1: 10.46096420288086, L3: 26.761152267456055\n",
      "Current prediction:  62.16765594482422 \n",
      "\n",
      "Iteration 545, Loss: 37.644859313964844, L1: 10.579713821411133, L3: 27.065147399902344\n",
      "Current prediction:  62.16606521606445 \n",
      "\n",
      "Iteration 546, Loss: 37.44172668457031, L1: 10.468432426452637, L3: 26.97329330444336\n",
      "Current prediction:  62.16477966308594 \n",
      "\n",
      "Iteration 547, Loss: 38.217620849609375, L1: 10.530061721801758, L3: 27.687559127807617\n",
      "Current prediction:  62.16331481933594 \n",
      "\n",
      "Iteration 548, Loss: 38.28702163696289, L1: 10.695596694946289, L3: 27.5914249420166\n",
      "Current prediction:  62.161739349365234 \n",
      "\n",
      "Iteration 549, Loss: 37.87911605834961, L1: 10.52209186553955, L3: 27.357025146484375\n",
      "Current prediction:  62.16061782836914 \n",
      "\n",
      "Iteration 550, Loss: 37.33555603027344, L1: 10.623598098754883, L3: 26.711959838867188\n",
      "Current prediction:  62.15973663330078 \n",
      "\n",
      "Iteration 551, Loss: 37.46043395996094, L1: 10.567825317382812, L3: 26.892606735229492\n",
      "Current prediction:  62.15900421142578 \n",
      "\n",
      "Iteration 552, Loss: 37.30372619628906, L1: 10.536310195922852, L3: 26.767417907714844\n",
      "Current prediction:  62.158653259277344 \n",
      "\n",
      "Iteration 553, Loss: 37.52118682861328, L1: 11.094391822814941, L3: 26.426795959472656\n",
      "Current prediction:  62.15863800048828 \n",
      "\n",
      "Iteration 554, Loss: 37.4986572265625, L1: 10.613690376281738, L3: 26.884967803955078\n",
      "Current prediction:  62.158729553222656 \n",
      "\n",
      "Iteration 555, Loss: 36.766883850097656, L1: 10.568171501159668, L3: 26.198713302612305\n",
      "Current prediction:  62.15916061401367 \n",
      "\n",
      "Iteration 556, Loss: 36.483245849609375, L1: 10.604999542236328, L3: 25.878244400024414\n",
      "Current prediction:  62.160247802734375 \n",
      "\n",
      "Iteration 557, Loss: 37.287315368652344, L1: 10.55904483795166, L3: 26.728271484375\n",
      "Current prediction:  62.16200637817383 \n",
      "\n",
      "Iteration 558, Loss: 37.5656623840332, L1: 10.585855484008789, L3: 26.979806900024414\n",
      "Current prediction:  62.164180755615234 \n",
      "\n",
      "Iteration 559, Loss: 40.2664680480957, L1: 14.237987518310547, L3: 26.028480529785156\n",
      "Current prediction:  62.166080474853516 \n",
      "\n",
      "Iteration 560, Loss: 37.02064895629883, L1: 10.568859100341797, L3: 26.45178985595703\n",
      "Current prediction:  62.16854476928711 \n",
      "\n",
      "Iteration 561, Loss: 37.82915496826172, L1: 10.597494125366211, L3: 27.231658935546875\n",
      "Current prediction:  62.17107009887695 \n",
      "\n",
      "Iteration 562, Loss: 37.72536849975586, L1: 10.549668312072754, L3: 27.17569923400879\n",
      "Current prediction:  62.17374038696289 \n",
      "\n",
      "Iteration 563, Loss: 37.90220642089844, L1: 10.553173065185547, L3: 27.349031448364258\n",
      "Current prediction:  62.17615509033203 \n",
      "\n",
      "Iteration 564, Loss: 36.98798751831055, L1: 10.58825969696045, L3: 26.399728775024414\n",
      "Current prediction:  62.178592681884766 \n",
      "\n",
      "Iteration 565, Loss: 36.16764831542969, L1: 10.582893371582031, L3: 25.58475685119629\n",
      "Current prediction:  62.18112564086914 \n",
      "\n",
      "Iteration 566, Loss: 37.26576614379883, L1: 10.496397972106934, L3: 26.76936912536621\n",
      "Current prediction:  62.18331527709961 \n",
      "\n",
      "Iteration 567, Loss: 37.4760856628418, L1: 10.607154846191406, L3: 26.86893081665039\n",
      "Current prediction:  62.185150146484375 \n",
      "\n",
      "Iteration 568, Loss: 37.69444274902344, L1: 10.508561134338379, L3: 27.185880661010742\n",
      "Current prediction:  62.18673324584961 \n",
      "\n",
      "Iteration 569, Loss: 37.60344314575195, L1: 10.57003402709961, L3: 27.033409118652344\n",
      "Current prediction:  62.18857955932617 \n",
      "\n",
      "Iteration 570, Loss: 37.6290283203125, L1: 10.532751083374023, L3: 27.096275329589844\n",
      "Current prediction:  62.190677642822266 \n",
      "\n",
      "Iteration 571, Loss: 37.28408432006836, L1: 10.489745140075684, L3: 26.79433822631836\n",
      "Current prediction:  62.19314193725586 \n",
      "\n",
      "Iteration 572, Loss: 37.140201568603516, L1: 10.503113746643066, L3: 26.637086868286133\n",
      "Current prediction:  62.195762634277344 \n",
      "\n",
      "Iteration 573, Loss: 37.320858001708984, L1: 10.563094139099121, L3: 26.75776481628418\n",
      "Current prediction:  62.19840621948242 \n",
      "\n",
      "Iteration 574, Loss: 38.9833984375, L1: 10.536622047424316, L3: 28.446775436401367\n",
      "Current prediction:  62.20103073120117 \n",
      "\n",
      "Iteration 575, Loss: 37.98485565185547, L1: 10.497941970825195, L3: 27.48691177368164\n",
      "Current prediction:  62.20408630371094 \n",
      "\n",
      "Iteration 576, Loss: 38.00368118286133, L1: 10.562589645385742, L3: 27.441091537475586\n",
      "Current prediction:  62.207122802734375 \n",
      "\n",
      "Iteration 577, Loss: 37.20622253417969, L1: 10.546881675720215, L3: 26.659339904785156\n",
      "Current prediction:  62.21047592163086 \n",
      "\n",
      "Iteration 578, Loss: 37.13555145263672, L1: 10.503952980041504, L3: 26.6315975189209\n",
      "Current prediction:  62.21419906616211 \n",
      "\n",
      "Iteration 579, Loss: 37.27544021606445, L1: 10.528668403625488, L3: 26.74677276611328\n",
      "Current prediction:  62.21784591674805 \n",
      "\n",
      "Iteration 580, Loss: 37.21441650390625, L1: 10.450143814086914, L3: 26.76427459716797\n",
      "Current prediction:  62.22157287597656 \n",
      "\n",
      "Iteration 581, Loss: 37.27949905395508, L1: 10.471692085266113, L3: 26.80780601501465\n",
      "Current prediction:  62.22525405883789 \n",
      "\n",
      "Iteration 582, Loss: 36.88662338256836, L1: 10.470534324645996, L3: 26.41609001159668\n",
      "Current prediction:  62.22930145263672 \n",
      "\n",
      "Iteration 583, Loss: 37.29775619506836, L1: 10.540574073791504, L3: 26.75718116760254\n",
      "Current prediction:  62.2334098815918 \n",
      "\n",
      "Iteration 584, Loss: 36.70304870605469, L1: 10.551650047302246, L3: 26.151399612426758\n",
      "Current prediction:  62.23768615722656 \n",
      "\n",
      "Iteration 585, Loss: 37.81991958618164, L1: 10.474663734436035, L3: 27.345256805419922\n",
      "Current prediction:  62.242122650146484 \n",
      "\n",
      "Iteration 586, Loss: 36.73420715332031, L1: 10.494874000549316, L3: 26.23933219909668\n",
      "Current prediction:  62.24650573730469 \n",
      "\n",
      "Iteration 587, Loss: 37.235294342041016, L1: 10.484904289245605, L3: 26.750391006469727\n",
      "Current prediction:  62.25053024291992 \n",
      "\n",
      "Iteration 588, Loss: 38.16810607910156, L1: 10.493108749389648, L3: 27.674999237060547\n",
      "Current prediction:  62.254310607910156 \n",
      "\n",
      "Iteration 589, Loss: 36.87913513183594, L1: 10.492734909057617, L3: 26.38640022277832\n",
      "Current prediction:  62.258079528808594 \n",
      "\n",
      "Iteration 590, Loss: 37.10601806640625, L1: 10.472013473510742, L3: 26.634002685546875\n",
      "Current prediction:  62.26178741455078 \n",
      "\n",
      "Iteration 591, Loss: 36.74795913696289, L1: 10.54216480255127, L3: 26.205795288085938\n",
      "Current prediction:  62.264984130859375 \n",
      "\n",
      "Iteration 592, Loss: 38.69367599487305, L1: 10.455745697021484, L3: 28.237930297851562\n",
      "Current prediction:  62.26779556274414 \n",
      "\n",
      "Iteration 593, Loss: 37.402191162109375, L1: 10.492055892944336, L3: 26.910137176513672\n",
      "Current prediction:  62.27037048339844 \n",
      "\n",
      "Iteration 594, Loss: 37.598480224609375, L1: 10.519157409667969, L3: 27.07932472229004\n",
      "Current prediction:  62.27263259887695 \n",
      "\n",
      "Iteration 595, Loss: 37.723388671875, L1: 10.478050231933594, L3: 27.245338439941406\n",
      "Current prediction:  62.27461624145508 \n",
      "\n",
      "Iteration 596, Loss: 36.83843994140625, L1: 10.397747039794922, L3: 26.440690994262695\n",
      "Current prediction:  62.27653503417969 \n",
      "\n",
      "Iteration 597, Loss: 37.405582427978516, L1: 10.435988426208496, L3: 26.969593048095703\n",
      "Current prediction:  62.27827072143555 \n",
      "\n",
      "Iteration 598, Loss: 37.218292236328125, L1: 10.48669719696045, L3: 26.73159408569336\n",
      "Current prediction:  62.27937316894531 \n",
      "\n",
      "Iteration 599, Loss: 37.602020263671875, L1: 10.43734073638916, L3: 27.16468048095703\n",
      "Current prediction:  62.280540466308594 \n",
      "\n",
      "Iteration 600, Loss: 36.36969757080078, L1: 10.457472801208496, L3: 25.91222381591797\n",
      "Current prediction:  62.28169250488281 \n",
      "\n",
      "Iteration 601, Loss: 36.78117370605469, L1: 10.388541221618652, L3: 26.39263153076172\n",
      "Current prediction:  62.28266143798828 \n",
      "\n",
      "Iteration 602, Loss: 37.01222229003906, L1: 10.412662506103516, L3: 26.59956169128418\n",
      "Current prediction:  62.283966064453125 \n",
      "\n",
      "Iteration 603, Loss: 37.23025131225586, L1: 10.50660514831543, L3: 26.72364616394043\n",
      "Current prediction:  62.28505325317383 \n",
      "\n",
      "Iteration 604, Loss: 38.01931381225586, L1: 10.379573822021484, L3: 27.639739990234375\n",
      "Current prediction:  62.28580093383789 \n",
      "\n",
      "Iteration 605, Loss: 37.05290222167969, L1: 10.503739356994629, L3: 26.549161911010742\n",
      "Current prediction:  62.286624908447266 \n",
      "\n",
      "Iteration 606, Loss: 38.21623611450195, L1: 10.508135795593262, L3: 27.708099365234375\n",
      "Current prediction:  62.287113189697266 \n",
      "\n",
      "Iteration 607, Loss: 36.78483581542969, L1: 10.367053985595703, L3: 26.417781829833984\n",
      "Current prediction:  62.287567138671875 \n",
      "\n",
      "Iteration 608, Loss: 37.703773498535156, L1: 10.439176559448242, L3: 27.264596939086914\n",
      "Current prediction:  62.28739547729492 \n",
      "\n",
      "Iteration 609, Loss: 37.287681579589844, L1: 10.392929077148438, L3: 26.894752502441406\n",
      "Current prediction:  62.28743362426758 \n",
      "\n",
      "Iteration 610, Loss: 38.17871856689453, L1: 10.391432762145996, L3: 27.78728675842285\n",
      "Current prediction:  62.28750991821289 \n",
      "\n",
      "Iteration 611, Loss: 35.9489631652832, L1: 10.375326156616211, L3: 25.573637008666992\n",
      "Current prediction:  62.28770065307617 \n",
      "\n",
      "Iteration 612, Loss: 37.854888916015625, L1: 10.405357360839844, L3: 27.44953155517578\n",
      "Current prediction:  62.28783416748047 \n",
      "\n",
      "Iteration 613, Loss: 38.5626106262207, L1: 10.833967208862305, L3: 27.7286434173584\n",
      "Current prediction:  62.28799819946289 \n",
      "\n",
      "Iteration 614, Loss: 36.18070602416992, L1: 10.376480102539062, L3: 25.80422592163086\n",
      "Current prediction:  62.287757873535156 \n",
      "\n",
      "Iteration 615, Loss: 37.244850158691406, L1: 10.496465682983398, L3: 26.748384475708008\n",
      "Current prediction:  62.28785705566406 \n",
      "\n",
      "Iteration 616, Loss: 38.67866134643555, L1: 10.760051727294922, L3: 27.918609619140625\n",
      "Current prediction:  62.28782653808594 \n",
      "\n",
      "Iteration 617, Loss: 37.75983428955078, L1: 10.569732666015625, L3: 27.190099716186523\n",
      "Current prediction:  61.75294494628906 \n",
      "\n",
      "Iteration 618, Loss: 38.30262756347656, L1: 10.652767181396484, L3: 27.649858474731445\n",
      "Current prediction:  61.602149963378906 \n",
      "\n",
      "Iteration 619, Loss: 38.42359161376953, L1: 10.950361251831055, L3: 27.473228454589844\n",
      "Current prediction:  61.60481643676758 \n",
      "\n",
      "Iteration 620, Loss: 37.61100769042969, L1: 11.166356086730957, L3: 26.444652557373047\n",
      "Current prediction:  61.608978271484375 \n",
      "\n",
      "Iteration 621, Loss: 37.11640167236328, L1: 11.215042114257812, L3: 25.90135955810547\n",
      "Current prediction:  61.614566802978516 \n",
      "\n",
      "Iteration 622, Loss: 38.68540573120117, L1: 11.077299118041992, L3: 27.60810661315918\n",
      "Current prediction:  61.62137222290039 \n",
      "\n",
      "Iteration 623, Loss: 37.4957275390625, L1: 10.829693794250488, L3: 26.666034698486328\n",
      "Current prediction:  61.629615783691406 \n",
      "\n",
      "Iteration 624, Loss: 37.110748291015625, L1: 10.860435485839844, L3: 26.250314712524414\n",
      "Current prediction:  61.63869857788086 \n",
      "\n",
      "Iteration 625, Loss: 38.073116302490234, L1: 10.841259956359863, L3: 27.231855392456055\n",
      "Current prediction:  61.648250579833984 \n",
      "\n",
      "Iteration 626, Loss: 37.38629913330078, L1: 10.862955093383789, L3: 26.523344039916992\n",
      "Current prediction:  61.65863037109375 \n",
      "\n",
      "Iteration 627, Loss: 38.177947998046875, L1: 10.859122276306152, L3: 27.318824768066406\n",
      "Current prediction:  61.66960525512695 \n",
      "\n",
      "Iteration 628, Loss: 39.0411376953125, L1: 11.673381805419922, L3: 27.367755889892578\n",
      "Current prediction:  61.680423736572266 \n",
      "\n",
      "Iteration 629, Loss: 37.98590850830078, L1: 10.867793083190918, L3: 27.11811637878418\n",
      "Current prediction:  61.69154357910156 \n",
      "\n",
      "Iteration 630, Loss: 36.761165618896484, L1: 10.955391883850098, L3: 25.805774688720703\n",
      "Current prediction:  61.70292663574219 \n",
      "\n",
      "Iteration 631, Loss: 37.2515869140625, L1: 11.244841575622559, L3: 26.006744384765625\n",
      "Current prediction:  61.714481353759766 \n",
      "\n",
      "Iteration 632, Loss: 37.70228958129883, L1: 11.027972221374512, L3: 26.674318313598633\n",
      "Current prediction:  61.72602081298828 \n",
      "\n",
      "Iteration 633, Loss: 37.5952033996582, L1: 11.025042533874512, L3: 26.570161819458008\n",
      "Current prediction:  60.92378234863281 \n",
      "\n",
      "Iteration 634, Loss: 38.341861724853516, L1: 11.253531455993652, L3: 27.08833122253418\n",
      "Current prediction:  60.93695068359375 \n",
      "\n",
      "Iteration 635, Loss: 37.9423828125, L1: 11.550094604492188, L3: 26.392288208007812\n",
      "Current prediction:  60.95128631591797 \n",
      "\n",
      "Iteration 636, Loss: 40.619178771972656, L1: 11.760186195373535, L3: 28.858993530273438\n",
      "Current prediction:  60.96699142456055 \n",
      "\n",
      "Iteration 637, Loss: 38.3974723815918, L1: 11.926432609558105, L3: 26.471040725708008\n",
      "Current prediction:  60.983524322509766 \n",
      "\n",
      "Iteration 638, Loss: 38.97027587890625, L1: 11.892240524291992, L3: 27.07803726196289\n",
      "Current prediction:  61.001102447509766 \n",
      "\n",
      "Iteration 639, Loss: 38.37797927856445, L1: 11.632763862609863, L3: 26.745216369628906\n",
      "Current prediction:  61.01945114135742 \n",
      "\n",
      "Iteration 640, Loss: 38.467750549316406, L1: 11.439765930175781, L3: 27.027984619140625\n",
      "Current prediction:  61.03841018676758 \n",
      "\n",
      "Iteration 641, Loss: 37.532379150390625, L1: 11.484977722167969, L3: 26.047401428222656\n",
      "Current prediction:  61.058536529541016 \n",
      "\n",
      "Iteration 642, Loss: 38.610687255859375, L1: 11.465517044067383, L3: 27.14516830444336\n",
      "Current prediction:  61.07962417602539 \n",
      "\n",
      "Iteration 643, Loss: 38.958065032958984, L1: 11.449846267700195, L3: 27.50821876525879\n",
      "Current prediction:  61.10095977783203 \n",
      "\n",
      "Iteration 644, Loss: 37.619503021240234, L1: 11.355205535888672, L3: 26.264297485351562\n",
      "Current prediction:  61.12289047241211 \n",
      "\n",
      "Iteration 645, Loss: 37.67792892456055, L1: 11.301018714904785, L3: 26.376909255981445\n",
      "Current prediction:  61.145076751708984 \n",
      "\n",
      "Iteration 646, Loss: 38.160865783691406, L1: 11.626285552978516, L3: 26.534582138061523\n",
      "Current prediction:  61.16703414916992 \n",
      "\n",
      "Iteration 647, Loss: 38.22894287109375, L1: 11.470149993896484, L3: 26.758792877197266\n",
      "Current prediction:  61.18922805786133 \n",
      "\n",
      "Iteration 648, Loss: 37.567344665527344, L1: 11.328714370727539, L3: 26.238628387451172\n",
      "Current prediction:  61.21158218383789 \n",
      "\n",
      "Iteration 649, Loss: 38.34153366088867, L1: 11.430583000183105, L3: 26.91094970703125\n",
      "Current prediction:  61.23398971557617 \n",
      "\n",
      "Iteration 650, Loss: 37.38715362548828, L1: 11.457944869995117, L3: 25.929208755493164\n",
      "Current prediction:  61.256587982177734 \n",
      "\n",
      "Iteration 651, Loss: 38.195579528808594, L1: 11.424530029296875, L3: 26.77104949951172\n",
      "Current prediction:  61.278900146484375 \n",
      "\n",
      "Iteration 652, Loss: 37.880760192871094, L1: 11.191967964172363, L3: 26.688793182373047\n",
      "Current prediction:  61.30079650878906 \n",
      "\n",
      "Iteration 653, Loss: 37.64897537231445, L1: 11.354063034057617, L3: 26.294912338256836\n",
      "Current prediction:  61.322601318359375 \n",
      "\n",
      "Iteration 654, Loss: 37.4852294921875, L1: 11.306267738342285, L3: 26.17896270751953\n",
      "Current prediction:  61.34430694580078 \n",
      "\n",
      "Iteration 655, Loss: 38.22919464111328, L1: 11.173131942749023, L3: 27.056062698364258\n",
      "Current prediction:  61.366058349609375 \n",
      "\n",
      "Iteration 656, Loss: 38.3409423828125, L1: 11.22236156463623, L3: 27.118581771850586\n",
      "Current prediction:  61.38776397705078 \n",
      "\n",
      "Iteration 657, Loss: 38.16432189941406, L1: 11.215831756591797, L3: 26.948490142822266\n",
      "Current prediction:  61.40953063964844 \n",
      "\n",
      "Iteration 658, Loss: 38.279361724853516, L1: 11.230071067810059, L3: 27.04928970336914\n",
      "Current prediction:  61.431278228759766 \n",
      "\n",
      "Iteration 659, Loss: 37.36546325683594, L1: 11.153810501098633, L3: 26.211650848388672\n",
      "Current prediction:  61.452693939208984 \n",
      "\n",
      "Iteration 660, Loss: 37.675716400146484, L1: 11.067843437194824, L3: 26.607872009277344\n",
      "Current prediction:  61.47389221191406 \n",
      "\n",
      "Iteration 661, Loss: 37.313385009765625, L1: 11.12350082397461, L3: 26.18988609313965\n",
      "Current prediction:  61.49454879760742 \n",
      "\n",
      "Iteration 662, Loss: 36.72361755371094, L1: 11.091904640197754, L3: 25.631711959838867\n",
      "Current prediction:  61.51498794555664 \n",
      "\n",
      "Iteration 663, Loss: 37.62725067138672, L1: 11.006108283996582, L3: 26.62114143371582\n",
      "Current prediction:  61.5349006652832 \n",
      "\n",
      "Iteration 664, Loss: 36.503780364990234, L1: 11.057953834533691, L3: 25.44582748413086\n",
      "Current prediction:  61.55418014526367 \n",
      "\n",
      "Iteration 665, Loss: 37.05438995361328, L1: 10.989439964294434, L3: 26.064950942993164\n",
      "Current prediction:  61.573123931884766 \n",
      "\n",
      "Iteration 666, Loss: 37.4443359375, L1: 11.066487312316895, L3: 26.377849578857422\n",
      "Current prediction:  61.59104919433594 \n",
      "\n",
      "Iteration 667, Loss: 37.92992401123047, L1: 10.907397270202637, L3: 27.02252769470215\n",
      "Current prediction:  61.608497619628906 \n",
      "\n",
      "Iteration 668, Loss: 36.54609298706055, L1: 10.88374137878418, L3: 25.662351608276367\n",
      "Current prediction:  61.62514114379883 \n",
      "\n",
      "Iteration 669, Loss: 36.51402282714844, L1: 11.061691284179688, L3: 25.45233154296875\n",
      "Current prediction:  61.64122772216797 \n",
      "\n",
      "Iteration 670, Loss: 37.40856170654297, L1: 11.040821075439453, L3: 26.367740631103516\n",
      "Current prediction:  61.65633010864258 \n",
      "\n",
      "Iteration 671, Loss: 37.505775451660156, L1: 11.110746383666992, L3: 26.395029067993164\n",
      "Current prediction:  61.67061996459961 \n",
      "\n",
      "Iteration 672, Loss: 38.03728103637695, L1: 11.255993843078613, L3: 26.781288146972656\n",
      "Current prediction:  61.68381118774414 \n",
      "\n",
      "Iteration 673, Loss: 36.62594223022461, L1: 11.036895751953125, L3: 25.589046478271484\n",
      "Current prediction:  61.69638442993164 \n",
      "\n",
      "Iteration 674, Loss: 38.08990478515625, L1: 11.058356285095215, L3: 27.03154754638672\n",
      "Current prediction:  61.70800018310547 \n",
      "\n",
      "Iteration 675, Loss: 38.18353271484375, L1: 11.10154914855957, L3: 27.081981658935547\n",
      "Current prediction:  61.71832275390625 \n",
      "\n",
      "Iteration 676, Loss: 36.73991394042969, L1: 10.941659927368164, L3: 25.798255920410156\n",
      "Current prediction:  61.728233337402344 \n",
      "\n",
      "Iteration 677, Loss: 38.281864166259766, L1: 11.137571334838867, L3: 27.1442928314209\n",
      "Current prediction:  61.73711013793945 \n",
      "\n",
      "Iteration 678, Loss: 38.257049560546875, L1: 11.152913093566895, L3: 27.104137420654297\n",
      "Current prediction:  61.745235443115234 \n",
      "\n",
      "Iteration 679, Loss: 37.3023567199707, L1: 10.966391563415527, L3: 26.335966110229492\n",
      "Current prediction:  61.752994537353516 \n",
      "\n",
      "Iteration 680, Loss: 36.921295166015625, L1: 11.147613525390625, L3: 25.773683547973633\n",
      "Current prediction:  61.76057052612305 \n",
      "\n",
      "Iteration 681, Loss: 38.07146453857422, L1: 10.9956636428833, L3: 27.0757999420166\n",
      "Current prediction:  61.767578125 \n",
      "\n",
      "Iteration 682, Loss: 37.909095764160156, L1: 11.080499649047852, L3: 26.828594207763672\n",
      "Current prediction:  61.774410247802734 \n",
      "\n",
      "Iteration 683, Loss: 37.97611618041992, L1: 11.025023460388184, L3: 26.951093673706055\n",
      "Current prediction:  61.780696868896484 \n",
      "\n",
      "Iteration 684, Loss: 37.987030029296875, L1: 11.031275749206543, L3: 26.955753326416016\n",
      "Current prediction:  61.78710174560547 \n",
      "\n",
      "Iteration 685, Loss: 37.82723617553711, L1: 10.988253593444824, L3: 26.83898162841797\n",
      "Current prediction:  61.793121337890625 \n",
      "\n",
      "Iteration 686, Loss: 38.820064544677734, L1: 10.925372123718262, L3: 27.89469337463379\n",
      "Current prediction:  61.798744201660156 \n",
      "\n",
      "Iteration 687, Loss: 38.09004211425781, L1: 10.909052848815918, L3: 27.18099021911621\n",
      "Current prediction:  61.80424118041992 \n",
      "\n",
      "Iteration 688, Loss: 37.331077575683594, L1: 10.940557479858398, L3: 26.390522003173828\n",
      "Current prediction:  61.80971908569336 \n",
      "\n",
      "Iteration 689, Loss: 38.25408172607422, L1: 10.9497709274292, L3: 27.304311752319336\n",
      "Current prediction:  61.8150634765625 \n",
      "\n",
      "Iteration 690, Loss: 37.02996063232422, L1: 10.821798324584961, L3: 26.208162307739258\n",
      "Current prediction:  61.82037353515625 \n",
      "\n",
      "Iteration 691, Loss: 38.0943603515625, L1: 10.925027847290039, L3: 27.169334411621094\n",
      "Current prediction:  61.8258056640625 \n",
      "\n",
      "Iteration 692, Loss: 37.618255615234375, L1: 10.824348449707031, L3: 26.793909072875977\n",
      "Current prediction:  61.83136749267578 \n",
      "\n",
      "Iteration 693, Loss: 37.05541229248047, L1: 10.793048858642578, L3: 26.26236343383789\n",
      "Current prediction:  61.8371467590332 \n",
      "\n",
      "Iteration 694, Loss: 37.04124069213867, L1: 10.634284019470215, L3: 26.40695571899414\n",
      "Current prediction:  61.84308624267578 \n",
      "\n",
      "Iteration 695, Loss: 37.897701263427734, L1: 10.607973098754883, L3: 27.28972816467285\n",
      "Current prediction:  61.84934616088867 \n",
      "\n",
      "Iteration 696, Loss: 36.75920867919922, L1: 10.718451499938965, L3: 26.04075813293457\n",
      "Current prediction:  61.85600662231445 \n",
      "\n",
      "Iteration 697, Loss: 37.37546920776367, L1: 10.747137069702148, L3: 26.628332138061523\n",
      "Current prediction:  61.86252212524414 \n",
      "\n",
      "Iteration 698, Loss: 36.141265869140625, L1: 10.664794921875, L3: 25.476470947265625\n",
      "Current prediction:  61.86935806274414 \n",
      "\n",
      "Iteration 699, Loss: 37.35198974609375, L1: 10.674601554870605, L3: 26.677387237548828\n",
      "Current prediction:  61.876590728759766 \n",
      "\n",
      "Iteration 700, Loss: 37.37117385864258, L1: 10.670126914978027, L3: 26.701045989990234\n",
      "Current prediction:  61.884029388427734 \n",
      "\n",
      "Iteration 701, Loss: 37.23725509643555, L1: 10.580711364746094, L3: 26.656543731689453\n",
      "Current prediction:  61.89174270629883 \n",
      "\n",
      "Iteration 702, Loss: 37.51476287841797, L1: 10.60975456237793, L3: 26.905010223388672\n",
      "Current prediction:  61.8994026184082 \n",
      "\n",
      "Iteration 703, Loss: 37.43128204345703, L1: 10.620844841003418, L3: 26.81043815612793\n",
      "Current prediction:  61.90721130371094 \n",
      "\n",
      "Iteration 704, Loss: 36.534175872802734, L1: 10.60960865020752, L3: 25.92456817626953\n",
      "Current prediction:  61.914920806884766 \n",
      "\n",
      "Iteration 705, Loss: 37.21216583251953, L1: 10.607324600219727, L3: 26.604841232299805\n",
      "Current prediction:  61.9225959777832 \n",
      "\n",
      "Iteration 706, Loss: 36.61107635498047, L1: 10.461099624633789, L3: 26.149974822998047\n",
      "Current prediction:  61.93010330200195 \n",
      "\n",
      "Iteration 707, Loss: 37.254905700683594, L1: 10.682987213134766, L3: 26.571918487548828\n",
      "Current prediction:  61.937381744384766 \n",
      "\n",
      "Iteration 708, Loss: 37.08234786987305, L1: 10.623950004577637, L3: 26.458396911621094\n",
      "Current prediction:  61.94474792480469 \n",
      "\n",
      "Iteration 709, Loss: 36.9705810546875, L1: 10.532414436340332, L3: 26.43816566467285\n",
      "Current prediction:  61.951900482177734 \n",
      "\n",
      "Iteration 710, Loss: 37.02267074584961, L1: 10.63598918914795, L3: 26.386680603027344\n",
      "Current prediction:  61.95813751220703 \n",
      "\n",
      "Iteration 711, Loss: 36.973873138427734, L1: 10.62574577331543, L3: 26.348127365112305\n",
      "Current prediction:  61.96406173706055 \n",
      "\n",
      "Iteration 712, Loss: 38.0636100769043, L1: 10.655749320983887, L3: 27.407859802246094\n",
      "Current prediction:  61.9697151184082 \n",
      "\n",
      "Iteration 713, Loss: 36.445648193359375, L1: 10.547273635864258, L3: 25.898372650146484\n",
      "Current prediction:  61.97535705566406 \n",
      "\n",
      "Iteration 714, Loss: 37.55830001831055, L1: 10.567173957824707, L3: 26.991125106811523\n",
      "Current prediction:  61.98042297363281 \n",
      "\n",
      "Iteration 715, Loss: 37.18989181518555, L1: 10.646804809570312, L3: 26.543087005615234\n",
      "Current prediction:  61.985206604003906 \n",
      "\n",
      "Iteration 716, Loss: 36.495113372802734, L1: 10.64749813079834, L3: 25.847614288330078\n",
      "Current prediction:  61.98982620239258 \n",
      "\n",
      "Iteration 717, Loss: 37.49633026123047, L1: 10.561269760131836, L3: 26.93505859375\n",
      "Current prediction:  61.993873596191406 \n",
      "\n",
      "Iteration 718, Loss: 37.760589599609375, L1: 10.684083938598633, L3: 27.07650375366211\n",
      "Current prediction:  61.9967155456543 \n",
      "\n",
      "Iteration 719, Loss: 36.785179138183594, L1: 10.814096450805664, L3: 25.97108268737793\n",
      "Current prediction:  61.99924850463867 \n",
      "\n",
      "Iteration 720, Loss: 37.41206359863281, L1: 10.787284851074219, L3: 26.624780654907227\n",
      "Current prediction:  62.00107955932617 \n",
      "\n",
      "Iteration 721, Loss: 36.829402923583984, L1: 11.000670433044434, L3: 25.828733444213867\n",
      "Current prediction:  62.00236129760742 \n",
      "\n",
      "Iteration 722, Loss: 37.978004455566406, L1: 10.64533805847168, L3: 27.332666397094727\n",
      "Current prediction:  62.003013610839844 \n",
      "\n",
      "Iteration 723, Loss: 37.713783264160156, L1: 10.444737434387207, L3: 27.269046783447266\n",
      "Current prediction:  62.00345230102539 \n",
      "\n",
      "Iteration 724, Loss: 37.05862045288086, L1: 10.617822647094727, L3: 26.440797805786133\n",
      "Current prediction:  62.00371551513672 \n",
      "\n",
      "Iteration 725, Loss: 36.9322509765625, L1: 10.64133358001709, L3: 26.290918350219727\n",
      "Current prediction:  62.00375747680664 \n",
      "\n",
      "Iteration 726, Loss: 36.811893463134766, L1: 10.574448585510254, L3: 26.237443923950195\n",
      "Current prediction:  62.003570556640625 \n",
      "\n",
      "Iteration 727, Loss: 36.72666549682617, L1: 10.473078727722168, L3: 26.25358772277832\n",
      "Current prediction:  62.003421783447266 \n",
      "\n",
      "Iteration 728, Loss: 36.91811752319336, L1: 10.597967147827148, L3: 26.32015037536621\n",
      "Current prediction:  62.00342559814453 \n",
      "\n",
      "Iteration 729, Loss: 37.71736526489258, L1: 10.536820411682129, L3: 27.180543899536133\n",
      "Current prediction:  62.00373077392578 \n",
      "\n",
      "Iteration 730, Loss: 37.07915496826172, L1: 10.512310028076172, L3: 26.566843032836914\n",
      "Current prediction:  62.00455093383789 \n",
      "\n",
      "Iteration 731, Loss: 37.983211517333984, L1: 10.512364387512207, L3: 27.47084617614746\n",
      "Current prediction:  62.00529479980469 \n",
      "\n",
      "Iteration 732, Loss: 37.513526916503906, L1: 10.530919075012207, L3: 26.982606887817383\n",
      "Current prediction:  62.00648880004883 \n",
      "\n",
      "Iteration 733, Loss: 36.901611328125, L1: 10.645606994628906, L3: 26.256006240844727\n",
      "Current prediction:  62.00776672363281 \n",
      "\n",
      "Iteration 734, Loss: 38.33324432373047, L1: 10.689826965332031, L3: 27.64341926574707\n",
      "Current prediction:  62.00908279418945 \n",
      "\n",
      "Iteration 735, Loss: 36.758216857910156, L1: 10.596473693847656, L3: 26.161741256713867\n",
      "Current prediction:  62.01097869873047 \n",
      "\n",
      "Iteration 736, Loss: 37.31202697753906, L1: 10.652138710021973, L3: 26.659887313842773\n",
      "Current prediction:  62.013282775878906 \n",
      "\n",
      "Iteration 737, Loss: 38.14256286621094, L1: 10.592995643615723, L3: 27.54956817626953\n",
      "Current prediction:  62.015830993652344 \n",
      "\n",
      "Iteration 738, Loss: 37.259315490722656, L1: 10.61161994934082, L3: 26.647693634033203\n",
      "Current prediction:  62.018959045410156 \n",
      "\n",
      "Iteration 739, Loss: 38.13474655151367, L1: 10.599516868591309, L3: 27.535228729248047\n",
      "Current prediction:  62.02234649658203 \n",
      "\n",
      "Iteration 740, Loss: 36.98731231689453, L1: 10.543069839477539, L3: 26.444244384765625\n",
      "Current prediction:  62.02579116821289 \n",
      "\n",
      "Iteration 741, Loss: 37.30615234375, L1: 10.607423782348633, L3: 26.698728561401367\n",
      "Current prediction:  62.029361724853516 \n",
      "\n",
      "Iteration 742, Loss: 38.51722717285156, L1: 10.593704223632812, L3: 27.923521041870117\n",
      "Current prediction:  62.033111572265625 \n",
      "\n",
      "Iteration 743, Loss: 36.90272903442383, L1: 10.557059288024902, L3: 26.34566879272461\n",
      "Current prediction:  62.037025451660156 \n",
      "\n",
      "Iteration 744, Loss: 37.55280685424805, L1: 10.536980628967285, L3: 27.015825271606445\n",
      "Current prediction:  62.04115295410156 \n",
      "\n",
      "Iteration 745, Loss: 37.39484786987305, L1: 10.509509086608887, L3: 26.885337829589844\n",
      "Current prediction:  62.04558563232422 \n",
      "\n",
      "Iteration 746, Loss: 37.025787353515625, L1: 10.461311340332031, L3: 26.56447410583496\n",
      "Current prediction:  62.05042266845703 \n",
      "\n",
      "Iteration 747, Loss: 37.44618225097656, L1: 10.525279998779297, L3: 26.920902252197266\n",
      "Current prediction:  62.05538558959961 \n",
      "\n",
      "Iteration 748, Loss: 36.75484085083008, L1: 10.518820762634277, L3: 26.236019134521484\n",
      "Current prediction:  62.06035614013672 \n",
      "\n",
      "Iteration 749, Loss: 37.83002853393555, L1: 10.566140174865723, L3: 27.263887405395508\n",
      "Current prediction:  62.0512809753418 \n",
      "\n",
      "Iteration 750, Loss: 37.22908020019531, L1: 10.62612533569336, L3: 26.60295295715332\n",
      "Current prediction:  61.8039665222168 \n",
      "\n",
      "Iteration 751, Loss: 38.22235870361328, L1: 10.521735191345215, L3: 27.700624465942383\n",
      "Current prediction:  61.54914474487305 \n",
      "\n",
      "Iteration 752, Loss: 36.93284225463867, L1: 10.589320182800293, L3: 26.343521118164062\n",
      "Current prediction:  61.53810119628906 \n",
      "\n",
      "Iteration 753, Loss: 36.777427673339844, L1: 10.618706703186035, L3: 26.158720016479492\n",
      "Current prediction:  61.54376983642578 \n",
      "\n",
      "Iteration 754, Loss: 37.19451904296875, L1: 10.589654922485352, L3: 26.604862213134766\n",
      "Current prediction:  61.55029296875 \n",
      "\n",
      "Iteration 755, Loss: 37.34206771850586, L1: 10.626282691955566, L3: 26.715784072875977\n",
      "Current prediction:  61.556907653808594 \n",
      "\n",
      "Iteration 756, Loss: 38.0673828125, L1: 10.619627952575684, L3: 27.447755813598633\n",
      "Current prediction:  61.56386184692383 \n",
      "\n",
      "Iteration 757, Loss: 37.33792495727539, L1: 10.560303688049316, L3: 26.77762222290039\n",
      "Current prediction:  61.57088088989258 \n",
      "\n",
      "Iteration 758, Loss: 36.61761474609375, L1: 10.570226669311523, L3: 26.04738998413086\n",
      "Current prediction:  61.57829666137695 \n",
      "\n",
      "Iteration 759, Loss: 38.251277923583984, L1: 10.583285331726074, L3: 27.667993545532227\n",
      "Current prediction:  61.58534622192383 \n",
      "\n",
      "Iteration 760, Loss: 37.90351867675781, L1: 10.604230880737305, L3: 27.29928970336914\n",
      "Current prediction:  61.59188461303711 \n",
      "\n",
      "Iteration 761, Loss: 37.431819915771484, L1: 10.525851249694824, L3: 26.905969619750977\n",
      "Current prediction:  61.59840393066406 \n",
      "\n",
      "Iteration 762, Loss: 36.185001373291016, L1: 10.558820724487305, L3: 25.62618064880371\n",
      "Current prediction:  61.60506057739258 \n",
      "\n",
      "Iteration 763, Loss: 37.32953643798828, L1: 10.505216598510742, L3: 26.824317932128906\n",
      "Current prediction:  61.611812591552734 \n",
      "\n",
      "Iteration 764, Loss: 37.356689453125, L1: 10.521394729614258, L3: 26.83529281616211\n",
      "Current prediction:  61.61890411376953 \n",
      "\n",
      "Iteration 765, Loss: 38.152503967285156, L1: 10.462461471557617, L3: 27.690040588378906\n",
      "Current prediction:  61.62771224975586 \n",
      "\n",
      "Iteration 766, Loss: 38.049217224121094, L1: 10.4986572265625, L3: 27.550561904907227\n",
      "Current prediction:  61.649513244628906 \n",
      "\n",
      "Iteration 767, Loss: 37.514556884765625, L1: 10.527358055114746, L3: 26.987199783325195\n",
      "Current prediction:  61.75365447998047 \n",
      "\n",
      "Iteration 768, Loss: 37.05360412597656, L1: 10.42325210571289, L3: 26.63035011291504\n",
      "Current prediction:  62.036991119384766 \n",
      "\n",
      "Iteration 769, Loss: 37.02021026611328, L1: 10.426213264465332, L3: 26.593996047973633\n",
      "Current prediction:  62.17414474487305 \n",
      "\n",
      "Iteration 770, Loss: 37.080108642578125, L1: 10.437767028808594, L3: 26.6423397064209\n",
      "Current prediction:  62.20020294189453 \n",
      "\n",
      "Iteration 771, Loss: 37.37413024902344, L1: 10.391807556152344, L3: 26.982322692871094\n",
      "Current prediction:  62.20775604248047 \n",
      "\n",
      "Iteration 772, Loss: 36.27196502685547, L1: 10.422737121582031, L3: 25.84922981262207\n",
      "Current prediction:  62.21352005004883 \n",
      "\n",
      "Iteration 773, Loss: 36.87632751464844, L1: 10.3709716796875, L3: 26.505353927612305\n",
      "Current prediction:  62.219032287597656 \n",
      "\n",
      "Iteration 774, Loss: 36.43762969970703, L1: 10.309300422668457, L3: 26.128328323364258\n",
      "Current prediction:  62.223915100097656 \n",
      "\n",
      "Iteration 775, Loss: 36.6868782043457, L1: 10.336764335632324, L3: 26.350112915039062\n",
      "Current prediction:  62.2286262512207 \n",
      "\n",
      "Iteration 776, Loss: 35.98981857299805, L1: 10.315842628479004, L3: 25.673974990844727\n",
      "Current prediction:  62.232975006103516 \n",
      "\n",
      "Iteration 777, Loss: 36.55820846557617, L1: 10.335939407348633, L3: 26.22226905822754\n",
      "Current prediction:  62.23678207397461 \n",
      "\n",
      "Iteration 778, Loss: 36.33544158935547, L1: 10.33194637298584, L3: 26.003496170043945\n",
      "Current prediction:  62.2398567199707 \n",
      "\n",
      "Iteration 779, Loss: 36.831390380859375, L1: 10.410972595214844, L3: 26.4204158782959\n",
      "Current prediction:  62.24245834350586 \n",
      "\n",
      "Iteration 780, Loss: 37.200660705566406, L1: 10.330133438110352, L3: 26.870525360107422\n",
      "Current prediction:  62.244239807128906 \n",
      "\n",
      "Iteration 781, Loss: 36.67787170410156, L1: 10.358409881591797, L3: 26.319461822509766\n",
      "Current prediction:  62.2452507019043 \n",
      "\n",
      "Iteration 782, Loss: 36.939510345458984, L1: 10.409313201904297, L3: 26.530197143554688\n",
      "Current prediction:  62.24577713012695 \n",
      "\n",
      "Iteration 783, Loss: 36.86412048339844, L1: 10.32298755645752, L3: 26.5411319732666\n",
      "Current prediction:  62.24586486816406 \n",
      "\n",
      "Iteration 784, Loss: 37.73147201538086, L1: 10.379128456115723, L3: 27.352344512939453\n",
      "Current prediction:  62.244991302490234 \n",
      "\n",
      "Iteration 785, Loss: 36.85428237915039, L1: 10.33155345916748, L3: 26.522727966308594\n",
      "Current prediction:  62.2435302734375 \n",
      "\n",
      "Iteration 786, Loss: 36.304405212402344, L1: 10.349837303161621, L3: 25.95456886291504\n",
      "Current prediction:  62.2419319152832 \n",
      "\n",
      "Iteration 787, Loss: 37.552738189697266, L1: 10.326958656311035, L3: 27.225780487060547\n",
      "Current prediction:  62.24031448364258 \n",
      "\n",
      "Iteration 788, Loss: 37.40437698364258, L1: 10.385333061218262, L3: 27.01904296875\n",
      "Current prediction:  62.23838424682617 \n",
      "\n",
      "Iteration 789, Loss: 36.56770706176758, L1: 10.402473449707031, L3: 26.165233612060547\n",
      "Current prediction:  62.2361946105957 \n",
      "\n",
      "Iteration 790, Loss: 36.51416778564453, L1: 10.362276077270508, L3: 26.15188980102539\n",
      "Current prediction:  62.234012603759766 \n",
      "\n",
      "Iteration 791, Loss: 37.679264068603516, L1: 10.278311729431152, L3: 27.40095329284668\n",
      "Current prediction:  62.23160934448242 \n",
      "\n",
      "Iteration 792, Loss: 37.1628303527832, L1: 10.251411437988281, L3: 26.911418914794922\n",
      "Current prediction:  62.229488372802734 \n",
      "\n",
      "Iteration 793, Loss: 37.106468200683594, L1: 10.290283203125, L3: 26.816184997558594\n",
      "Current prediction:  62.227725982666016 \n",
      "\n",
      "Iteration 794, Loss: 36.41069030761719, L1: 10.341556549072266, L3: 26.06913185119629\n",
      "Current prediction:  62.22620391845703 \n",
      "\n",
      "Iteration 795, Loss: 36.31934356689453, L1: 10.3306245803833, L3: 25.988719940185547\n",
      "Current prediction:  62.224979400634766 \n",
      "\n",
      "Iteration 796, Loss: 36.942787170410156, L1: 10.378504753112793, L3: 26.564281463623047\n",
      "Current prediction:  62.22427749633789 \n",
      "\n",
      "Iteration 797, Loss: 37.67864227294922, L1: 10.34254264831543, L3: 27.336097717285156\n",
      "Current prediction:  62.223243713378906 \n",
      "\n",
      "Iteration 798, Loss: 38.100284576416016, L1: 10.312749862670898, L3: 27.787534713745117\n",
      "Current prediction:  62.22244644165039 \n",
      "\n",
      "Iteration 799, Loss: 37.04267120361328, L1: 10.305715560913086, L3: 26.736955642700195\n",
      "Current prediction:  62.22188186645508 \n",
      "\n",
      "Iteration 800, Loss: 37.188865661621094, L1: 10.31948471069336, L3: 26.869380950927734\n",
      "Current prediction:  62.22172546386719 \n",
      "\n",
      "Iteration 801, Loss: 36.498138427734375, L1: 10.36760425567627, L3: 26.130535125732422\n",
      "Current prediction:  62.22211456298828 \n",
      "\n",
      "Iteration 802, Loss: 37.01474380493164, L1: 10.39966106414795, L3: 26.615083694458008\n",
      "Current prediction:  62.22321319580078 \n",
      "\n",
      "Iteration 803, Loss: 37.85051727294922, L1: 10.34349536895752, L3: 27.507020950317383\n",
      "Current prediction:  62.22443389892578 \n",
      "\n",
      "Iteration 804, Loss: 37.918853759765625, L1: 10.321183204650879, L3: 27.59766960144043\n",
      "Current prediction:  62.225948333740234 \n",
      "\n",
      "Iteration 805, Loss: 37.530094146728516, L1: 10.379544258117676, L3: 27.150550842285156\n",
      "Current prediction:  62.22771072387695 \n",
      "\n",
      "Iteration 806, Loss: 36.9368782043457, L1: 10.378571510314941, L3: 26.558305740356445\n",
      "Current prediction:  62.229251861572266 \n",
      "\n",
      "Iteration 807, Loss: 37.25889587402344, L1: 10.294682502746582, L3: 26.96421241760254\n",
      "Current prediction:  62.23060989379883 \n",
      "\n",
      "Iteration 808, Loss: 37.572959899902344, L1: 10.366365432739258, L3: 27.206594467163086\n",
      "Current prediction:  62.232017517089844 \n",
      "\n",
      "Iteration 809, Loss: 37.60526657104492, L1: 10.366026878356934, L3: 27.239240646362305\n",
      "Current prediction:  62.233306884765625 \n",
      "\n",
      "Iteration 810, Loss: 37.61796569824219, L1: 10.268386840820312, L3: 27.349578857421875\n",
      "Current prediction:  62.23445510864258 \n",
      "\n",
      "Iteration 811, Loss: 37.41895294189453, L1: 10.32883071899414, L3: 27.090124130249023\n",
      "Current prediction:  62.23528289794922 \n",
      "\n",
      "Iteration 812, Loss: 37.10962677001953, L1: 10.306293487548828, L3: 26.80333137512207\n",
      "Current prediction:  62.235713958740234 \n",
      "\n",
      "Iteration 813, Loss: 36.911441802978516, L1: 10.351300239562988, L3: 26.56014060974121\n",
      "Current prediction:  62.236270904541016 \n",
      "\n",
      "Iteration 814, Loss: 36.33837890625, L1: 10.394647598266602, L3: 25.94373321533203\n",
      "Current prediction:  62.23708724975586 \n",
      "\n",
      "Iteration 815, Loss: 37.21139144897461, L1: 10.360302925109863, L3: 26.85108757019043\n",
      "Current prediction:  62.23764419555664 \n",
      "\n",
      "Iteration 816, Loss: 37.13526916503906, L1: 10.322976112365723, L3: 26.812294006347656\n",
      "Current prediction:  62.238121032714844 \n",
      "\n",
      "Iteration 817, Loss: 36.908164978027344, L1: 10.300688743591309, L3: 26.60747528076172\n",
      "Current prediction:  62.23868942260742 \n",
      "\n",
      "Iteration 818, Loss: 36.93046951293945, L1: 10.287206649780273, L3: 26.64326286315918\n",
      "Current prediction:  62.239105224609375 \n",
      "\n",
      "Iteration 819, Loss: 38.090641021728516, L1: 10.308797836303711, L3: 27.781843185424805\n",
      "Current prediction:  62.23888397216797 \n",
      "\n",
      "Iteration 820, Loss: 36.36970138549805, L1: 10.34675121307373, L3: 26.02294921875\n",
      "Current prediction:  62.23882293701172 \n",
      "\n",
      "Iteration 821, Loss: 39.24152374267578, L1: 12.175928115844727, L3: 27.065593719482422\n",
      "Current prediction:  62.23889923095703 \n",
      "\n",
      "Iteration 822, Loss: 37.075592041015625, L1: 10.311737060546875, L3: 26.763856887817383\n",
      "Current prediction:  62.2391242980957 \n",
      "\n",
      "Iteration 823, Loss: 36.96418762207031, L1: 10.350259780883789, L3: 26.613927841186523\n",
      "Current prediction:  62.23960876464844 \n",
      "\n",
      "Iteration 824, Loss: 36.94340515136719, L1: 10.389230728149414, L3: 26.55417251586914\n",
      "Current prediction:  62.24076843261719 \n",
      "\n",
      "Iteration 825, Loss: 37.54507827758789, L1: 10.367980003356934, L3: 27.177099227905273\n",
      "Current prediction:  62.242042541503906 \n",
      "\n",
      "Iteration 826, Loss: 37.563270568847656, L1: 10.359026908874512, L3: 27.20424461364746\n",
      "Current prediction:  62.24298095703125 \n",
      "\n",
      "Iteration 827, Loss: 36.827964782714844, L1: 10.40270709991455, L3: 26.425256729125977\n",
      "Current prediction:  62.23869323730469 \n",
      "\n",
      "Iteration 828, Loss: 37.40117263793945, L1: 10.375923156738281, L3: 27.025249481201172\n",
      "Current prediction:  62.210723876953125 \n",
      "\n",
      "Iteration 829, Loss: 36.679874420166016, L1: 10.414322853088379, L3: 26.265552520751953\n",
      "Current prediction:  62.133155822753906 \n",
      "\n",
      "Iteration 830, Loss: 38.67254638671875, L1: 10.388967514038086, L3: 28.283578872680664\n",
      "Current prediction:  62.07322311401367 \n",
      "\n",
      "Iteration 831, Loss: 36.88030242919922, L1: 10.397156715393066, L3: 26.48314666748047\n",
      "Current prediction:  62.02622985839844 \n",
      "\n",
      "Iteration 832, Loss: 37.49531555175781, L1: 10.379068374633789, L3: 27.116249084472656\n",
      "Current prediction:  62.02427673339844 \n",
      "\n",
      "Iteration 833, Loss: 36.70843505859375, L1: 10.38097095489502, L3: 26.327463150024414\n",
      "Current prediction:  62.03361892700195 \n",
      "\n",
      "Iteration 834, Loss: 37.408695220947266, L1: 10.37124252319336, L3: 27.037452697753906\n",
      "Current prediction:  62.066505432128906 \n",
      "\n",
      "Iteration 835, Loss: 37.41749954223633, L1: 10.390547752380371, L3: 27.02695083618164\n",
      "Current prediction:  62.12037658691406 \n",
      "\n",
      "Iteration 836, Loss: 37.51292419433594, L1: 10.382494926452637, L3: 27.130428314208984\n",
      "Current prediction:  62.18866729736328 \n",
      "\n",
      "Iteration 837, Loss: 37.97037887573242, L1: 10.343392372131348, L3: 27.62698745727539\n",
      "Current prediction:  62.23463821411133 \n",
      "\n",
      "Iteration 838, Loss: 37.80963134765625, L1: 10.363138198852539, L3: 27.44649314880371\n",
      "Current prediction:  62.253665924072266 \n",
      "\n",
      "Iteration 839, Loss: 37.07814025878906, L1: 10.325072288513184, L3: 26.753068923950195\n",
      "Current prediction:  62.270503997802734 \n",
      "\n",
      "Iteration 840, Loss: 37.47636032104492, L1: 10.339654922485352, L3: 27.13670539855957\n",
      "Current prediction:  62.275909423828125 \n",
      "\n",
      "Iteration 841, Loss: 37.2078857421875, L1: 10.326644897460938, L3: 26.881240844726562\n",
      "Current prediction:  62.28502655029297 \n",
      "\n",
      "Iteration 842, Loss: 36.625770568847656, L1: 10.272335052490234, L3: 26.353437423706055\n",
      "Current prediction:  62.29415512084961 \n",
      "\n",
      "Iteration 843, Loss: 37.21195983886719, L1: 10.289321899414062, L3: 26.922637939453125\n",
      "Current prediction:  62.299617767333984 \n",
      "\n",
      "Iteration 844, Loss: 37.450286865234375, L1: 10.275388717651367, L3: 27.174896240234375\n",
      "Current prediction:  62.30322265625 \n",
      "\n",
      "Iteration 845, Loss: 36.98833084106445, L1: 10.243827819824219, L3: 26.744503021240234\n",
      "Current prediction:  62.30607986450195 \n",
      "\n",
      "Iteration 846, Loss: 36.836875915527344, L1: 10.236343383789062, L3: 26.60053062438965\n",
      "Current prediction:  62.30863571166992 \n",
      "\n",
      "Iteration 847, Loss: 38.44007110595703, L1: 10.311508178710938, L3: 28.128562927246094\n",
      "Current prediction:  62.31035232543945 \n",
      "\n",
      "Iteration 848, Loss: 36.088775634765625, L1: 10.241296768188477, L3: 25.84748077392578\n",
      "Current prediction:  62.31201171875 \n",
      "\n",
      "Iteration 849, Loss: 37.2132568359375, L1: 10.253644943237305, L3: 26.959611892700195\n",
      "Current prediction:  62.31355667114258 \n",
      "\n",
      "Iteration 850, Loss: 37.97932052612305, L1: 10.269993782043457, L3: 27.709327697753906\n",
      "Current prediction:  62.31441116333008 \n",
      "\n",
      "Iteration 851, Loss: 36.40483093261719, L1: 10.260199546813965, L3: 26.144630432128906\n",
      "Current prediction:  62.31486129760742 \n",
      "\n",
      "Iteration 852, Loss: 35.696041107177734, L1: 10.237488746643066, L3: 25.458553314208984\n",
      "Current prediction:  62.315521240234375 \n",
      "\n",
      "Iteration 853, Loss: 36.87773132324219, L1: 10.258794784545898, L3: 26.618934631347656\n",
      "Current prediction:  62.315589904785156 \n",
      "\n",
      "Iteration 854, Loss: 37.00782775878906, L1: 10.189943313598633, L3: 26.81788444519043\n",
      "Current prediction:  62.31488800048828 \n",
      "\n",
      "Iteration 855, Loss: 37.30617904663086, L1: 10.191144943237305, L3: 27.115034103393555\n",
      "Current prediction:  62.31373596191406 \n",
      "\n",
      "Iteration 856, Loss: 37.45319747924805, L1: 10.23060131072998, L3: 27.222597122192383\n",
      "Current prediction:  62.31238555908203 \n",
      "\n",
      "Iteration 857, Loss: 36.92434310913086, L1: 10.201416969299316, L3: 26.722925186157227\n",
      "Current prediction:  62.310577392578125 \n",
      "\n",
      "Iteration 858, Loss: 37.032188415527344, L1: 10.2344331741333, L3: 26.79775619506836\n",
      "Current prediction:  62.30870056152344 \n",
      "\n",
      "Iteration 859, Loss: 38.206214904785156, L1: 10.206944465637207, L3: 27.999271392822266\n",
      "Current prediction:  62.3068733215332 \n",
      "\n",
      "Iteration 860, Loss: 37.19795608520508, L1: 10.309258460998535, L3: 26.888696670532227\n",
      "Current prediction:  62.304988861083984 \n",
      "\n",
      "Iteration 861, Loss: 37.215797424316406, L1: 10.215164184570312, L3: 27.00063133239746\n",
      "Current prediction:  62.30305480957031 \n",
      "\n",
      "Iteration 862, Loss: 36.28822708129883, L1: 10.23078727722168, L3: 26.05743980407715\n",
      "Current prediction:  62.30108642578125 \n",
      "\n",
      "Iteration 863, Loss: 37.68643569946289, L1: 10.235072135925293, L3: 27.45136260986328\n",
      "Current prediction:  62.299102783203125 \n",
      "\n",
      "Iteration 864, Loss: 36.72100067138672, L1: 10.282297134399414, L3: 26.438705444335938\n",
      "Current prediction:  62.296897888183594 \n",
      "\n",
      "Iteration 865, Loss: 37.56953430175781, L1: 10.30962085723877, L3: 27.259912490844727\n",
      "Current prediction:  62.29450225830078 \n",
      "\n",
      "Iteration 866, Loss: 37.13518524169922, L1: 10.332501411437988, L3: 26.802684783935547\n",
      "Current prediction:  62.291629791259766 \n",
      "\n",
      "Iteration 867, Loss: 36.48182678222656, L1: 10.412684440612793, L3: 26.069143295288086\n",
      "Current prediction:  62.288326263427734 \n",
      "\n",
      "Iteration 868, Loss: 37.19585418701172, L1: 10.303104400634766, L3: 26.892749786376953\n",
      "Current prediction:  62.285701751708984 \n",
      "\n",
      "Iteration 869, Loss: 37.199989318847656, L1: 10.2765531539917, L3: 26.923437118530273\n",
      "Current prediction:  62.28302001953125 \n",
      "\n",
      "Iteration 870, Loss: 37.45857620239258, L1: 10.220337867736816, L3: 27.238237380981445\n",
      "Current prediction:  62.2802848815918 \n",
      "\n",
      "Iteration 871, Loss: 37.47084045410156, L1: 10.327497482299805, L3: 27.143342971801758\n",
      "Current prediction:  62.277774810791016 \n",
      "\n",
      "Iteration 872, Loss: 37.34611511230469, L1: 10.264957427978516, L3: 27.08115577697754\n",
      "Current prediction:  62.275142669677734 \n",
      "\n",
      "Iteration 873, Loss: 36.17426300048828, L1: 10.355751991271973, L3: 25.818511962890625\n",
      "Current prediction:  62.272525787353516 \n",
      "\n",
      "Iteration 874, Loss: 37.30004119873047, L1: 10.328779220581055, L3: 26.97126007080078\n",
      "Current prediction:  62.26994323730469 \n",
      "\n",
      "Iteration 875, Loss: 36.12962341308594, L1: 10.364046096801758, L3: 25.76557731628418\n",
      "Current prediction:  62.2657470703125 \n",
      "\n",
      "Iteration 876, Loss: 37.321311950683594, L1: 10.35206413269043, L3: 26.969247817993164\n",
      "Current prediction:  62.25170135498047 \n",
      "\n",
      "Iteration 877, Loss: 36.218955993652344, L1: 10.385018348693848, L3: 25.83393669128418\n",
      "Current prediction:  62.22606658935547 \n",
      "\n",
      "Iteration 878, Loss: 37.82000732421875, L1: 10.374889373779297, L3: 27.44511604309082\n",
      "Current prediction:  62.19843673706055 \n",
      "\n",
      "Iteration 879, Loss: 37.28032302856445, L1: 10.365883827209473, L3: 26.914438247680664\n",
      "Current prediction:  62.16716003417969 \n",
      "\n",
      "Iteration 880, Loss: 36.85554504394531, L1: 10.402533531188965, L3: 26.453012466430664\n",
      "Current prediction:  62.14188766479492 \n",
      "\n",
      "Iteration 881, Loss: 37.42123031616211, L1: 10.371780395507812, L3: 27.049449920654297\n",
      "Current prediction:  62.16709899902344 \n",
      "\n",
      "Iteration 882, Loss: 36.17865753173828, L1: 10.361719131469727, L3: 25.816938400268555\n",
      "Current prediction:  62.19524002075195 \n",
      "\n",
      "Iteration 883, Loss: 37.06915283203125, L1: 10.3925199508667, L3: 26.676633834838867\n",
      "Current prediction:  62.224205017089844 \n",
      "\n",
      "Iteration 884, Loss: 37.5855827331543, L1: 10.39248275756836, L3: 27.193099975585938\n",
      "Current prediction:  62.24571990966797 \n",
      "\n",
      "Iteration 885, Loss: 37.81000518798828, L1: 10.364035606384277, L3: 27.445968627929688\n",
      "Current prediction:  62.26384735107422 \n",
      "\n",
      "Iteration 886, Loss: 37.50922393798828, L1: 10.347954750061035, L3: 27.16126823425293\n",
      "Current prediction:  62.27955627441406 \n",
      "\n",
      "Iteration 887, Loss: 35.7337646484375, L1: 10.342733383178711, L3: 25.391029357910156\n",
      "Current prediction:  62.291358947753906 \n",
      "\n",
      "Iteration 888, Loss: 36.97742462158203, L1: 10.320039749145508, L3: 26.657384872436523\n",
      "Current prediction:  62.299835205078125 \n",
      "\n",
      "Iteration 889, Loss: 38.13942337036133, L1: 10.321915626525879, L3: 27.817508697509766\n",
      "Current prediction:  62.306884765625 \n",
      "\n",
      "Iteration 890, Loss: 36.591796875, L1: 10.327224731445312, L3: 26.26457405090332\n",
      "Current prediction:  62.31234359741211 \n",
      "\n",
      "Iteration 891, Loss: 37.49835205078125, L1: 10.354161262512207, L3: 27.144189834594727\n",
      "Current prediction:  62.31731414794922 \n",
      "\n",
      "Iteration 892, Loss: 37.9677734375, L1: 10.310554504394531, L3: 27.65721893310547\n",
      "Current prediction:  62.3216667175293 \n",
      "\n",
      "Iteration 893, Loss: 37.92428207397461, L1: 10.337156295776367, L3: 27.587125778198242\n",
      "Current prediction:  62.325557708740234 \n",
      "\n",
      "Iteration 894, Loss: 37.577735900878906, L1: 10.34047794342041, L3: 27.237258911132812\n",
      "Current prediction:  62.3290901184082 \n",
      "\n",
      "Iteration 895, Loss: 37.146636962890625, L1: 10.299444198608398, L3: 26.84719467163086\n",
      "Current prediction:  62.332763671875 \n",
      "\n",
      "Iteration 896, Loss: 36.62590789794922, L1: 10.325900077819824, L3: 26.300006866455078\n",
      "Current prediction:  62.336334228515625 \n",
      "\n",
      "Iteration 897, Loss: 36.68132400512695, L1: 10.269383430480957, L3: 26.411941528320312\n",
      "Current prediction:  62.339744567871094 \n",
      "\n",
      "Iteration 898, Loss: 37.386051177978516, L1: 10.179587364196777, L3: 27.206464767456055\n",
      "Current prediction:  62.342350006103516 \n",
      "\n",
      "Iteration 899, Loss: 36.696136474609375, L1: 10.249284744262695, L3: 26.446849822998047\n",
      "Current prediction:  62.3443603515625 \n",
      "\n",
      "Iteration 900, Loss: 37.449501037597656, L1: 10.259921073913574, L3: 27.189579010009766\n",
      "Current prediction:  62.34660720825195 \n",
      "\n",
      "Iteration 901, Loss: 38.116756439208984, L1: 10.21951675415039, L3: 27.897239685058594\n",
      "Current prediction:  62.34855651855469 \n",
      "\n",
      "Iteration 902, Loss: 36.999961853027344, L1: 10.328116416931152, L3: 26.671846389770508\n",
      "Current prediction:  62.3504638671875 \n",
      "\n",
      "Iteration 903, Loss: 36.6674690246582, L1: 10.214280128479004, L3: 26.453189849853516\n",
      "Current prediction:  62.35197067260742 \n",
      "\n",
      "Iteration 904, Loss: 37.09443283081055, L1: 10.200318336486816, L3: 26.894115447998047\n",
      "Current prediction:  62.3531494140625 \n",
      "\n",
      "Iteration 905, Loss: 36.21417999267578, L1: 10.188379287719727, L3: 26.025802612304688\n",
      "Current prediction:  62.354026794433594 \n",
      "\n",
      "Iteration 906, Loss: 37.169395446777344, L1: 10.226408004760742, L3: 26.9429874420166\n",
      "Current prediction:  62.3549690246582 \n",
      "\n",
      "Iteration 907, Loss: 37.3731575012207, L1: 10.298666000366211, L3: 27.074491500854492\n",
      "Current prediction:  62.3558464050293 \n",
      "\n",
      "Iteration 908, Loss: 37.10712814331055, L1: 10.26807689666748, L3: 26.83905029296875\n",
      "Current prediction:  62.35673904418945 \n",
      "\n",
      "Iteration 909, Loss: 36.94244384765625, L1: 10.275440216064453, L3: 26.66700553894043\n",
      "Current prediction:  62.35747146606445 \n",
      "\n",
      "Iteration 910, Loss: 37.43231964111328, L1: 10.846508026123047, L3: 26.5858097076416\n",
      "Current prediction:  62.35830307006836 \n",
      "\n",
      "Iteration 911, Loss: 38.55255889892578, L1: 10.276778221130371, L3: 28.275781631469727\n",
      "Current prediction:  62.358238220214844 \n",
      "\n",
      "Iteration 912, Loss: 37.74774932861328, L1: 10.2616548538208, L3: 27.486093521118164\n",
      "Current prediction:  62.35749435424805 \n",
      "\n",
      "Iteration 913, Loss: 37.900753021240234, L1: 10.16594409942627, L3: 27.73480987548828\n",
      "Current prediction:  62.35589599609375 \n",
      "\n",
      "Iteration 914, Loss: 36.54587173461914, L1: 10.234911918640137, L3: 26.310958862304688\n",
      "Current prediction:  62.354209899902344 \n",
      "\n",
      "Iteration 915, Loss: 37.4135856628418, L1: 10.272356986999512, L3: 27.14122772216797\n",
      "Current prediction:  62.35176086425781 \n",
      "\n",
      "Iteration 916, Loss: 38.04132080078125, L1: 10.397544860839844, L3: 27.643775939941406\n",
      "Current prediction:  62.348968505859375 \n",
      "\n",
      "Iteration 917, Loss: 37.090476989746094, L1: 10.346281051635742, L3: 26.74419403076172\n",
      "Current prediction:  62.34604263305664 \n",
      "\n",
      "Iteration 918, Loss: 36.130645751953125, L1: 10.305846214294434, L3: 25.824800491333008\n",
      "Current prediction:  62.34344482421875 \n",
      "\n",
      "Iteration 919, Loss: 36.64228057861328, L1: 10.303987503051758, L3: 26.338293075561523\n",
      "Current prediction:  62.34040451049805 \n",
      "\n",
      "Iteration 920, Loss: 36.35626220703125, L1: 10.270851135253906, L3: 26.085412979125977\n",
      "Current prediction:  62.337852478027344 \n",
      "\n",
      "Iteration 921, Loss: 38.212947845458984, L1: 10.38575267791748, L3: 27.827194213867188\n",
      "Current prediction:  62.12701416015625 \n",
      "\n",
      "Iteration 922, Loss: 36.117820739746094, L1: 10.463556289672852, L3: 25.65426254272461\n",
      "Current prediction:  61.75968933105469 \n",
      "\n",
      "Iteration 923, Loss: 37.42884063720703, L1: 10.588144302368164, L3: 26.840696334838867\n",
      "Current prediction:  61.75876235961914 \n",
      "\n",
      "Iteration 924, Loss: 38.36882019042969, L1: 10.62075138092041, L3: 27.74806785583496\n",
      "Current prediction:  61.75890350341797 \n",
      "\n",
      "Iteration 925, Loss: 38.48472213745117, L1: 10.64787483215332, L3: 27.83684730529785\n",
      "Current prediction:  61.75920486450195 \n",
      "\n",
      "Iteration 926, Loss: 37.43846130371094, L1: 10.690445899963379, L3: 26.748016357421875\n",
      "Current prediction:  61.759918212890625 \n",
      "\n",
      "Iteration 927, Loss: 36.39289474487305, L1: 10.601801872253418, L3: 25.791093826293945\n",
      "Current prediction:  61.7610969543457 \n",
      "\n",
      "Iteration 928, Loss: 37.12232208251953, L1: 10.731475830078125, L3: 26.390846252441406\n",
      "Current prediction:  61.76295471191406 \n",
      "\n",
      "Iteration 929, Loss: 37.17711639404297, L1: 10.682844161987305, L3: 26.494272232055664\n",
      "Current prediction:  61.765811920166016 \n",
      "\n",
      "Iteration 930, Loss: 36.527549743652344, L1: 10.870983123779297, L3: 25.65656852722168\n",
      "Current prediction:  61.76906967163086 \n",
      "\n",
      "Iteration 931, Loss: 37.80815887451172, L1: 10.752711296081543, L3: 27.05544662475586\n",
      "Current prediction:  61.77292251586914 \n",
      "\n",
      "Iteration 932, Loss: 37.64061737060547, L1: 10.70905590057373, L3: 26.931562423706055\n",
      "Current prediction:  61.77727127075195 \n",
      "\n",
      "Iteration 933, Loss: 37.19212341308594, L1: 10.895198822021484, L3: 26.296924591064453\n",
      "Current prediction:  61.781864166259766 \n",
      "\n",
      "Iteration 934, Loss: 38.28237533569336, L1: 10.725838661193848, L3: 27.556537628173828\n",
      "Current prediction:  61.786468505859375 \n",
      "\n",
      "Iteration 935, Loss: 36.86540985107422, L1: 10.785490036010742, L3: 26.079917907714844\n",
      "Current prediction:  61.79100799560547 \n",
      "\n",
      "Iteration 936, Loss: 36.299800872802734, L1: 11.021941184997559, L3: 25.27785873413086\n",
      "Current prediction:  61.795867919921875 \n",
      "\n",
      "Iteration 937, Loss: 38.09275817871094, L1: 10.92378044128418, L3: 27.168977737426758\n",
      "Current prediction:  61.800777435302734 \n",
      "\n",
      "Iteration 938, Loss: 37.2218017578125, L1: 11.00007152557373, L3: 26.221731185913086\n",
      "Current prediction:  61.8062744140625 \n",
      "\n",
      "Iteration 939, Loss: 37.20018005371094, L1: 10.719244003295898, L3: 26.480937957763672\n",
      "Current prediction:  61.81196212768555 \n",
      "\n",
      "Iteration 940, Loss: 37.8190803527832, L1: 10.873208999633789, L3: 26.945871353149414\n",
      "Current prediction:  61.81760787963867 \n",
      "\n",
      "Iteration 941, Loss: 37.08747100830078, L1: 10.564010620117188, L3: 26.52345848083496\n",
      "Current prediction:  61.822975158691406 \n",
      "\n",
      "Iteration 942, Loss: 37.937042236328125, L1: 10.701339721679688, L3: 27.235700607299805\n",
      "Current prediction:  61.82851791381836 \n",
      "\n",
      "Iteration 943, Loss: 36.67133331298828, L1: 10.855842590332031, L3: 25.81549072265625\n",
      "Current prediction:  61.824859619140625 \n",
      "\n",
      "Iteration 944, Loss: 37.05739974975586, L1: 10.905369758605957, L3: 26.15203094482422\n",
      "Current prediction:  61.30245590209961 \n",
      "\n",
      "Iteration 945, Loss: 37.33646011352539, L1: 10.922259330749512, L3: 26.414199829101562\n",
      "Current prediction:  61.30311584472656 \n",
      "\n",
      "Iteration 946, Loss: 38.26526641845703, L1: 10.95116901397705, L3: 27.314096450805664\n",
      "Current prediction:  61.310489654541016 \n",
      "\n",
      "Iteration 947, Loss: 38.103309631347656, L1: 10.866809844970703, L3: 27.236499786376953\n",
      "Current prediction:  61.31813430786133 \n",
      "\n",
      "Iteration 948, Loss: 37.59356689453125, L1: 10.89942455291748, L3: 26.694141387939453\n",
      "Current prediction:  61.32653045654297 \n",
      "\n",
      "Iteration 949, Loss: 38.61595916748047, L1: 10.885353088378906, L3: 27.730606079101562\n",
      "Current prediction:  61.33441925048828 \n",
      "\n",
      "Iteration 950, Loss: 37.21745300292969, L1: 10.755146980285645, L3: 26.46230697631836\n",
      "Current prediction:  61.34233093261719 \n",
      "\n",
      "Iteration 951, Loss: 37.64408493041992, L1: 10.661408424377441, L3: 26.982675552368164\n",
      "Current prediction:  61.82601547241211 \n",
      "\n",
      "Iteration 952, Loss: 36.35429382324219, L1: 10.695556640625, L3: 25.658735275268555\n",
      "Current prediction:  61.93062973022461 \n",
      "\n",
      "Iteration 953, Loss: 36.75459289550781, L1: 10.559589385986328, L3: 26.195003509521484\n",
      "Current prediction:  61.9376335144043 \n",
      "\n",
      "Iteration 954, Loss: 37.32263946533203, L1: 10.463701248168945, L3: 26.858938217163086\n",
      "Current prediction:  61.944183349609375 \n",
      "\n",
      "Iteration 955, Loss: 37.40394592285156, L1: 10.44832992553711, L3: 26.955617904663086\n",
      "Current prediction:  61.95008850097656 \n",
      "\n",
      "Iteration 956, Loss: 37.876502990722656, L1: 10.367332458496094, L3: 27.509172439575195\n",
      "Current prediction:  61.955387115478516 \n",
      "\n",
      "Iteration 957, Loss: 36.738067626953125, L1: 10.332056045532227, L3: 26.4060115814209\n",
      "Current prediction:  61.959983825683594 \n",
      "\n",
      "Iteration 958, Loss: 38.72845458984375, L1: 10.37647819519043, L3: 28.351974487304688\n",
      "Current prediction:  61.963844299316406 \n",
      "\n",
      "Iteration 959, Loss: 36.12730026245117, L1: 10.453485488891602, L3: 25.67381477355957\n",
      "Current prediction:  61.96738815307617 \n",
      "\n",
      "Iteration 960, Loss: 36.73765563964844, L1: 10.496119499206543, L3: 26.241535186767578\n",
      "Current prediction:  61.96989440917969 \n",
      "\n",
      "Iteration 961, Loss: 38.332923889160156, L1: 10.445732116699219, L3: 27.887189865112305\n",
      "Current prediction:  61.97160339355469 \n",
      "\n",
      "Iteration 962, Loss: 37.356605529785156, L1: 10.44382381439209, L3: 26.91278076171875\n",
      "Current prediction:  61.97249221801758 \n",
      "\n",
      "Iteration 963, Loss: 37.732242584228516, L1: 10.462784767150879, L3: 27.269458770751953\n",
      "Current prediction:  61.97242736816406 \n",
      "\n",
      "Iteration 964, Loss: 36.975990295410156, L1: 10.473581314086914, L3: 26.50240707397461\n",
      "Current prediction:  61.97237777709961 \n",
      "\n",
      "Iteration 965, Loss: 37.06653594970703, L1: 10.47305679321289, L3: 26.593481063842773\n",
      "Current prediction:  61.97177505493164 \n",
      "\n",
      "Iteration 966, Loss: 37.71465301513672, L1: 10.496496200561523, L3: 27.218158721923828\n",
      "Current prediction:  61.970848083496094 \n",
      "\n",
      "Iteration 967, Loss: 36.46630859375, L1: 10.392516136169434, L3: 26.07379150390625\n",
      "Current prediction:  61.970115661621094 \n",
      "\n",
      "Iteration 968, Loss: 37.82730484008789, L1: 10.253666877746582, L3: 27.573637008666992\n",
      "Current prediction:  61.9687385559082 \n",
      "\n",
      "Iteration 969, Loss: 39.09757995605469, L1: 10.441097259521484, L3: 28.65648078918457\n",
      "Current prediction:  61.96660614013672 \n",
      "\n",
      "Iteration 970, Loss: 36.68152618408203, L1: 10.399824142456055, L3: 26.281700134277344\n",
      "Current prediction:  61.983604431152344 \n",
      "\n",
      "Iteration 971, Loss: 36.87858963012695, L1: 10.105606079101562, L3: 26.77298355102539\n",
      "Current prediction:  62.157676696777344 \n",
      "\n",
      "Iteration 972, Loss: 36.7623291015625, L1: 10.109201431274414, L3: 26.653125762939453\n",
      "Current prediction:  62.15479278564453 \n",
      "\n",
      "Iteration 973, Loss: 35.88034439086914, L1: 10.05373764038086, L3: 25.82660675048828\n",
      "Current prediction:  62.15290832519531 \n",
      "\n",
      "Iteration 974, Loss: 37.02040481567383, L1: 10.001627922058105, L3: 27.018775939941406\n",
      "Current prediction:  62.156002044677734 \n",
      "\n",
      "Iteration 975, Loss: 36.52145767211914, L1: 10.03622055053711, L3: 26.48523712158203\n",
      "Current prediction:  62.17975997924805 \n",
      "\n",
      "Iteration 976, Loss: 36.936012268066406, L1: 10.039264678955078, L3: 26.896747589111328\n",
      "Current prediction:  62.26594161987305 \n",
      "\n",
      "Iteration 977, Loss: 37.310462951660156, L1: 10.220735549926758, L3: 27.0897274017334\n",
      "Current prediction:  62.412086486816406 \n",
      "\n",
      "Iteration 978, Loss: 37.020912170410156, L1: 10.093475341796875, L3: 26.92743682861328\n",
      "Current prediction:  62.554195404052734 \n",
      "\n",
      "Iteration 979, Loss: 38.09911346435547, L1: 10.098777770996094, L3: 28.000337600708008\n",
      "Current prediction:  62.623775482177734 \n",
      "\n",
      "Iteration 980, Loss: 37.42536163330078, L1: 10.009220123291016, L3: 27.4161434173584\n",
      "Current prediction:  62.643898010253906 \n",
      "\n",
      "Iteration 981, Loss: 38.001075744628906, L1: 10.042495727539062, L3: 27.95857810974121\n",
      "Current prediction:  62.64518356323242 \n",
      "\n",
      "Iteration 982, Loss: 36.764217376708984, L1: 10.033792495727539, L3: 26.730424880981445\n",
      "Current prediction:  62.64018249511719 \n",
      "\n",
      "Iteration 983, Loss: 37.64836883544922, L1: 10.060302734375, L3: 27.58806800842285\n",
      "Current prediction:  62.629981994628906 \n",
      "\n",
      "Iteration 984, Loss: 37.974609375, L1: 10.189860343933105, L3: 27.78474998474121\n",
      "Current prediction:  62.61839294433594 \n",
      "\n",
      "Iteration 985, Loss: 37.676841735839844, L1: 10.06157398223877, L3: 27.61526870727539\n",
      "Current prediction:  62.60511779785156 \n",
      "\n",
      "Iteration 986, Loss: 36.38823699951172, L1: 10.105008125305176, L3: 26.283227920532227\n",
      "Current prediction:  62.59328079223633 \n",
      "\n",
      "Iteration 987, Loss: 37.2949104309082, L1: 10.134592056274414, L3: 27.16031837463379\n",
      "Current prediction:  62.58506774902344 \n",
      "\n",
      "Iteration 988, Loss: 36.91301727294922, L1: 10.085395812988281, L3: 26.827621459960938\n",
      "Current prediction:  62.57793045043945 \n",
      "\n",
      "Iteration 989, Loss: 36.74213409423828, L1: 10.119043350219727, L3: 26.623090744018555\n",
      "Current prediction:  62.5770149230957 \n",
      "\n",
      "Iteration 990, Loss: 36.68788146972656, L1: 10.115175247192383, L3: 26.57270622253418\n",
      "Current prediction:  62.58208084106445 \n",
      "\n",
      "Iteration 991, Loss: 37.09187698364258, L1: 10.106318473815918, L3: 26.985559463500977\n",
      "Current prediction:  62.58863067626953 \n",
      "\n",
      "Iteration 992, Loss: 36.97080993652344, L1: 10.123420715332031, L3: 26.84739112854004\n",
      "Current prediction:  62.5938835144043 \n",
      "\n",
      "Iteration 993, Loss: 37.17630386352539, L1: 10.075284004211426, L3: 27.10101890563965\n",
      "Current prediction:  62.594139099121094 \n",
      "\n",
      "Iteration 994, Loss: 37.83675765991211, L1: 10.069087982177734, L3: 27.767669677734375\n",
      "Current prediction:  62.592342376708984 \n",
      "\n",
      "Iteration 995, Loss: 36.61077117919922, L1: 10.019874572753906, L3: 26.590896606445312\n",
      "Current prediction:  62.589683532714844 \n",
      "\n",
      "Iteration 996, Loss: 37.418365478515625, L1: 10.050371170043945, L3: 27.367996215820312\n",
      "Current prediction:  62.58632278442383 \n",
      "\n",
      "Iteration 997, Loss: 37.711395263671875, L1: 10.05925178527832, L3: 27.652141571044922\n",
      "Current prediction:  62.582637786865234 \n",
      "\n",
      "Iteration 998, Loss: 37.88047409057617, L1: 10.187076568603516, L3: 27.693397521972656\n",
      "Current prediction:  62.57865524291992 \n",
      "\n",
      "â†³ LR reduced to 1.0e-03 at iteration 1000 \n",
      "\n",
      "Iteration 999, Loss: 37.91288375854492, L1: 10.054646492004395, L3: 27.858238220214844\n",
      "Current prediction:  62.5741081237793 \n",
      "\n",
      "Iteration 1000, Loss: 37.439903259277344, L1: 10.010564804077148, L3: 27.429340362548828\n",
      "Current prediction:  62.569549560546875 \n",
      "\n",
      "Iteration 1001, Loss: 36.83643341064453, L1: 10.117125511169434, L3: 26.719308853149414\n",
      "Current prediction:  62.56533432006836 \n",
      "\n",
      "Iteration 1002, Loss: 37.63854217529297, L1: 10.120622634887695, L3: 27.517919540405273\n",
      "Current prediction:  62.56080627441406 \n",
      "\n",
      "Iteration 1003, Loss: 36.601959228515625, L1: 10.06174087524414, L3: 26.54021644592285\n",
      "Current prediction:  62.55642318725586 \n",
      "\n",
      "Iteration 1004, Loss: 37.12904357910156, L1: 10.07491683959961, L3: 27.054128646850586\n",
      "Current prediction:  62.5517578125 \n",
      "\n",
      "Iteration 1005, Loss: 37.86216354370117, L1: 10.186902046203613, L3: 27.675262451171875\n",
      "Current prediction:  62.54690170288086 \n",
      "\n",
      "Iteration 1006, Loss: 37.01567459106445, L1: 10.122480392456055, L3: 26.8931941986084\n",
      "Current prediction:  62.542137145996094 \n",
      "\n",
      "Iteration 1007, Loss: 36.97809982299805, L1: 10.09961986541748, L3: 26.878480911254883\n",
      "Current prediction:  62.53782653808594 \n",
      "\n",
      "Iteration 1008, Loss: 37.52375411987305, L1: 10.074767112731934, L3: 27.44898796081543\n",
      "Current prediction:  62.53305435180664 \n",
      "\n",
      "Iteration 1009, Loss: 37.407535552978516, L1: 10.077936172485352, L3: 27.329599380493164\n",
      "Current prediction:  62.52796173095703 \n",
      "\n",
      "Iteration 1010, Loss: 36.953941345214844, L1: 10.119827270507812, L3: 26.8341121673584\n",
      "Current prediction:  62.52341842651367 \n",
      "\n",
      "Iteration 1011, Loss: 37.71281433105469, L1: 10.137693405151367, L3: 27.57512092590332\n",
      "Current prediction:  62.518924713134766 \n",
      "\n",
      "Iteration 1012, Loss: 38.18556213378906, L1: 10.120685577392578, L3: 28.06487464904785\n",
      "Current prediction:  62.514278411865234 \n",
      "\n",
      "Iteration 1013, Loss: 35.5040283203125, L1: 10.11097240447998, L3: 25.393056869506836\n",
      "Current prediction:  62.51017761230469 \n",
      "\n",
      "Iteration 1014, Loss: 37.20148849487305, L1: 10.048432350158691, L3: 27.153057098388672\n",
      "Current prediction:  62.50642395019531 \n",
      "\n",
      "Iteration 1015, Loss: 36.27411651611328, L1: 10.147911071777344, L3: 26.126205444335938\n",
      "Current prediction:  62.50297927856445 \n",
      "\n",
      "Iteration 1016, Loss: 37.84234619140625, L1: 10.054015159606934, L3: 27.788331985473633\n",
      "Current prediction:  62.49943161010742 \n",
      "\n",
      "Iteration 1017, Loss: 36.692237854003906, L1: 10.080322265625, L3: 26.611915588378906\n",
      "Current prediction:  62.495643615722656 \n",
      "\n",
      "Iteration 1018, Loss: 35.794010162353516, L1: 10.10583782196045, L3: 25.688173294067383\n",
      "Current prediction:  62.49283981323242 \n",
      "\n",
      "Iteration 1019, Loss: 36.929893493652344, L1: 10.108884811401367, L3: 26.821008682250977\n",
      "Current prediction:  62.49029541015625 \n",
      "\n",
      "Iteration 1020, Loss: 37.123008728027344, L1: 10.100118637084961, L3: 27.02288818359375\n",
      "Current prediction:  62.48789978027344 \n",
      "\n",
      "Iteration 1021, Loss: 36.6417121887207, L1: 10.174098014831543, L3: 26.467613220214844\n",
      "Current prediction:  62.485382080078125 \n",
      "\n",
      "Iteration 1022, Loss: 38.17058563232422, L1: 10.138021469116211, L3: 28.032562255859375\n",
      "Current prediction:  62.48240280151367 \n",
      "\n",
      "Iteration 1023, Loss: 36.625152587890625, L1: 10.14782428741455, L3: 26.477327346801758\n",
      "Current prediction:  62.47969436645508 \n",
      "\n",
      "Iteration 1024, Loss: 37.20755386352539, L1: 10.127456665039062, L3: 27.080097198486328\n",
      "Current prediction:  62.476654052734375 \n",
      "\n",
      "Iteration 1025, Loss: 37.09357452392578, L1: 10.230380058288574, L3: 26.86319351196289\n",
      "Current prediction:  62.471656799316406 \n",
      "\n",
      "Iteration 1026, Loss: 37.111446380615234, L1: 10.237772941589355, L3: 26.873672485351562\n",
      "Current prediction:  62.46559143066406 \n",
      "\n",
      "Iteration 1027, Loss: 38.047340393066406, L1: 10.168463706970215, L3: 27.878877639770508\n",
      "Current prediction:  62.45661926269531 \n",
      "\n",
      "Iteration 1028, Loss: 37.162193298339844, L1: 10.319592475891113, L3: 26.842599868774414\n",
      "Current prediction:  62.44557189941406 \n",
      "\n",
      "Iteration 1029, Loss: 37.86240768432617, L1: 10.217641830444336, L3: 27.644765853881836\n",
      "Current prediction:  62.42014694213867 \n",
      "\n",
      "Iteration 1030, Loss: 36.961875915527344, L1: 10.214495658874512, L3: 26.74738121032715\n",
      "Current prediction:  62.39455795288086 \n",
      "\n",
      "Iteration 1031, Loss: 37.95970153808594, L1: 10.18088150024414, L3: 27.77882194519043\n",
      "Current prediction:  62.35066604614258 \n",
      "\n",
      "Iteration 1032, Loss: 37.47199249267578, L1: 10.228193283081055, L3: 27.243797302246094\n",
      "Current prediction:  62.31773376464844 \n",
      "\n",
      "Iteration 1033, Loss: 36.7620735168457, L1: 10.248910903930664, L3: 26.51316261291504\n",
      "Current prediction:  62.28639221191406 \n",
      "\n",
      "Iteration 1034, Loss: 35.70989227294922, L1: 10.195240020751953, L3: 25.514652252197266\n",
      "Current prediction:  62.31296920776367 \n",
      "\n",
      "Iteration 1035, Loss: 36.89535903930664, L1: 10.215920448303223, L3: 26.6794376373291\n",
      "Current prediction:  62.27632141113281 \n",
      "\n",
      "Iteration 1036, Loss: 36.03082275390625, L1: 10.229044914245605, L3: 25.801776885986328\n",
      "Current prediction:  62.251197814941406 \n",
      "\n",
      "Iteration 1037, Loss: 37.76152038574219, L1: 10.274971961975098, L3: 27.486547470092773\n",
      "Current prediction:  62.242156982421875 \n",
      "\n",
      "Iteration 1038, Loss: 37.9559326171875, L1: 10.351612091064453, L3: 27.604318618774414\n",
      "Current prediction:  62.26687240600586 \n",
      "\n",
      "Iteration 1039, Loss: 37.41551971435547, L1: 10.28264045715332, L3: 27.13288116455078\n",
      "Current prediction:  62.31233215332031 \n",
      "\n",
      "Iteration 1040, Loss: 36.52640914916992, L1: 10.22045612335205, L3: 26.305953979492188\n",
      "Current prediction:  62.38750076293945 \n",
      "\n",
      "Iteration 1041, Loss: 36.95525360107422, L1: 10.291743278503418, L3: 26.663509368896484\n",
      "Current prediction:  62.41879653930664 \n",
      "\n",
      "Iteration 1042, Loss: 36.73103713989258, L1: 10.253994941711426, L3: 26.47704315185547\n",
      "Current prediction:  62.43010711669922 \n",
      "\n",
      "Iteration 1043, Loss: 36.2722282409668, L1: 10.155868530273438, L3: 26.11635971069336\n",
      "Current prediction:  62.435123443603516 \n",
      "\n",
      "Iteration 1044, Loss: 38.66937255859375, L1: 10.271158218383789, L3: 28.398216247558594\n",
      "Current prediction:  62.43691635131836 \n",
      "\n",
      "Iteration 1045, Loss: 36.473079681396484, L1: 10.232532501220703, L3: 26.24054718017578\n",
      "Current prediction:  62.43622589111328 \n",
      "\n",
      "Iteration 1046, Loss: 37.201515197753906, L1: 10.214547157287598, L3: 26.986968994140625\n",
      "Current prediction:  62.43618392944336 \n",
      "\n",
      "Iteration 1047, Loss: 37.76551055908203, L1: 10.248685836791992, L3: 27.51682472229004\n",
      "Current prediction:  62.43674087524414 \n",
      "\n",
      "Iteration 1048, Loss: 37.167972564697266, L1: 10.184967041015625, L3: 26.98300552368164\n",
      "Current prediction:  62.43796920776367 \n",
      "\n",
      "Iteration 1049, Loss: 37.26506423950195, L1: 10.183115005493164, L3: 27.08194923400879\n",
      "Current prediction:  62.43906021118164 \n",
      "\n",
      "Iteration 1050, Loss: 36.58198928833008, L1: 10.215194702148438, L3: 26.36679458618164\n",
      "Current prediction:  62.43963623046875 \n",
      "\n",
      "Iteration 1051, Loss: 37.49068832397461, L1: 10.174513816833496, L3: 27.31617546081543\n",
      "Current prediction:  62.440040588378906 \n",
      "\n",
      "Iteration 1052, Loss: 35.89031219482422, L1: 10.187355041503906, L3: 25.702957153320312\n",
      "Current prediction:  62.44087219238281 \n",
      "\n",
      "Iteration 1053, Loss: 37.0706787109375, L1: 10.203652381896973, L3: 26.86702537536621\n",
      "Current prediction:  62.44188690185547 \n",
      "\n",
      "Iteration 1054, Loss: 37.65409851074219, L1: 10.184815406799316, L3: 27.469282150268555\n",
      "Current prediction:  62.443504333496094 \n",
      "\n",
      "Iteration 1055, Loss: 36.06857681274414, L1: 10.162606239318848, L3: 25.905969619750977\n",
      "Current prediction:  62.44548797607422 \n",
      "\n",
      "Iteration 1056, Loss: 36.386077880859375, L1: 10.126519203186035, L3: 26.259559631347656\n",
      "Current prediction:  62.44711685180664 \n",
      "\n",
      "Iteration 1057, Loss: 37.40359878540039, L1: 10.177597999572754, L3: 27.22599983215332\n",
      "Current prediction:  62.44915008544922 \n",
      "\n",
      "Iteration 1058, Loss: 37.65847396850586, L1: 10.352377891540527, L3: 27.30609703063965\n",
      "Current prediction:  62.45121765136719 \n",
      "\n",
      "Iteration 1059, Loss: 36.3798828125, L1: 10.124970436096191, L3: 26.254913330078125\n",
      "Current prediction:  62.45337677001953 \n",
      "\n",
      "Iteration 1060, Loss: 37.823158264160156, L1: 10.114876747131348, L3: 27.708280563354492\n",
      "Current prediction:  62.45506286621094 \n",
      "\n",
      "Iteration 1061, Loss: 37.234466552734375, L1: 10.139451026916504, L3: 27.095016479492188\n",
      "Current prediction:  62.45659255981445 \n",
      "\n",
      "Iteration 1062, Loss: 37.673744201660156, L1: 10.154167175292969, L3: 27.51957893371582\n",
      "Current prediction:  62.45771026611328 \n",
      "\n",
      "Iteration 1063, Loss: 37.4007568359375, L1: 10.266487121582031, L3: 27.134267807006836\n",
      "Current prediction:  62.45787811279297 \n",
      "\n",
      "Iteration 1064, Loss: 37.88199996948242, L1: 10.222378730773926, L3: 27.659622192382812\n",
      "Current prediction:  62.45676803588867 \n",
      "\n",
      "Iteration 1065, Loss: 36.14029312133789, L1: 10.336541175842285, L3: 25.80375099182129\n",
      "Current prediction:  62.45451736450195 \n",
      "\n",
      "Iteration 1066, Loss: 37.191646575927734, L1: 10.223536491394043, L3: 26.968109130859375\n",
      "Current prediction:  62.45480728149414 \n",
      "\n",
      "Iteration 1067, Loss: 36.01642990112305, L1: 10.282008171081543, L3: 25.734420776367188\n",
      "Current prediction:  62.45584487915039 \n",
      "\n",
      "Iteration 1068, Loss: 36.59716796875, L1: 10.266681671142578, L3: 26.330488204956055\n",
      "Current prediction:  62.4599609375 \n",
      "\n",
      "Iteration 1069, Loss: 37.33527374267578, L1: 10.341841697692871, L3: 26.993431091308594\n",
      "Current prediction:  62.2371826171875 \n",
      "\n",
      "Iteration 1070, Loss: 37.51709747314453, L1: 10.379164695739746, L3: 27.13793182373047\n",
      "Current prediction:  61.90056610107422 \n",
      "\n",
      "Iteration 1071, Loss: 36.749752044677734, L1: 10.41207218170166, L3: 26.337678909301758\n",
      "Current prediction:  61.90740966796875 \n",
      "\n",
      "Iteration 1072, Loss: 37.22276306152344, L1: 10.448596000671387, L3: 26.774166107177734\n",
      "Current prediction:  61.914058685302734 \n",
      "\n",
      "Iteration 1073, Loss: 38.099185943603516, L1: 10.480607032775879, L3: 27.61857795715332\n",
      "Current prediction:  61.92121124267578 \n",
      "\n",
      "Iteration 1074, Loss: 37.11918640136719, L1: 10.498806953430176, L3: 26.620378494262695\n",
      "Current prediction:  61.9285774230957 \n",
      "\n",
      "Iteration 1075, Loss: 37.61405563354492, L1: 10.489068984985352, L3: 27.12498664855957\n",
      "Current prediction:  61.93666458129883 \n",
      "\n",
      "Iteration 1076, Loss: 37.180992126464844, L1: 10.588912963867188, L3: 26.59208106994629\n",
      "Current prediction:  61.94533920288086 \n",
      "\n",
      "Iteration 1077, Loss: 37.321624755859375, L1: 10.535155296325684, L3: 26.786468505859375\n",
      "Current prediction:  61.95443344116211 \n",
      "\n",
      "Iteration 1078, Loss: 37.29286193847656, L1: 10.511900901794434, L3: 26.780961990356445\n",
      "Current prediction:  61.96408462524414 \n",
      "\n",
      "Iteration 1079, Loss: 38.18394470214844, L1: 10.55120849609375, L3: 27.632734298706055\n",
      "Current prediction:  61.973995208740234 \n",
      "\n",
      "Iteration 1080, Loss: 36.118045806884766, L1: 10.457863807678223, L3: 25.66018295288086\n",
      "Current prediction:  61.98432922363281 \n",
      "\n",
      "Iteration 1081, Loss: 37.21717071533203, L1: 10.48410415649414, L3: 26.733068466186523\n",
      "Current prediction:  61.9942741394043 \n",
      "\n",
      "Iteration 1082, Loss: 37.9155158996582, L1: 10.515801429748535, L3: 27.399715423583984\n",
      "Current prediction:  62.00384521484375 \n",
      "\n",
      "Iteration 1083, Loss: 37.05955123901367, L1: 10.501500129699707, L3: 26.55805015563965\n",
      "Current prediction:  62.013492584228516 \n",
      "\n",
      "Iteration 1084, Loss: 37.13261795043945, L1: 10.439953804016113, L3: 26.692663192749023\n",
      "Current prediction:  62.02311325073242 \n",
      "\n",
      "Iteration 1085, Loss: 36.76446533203125, L1: 10.50710678100586, L3: 26.257356643676758\n",
      "Current prediction:  62.03260803222656 \n",
      "\n",
      "Iteration 1086, Loss: 37.95416259765625, L1: 10.449883460998535, L3: 27.5042781829834\n",
      "Current prediction:  62.04189682006836 \n",
      "\n",
      "Iteration 1087, Loss: 37.99951171875, L1: 10.565400123596191, L3: 27.434112548828125\n",
      "Current prediction:  62.0507698059082 \n",
      "\n",
      "Iteration 1088, Loss: 37.664955139160156, L1: 10.558976173400879, L3: 27.10597801208496\n",
      "Current prediction:  62.05921936035156 \n",
      "\n",
      "Iteration 1089, Loss: 37.38072204589844, L1: 10.534186363220215, L3: 26.84653663635254\n",
      "Current prediction:  62.06801986694336 \n",
      "\n",
      "Iteration 1090, Loss: 36.108604431152344, L1: 10.590836524963379, L3: 25.51776885986328\n",
      "Current prediction:  62.07695770263672 \n",
      "\n",
      "Iteration 1091, Loss: 38.151161193847656, L1: 10.542190551757812, L3: 27.60896873474121\n",
      "Current prediction:  62.08556365966797 \n",
      "\n",
      "Iteration 1092, Loss: 37.16387176513672, L1: 10.537307739257812, L3: 26.626562118530273\n",
      "Current prediction:  62.09272384643555 \n",
      "\n",
      "Iteration 1093, Loss: 38.866981506347656, L1: 10.52218246459961, L3: 28.34480094909668\n",
      "Current prediction:  62.09890365600586 \n",
      "\n",
      "Iteration 1094, Loss: 37.590049743652344, L1: 10.467500686645508, L3: 27.122549057006836\n",
      "Current prediction:  62.10468673706055 \n",
      "\n",
      "Iteration 1095, Loss: 38.104774475097656, L1: 10.54254150390625, L3: 27.56223487854004\n",
      "Current prediction:  62.109989166259766 \n",
      "\n",
      "Iteration 1096, Loss: 36.75069046020508, L1: 10.461784362792969, L3: 26.28890609741211\n",
      "Current prediction:  62.11600112915039 \n",
      "\n",
      "Iteration 1097, Loss: 38.172523498535156, L1: 10.575323104858398, L3: 27.59720230102539\n",
      "Current prediction:  62.12232971191406 \n",
      "\n",
      "Iteration 1098, Loss: 37.14091491699219, L1: 10.627604484558105, L3: 26.513309478759766\n",
      "Current prediction:  62.128868103027344 \n",
      "\n",
      "Iteration 1099, Loss: 37.704166412353516, L1: 10.49524211883545, L3: 27.208925247192383\n",
      "Current prediction:  62.124656677246094 \n",
      "\n",
      "Iteration 1100, Loss: 36.40135192871094, L1: 10.56725025177002, L3: 25.8341007232666\n",
      "Current prediction:  61.850006103515625 \n",
      "\n",
      "Iteration 1101, Loss: 36.97431182861328, L1: 10.493927001953125, L3: 26.480384826660156\n",
      "Current prediction:  61.71683883666992 \n",
      "\n",
      "Iteration 1102, Loss: 37.097293853759766, L1: 10.660101890563965, L3: 26.437191009521484\n",
      "Current prediction:  61.71672821044922 \n",
      "\n",
      "Iteration 1103, Loss: 38.765621185302734, L1: 10.610645294189453, L3: 28.15497589111328\n",
      "Current prediction:  61.725337982177734 \n",
      "\n",
      "Iteration 1104, Loss: 36.81018829345703, L1: 10.580227851867676, L3: 26.22995948791504\n",
      "Current prediction:  61.73577880859375 \n",
      "\n",
      "Iteration 1105, Loss: 37.485992431640625, L1: 10.413650512695312, L3: 27.072343826293945\n",
      "Current prediction:  61.7464485168457 \n",
      "\n",
      "Iteration 1106, Loss: 37.05387496948242, L1: 10.499873161315918, L3: 26.554000854492188\n",
      "Current prediction:  61.757347106933594 \n",
      "\n",
      "Iteration 1107, Loss: 36.626441955566406, L1: 10.476329803466797, L3: 26.150114059448242\n",
      "Current prediction:  61.768089294433594 \n",
      "\n",
      "Iteration 1108, Loss: 36.77843475341797, L1: 10.449319839477539, L3: 26.329113006591797\n",
      "Current prediction:  61.778656005859375 \n",
      "\n",
      "Iteration 1109, Loss: 37.64180374145508, L1: 10.454471588134766, L3: 27.187332153320312\n",
      "Current prediction:  61.789798736572266 \n",
      "\n",
      "Iteration 1110, Loss: 37.254058837890625, L1: 10.376325607299805, L3: 26.877735137939453\n",
      "Current prediction:  61.803733825683594 \n",
      "\n",
      "Iteration 1111, Loss: 36.717811584472656, L1: 10.372879028320312, L3: 26.344934463500977\n",
      "Current prediction:  61.835208892822266 \n",
      "\n",
      "Iteration 1112, Loss: 37.02378845214844, L1: 10.417545318603516, L3: 26.606243133544922\n",
      "Current prediction:  61.92730712890625 \n",
      "\n",
      "Iteration 1113, Loss: 36.88822937011719, L1: 10.378644943237305, L3: 26.509584426879883\n",
      "Current prediction:  62.08918380737305 \n",
      "\n",
      "Iteration 1114, Loss: 36.82988739013672, L1: 10.28425121307373, L3: 26.545635223388672\n",
      "Current prediction:  62.245216369628906 \n",
      "\n",
      "Iteration 1115, Loss: 38.26737976074219, L1: 10.245908737182617, L3: 28.021469116210938\n",
      "Current prediction:  62.28858947753906 \n",
      "\n",
      "Iteration 1116, Loss: 37.11763381958008, L1: 10.270367622375488, L3: 26.847265243530273\n",
      "Current prediction:  62.301414489746094 \n",
      "\n",
      "Iteration 1117, Loss: 37.00822830200195, L1: 10.270711898803711, L3: 26.737516403198242\n",
      "Current prediction:  62.30985641479492 \n",
      "\n",
      "Iteration 1118, Loss: 37.74847412109375, L1: 10.286264419555664, L3: 27.46221160888672\n",
      "Current prediction:  62.200408935546875 \n",
      "\n",
      "Iteration 1119, Loss: 37.287384033203125, L1: 10.275216102600098, L3: 27.012168884277344\n",
      "Current prediction:  61.80051803588867 \n",
      "\n",
      "Iteration 1120, Loss: 37.06502914428711, L1: 10.332197189331055, L3: 26.732831954956055\n",
      "Current prediction:  61.79221725463867 \n",
      "\n",
      "Iteration 1121, Loss: 37.30857849121094, L1: 10.294431686401367, L3: 27.01414680480957\n",
      "Current prediction:  61.79991149902344 \n",
      "\n",
      "Iteration 1122, Loss: 37.33993148803711, L1: 10.371132850646973, L3: 26.96879768371582\n",
      "Current prediction:  61.80705261230469 \n",
      "\n",
      "Iteration 1123, Loss: 36.89524841308594, L1: 10.342501640319824, L3: 26.55274772644043\n",
      "Current prediction:  61.814056396484375 \n",
      "\n",
      "Iteration 1124, Loss: 37.56925964355469, L1: 10.26815414428711, L3: 27.301105499267578\n",
      "Current prediction:  61.82090759277344 \n",
      "\n",
      "Iteration 1125, Loss: 37.77853012084961, L1: 10.24361515045166, L3: 27.534915924072266\n",
      "Current prediction:  61.82728958129883 \n",
      "\n",
      "Iteration 1126, Loss: 37.54969787597656, L1: 10.300074577331543, L3: 27.249624252319336\n",
      "Current prediction:  61.83308792114258 \n",
      "\n",
      "Iteration 1127, Loss: 36.97114562988281, L1: 10.271086692810059, L3: 26.70005989074707\n",
      "Current prediction:  61.838748931884766 \n",
      "\n",
      "Iteration 1128, Loss: 37.818519592285156, L1: 10.343276977539062, L3: 27.475244522094727\n",
      "Current prediction:  61.84349822998047 \n",
      "\n",
      "Iteration 1129, Loss: 36.690208435058594, L1: 10.227882385253906, L3: 26.46232795715332\n",
      "Current prediction:  61.84686279296875 \n",
      "\n",
      "Iteration 1130, Loss: 36.194156646728516, L1: 10.283276557922363, L3: 25.910879135131836\n",
      "Current prediction:  61.84941482543945 \n",
      "\n",
      "Iteration 1131, Loss: 37.49617385864258, L1: 10.335611343383789, L3: 27.16056251525879\n",
      "Current prediction:  61.85150146484375 \n",
      "\n",
      "Iteration 1132, Loss: 39.07094955444336, L1: 10.211686134338379, L3: 28.859262466430664\n",
      "Current prediction:  61.8554801940918 \n",
      "\n",
      "Iteration 1133, Loss: 36.573951721191406, L1: 10.308295249938965, L3: 26.265655517578125\n",
      "Current prediction:  61.89848709106445 \n",
      "\n",
      "Iteration 1134, Loss: 36.867431640625, L1: 10.140995025634766, L3: 26.7264347076416\n",
      "Current prediction:  62.2239875793457 \n",
      "\n",
      "Iteration 1135, Loss: 36.543514251708984, L1: 10.180471420288086, L3: 26.3630428314209\n",
      "Current prediction:  62.38311767578125 \n",
      "\n",
      "Iteration 1136, Loss: 37.85457992553711, L1: 10.19148063659668, L3: 27.66309928894043\n",
      "Current prediction:  62.3976936340332 \n",
      "\n",
      "Iteration 1137, Loss: 36.765132904052734, L1: 10.189059257507324, L3: 26.576074600219727\n",
      "Current prediction:  62.398597717285156 \n",
      "\n",
      "Iteration 1138, Loss: 36.60315704345703, L1: 10.153465270996094, L3: 26.44969367980957\n",
      "Current prediction:  62.3983268737793 \n",
      "\n",
      "Iteration 1139, Loss: 36.67367172241211, L1: 10.1662015914917, L3: 26.507469177246094\n",
      "Current prediction:  62.39802169799805 \n",
      "\n",
      "Iteration 1140, Loss: 36.58171844482422, L1: 10.145000457763672, L3: 26.436716079711914\n",
      "Current prediction:  62.39776611328125 \n",
      "\n",
      "Iteration 1141, Loss: 36.42115783691406, L1: 10.139359474182129, L3: 26.28179931640625\n",
      "Current prediction:  62.397682189941406 \n",
      "\n",
      "Iteration 1142, Loss: 37.02593994140625, L1: 10.115948677062988, L3: 26.909990310668945\n",
      "Current prediction:  62.39744567871094 \n",
      "\n",
      "Iteration 1143, Loss: 36.70214080810547, L1: 10.132343292236328, L3: 26.569799423217773\n",
      "Current prediction:  62.397254943847656 \n",
      "\n",
      "Iteration 1144, Loss: 37.14131164550781, L1: 10.125471115112305, L3: 27.015840530395508\n",
      "Current prediction:  62.39752960205078 \n",
      "\n",
      "Iteration 1145, Loss: 36.88524627685547, L1: 10.13758659362793, L3: 26.74765968322754\n",
      "Current prediction:  62.397865295410156 \n",
      "\n",
      "Iteration 1146, Loss: 37.02954864501953, L1: 10.111793518066406, L3: 26.917757034301758\n",
      "Current prediction:  62.39798355102539 \n",
      "\n",
      "Iteration 1147, Loss: 36.48761749267578, L1: 10.114106178283691, L3: 26.373512268066406\n",
      "Current prediction:  62.39866638183594 \n",
      "\n",
      "Iteration 1148, Loss: 37.85765838623047, L1: 10.184438705444336, L3: 27.673219680786133\n",
      "Current prediction:  62.399742126464844 \n",
      "\n",
      "Iteration 1149, Loss: 36.90739440917969, L1: 10.184784889221191, L3: 26.72260856628418\n",
      "Current prediction:  62.40100860595703 \n",
      "\n",
      "Iteration 1150, Loss: 36.69647216796875, L1: 10.177956581115723, L3: 26.518516540527344\n",
      "Current prediction:  62.4022216796875 \n",
      "\n",
      "Iteration 1151, Loss: 38.184104919433594, L1: 10.141987800598145, L3: 28.042118072509766\n",
      "Current prediction:  62.403682708740234 \n",
      "\n",
      "Iteration 1152, Loss: 37.43711853027344, L1: 10.151888847351074, L3: 27.28523063659668\n",
      "Current prediction:  62.405357360839844 \n",
      "\n",
      "Iteration 1153, Loss: 37.030418395996094, L1: 10.119141578674316, L3: 26.91127586364746\n",
      "Current prediction:  62.407257080078125 \n",
      "\n",
      "Iteration 1154, Loss: 36.899845123291016, L1: 10.10021686553955, L3: 26.79962730407715\n",
      "Current prediction:  62.40916061401367 \n",
      "\n",
      "Iteration 1155, Loss: 37.277462005615234, L1: 10.12968635559082, L3: 27.147775650024414\n",
      "Current prediction:  62.41169738769531 \n",
      "\n",
      "Iteration 1156, Loss: 37.39796447753906, L1: 10.097912788391113, L3: 27.300052642822266\n",
      "Current prediction:  62.413272857666016 \n",
      "\n",
      "Iteration 1157, Loss: 36.99893569946289, L1: 10.133627891540527, L3: 26.865306854248047\n",
      "Current prediction:  62.414405822753906 \n",
      "\n",
      "Iteration 1158, Loss: 36.70428466796875, L1: 10.110773086547852, L3: 26.593509674072266\n",
      "Current prediction:  62.41565704345703 \n",
      "\n",
      "Iteration 1159, Loss: 37.069557189941406, L1: 10.100042343139648, L3: 26.969514846801758\n",
      "Current prediction:  62.4170036315918 \n",
      "\n",
      "Iteration 1160, Loss: 37.65717697143555, L1: 10.10037899017334, L3: 27.55679702758789\n",
      "Current prediction:  62.418418884277344 \n",
      "\n",
      "Iteration 1161, Loss: 37.00275802612305, L1: 10.084016799926758, L3: 26.91874122619629\n",
      "Current prediction:  62.419593811035156 \n",
      "\n",
      "Iteration 1162, Loss: 36.486488342285156, L1: 10.043856620788574, L3: 26.442630767822266\n",
      "Current prediction:  62.42066192626953 \n",
      "\n",
      "Iteration 1163, Loss: 36.53324508666992, L1: 10.117185592651367, L3: 26.416059494018555\n",
      "Current prediction:  62.42186737060547 \n",
      "\n",
      "Iteration 1164, Loss: 36.57655715942383, L1: 10.100571632385254, L3: 26.475984573364258\n",
      "Current prediction:  62.42287063598633 \n",
      "\n",
      "Iteration 1165, Loss: 36.55690383911133, L1: 10.09547233581543, L3: 26.4614315032959\n",
      "Current prediction:  62.423988342285156 \n",
      "\n",
      "Iteration 1166, Loss: 37.19260787963867, L1: 10.049705505371094, L3: 27.142902374267578\n",
      "Current prediction:  62.42361068725586 \n",
      "\n",
      "Iteration 1167, Loss: 37.0224723815918, L1: 10.152726173400879, L3: 26.869747161865234\n",
      "Current prediction:  62.42179489135742 \n",
      "\n",
      "Iteration 1168, Loss: 37.37636947631836, L1: 10.12848949432373, L3: 27.247880935668945\n",
      "Current prediction:  62.4139289855957 \n",
      "\n",
      "Iteration 1169, Loss: 36.961181640625, L1: 10.106389999389648, L3: 26.854793548583984\n",
      "Current prediction:  62.40351486206055 \n",
      "\n",
      "Iteration 1170, Loss: 37.62098693847656, L1: 10.093873023986816, L3: 27.52711296081543\n",
      "Current prediction:  62.39519119262695 \n",
      "\n",
      "Iteration 1171, Loss: 37.429603576660156, L1: 10.126214027404785, L3: 27.303390502929688\n",
      "Current prediction:  62.40492248535156 \n",
      "\n",
      "Iteration 1172, Loss: 37.289913177490234, L1: 10.11299991607666, L3: 27.176912307739258\n",
      "Current prediction:  62.406185150146484 \n",
      "\n",
      "Iteration 1173, Loss: 35.271934509277344, L1: 10.141609191894531, L3: 25.130327224731445\n",
      "Current prediction:  62.41851043701172 \n",
      "\n",
      "Iteration 1174, Loss: 36.87232971191406, L1: 10.144688606262207, L3: 26.72764015197754\n",
      "Current prediction:  62.419944763183594 \n",
      "\n",
      "Iteration 1175, Loss: 37.127750396728516, L1: 10.126972198486328, L3: 27.000778198242188\n",
      "Current prediction:  62.41910934448242 \n",
      "\n",
      "Iteration 1176, Loss: 36.728519439697266, L1: 10.156195640563965, L3: 26.572324752807617\n",
      "Current prediction:  62.41678237915039 \n",
      "\n",
      "Iteration 1177, Loss: 36.11240768432617, L1: 10.144844055175781, L3: 25.96756362915039\n",
      "Current prediction:  62.421470642089844 \n",
      "\n",
      "Iteration 1178, Loss: 36.96202087402344, L1: 10.212629318237305, L3: 26.749391555786133\n",
      "Current prediction:  62.430110931396484 \n",
      "\n",
      "Iteration 1179, Loss: 36.634483337402344, L1: 10.186904907226562, L3: 26.447580337524414\n",
      "Current prediction:  62.43512725830078 \n",
      "\n",
      "Iteration 1180, Loss: 37.29374694824219, L1: 10.115213394165039, L3: 27.17853355407715\n",
      "Current prediction:  62.4364128112793 \n",
      "\n",
      "Iteration 1181, Loss: 37.215091705322266, L1: 10.226778984069824, L3: 26.988311767578125\n",
      "Current prediction:  62.43966293334961 \n",
      "\n",
      "Iteration 1182, Loss: 36.77861022949219, L1: 10.948004722595215, L3: 25.830604553222656\n",
      "Current prediction:  62.44379806518555 \n",
      "\n",
      "Iteration 1183, Loss: 36.86442565917969, L1: 10.846050262451172, L3: 26.018373489379883\n",
      "Current prediction:  61.14539337158203 \n",
      "\n",
      "Iteration 1184, Loss: 37.31840133666992, L1: 10.765220642089844, L3: 26.553180694580078\n",
      "Current prediction:  61.06940460205078 \n",
      "\n",
      "Iteration 1185, Loss: 39.65755844116211, L1: 11.529943466186523, L3: 28.127614974975586\n",
      "Current prediction:  61.08025360107422 \n",
      "\n",
      "Iteration 1186, Loss: 39.613975524902344, L1: 11.956864356994629, L3: 27.6571102142334\n",
      "Current prediction:  61.09550094604492 \n",
      "\n",
      "Iteration 1187, Loss: 39.388671875, L1: 11.424956321716309, L3: 27.963716506958008\n",
      "Current prediction:  61.11437225341797 \n",
      "\n",
      "Iteration 1188, Loss: 40.623382568359375, L1: 12.57645034790039, L3: 28.046934127807617\n",
      "Current prediction:  61.13679504394531 \n",
      "\n",
      "Iteration 1189, Loss: 40.0860595703125, L1: 11.897452354431152, L3: 28.18860626220703\n",
      "Current prediction:  61.1609992980957 \n",
      "\n",
      "Iteration 1190, Loss: 38.31463623046875, L1: 10.659729957580566, L3: 27.654905319213867\n",
      "Current prediction:  62.56966781616211 \n",
      "\n",
      "Iteration 1191, Loss: 37.446136474609375, L1: 10.504387855529785, L3: 26.941749572753906\n",
      "Current prediction:  62.589508056640625 \n",
      "\n",
      "Iteration 1192, Loss: 37.868682861328125, L1: 10.994665145874023, L3: 26.874019622802734\n",
      "Current prediction:  62.60579299926758 \n",
      "\n",
      "Iteration 1193, Loss: 38.21435546875, L1: 10.288131713867188, L3: 27.92622184753418\n",
      "Current prediction:  62.617950439453125 \n",
      "\n",
      "Iteration 1194, Loss: 38.18724822998047, L1: 10.110128402709961, L3: 28.07712173461914\n",
      "Current prediction:  62.62643814086914 \n",
      "\n",
      "Iteration 1195, Loss: 37.157405853271484, L1: 9.896581649780273, L3: 27.26082420349121\n",
      "Current prediction:  62.6317024230957 \n",
      "\n",
      "Iteration 1196, Loss: 37.80345153808594, L1: 9.92646598815918, L3: 27.876983642578125\n",
      "Current prediction:  62.634178161621094 \n",
      "\n",
      "Iteration 1197, Loss: 37.430152893066406, L1: 9.95148754119873, L3: 27.478666305541992\n",
      "Current prediction:  62.634056091308594 \n",
      "\n",
      "Iteration 1198, Loss: 37.100101470947266, L1: 10.015889167785645, L3: 27.084213256835938\n",
      "Current prediction:  62.632652282714844 \n",
      "\n",
      "Iteration 1199, Loss: 37.08575439453125, L1: 9.85594367980957, L3: 27.229812622070312\n",
      "Current prediction:  62.62968444824219 \n",
      "\n",
      "Iteration 1200, Loss: 36.92421340942383, L1: 9.880908012390137, L3: 27.043306350708008\n",
      "Current prediction:  62.62228012084961 \n",
      "\n",
      "Iteration 1201, Loss: 37.3654899597168, L1: 9.944916725158691, L3: 27.420574188232422\n",
      "Current prediction:  62.4705810546875 \n",
      "\n",
      "Iteration 1202, Loss: 36.11912536621094, L1: 10.005720138549805, L3: 26.113407135009766\n",
      "Current prediction:  62.18833923339844 \n",
      "\n",
      "Iteration 1203, Loss: 38.42283248901367, L1: 9.947846412658691, L3: 28.474987030029297\n",
      "Current prediction:  62.16884994506836 \n",
      "\n",
      "Iteration 1204, Loss: 36.98101043701172, L1: 10.034695625305176, L3: 26.94631576538086\n",
      "Current prediction:  62.1618537902832 \n",
      "\n",
      "Iteration 1205, Loss: 38.28987503051758, L1: 10.486503601074219, L3: 27.80337142944336\n",
      "Current prediction:  62.15481185913086 \n",
      "\n",
      "Iteration 1206, Loss: 37.843666076660156, L1: 10.195924758911133, L3: 27.647741317749023\n",
      "Current prediction:  62.147674560546875 \n",
      "\n",
      "Iteration 1207, Loss: 37.69427490234375, L1: 10.330438613891602, L3: 27.36383819580078\n",
      "Current prediction:  62.140594482421875 \n",
      "\n",
      "Iteration 1208, Loss: 37.39940643310547, L1: 10.269742965698242, L3: 27.12966537475586\n",
      "Current prediction:  61.62057876586914 \n",
      "\n",
      "Iteration 1209, Loss: 36.35135269165039, L1: 10.50669002532959, L3: 25.844661712646484\n",
      "Current prediction:  61.17779541015625 \n",
      "\n",
      "Iteration 1210, Loss: 37.175899505615234, L1: 10.826123237609863, L3: 26.349775314331055\n",
      "Current prediction:  60.8003044128418 \n",
      "\n",
      "Iteration 1211, Loss: 37.1230583190918, L1: 10.858421325683594, L3: 26.264636993408203\n",
      "Current prediction:  60.76967239379883 \n",
      "\n",
      "Iteration 1212, Loss: 37.08997344970703, L1: 10.69339370727539, L3: 26.396577835083008\n",
      "Current prediction:  60.7002067565918 \n",
      "\n",
      "Iteration 1213, Loss: 38.04073715209961, L1: 10.69588565826416, L3: 27.344850540161133\n",
      "Current prediction:  60.53736877441406 \n",
      "\n",
      "Iteration 1214, Loss: 37.88518142700195, L1: 10.64509391784668, L3: 27.240087509155273\n",
      "Current prediction:  60.430931091308594 \n",
      "\n",
      "Iteration 1215, Loss: 38.078887939453125, L1: 10.677680969238281, L3: 27.401208877563477\n",
      "Current prediction:  60.50436782836914 \n",
      "\n",
      "Iteration 1216, Loss: 37.81509780883789, L1: 10.63082218170166, L3: 27.184274673461914\n",
      "Current prediction:  60.72149658203125 \n",
      "\n",
      "Iteration 1217, Loss: 37.79255676269531, L1: 10.552715301513672, L3: 27.239843368530273\n",
      "Current prediction:  60.832916259765625 \n",
      "\n",
      "Iteration 1218, Loss: 36.0136604309082, L1: 10.450812339782715, L3: 25.562847137451172\n",
      "Current prediction:  60.97087097167969 \n",
      "\n",
      "Iteration 1219, Loss: 36.610008239746094, L1: 10.460741996765137, L3: 26.14926528930664\n",
      "Current prediction:  61.54728698730469 \n",
      "\n",
      "Iteration 1220, Loss: 36.45427322387695, L1: 10.469077110290527, L3: 25.98519515991211\n",
      "Current prediction:  62.061683654785156 \n",
      "\n",
      "Iteration 1221, Loss: 37.36183166503906, L1: 10.452232360839844, L3: 26.90959930419922\n",
      "Current prediction:  61.908634185791016 \n",
      "\n",
      "Iteration 1222, Loss: 37.1097297668457, L1: 10.501105308532715, L3: 26.608625411987305\n",
      "Current prediction:  61.9390754699707 \n",
      "\n",
      "Iteration 1223, Loss: 37.37093734741211, L1: 10.689535140991211, L3: 26.6814022064209\n",
      "Current prediction:  61.957637786865234 \n",
      "\n",
      "Iteration 1224, Loss: 36.816715240478516, L1: 10.571199417114258, L3: 26.245515823364258\n",
      "Current prediction:  61.97461700439453 \n",
      "\n",
      "Iteration 1225, Loss: 37.34199905395508, L1: 10.616376876831055, L3: 26.725622177124023\n",
      "Current prediction:  61.99055862426758 \n",
      "\n",
      "Iteration 1226, Loss: 37.32732009887695, L1: 10.762953758239746, L3: 26.56436538696289\n",
      "Current prediction:  62.005558013916016 \n",
      "\n",
      "Iteration 1227, Loss: 38.0650520324707, L1: 10.613995552062988, L3: 27.4510555267334\n",
      "Current prediction:  62.01948165893555 \n",
      "\n",
      "Iteration 1228, Loss: 35.94508361816406, L1: 10.579184532165527, L3: 25.36590003967285\n",
      "Current prediction:  62.03234100341797 \n",
      "\n",
      "Iteration 1229, Loss: 37.42660140991211, L1: 10.607170104980469, L3: 26.81943130493164\n",
      "Current prediction:  62.04391098022461 \n",
      "\n",
      "Iteration 1230, Loss: 36.70384979248047, L1: 10.507678031921387, L3: 26.196170806884766\n",
      "Current prediction:  62.05409240722656 \n",
      "\n",
      "Iteration 1231, Loss: 36.6920166015625, L1: 10.771171569824219, L3: 25.92084312438965\n",
      "Current prediction:  62.063194274902344 \n",
      "\n",
      "Iteration 1232, Loss: 36.297855377197266, L1: 10.637450218200684, L3: 25.6604061126709\n",
      "Current prediction:  62.071929931640625 \n",
      "\n",
      "Iteration 1233, Loss: 36.602928161621094, L1: 10.614580154418945, L3: 25.98834991455078\n",
      "Current prediction:  62.079654693603516 \n",
      "\n",
      "Iteration 1234, Loss: 37.31718063354492, L1: 10.4975004196167, L3: 26.81968116760254\n",
      "Current prediction:  62.08649826049805 \n",
      "\n",
      "Iteration 1235, Loss: 36.656158447265625, L1: 10.605443954467773, L3: 26.05071258544922\n",
      "Current prediction:  62.09299087524414 \n",
      "\n",
      "Iteration 1236, Loss: 37.51882553100586, L1: 10.62934398651123, L3: 26.889480590820312\n",
      "Current prediction:  62.098365783691406 \n",
      "\n",
      "Iteration 1237, Loss: 36.914859771728516, L1: 10.710590362548828, L3: 26.204269409179688\n",
      "Current prediction:  62.103271484375 \n",
      "\n",
      "Iteration 1238, Loss: 36.616004943847656, L1: 10.656810760498047, L3: 25.959196090698242\n",
      "Current prediction:  62.10803985595703 \n",
      "\n",
      "Iteration 1239, Loss: 37.66865158081055, L1: 10.59012508392334, L3: 27.07852554321289\n",
      "Current prediction:  62.11145782470703 \n",
      "\n",
      "Iteration 1240, Loss: 37.23622512817383, L1: 10.641024589538574, L3: 26.595199584960938\n",
      "Current prediction:  62.11418914794922 \n",
      "\n",
      "Iteration 1241, Loss: 36.788551330566406, L1: 10.44399356842041, L3: 26.344558715820312\n",
      "Current prediction:  62.11521530151367 \n",
      "\n",
      "Iteration 1242, Loss: 36.86646270751953, L1: 10.547158241271973, L3: 26.319305419921875\n",
      "Current prediction:  62.11555099487305 \n",
      "\n",
      "Iteration 1243, Loss: 37.01271057128906, L1: 10.45753288269043, L3: 26.555177688598633\n",
      "Current prediction:  62.11597442626953 \n",
      "\n",
      "Iteration 1244, Loss: 36.1612434387207, L1: 10.496427536010742, L3: 25.66481590270996\n",
      "Current prediction:  62.12137985229492 \n",
      "\n",
      "Iteration 1245, Loss: 35.91544723510742, L1: 10.52214241027832, L3: 25.3933048248291\n",
      "Current prediction:  62.135337829589844 \n",
      "\n",
      "Iteration 1246, Loss: 36.686588287353516, L1: 10.464911460876465, L3: 26.221677780151367\n",
      "Current prediction:  62.14777755737305 \n",
      "\n",
      "Iteration 1247, Loss: 37.95906448364258, L1: 10.514120101928711, L3: 27.444944381713867\n",
      "Current prediction:  62.15833282470703 \n",
      "\n",
      "Iteration 1248, Loss: 36.74003601074219, L1: 10.49709701538086, L3: 26.24294090270996\n",
      "Current prediction:  62.169673919677734 \n",
      "\n",
      "Iteration 1249, Loss: 37.17289733886719, L1: 10.473917961120605, L3: 26.698978424072266\n",
      "Current prediction:  62.1810302734375 \n",
      "\n",
      "Iteration 1250, Loss: 35.58184051513672, L1: 10.443272590637207, L3: 25.138566970825195\n",
      "Current prediction:  62.19289016723633 \n",
      "\n",
      "Iteration 1251, Loss: 37.46593475341797, L1: 10.477396965026855, L3: 26.988536834716797\n",
      "Current prediction:  62.204925537109375 \n",
      "\n",
      "Iteration 1252, Loss: 36.490631103515625, L1: 10.4302339553833, L3: 26.06039810180664\n",
      "Current prediction:  62.216949462890625 \n",
      "\n",
      "Iteration 1253, Loss: 37.065067291259766, L1: 10.352773666381836, L3: 26.71229362487793\n",
      "Current prediction:  62.228458404541016 \n",
      "\n",
      "Iteration 1254, Loss: 37.983272552490234, L1: 10.332539558410645, L3: 27.650732040405273\n",
      "Current prediction:  62.24003601074219 \n",
      "\n",
      "Iteration 1255, Loss: 36.91873550415039, L1: 10.321938514709473, L3: 26.5967960357666\n",
      "Current prediction:  62.25581741333008 \n",
      "\n",
      "Iteration 1256, Loss: 37.070804595947266, L1: 10.301087379455566, L3: 26.769716262817383\n",
      "Current prediction:  62.27375411987305 \n",
      "\n",
      "Iteration 1257, Loss: 37.35253143310547, L1: 10.266387939453125, L3: 27.086143493652344\n",
      "Current prediction:  62.29054641723633 \n",
      "\n",
      "Iteration 1258, Loss: 36.53670883178711, L1: 10.20804500579834, L3: 26.328662872314453\n",
      "Current prediction:  62.306087493896484 \n",
      "\n",
      "Iteration 1259, Loss: 37.67629623413086, L1: 10.291764259338379, L3: 27.384531021118164\n",
      "Current prediction:  62.31975555419922 \n",
      "\n",
      "Iteration 1260, Loss: 36.19439697265625, L1: 10.168956756591797, L3: 26.02543830871582\n",
      "Current prediction:  62.332489013671875 \n",
      "\n",
      "Iteration 1261, Loss: 37.39616012573242, L1: 10.186187744140625, L3: 27.209972381591797\n",
      "Current prediction:  62.34438705444336 \n",
      "\n",
      "Iteration 1262, Loss: 36.50136184692383, L1: 10.186123847961426, L3: 26.315237045288086\n",
      "Current prediction:  62.355125427246094 \n",
      "\n",
      "Iteration 1263, Loss: 36.81654357910156, L1: 10.097970008850098, L3: 26.71857261657715\n",
      "Current prediction:  62.364994049072266 \n",
      "\n",
      "Iteration 1264, Loss: 36.49801254272461, L1: 10.14914321899414, L3: 26.34886932373047\n",
      "Current prediction:  62.37357711791992 \n",
      "\n",
      "Iteration 1265, Loss: 38.204158782958984, L1: 10.183619499206543, L3: 28.020540237426758\n",
      "Current prediction:  62.38116455078125 \n",
      "\n",
      "Iteration 1266, Loss: 36.750640869140625, L1: 10.152961730957031, L3: 26.59767723083496\n",
      "Current prediction:  62.38796615600586 \n",
      "\n",
      "Iteration 1267, Loss: 36.89918518066406, L1: 10.12218952178955, L3: 26.776996612548828\n",
      "Current prediction:  62.39389419555664 \n",
      "\n",
      "Iteration 1268, Loss: 36.860511779785156, L1: 10.14255428314209, L3: 26.717958450317383\n",
      "Current prediction:  62.398582458496094 \n",
      "\n",
      "Iteration 1269, Loss: 36.370567321777344, L1: 10.09826374053955, L3: 26.272302627563477\n",
      "Current prediction:  62.40268325805664 \n",
      "\n",
      "Iteration 1270, Loss: 37.37849426269531, L1: 10.105474472045898, L3: 27.273021697998047\n",
      "Current prediction:  62.405155181884766 \n",
      "\n",
      "Iteration 1271, Loss: 37.733917236328125, L1: 10.085437774658203, L3: 27.64847755432129\n",
      "Current prediction:  62.40608596801758 \n",
      "\n",
      "Iteration 1272, Loss: 37.86496353149414, L1: 10.077376365661621, L3: 27.787588119506836\n",
      "Current prediction:  62.39912033081055 \n",
      "\n",
      "Iteration 1273, Loss: 36.07184982299805, L1: 10.257454872131348, L3: 25.814393997192383\n",
      "Current prediction:  62.34963607788086 \n",
      "\n",
      "Iteration 1274, Loss: 37.19598388671875, L1: 10.109481811523438, L3: 27.08650016784668\n",
      "Current prediction:  62.22811508178711 \n",
      "\n",
      "Iteration 1275, Loss: 36.83382034301758, L1: 10.08750057220459, L3: 26.746318817138672\n",
      "Current prediction:  62.09003448486328 \n",
      "\n",
      "Iteration 1276, Loss: 37.3531494140625, L1: 10.064166069030762, L3: 27.288984298706055\n",
      "Current prediction:  61.9743766784668 \n",
      "\n",
      "Iteration 1277, Loss: 36.845306396484375, L1: 10.108926773071289, L3: 26.73638153076172\n",
      "Current prediction:  61.93576431274414 \n",
      "\n",
      "Iteration 1278, Loss: 36.651309967041016, L1: 10.128829002380371, L3: 26.52248191833496\n",
      "Current prediction:  61.9076042175293 \n",
      "\n",
      "Iteration 1279, Loss: 37.248008728027344, L1: 10.231385231018066, L3: 27.01662254333496\n",
      "Current prediction:  61.89700698852539 \n",
      "\n",
      "Iteration 1280, Loss: 37.06590270996094, L1: 10.202924728393555, L3: 26.862977981567383\n",
      "Current prediction:  61.89678192138672 \n",
      "\n",
      "Iteration 1281, Loss: 37.53284454345703, L1: 10.115461349487305, L3: 27.41738510131836\n",
      "Current prediction:  61.92033004760742 \n",
      "\n",
      "Iteration 1282, Loss: 37.61372375488281, L1: 10.14013671875, L3: 27.473587036132812\n",
      "Current prediction:  61.96249771118164 \n",
      "\n",
      "Iteration 1283, Loss: 37.598060607910156, L1: 10.176445960998535, L3: 27.421615600585938\n",
      "Current prediction:  62.048397064208984 \n",
      "\n",
      "Iteration 1284, Loss: 36.695533752441406, L1: 10.300411224365234, L3: 26.395124435424805\n",
      "Current prediction:  62.22422409057617 \n",
      "\n",
      "Iteration 1285, Loss: 37.57406234741211, L1: 10.213127136230469, L3: 27.36093521118164\n",
      "Current prediction:  62.351158142089844 \n",
      "\n",
      "Iteration 1286, Loss: 36.756221771240234, L1: 10.101522445678711, L3: 26.654699325561523\n",
      "Current prediction:  62.381736755371094 \n",
      "\n",
      "Iteration 1287, Loss: 36.7821044921875, L1: 10.191431045532227, L3: 26.590675354003906\n",
      "Current prediction:  62.38347244262695 \n",
      "\n",
      "Iteration 1288, Loss: 37.57796859741211, L1: 10.175634384155273, L3: 27.402334213256836\n",
      "Current prediction:  62.381919860839844 \n",
      "\n",
      "Iteration 1289, Loss: 37.7203483581543, L1: 10.145447731018066, L3: 27.574899673461914\n",
      "Current prediction:  62.38020706176758 \n",
      "\n",
      "Iteration 1290, Loss: 37.89319610595703, L1: 10.17082691192627, L3: 27.722368240356445\n",
      "Current prediction:  62.378570556640625 \n",
      "\n",
      "Iteration 1291, Loss: 36.87299728393555, L1: 10.223976135253906, L3: 26.64902114868164\n",
      "Current prediction:  62.377506256103516 \n",
      "\n",
      "Iteration 1292, Loss: 36.814430236816406, L1: 10.141836166381836, L3: 26.672595977783203\n",
      "Current prediction:  62.376564025878906 \n",
      "\n",
      "Iteration 1293, Loss: 37.30922317504883, L1: 10.122859001159668, L3: 27.186365127563477\n",
      "Current prediction:  62.37601089477539 \n",
      "\n",
      "Iteration 1294, Loss: 37.54117202758789, L1: 10.201699256896973, L3: 27.3394718170166\n",
      "Current prediction:  62.374794006347656 \n",
      "\n",
      "Iteration 1295, Loss: 37.52843475341797, L1: 10.141201972961426, L3: 27.38723373413086\n",
      "Current prediction:  62.37211990356445 \n",
      "\n",
      "Iteration 1296, Loss: 37.270965576171875, L1: 10.09765911102295, L3: 27.17330551147461\n",
      "Current prediction:  62.369354248046875 \n",
      "\n",
      "Iteration 1297, Loss: 36.17821502685547, L1: 10.186993598937988, L3: 25.991222381591797\n",
      "Current prediction:  62.371768951416016 \n",
      "\n",
      "Iteration 1298, Loss: 37.03716278076172, L1: 10.067488670349121, L3: 26.969675064086914\n",
      "Current prediction:  62.3751220703125 \n",
      "\n",
      "Iteration 1299, Loss: 37.10682678222656, L1: 10.113771438598633, L3: 26.993053436279297\n",
      "Current prediction:  62.379730224609375 \n",
      "\n",
      "Iteration 1300, Loss: 37.619869232177734, L1: 10.074350357055664, L3: 27.54551887512207\n",
      "Current prediction:  62.383949279785156 \n",
      "\n",
      "Iteration 1301, Loss: 37.9238166809082, L1: 9.985751152038574, L3: 27.938066482543945\n",
      "Current prediction:  62.38783645629883 \n",
      "\n",
      "Iteration 1302, Loss: 36.67748260498047, L1: 10.15729808807373, L3: 26.520183563232422\n",
      "Current prediction:  62.39048767089844 \n",
      "\n",
      "Iteration 1303, Loss: 36.50575637817383, L1: 10.01192855834961, L3: 26.49382781982422\n",
      "Current prediction:  62.39359664916992 \n",
      "\n",
      "Iteration 1304, Loss: 38.00099182128906, L1: 10.046897888183594, L3: 27.95409393310547\n",
      "Current prediction:  62.396488189697266 \n",
      "\n",
      "Iteration 1305, Loss: 36.78969192504883, L1: 10.017537117004395, L3: 26.772153854370117\n",
      "Current prediction:  62.398895263671875 \n",
      "\n",
      "Iteration 1306, Loss: 36.28822708129883, L1: 10.000807762145996, L3: 26.287418365478516\n",
      "Current prediction:  62.400428771972656 \n",
      "\n",
      "Iteration 1307, Loss: 36.7031135559082, L1: 9.989795684814453, L3: 26.71331787109375\n",
      "Current prediction:  62.401363372802734 \n",
      "\n",
      "Iteration 1308, Loss: 37.253944396972656, L1: 10.022543907165527, L3: 27.231399536132812\n",
      "Current prediction:  62.401756286621094 \n",
      "\n",
      "Iteration 1309, Loss: 36.396522521972656, L1: 10.011369705200195, L3: 26.385150909423828\n",
      "Current prediction:  62.40200424194336 \n",
      "\n",
      "Iteration 1310, Loss: 37.52204513549805, L1: 10.018614768981934, L3: 27.50343132019043\n",
      "Current prediction:  62.4024772644043 \n",
      "\n",
      "Iteration 1311, Loss: 36.46912384033203, L1: 9.97612190246582, L3: 26.493000030517578\n",
      "Current prediction:  62.402320861816406 \n",
      "\n",
      "Iteration 1312, Loss: 37.20527648925781, L1: 9.98132610321045, L3: 27.223949432373047\n",
      "Current prediction:  62.40080642700195 \n",
      "\n",
      "Iteration 1313, Loss: 37.8778076171875, L1: 10.015759468078613, L3: 27.862049102783203\n",
      "Current prediction:  62.39485168457031 \n",
      "\n",
      "Iteration 1314, Loss: 37.12272644042969, L1: 9.99824333190918, L3: 27.12448501586914\n",
      "Current prediction:  62.391357421875 \n",
      "\n",
      "Iteration 1315, Loss: 37.365501403808594, L1: 9.968605995178223, L3: 27.396894454956055\n",
      "Current prediction:  62.386959075927734 \n",
      "\n",
      "Iteration 1316, Loss: 35.48440933227539, L1: 9.974607467651367, L3: 25.509801864624023\n",
      "Current prediction:  62.377769470214844 \n",
      "\n",
      "Iteration 1317, Loss: 36.40545654296875, L1: 9.976383209228516, L3: 26.4290714263916\n",
      "Current prediction:  62.36838150024414 \n",
      "\n",
      "Iteration 1318, Loss: 36.39981460571289, L1: 9.989374160766602, L3: 26.41044044494629\n",
      "Current prediction:  62.30830383300781 \n",
      "\n",
      "Iteration 1319, Loss: 36.80058670043945, L1: 9.990965843200684, L3: 26.809621810913086\n",
      "Current prediction:  62.24684524536133 \n",
      "\n",
      "Iteration 1320, Loss: 36.15736389160156, L1: 9.99520492553711, L3: 26.162158966064453\n",
      "Current prediction:  62.18975830078125 \n",
      "\n",
      "Iteration 1321, Loss: 36.8037109375, L1: 9.97248649597168, L3: 26.83122444152832\n",
      "Current prediction:  62.10726547241211 \n",
      "\n",
      "Iteration 1322, Loss: 36.687015533447266, L1: 10.021912574768066, L3: 26.665103912353516\n",
      "Current prediction:  62.113460540771484 \n",
      "\n",
      "Iteration 1323, Loss: 37.21519088745117, L1: 9.986597061157227, L3: 27.228593826293945\n",
      "Current prediction:  62.18849182128906 \n",
      "\n",
      "Iteration 1324, Loss: 36.875633239746094, L1: 10.032431602478027, L3: 26.84320068359375\n",
      "Current prediction:  62.2686767578125 \n",
      "\n",
      "Iteration 1325, Loss: 38.013465881347656, L1: 9.980700492858887, L3: 28.032766342163086\n",
      "Current prediction:  62.32620620727539 \n",
      "\n",
      "Iteration 1326, Loss: 37.420997619628906, L1: 9.990318298339844, L3: 27.430681228637695\n",
      "Current prediction:  62.33170700073242 \n",
      "\n",
      "Iteration 1327, Loss: 37.301727294921875, L1: 9.980140686035156, L3: 27.321584701538086\n",
      "Current prediction:  62.33938980102539 \n",
      "\n",
      "Iteration 1328, Loss: 37.111751556396484, L1: 9.99250316619873, L3: 27.11924934387207\n",
      "Current prediction:  62.35110092163086 \n",
      "\n",
      "Iteration 1329, Loss: 36.47516632080078, L1: 10.000810623168945, L3: 26.474355697631836\n",
      "Current prediction:  62.34967041015625 \n",
      "\n",
      "Iteration 1330, Loss: 36.532100677490234, L1: 9.94271183013916, L3: 26.58938980102539\n",
      "Current prediction:  62.31716537475586 \n",
      "\n",
      "Iteration 1331, Loss: 36.423500061035156, L1: 10.03499698638916, L3: 26.388504028320312\n",
      "Current prediction:  62.28278350830078 \n",
      "\n",
      "Iteration 1332, Loss: 37.26605224609375, L1: 9.980695724487305, L3: 27.285356521606445\n",
      "Current prediction:  62.22775650024414 \n",
      "\n",
      "Iteration 1333, Loss: 36.51451110839844, L1: 10.056572914123535, L3: 26.457937240600586\n",
      "Current prediction:  62.140899658203125 \n",
      "\n",
      "Iteration 1334, Loss: 36.57686996459961, L1: 10.012227058410645, L3: 26.56464385986328\n",
      "Current prediction:  62.087886810302734 \n",
      "\n",
      "Iteration 1335, Loss: 37.4338493347168, L1: 10.016423225402832, L3: 27.41742515563965\n",
      "Current prediction:  62.15709686279297 \n",
      "\n",
      "Iteration 1336, Loss: 36.34239959716797, L1: 10.036361694335938, L3: 26.306039810180664\n",
      "Current prediction:  62.259552001953125 \n",
      "\n",
      "Iteration 1337, Loss: 37.0462532043457, L1: 9.989180564880371, L3: 27.057071685791016\n",
      "Current prediction:  62.294986724853516 \n",
      "\n",
      "Iteration 1338, Loss: 36.18647003173828, L1: 10.014652252197266, L3: 26.17181968688965\n",
      "Current prediction:  62.31568145751953 \n",
      "\n",
      "Iteration 1339, Loss: 35.5384521484375, L1: 10.006648063659668, L3: 25.53180503845215\n",
      "Current prediction:  62.32638168334961 \n",
      "\n",
      "Iteration 1340, Loss: 37.38705825805664, L1: 10.085443496704102, L3: 27.30161476135254\n",
      "Current prediction:  62.33709716796875 \n",
      "\n",
      "Iteration 1341, Loss: 37.81919860839844, L1: 10.040534019470215, L3: 27.77866554260254\n",
      "Current prediction:  62.33740997314453 \n",
      "\n",
      "Iteration 1342, Loss: 36.51967239379883, L1: 10.018465995788574, L3: 26.501205444335938\n",
      "Current prediction:  62.33489990234375 \n",
      "\n",
      "Iteration 1343, Loss: 36.667972564697266, L1: 10.092926979064941, L3: 26.575044631958008\n",
      "Current prediction:  62.3310546875 \n",
      "\n",
      "Iteration 1344, Loss: 35.976280212402344, L1: 10.05958366394043, L3: 25.91669464111328\n",
      "Current prediction:  62.32695388793945 \n",
      "\n",
      "Iteration 1345, Loss: 37.00320816040039, L1: 10.059309005737305, L3: 26.943899154663086\n",
      "Current prediction:  62.32218933105469 \n",
      "\n",
      "Iteration 1346, Loss: 36.98705291748047, L1: 10.088414192199707, L3: 26.898639678955078\n",
      "Current prediction:  62.31741714477539 \n",
      "\n",
      "Iteration 1347, Loss: 35.88644790649414, L1: 10.065485954284668, L3: 25.820960998535156\n",
      "Current prediction:  62.31289291381836 \n",
      "\n",
      "Iteration 1348, Loss: 37.418304443359375, L1: 10.124687194824219, L3: 27.293615341186523\n",
      "Current prediction:  62.30802536010742 \n",
      "\n",
      "Iteration 1349, Loss: 37.909732818603516, L1: 10.064620018005371, L3: 27.845111846923828\n",
      "Current prediction:  62.30281066894531 \n",
      "\n",
      "Iteration 1350, Loss: 36.12571334838867, L1: 10.054341316223145, L3: 26.071372985839844\n",
      "Current prediction:  62.29757308959961 \n",
      "\n",
      "Iteration 1351, Loss: 36.34627914428711, L1: 10.036930084228516, L3: 26.309349060058594\n",
      "Current prediction:  62.29383850097656 \n",
      "\n",
      "Iteration 1352, Loss: 37.805843353271484, L1: 10.071074485778809, L3: 27.73476791381836\n",
      "Current prediction:  62.29073715209961 \n",
      "\n",
      "Iteration 1353, Loss: 36.84285354614258, L1: 10.05301284790039, L3: 26.789840698242188\n",
      "Current prediction:  62.28809356689453 \n",
      "\n",
      "Iteration 1354, Loss: 36.9715576171875, L1: 10.115361213684082, L3: 26.856197357177734\n",
      "Current prediction:  62.28501892089844 \n",
      "\n",
      "Iteration 1355, Loss: 36.487815856933594, L1: 10.085256576538086, L3: 26.402559280395508\n",
      "Current prediction:  62.28219223022461 \n",
      "\n",
      "Iteration 1356, Loss: 37.06332778930664, L1: 10.056709289550781, L3: 27.00661849975586\n",
      "Current prediction:  62.27930450439453 \n",
      "\n",
      "Iteration 1357, Loss: 37.40361785888672, L1: 10.094647407531738, L3: 27.308969497680664\n",
      "Current prediction:  62.276668548583984 \n",
      "\n",
      "Iteration 1358, Loss: 36.4671630859375, L1: 10.091561317443848, L3: 26.375600814819336\n",
      "Current prediction:  62.277400970458984 \n",
      "\n",
      "Iteration 1359, Loss: 36.609458923339844, L1: 10.113029479980469, L3: 26.496427536010742\n",
      "Current prediction:  62.274539947509766 \n",
      "\n",
      "Iteration 1360, Loss: 36.12724685668945, L1: 10.106266975402832, L3: 26.020978927612305\n",
      "Current prediction:  62.27511978149414 \n",
      "\n",
      "Iteration 1361, Loss: 36.42896270751953, L1: 10.096790313720703, L3: 26.332172393798828\n",
      "Current prediction:  62.277862548828125 \n",
      "\n",
      "Iteration 1362, Loss: 37.38976287841797, L1: 10.158340454101562, L3: 27.23142433166504\n",
      "Current prediction:  62.28105926513672 \n",
      "\n",
      "Iteration 1363, Loss: 36.89313888549805, L1: 10.104321479797363, L3: 26.788816452026367\n",
      "Current prediction:  62.2850456237793 \n",
      "\n",
      "Iteration 1364, Loss: 36.75291442871094, L1: 10.076894760131836, L3: 26.6760196685791\n",
      "Current prediction:  62.288936614990234 \n",
      "\n",
      "Iteration 1365, Loss: 37.17548370361328, L1: 10.118356704711914, L3: 27.057125091552734\n",
      "Current prediction:  62.29294967651367 \n",
      "\n",
      "Iteration 1366, Loss: 36.47937774658203, L1: 10.120122909545898, L3: 26.3592529296875\n",
      "Current prediction:  62.297096252441406 \n",
      "\n",
      "Iteration 1367, Loss: 36.920894622802734, L1: 10.12197208404541, L3: 26.798921585083008\n",
      "Current prediction:  62.301204681396484 \n",
      "\n",
      "Iteration 1368, Loss: 35.81336212158203, L1: 10.061005592346191, L3: 25.752355575561523\n",
      "Current prediction:  62.3049430847168 \n",
      "\n",
      "Iteration 1369, Loss: 37.278343200683594, L1: 10.091073036193848, L3: 27.18726921081543\n",
      "Current prediction:  62.30892562866211 \n",
      "\n",
      "Iteration 1370, Loss: 35.74991226196289, L1: 10.055710792541504, L3: 25.694202423095703\n",
      "Current prediction:  62.312992095947266 \n",
      "\n",
      "Iteration 1371, Loss: 38.20526123046875, L1: 10.080620765686035, L3: 28.12464141845703\n",
      "Current prediction:  62.3161735534668 \n",
      "\n",
      "Iteration 1372, Loss: 37.082862854003906, L1: 10.04216194152832, L3: 27.04070281982422\n",
      "Current prediction:  62.319480895996094 \n",
      "\n",
      "Iteration 1373, Loss: 36.69878387451172, L1: 10.064102172851562, L3: 26.63468360900879\n",
      "Current prediction:  62.322959899902344 \n",
      "\n",
      "Iteration 1374, Loss: 37.18695068359375, L1: 10.011466026306152, L3: 27.17548370361328\n",
      "Current prediction:  62.32639694213867 \n",
      "\n",
      "Iteration 1375, Loss: 36.10845947265625, L1: 10.050950050354004, L3: 26.057510375976562\n",
      "Current prediction:  62.33005142211914 \n",
      "\n",
      "Iteration 1376, Loss: 35.72780990600586, L1: 10.030795097351074, L3: 25.6970157623291\n",
      "Current prediction:  62.33365249633789 \n",
      "\n",
      "Iteration 1377, Loss: 36.07607650756836, L1: 9.987481117248535, L3: 26.088594436645508\n",
      "Current prediction:  62.33686065673828 \n",
      "\n",
      "Iteration 1378, Loss: 36.53422164916992, L1: 9.979531288146973, L3: 26.554691314697266\n",
      "Current prediction:  62.33951950073242 \n",
      "\n",
      "Iteration 1379, Loss: 37.59174728393555, L1: 9.973072052001953, L3: 27.618675231933594\n",
      "Current prediction:  62.34123611450195 \n",
      "\n",
      "Iteration 1380, Loss: 36.625732421875, L1: 9.941690444946289, L3: 26.684043884277344\n",
      "Current prediction:  62.34220504760742 \n",
      "\n",
      "Iteration 1381, Loss: 37.70330810546875, L1: 9.981019973754883, L3: 27.7222900390625\n",
      "Current prediction:  62.3432502746582 \n",
      "\n",
      "Iteration 1382, Loss: 36.588134765625, L1: 9.995924949645996, L3: 26.592208862304688\n",
      "Current prediction:  62.3436164855957 \n",
      "\n",
      "Iteration 1383, Loss: 36.22120666503906, L1: 9.950386047363281, L3: 26.270822525024414\n",
      "Current prediction:  62.34371566772461 \n",
      "\n",
      "Iteration 1384, Loss: 35.95165252685547, L1: 10.018500328063965, L3: 25.933151245117188\n",
      "Current prediction:  62.34425354003906 \n",
      "\n",
      "Iteration 1385, Loss: 36.753082275390625, L1: 9.950328826904297, L3: 26.802753448486328\n",
      "Current prediction:  62.34363555908203 \n",
      "\n",
      "Iteration 1386, Loss: 37.46601486206055, L1: 9.960723876953125, L3: 27.505290985107422\n",
      "Current prediction:  62.3421630859375 \n",
      "\n",
      "Iteration 1387, Loss: 37.931175231933594, L1: 10.023397445678711, L3: 27.907777786254883\n",
      "Current prediction:  62.3402099609375 \n",
      "\n",
      "Iteration 1388, Loss: 37.112754821777344, L1: 10.013238906860352, L3: 27.099517822265625\n",
      "Current prediction:  62.337623596191406 \n",
      "\n",
      "Iteration 1389, Loss: 37.58403015136719, L1: 9.993109703063965, L3: 27.590919494628906\n",
      "Current prediction:  62.335182189941406 \n",
      "\n",
      "Iteration 1390, Loss: 37.2868537902832, L1: 9.960470199584961, L3: 27.326383590698242\n",
      "Current prediction:  62.33251953125 \n",
      "\n",
      "Iteration 1391, Loss: 36.59822463989258, L1: 9.985055923461914, L3: 26.613168716430664\n",
      "Current prediction:  62.32975387573242 \n",
      "\n",
      "Iteration 1392, Loss: 37.13027572631836, L1: 10.003188133239746, L3: 27.127086639404297\n",
      "Current prediction:  62.32681655883789 \n",
      "\n",
      "Iteration 1393, Loss: 36.436729431152344, L1: 9.997880935668945, L3: 26.43885040283203\n",
      "Current prediction:  62.321468353271484 \n",
      "\n",
      "Iteration 1394, Loss: 36.74421691894531, L1: 10.012473106384277, L3: 26.73174285888672\n",
      "Current prediction:  62.29378128051758 \n",
      "\n",
      "Iteration 1395, Loss: 35.374183654785156, L1: 9.97937297821045, L3: 25.39480972290039\n",
      "Current prediction:  62.1843147277832 \n",
      "\n",
      "Iteration 1396, Loss: 37.07750701904297, L1: 10.046143531799316, L3: 27.031362533569336\n",
      "Current prediction:  62.07500457763672 \n",
      "\n",
      "Iteration 1397, Loss: 36.903438568115234, L1: 10.06050968170166, L3: 26.842927932739258\n",
      "Current prediction:  61.95060348510742 \n",
      "\n",
      "Iteration 1398, Loss: 37.2308349609375, L1: 10.002580642700195, L3: 27.228252410888672\n",
      "Current prediction:  61.90198516845703 \n",
      "\n",
      "Iteration 1399, Loss: 36.51359939575195, L1: 10.052318572998047, L3: 26.461280822753906\n",
      "Current prediction:  61.87843322753906 \n",
      "\n",
      "Iteration 1400, Loss: 37.36213684082031, L1: 10.077790260314941, L3: 27.284347534179688\n",
      "Current prediction:  61.842674255371094 \n",
      "\n",
      "Iteration 1401, Loss: 36.912818908691406, L1: 10.040956497192383, L3: 26.871862411499023\n",
      "Current prediction:  61.84101486206055 \n",
      "\n",
      "Iteration 1402, Loss: 37.5836296081543, L1: 10.063177108764648, L3: 27.52045249938965\n",
      "Current prediction:  61.9123420715332 \n",
      "\n",
      "Iteration 1403, Loss: 36.562625885009766, L1: 10.044678688049316, L3: 26.517946243286133\n",
      "Current prediction:  62.101680755615234 \n",
      "\n",
      "Iteration 1404, Loss: 37.179039001464844, L1: 10.009032249450684, L3: 27.170005798339844\n",
      "Current prediction:  62.25101852416992 \n",
      "\n",
      "Iteration 1405, Loss: 36.052345275878906, L1: 10.02750015258789, L3: 26.02484703063965\n",
      "Current prediction:  62.28112030029297 \n",
      "\n",
      "Iteration 1406, Loss: 36.46534729003906, L1: 9.98220443725586, L3: 26.483142852783203\n",
      "Current prediction:  62.2839241027832 \n",
      "\n",
      "Iteration 1407, Loss: 38.09511184692383, L1: 10.013012886047363, L3: 28.08209991455078\n",
      "Current prediction:  62.301368713378906 \n",
      "\n",
      "Iteration 1408, Loss: 36.90094757080078, L1: 10.036887168884277, L3: 26.864059448242188\n",
      "Current prediction:  62.30559158325195 \n",
      "\n",
      "Iteration 1409, Loss: 37.00558853149414, L1: 10.10293197631836, L3: 26.90265655517578\n",
      "Current prediction:  62.3092155456543 \n",
      "\n",
      "Iteration 1410, Loss: 37.66637420654297, L1: 9.972681045532227, L3: 27.693693161010742\n",
      "Current prediction:  62.311790466308594 \n",
      "\n",
      "Iteration 1411, Loss: 37.043487548828125, L1: 9.97690200805664, L3: 27.066587448120117\n",
      "Current prediction:  62.3117790222168 \n",
      "\n",
      "Iteration 1412, Loss: 37.377803802490234, L1: 10.013298034667969, L3: 27.364505767822266\n",
      "Current prediction:  62.31145095825195 \n",
      "\n",
      "Iteration 1413, Loss: 36.67024230957031, L1: 9.983536720275879, L3: 26.68670654296875\n",
      "Current prediction:  62.310325622558594 \n",
      "\n",
      "Iteration 1414, Loss: 37.6324462890625, L1: 9.98670482635498, L3: 27.645740509033203\n",
      "Current prediction:  62.309085845947266 \n",
      "\n",
      "Iteration 1415, Loss: 37.691078186035156, L1: 10.010477066040039, L3: 27.680599212646484\n",
      "Current prediction:  62.30751419067383 \n",
      "\n",
      "Iteration 1416, Loss: 36.0069465637207, L1: 10.006172180175781, L3: 26.000774383544922\n",
      "Current prediction:  62.30644226074219 \n",
      "\n",
      "Iteration 1417, Loss: 36.68757247924805, L1: 10.01330280303955, L3: 26.67426872253418\n",
      "Current prediction:  62.3046760559082 \n",
      "\n",
      "Iteration 1418, Loss: 36.263031005859375, L1: 10.026756286621094, L3: 26.23627281188965\n",
      "Current prediction:  62.303165435791016 \n",
      "\n",
      "Iteration 1419, Loss: 37.88252639770508, L1: 10.006933212280273, L3: 27.875593185424805\n",
      "Current prediction:  62.301631927490234 \n",
      "\n",
      "Iteration 1420, Loss: 36.80100631713867, L1: 10.003796577453613, L3: 26.797208786010742\n",
      "Current prediction:  62.299774169921875 \n",
      "\n",
      "Iteration 1421, Loss: 36.514244079589844, L1: 10.003623008728027, L3: 26.510622024536133\n",
      "Current prediction:  62.29861068725586 \n",
      "\n",
      "Iteration 1422, Loss: 36.87330627441406, L1: 10.019607543945312, L3: 26.853696823120117\n",
      "Current prediction:  62.297977447509766 \n",
      "\n",
      "Iteration 1423, Loss: 36.453433990478516, L1: 10.025298118591309, L3: 26.428136825561523\n",
      "Current prediction:  62.297386169433594 \n",
      "\n",
      "Iteration 1424, Loss: 36.505104064941406, L1: 10.040096282958984, L3: 26.465009689331055\n",
      "Current prediction:  62.29669189453125 \n",
      "\n",
      "Iteration 1425, Loss: 36.730770111083984, L1: 10.010501861572266, L3: 26.72026824951172\n",
      "Current prediction:  62.296051025390625 \n",
      "\n",
      "Iteration 1426, Loss: 37.3308219909668, L1: 9.995059967041016, L3: 27.33576202392578\n",
      "Current prediction:  62.295921325683594 \n",
      "\n",
      "Iteration 1427, Loss: 37.249664306640625, L1: 10.00990104675293, L3: 27.239763259887695\n",
      "Current prediction:  62.295692443847656 \n",
      "\n",
      "Iteration 1428, Loss: 36.86396789550781, L1: 10.057597160339355, L3: 26.806371688842773\n",
      "Current prediction:  62.29566192626953 \n",
      "\n",
      "Iteration 1429, Loss: 36.62425231933594, L1: 10.035757064819336, L3: 26.588497161865234\n",
      "Current prediction:  62.29551315307617 \n",
      "\n",
      "Iteration 1430, Loss: 38.02798843383789, L1: 10.024337768554688, L3: 28.003650665283203\n",
      "Current prediction:  62.2952880859375 \n",
      "\n",
      "Iteration 1431, Loss: 36.65514373779297, L1: 10.02483081817627, L3: 26.630311965942383\n",
      "Current prediction:  62.29549026489258 \n",
      "\n",
      "Iteration 1432, Loss: 36.71087646484375, L1: 10.017129898071289, L3: 26.693744659423828\n",
      "Current prediction:  62.29570388793945 \n",
      "\n",
      "Iteration 1433, Loss: 37.210567474365234, L1: 10.052055358886719, L3: 27.158512115478516\n",
      "Current prediction:  62.29642868041992 \n",
      "\n",
      "Iteration 1434, Loss: 37.10114288330078, L1: 10.064896583557129, L3: 27.03624725341797\n",
      "Current prediction:  62.296630859375 \n",
      "\n",
      "Iteration 1435, Loss: 37.009639739990234, L1: 10.048575401306152, L3: 26.961063385009766\n",
      "Current prediction:  62.2972412109375 \n",
      "\n",
      "Iteration 1436, Loss: 36.87939453125, L1: 10.011781692504883, L3: 26.86761474609375\n",
      "Current prediction:  62.297637939453125 \n",
      "\n",
      "Iteration 1437, Loss: 37.45918273925781, L1: 10.054759979248047, L3: 27.404422760009766\n",
      "Current prediction:  62.296878814697266 \n",
      "\n",
      "Iteration 1438, Loss: 36.70269012451172, L1: 10.059151649475098, L3: 26.643537521362305\n",
      "Current prediction:  62.29386520385742 \n",
      "\n",
      "Iteration 1439, Loss: 36.328067779541016, L1: 10.013840675354004, L3: 26.314228057861328\n",
      "Current prediction:  62.2886848449707 \n",
      "\n",
      "Iteration 1440, Loss: 37.2385368347168, L1: 9.994220733642578, L3: 27.24431610107422\n",
      "Current prediction:  62.27769470214844 \n",
      "\n",
      "Iteration 1441, Loss: 37.053489685058594, L1: 10.0118408203125, L3: 27.041650772094727\n",
      "Current prediction:  62.234832763671875 \n",
      "\n",
      "Iteration 1442, Loss: 36.48309326171875, L1: 10.033857345581055, L3: 26.449237823486328\n",
      "Current prediction:  62.15581512451172 \n",
      "\n",
      "Iteration 1443, Loss: 37.090187072753906, L1: 10.016979217529297, L3: 27.073209762573242\n",
      "Current prediction:  62.06698226928711 \n",
      "\n",
      "Iteration 1444, Loss: 36.051090240478516, L1: 10.030820846557617, L3: 26.0202693939209\n",
      "Current prediction:  62.001380920410156 \n",
      "\n",
      "Iteration 1445, Loss: 36.784969329833984, L1: 9.97579288482666, L3: 26.80917739868164\n",
      "Current prediction:  62.07058334350586 \n",
      "\n",
      "Iteration 1446, Loss: 36.41101837158203, L1: 10.039535522460938, L3: 26.37148094177246\n",
      "Current prediction:  62.152061462402344 \n",
      "\n",
      "Iteration 1447, Loss: 36.50756072998047, L1: 10.014735221862793, L3: 26.49282455444336\n",
      "Current prediction:  62.20756149291992 \n",
      "\n",
      "Iteration 1448, Loss: 36.877220153808594, L1: 10.015945434570312, L3: 26.86127471923828\n",
      "Current prediction:  62.254905700683594 \n",
      "\n",
      "Iteration 1449, Loss: 36.25745391845703, L1: 9.986299514770508, L3: 26.27115249633789\n",
      "Current prediction:  62.27843475341797 \n",
      "\n",
      "Iteration 1450, Loss: 37.204803466796875, L1: 9.957472801208496, L3: 27.247331619262695\n",
      "Current prediction:  62.28056716918945 \n",
      "\n",
      "Iteration 1451, Loss: 37.078590393066406, L1: 9.989279747009277, L3: 27.089309692382812\n",
      "Current prediction:  62.28725814819336 \n",
      "\n",
      "Iteration 1452, Loss: 36.65951156616211, L1: 9.966626167297363, L3: 26.69288444519043\n",
      "Current prediction:  62.30121612548828 \n",
      "\n",
      "Iteration 1453, Loss: 35.863121032714844, L1: 9.972420692443848, L3: 25.89069938659668\n",
      "Current prediction:  62.30929946899414 \n",
      "\n",
      "Iteration 1454, Loss: 37.78342819213867, L1: 9.977705955505371, L3: 27.805721282958984\n",
      "Current prediction:  62.31411361694336 \n",
      "\n",
      "Iteration 1455, Loss: 36.783382415771484, L1: 9.983166694641113, L3: 26.800214767456055\n",
      "Current prediction:  62.3165283203125 \n",
      "\n",
      "Iteration 1456, Loss: 36.2062873840332, L1: 9.993617057800293, L3: 26.212671279907227\n",
      "Current prediction:  62.318687438964844 \n",
      "\n",
      "Iteration 1457, Loss: 36.10197830200195, L1: 9.939129829406738, L3: 26.16284942626953\n",
      "Current prediction:  62.32087326049805 \n",
      "\n",
      "Iteration 1458, Loss: 36.836875915527344, L1: 9.986927032470703, L3: 26.849950790405273\n",
      "Current prediction:  62.32208251953125 \n",
      "\n",
      "Iteration 1459, Loss: 36.449256896972656, L1: 9.99203872680664, L3: 26.457216262817383\n",
      "Current prediction:  62.32282257080078 \n",
      "\n",
      "Iteration 1460, Loss: 36.451133728027344, L1: 9.992324829101562, L3: 26.45880699157715\n",
      "Current prediction:  62.322322845458984 \n",
      "\n",
      "Iteration 1461, Loss: 36.83881759643555, L1: 10.018965721130371, L3: 26.81985092163086\n",
      "Current prediction:  62.31982421875 \n",
      "\n",
      "Iteration 1462, Loss: 37.22251510620117, L1: 9.974296569824219, L3: 27.248218536376953\n",
      "Current prediction:  62.31874465942383 \n",
      "\n",
      "Iteration 1463, Loss: 35.793426513671875, L1: 9.995473861694336, L3: 25.797950744628906\n",
      "Current prediction:  62.32036209106445 \n",
      "\n",
      "Iteration 1464, Loss: 37.071990966796875, L1: 9.999969482421875, L3: 27.072021484375\n",
      "Current prediction:  62.31683349609375 \n",
      "\n",
      "Iteration 1465, Loss: 37.58579635620117, L1: 10.033316612243652, L3: 27.552478790283203\n",
      "Current prediction:  62.31390380859375 \n",
      "\n",
      "Iteration 1466, Loss: 35.6558952331543, L1: 10.014741897583008, L3: 25.64115333557129\n",
      "Current prediction:  62.312538146972656 \n",
      "\n",
      "Iteration 1467, Loss: 36.33387756347656, L1: 9.995359420776367, L3: 26.338518142700195\n",
      "Current prediction:  62.30171203613281 \n",
      "\n",
      "Iteration 1468, Loss: 35.48666763305664, L1: 10.019503593444824, L3: 25.4671630859375\n",
      "Current prediction:  62.26652526855469 \n",
      "\n",
      "Iteration 1469, Loss: 35.79060363769531, L1: 9.987945556640625, L3: 25.80265998840332\n",
      "Current prediction:  62.168190002441406 \n",
      "\n",
      "Iteration 1470, Loss: 37.808197021484375, L1: 9.989261627197266, L3: 27.818937301635742\n",
      "Current prediction:  62.03679656982422 \n",
      "\n",
      "Iteration 1471, Loss: 37.69353485107422, L1: 9.996601104736328, L3: 27.696931838989258\n",
      "Current prediction:  61.9612922668457 \n",
      "\n",
      "Iteration 1472, Loss: 37.598731994628906, L1: 9.988801956176758, L3: 27.60993194580078\n",
      "Current prediction:  61.93074035644531 \n",
      "\n",
      "Iteration 1473, Loss: 37.12977981567383, L1: 10.007500648498535, L3: 27.122278213500977\n",
      "Current prediction:  61.96867370605469 \n",
      "\n",
      "Iteration 1474, Loss: 36.42631530761719, L1: 9.984613418579102, L3: 26.441699981689453\n",
      "Current prediction:  62.04904556274414 \n",
      "\n",
      "Iteration 1475, Loss: 36.153987884521484, L1: 9.978442192077637, L3: 26.175546646118164\n",
      "Current prediction:  62.15635681152344 \n",
      "\n",
      "Iteration 1476, Loss: 37.34154510498047, L1: 9.962533950805664, L3: 27.379013061523438\n",
      "Current prediction:  62.23984146118164 \n",
      "\n",
      "Iteration 1477, Loss: 37.456668853759766, L1: 9.947653770446777, L3: 27.509016036987305\n",
      "Current prediction:  62.2658805847168 \n",
      "\n",
      "Iteration 1478, Loss: 38.13970184326172, L1: 9.933897018432617, L3: 28.20580291748047\n",
      "Current prediction:  62.30617141723633 \n",
      "\n",
      "Iteration 1479, Loss: 36.02381134033203, L1: 9.9160795211792, L3: 26.107730865478516\n",
      "Current prediction:  62.34457015991211 \n",
      "\n",
      "Iteration 1480, Loss: 37.090797424316406, L1: 9.908987998962402, L3: 27.181808471679688\n",
      "Current prediction:  62.354244232177734 \n",
      "\n",
      "Iteration 1481, Loss: 35.46185302734375, L1: 9.915879249572754, L3: 25.54597282409668\n",
      "Current prediction:  62.357601165771484 \n",
      "\n",
      "Iteration 1482, Loss: 36.53870391845703, L1: 9.914199829101562, L3: 26.624502182006836\n",
      "Current prediction:  62.36162185668945 \n",
      "\n",
      "Iteration 1483, Loss: 36.141082763671875, L1: 9.931824684143066, L3: 26.209259033203125\n",
      "Current prediction:  62.3625373840332 \n",
      "\n",
      "Iteration 1484, Loss: 37.34113311767578, L1: 9.947650909423828, L3: 27.393484115600586\n",
      "Current prediction:  62.35898971557617 \n",
      "\n",
      "Iteration 1485, Loss: 36.23313522338867, L1: 9.877490043640137, L3: 26.35564422607422\n",
      "Current prediction:  62.35688018798828 \n",
      "\n",
      "Iteration 1486, Loss: 37.28245544433594, L1: 9.961381912231445, L3: 27.321075439453125\n",
      "Current prediction:  62.355892181396484 \n",
      "\n",
      "Iteration 1487, Loss: 36.18108367919922, L1: 9.942546844482422, L3: 26.238534927368164\n",
      "Current prediction:  62.35623550415039 \n",
      "\n",
      "Iteration 1488, Loss: 37.11936950683594, L1: 9.929946899414062, L3: 27.189424514770508\n",
      "Current prediction:  62.35836410522461 \n",
      "\n",
      "Iteration 1489, Loss: 36.8327751159668, L1: 9.94944953918457, L3: 26.883325576782227\n",
      "Current prediction:  62.355262756347656 \n",
      "\n",
      "Iteration 1490, Loss: 37.498199462890625, L1: 9.918251037597656, L3: 27.579946517944336\n",
      "Current prediction:  62.34886169433594 \n",
      "\n",
      "Iteration 1491, Loss: 37.287513732910156, L1: 9.94117546081543, L3: 27.346338272094727\n",
      "Current prediction:  62.33505630493164 \n",
      "\n",
      "Iteration 1492, Loss: 35.210514068603516, L1: 9.93816089630127, L3: 25.27235221862793\n",
      "Current prediction:  62.2811164855957 \n",
      "\n",
      "Iteration 1493, Loss: 36.8170166015625, L1: 9.960969924926758, L3: 26.856046676635742\n",
      "Current prediction:  62.15640640258789 \n",
      "\n",
      "Iteration 1494, Loss: 35.59613800048828, L1: 9.980812072753906, L3: 25.615327835083008\n",
      "Current prediction:  61.991050720214844 \n",
      "\n",
      "Iteration 1495, Loss: 36.02543258666992, L1: 9.951851844787598, L3: 26.073579788208008\n",
      "Current prediction:  61.97153854370117 \n",
      "\n",
      "Iteration 1496, Loss: 37.00115966796875, L1: 9.955082893371582, L3: 27.04607582092285\n",
      "Current prediction:  62.05501937866211 \n",
      "\n",
      "Iteration 1497, Loss: 37.11835479736328, L1: 9.984010696411133, L3: 27.13434600830078\n",
      "Current prediction:  62.145042419433594 \n",
      "\n",
      "Iteration 1498, Loss: 36.51535415649414, L1: 9.973002433776855, L3: 26.54235076904297\n",
      "Current prediction:  62.17395782470703 \n",
      "\n",
      "Iteration 1499, Loss: 36.35531997680664, L1: 9.987038612365723, L3: 26.3682804107666\n",
      "Current prediction:  62.18559265136719 \n",
      "\n",
      "Iteration 1500, Loss: 36.29456329345703, L1: 9.940705299377441, L3: 26.353858947753906\n",
      "Current prediction:  62.163570404052734 \n",
      "\n",
      "Iteration 1501, Loss: 36.374977111816406, L1: 9.974087715148926, L3: 26.400888442993164\n",
      "Current prediction:  62.23420715332031 \n",
      "\n",
      "Iteration 1502, Loss: 35.92188262939453, L1: 9.962868690490723, L3: 25.959014892578125\n",
      "Current prediction:  62.281097412109375 \n",
      "\n",
      "Iteration 1503, Loss: 36.250633239746094, L1: 9.95556926727295, L3: 26.29506492614746\n",
      "Current prediction:  62.300315856933594 \n",
      "\n",
      "Iteration 1504, Loss: 37.48967742919922, L1: 9.959263801574707, L3: 27.530412673950195\n",
      "Current prediction:  62.315799713134766 \n",
      "\n",
      "Iteration 1505, Loss: 36.55622100830078, L1: 9.949495315551758, L3: 26.606725692749023\n",
      "Current prediction:  62.32939910888672 \n",
      "\n",
      "Iteration 1506, Loss: 37.29508590698242, L1: 9.958379745483398, L3: 27.336706161499023\n",
      "Current prediction:  62.331966400146484 \n",
      "\n",
      "Iteration 1507, Loss: 36.70915985107422, L1: 9.942851066589355, L3: 26.76630973815918\n",
      "Current prediction:  62.32670211791992 \n",
      "\n",
      "Iteration 1508, Loss: 36.3129768371582, L1: 9.94738483428955, L3: 26.36559295654297\n",
      "Current prediction:  62.30405044555664 \n",
      "\n",
      "Iteration 1509, Loss: 36.58095169067383, L1: 9.927619934082031, L3: 26.653331756591797\n",
      "Current prediction:  62.29670333862305 \n",
      "\n",
      "Iteration 1510, Loss: 37.227012634277344, L1: 9.970855712890625, L3: 27.25615692138672\n",
      "Current prediction:  62.30377960205078 \n",
      "\n",
      "Iteration 1511, Loss: 37.340450286865234, L1: 9.952118873596191, L3: 27.388330459594727\n",
      "Current prediction:  62.267112731933594 \n",
      "\n",
      "Iteration 1512, Loss: 36.74335861206055, L1: 9.920422554016113, L3: 26.82293701171875\n",
      "Current prediction:  62.19137191772461 \n",
      "\n",
      "Iteration 1513, Loss: 36.818702697753906, L1: 9.956130027770996, L3: 26.862571716308594\n",
      "Current prediction:  62.07537841796875 \n",
      "\n",
      "Iteration 1514, Loss: 36.58052062988281, L1: 9.936532974243164, L3: 26.64398765563965\n",
      "Current prediction:  61.98046112060547 \n",
      "\n",
      "Iteration 1515, Loss: 36.53703689575195, L1: 9.95931339263916, L3: 26.57772445678711\n",
      "Current prediction:  61.9066047668457 \n",
      "\n",
      "Iteration 1516, Loss: 37.77959442138672, L1: 9.959312438964844, L3: 27.820281982421875\n",
      "Current prediction:  61.95003890991211 \n",
      "\n",
      "Iteration 1517, Loss: 36.22468948364258, L1: 10.000824928283691, L3: 26.223865509033203\n",
      "Current prediction:  62.10662078857422 \n",
      "\n",
      "Iteration 1518, Loss: 37.707923889160156, L1: 9.893163681030273, L3: 27.814762115478516\n",
      "Current prediction:  62.23824691772461 \n",
      "\n",
      "Iteration 1519, Loss: 36.209957122802734, L1: 9.924854278564453, L3: 26.28510284423828\n",
      "Current prediction:  62.31635665893555 \n",
      "\n",
      "Iteration 1520, Loss: 37.213172912597656, L1: 9.991333961486816, L3: 27.221837997436523\n",
      "Current prediction:  62.33185958862305 \n",
      "\n",
      "Iteration 1521, Loss: 36.935142517089844, L1: 9.897424697875977, L3: 27.037715911865234\n",
      "Current prediction:  62.333961486816406 \n",
      "\n",
      "Iteration 1522, Loss: 36.06708526611328, L1: 9.907005310058594, L3: 26.160079956054688\n",
      "Current prediction:  62.33384323120117 \n",
      "\n",
      "Iteration 1523, Loss: 36.871456146240234, L1: 9.957120895385742, L3: 26.914335250854492\n",
      "Current prediction:  62.33236312866211 \n",
      "\n",
      "Iteration 1524, Loss: 36.28831481933594, L1: 9.911609649658203, L3: 26.376707077026367\n",
      "Current prediction:  62.33000564575195 \n",
      "\n",
      "Iteration 1525, Loss: 36.44548034667969, L1: 9.949528694152832, L3: 26.495952606201172\n",
      "Current prediction:  62.326904296875 \n",
      "\n",
      "Iteration 1526, Loss: 36.47041702270508, L1: 9.924639701843262, L3: 26.545778274536133\n",
      "Current prediction:  62.323638916015625 \n",
      "\n",
      "Iteration 1527, Loss: 36.792015075683594, L1: 9.927322387695312, L3: 26.864694595336914\n",
      "Current prediction:  62.32064437866211 \n",
      "\n",
      "Iteration 1528, Loss: 36.627349853515625, L1: 9.9183988571167, L3: 26.70895004272461\n",
      "Current prediction:  62.317970275878906 \n",
      "\n",
      "Iteration 1529, Loss: 36.82283020019531, L1: 9.94839096069336, L3: 26.874441146850586\n",
      "Current prediction:  62.315303802490234 \n",
      "\n",
      "Iteration 1530, Loss: 36.10197448730469, L1: 9.922894477844238, L3: 26.179080963134766\n",
      "Current prediction:  62.312564849853516 \n",
      "\n",
      "Iteration 1531, Loss: 37.15067672729492, L1: 9.966448783874512, L3: 27.184228897094727\n",
      "Current prediction:  62.30966567993164 \n",
      "\n",
      "Iteration 1532, Loss: 36.0828857421875, L1: 9.979205131530762, L3: 26.103681564331055\n",
      "Current prediction:  62.3068733215332 \n",
      "\n",
      "Iteration 1533, Loss: 37.19676208496094, L1: 9.950167655944824, L3: 27.246593475341797\n",
      "Current prediction:  62.30401611328125 \n",
      "\n",
      "Iteration 1534, Loss: 37.920597076416016, L1: 9.95379638671875, L3: 27.966800689697266\n",
      "Current prediction:  62.300052642822266 \n",
      "\n",
      "Iteration 1535, Loss: 36.861751556396484, L1: 10.032340049743652, L3: 26.82941246032715\n",
      "Current prediction:  62.296573638916016 \n",
      "\n",
      "Iteration 1536, Loss: 36.818565368652344, L1: 9.944337844848633, L3: 26.87422752380371\n",
      "Current prediction:  62.29267501831055 \n",
      "\n",
      "Iteration 1537, Loss: 36.46582794189453, L1: 9.9637451171875, L3: 26.5020809173584\n",
      "Current prediction:  62.28936767578125 \n",
      "\n",
      "Iteration 1538, Loss: 36.54176712036133, L1: 9.98366641998291, L3: 26.5580997467041\n",
      "Current prediction:  62.286041259765625 \n",
      "\n",
      "Iteration 1539, Loss: 37.123199462890625, L1: 9.968171119689941, L3: 27.155027389526367\n",
      "Current prediction:  62.28265380859375 \n",
      "\n",
      "Iteration 1540, Loss: 36.725372314453125, L1: 9.973284721374512, L3: 26.752086639404297\n",
      "Current prediction:  62.27980422973633 \n",
      "\n",
      "Iteration 1541, Loss: 37.57294464111328, L1: 9.995985984802246, L3: 27.57695960998535\n",
      "Current prediction:  62.27572250366211 \n",
      "\n",
      "Iteration 1542, Loss: 36.418212890625, L1: 10.018424034118652, L3: 26.39978790283203\n",
      "Current prediction:  62.27231979370117 \n",
      "\n",
      "Iteration 1543, Loss: 36.72801971435547, L1: 10.00387191772461, L3: 26.724149703979492\n",
      "Current prediction:  62.27020263671875 \n",
      "\n",
      "Iteration 1544, Loss: 36.702308654785156, L1: 10.03902530670166, L3: 26.663284301757812\n",
      "Current prediction:  62.26945114135742 \n",
      "\n",
      "Iteration 1545, Loss: 36.40393829345703, L1: 10.001705169677734, L3: 26.40223503112793\n",
      "Current prediction:  62.26939392089844 \n",
      "\n",
      "Iteration 1546, Loss: 36.448387145996094, L1: 10.014970779418945, L3: 26.43341636657715\n",
      "Current prediction:  62.269954681396484 \n",
      "\n",
      "Iteration 1547, Loss: 36.59872055053711, L1: 10.018452644348145, L3: 26.58026885986328\n",
      "Current prediction:  62.27045440673828 \n",
      "\n",
      "Iteration 1548, Loss: 36.282936096191406, L1: 10.001283645629883, L3: 26.28165054321289\n",
      "Current prediction:  62.271949768066406 \n",
      "\n",
      "Iteration 1549, Loss: 36.90263366699219, L1: 10.013725280761719, L3: 26.888906478881836\n",
      "Current prediction:  62.27325439453125 \n",
      "\n",
      "Iteration 1550, Loss: 36.6245231628418, L1: 10.034573554992676, L3: 26.589950561523438\n",
      "Current prediction:  62.27423858642578 \n",
      "\n",
      "Iteration 1551, Loss: 36.022552490234375, L1: 10.005093574523926, L3: 26.017459869384766\n",
      "Current prediction:  62.27278518676758 \n",
      "\n",
      "Iteration 1552, Loss: 36.50695037841797, L1: 10.494485855102539, L3: 26.012466430664062\n",
      "Current prediction:  62.165740966796875 \n",
      "\n",
      "Iteration 1553, Loss: 39.085289001464844, L1: 10.881356239318848, L3: 28.20393180847168\n",
      "Current prediction:  61.43210220336914 \n",
      "\n",
      "Iteration 1554, Loss: 37.50767135620117, L1: 10.542219161987305, L3: 26.965452194213867\n",
      "Current prediction:  59.34392547607422 \n",
      "\n",
      "Iteration 1555, Loss: 39.690948486328125, L1: 11.524799346923828, L3: 28.166149139404297\n",
      "Current prediction:  59.35479736328125 \n",
      "\n",
      "Iteration 1556, Loss: 39.94800567626953, L1: 11.825955390930176, L3: 28.12204933166504\n",
      "Current prediction:  59.408355712890625 \n",
      "\n",
      "Iteration 1557, Loss: 38.51800537109375, L1: 11.154338836669922, L3: 27.36366844177246\n",
      "Current prediction:  59.88232421875 \n",
      "\n",
      "Iteration 1558, Loss: 38.63006591796875, L1: 11.141845703125, L3: 27.488218307495117\n",
      "Current prediction:  60.862430572509766 \n",
      "\n",
      "Iteration 1559, Loss: 39.001224517822266, L1: 11.061018943786621, L3: 27.94020652770996\n",
      "Current prediction:  61.0797233581543 \n",
      "\n",
      "Iteration 1560, Loss: 39.188228607177734, L1: 11.077081680297852, L3: 28.111146926879883\n",
      "Current prediction:  61.1255989074707 \n",
      "\n",
      "Iteration 1561, Loss: 38.0671501159668, L1: 10.959879875183105, L3: 27.107269287109375\n",
      "Current prediction:  61.16321563720703 \n",
      "\n",
      "Iteration 1562, Loss: 36.97407531738281, L1: 10.984631538391113, L3: 25.989442825317383\n",
      "Current prediction:  61.201717376708984 \n",
      "\n",
      "Iteration 1563, Loss: 37.42786407470703, L1: 10.913778305053711, L3: 26.514083862304688\n",
      "Current prediction:  61.240699768066406 \n",
      "\n",
      "Iteration 1564, Loss: 37.82753372192383, L1: 10.88208293914795, L3: 26.945449829101562\n",
      "Current prediction:  61.28002166748047 \n",
      "\n",
      "Iteration 1565, Loss: 36.88676452636719, L1: 10.950275421142578, L3: 25.936487197875977\n",
      "Current prediction:  61.31821060180664 \n",
      "\n",
      "Iteration 1566, Loss: 37.7653923034668, L1: 10.960518836975098, L3: 26.804874420166016\n",
      "Current prediction:  61.35514831542969 \n",
      "\n",
      "Iteration 1567, Loss: 36.55595016479492, L1: 10.930066108703613, L3: 25.625883102416992\n",
      "Current prediction:  61.390743255615234 \n",
      "\n",
      "Iteration 1568, Loss: 38.336387634277344, L1: 10.913187980651855, L3: 27.423200607299805\n",
      "Current prediction:  61.42390441894531 \n",
      "\n",
      "Iteration 1569, Loss: 37.86759948730469, L1: 11.084386825561523, L3: 26.783212661743164\n",
      "Current prediction:  61.45438766479492 \n",
      "\n",
      "Iteration 1570, Loss: 38.66120147705078, L1: 10.892425537109375, L3: 27.768775939941406\n",
      "Current prediction:  61.48263931274414 \n",
      "\n",
      "Iteration 1571, Loss: 36.95014572143555, L1: 10.827725410461426, L3: 26.122421264648438\n",
      "Current prediction:  61.508934020996094 \n",
      "\n",
      "Iteration 1572, Loss: 37.43575668334961, L1: 10.95373249053955, L3: 26.482025146484375\n",
      "Current prediction:  61.53346633911133 \n",
      "\n",
      "Iteration 1573, Loss: 37.66290283203125, L1: 11.08080768585205, L3: 26.582096099853516\n",
      "Current prediction:  61.555606842041016 \n",
      "\n",
      "Iteration 1574, Loss: 37.379966735839844, L1: 10.870987892150879, L3: 26.50897789001465\n",
      "Current prediction:  61.57529067993164 \n",
      "\n",
      "Iteration 1575, Loss: 37.08852767944336, L1: 11.034147262573242, L3: 26.054380416870117\n",
      "Current prediction:  61.593505859375 \n",
      "\n",
      "Iteration 1576, Loss: 36.92790603637695, L1: 11.201310157775879, L3: 25.726594924926758\n",
      "Current prediction:  61.61039352416992 \n",
      "\n",
      "Iteration 1577, Loss: 37.8080940246582, L1: 10.745104789733887, L3: 27.06298828125\n",
      "Current prediction:  61.625370025634766 \n",
      "\n",
      "Iteration 1578, Loss: 36.63137435913086, L1: 10.649740219116211, L3: 25.98163414001465\n",
      "Current prediction:  61.6392707824707 \n",
      "\n",
      "Iteration 1579, Loss: 36.98619842529297, L1: 10.61373519897461, L3: 26.37246322631836\n",
      "Current prediction:  61.65167236328125 \n",
      "\n",
      "Iteration 1580, Loss: 37.565528869628906, L1: 10.803311347961426, L3: 26.762218475341797\n",
      "Current prediction:  61.662288665771484 \n",
      "\n",
      "Iteration 1581, Loss: 36.97649002075195, L1: 10.356057167053223, L3: 26.620433807373047\n",
      "Current prediction:  61.672088623046875 \n",
      "\n",
      "Iteration 1582, Loss: 36.17662811279297, L1: 10.671021461486816, L3: 25.505605697631836\n",
      "Current prediction:  61.68172073364258 \n",
      "\n",
      "Iteration 1583, Loss: 36.650779724121094, L1: 10.435653686523438, L3: 26.21512794494629\n",
      "Current prediction:  61.69138717651367 \n",
      "\n",
      "Iteration 1584, Loss: 36.816673278808594, L1: 10.426337242126465, L3: 26.390335083007812\n",
      "Current prediction:  61.70044708251953 \n",
      "\n",
      "Iteration 1585, Loss: 36.73843002319336, L1: 10.413818359375, L3: 26.32461166381836\n",
      "Current prediction:  61.70755386352539 \n",
      "\n",
      "Iteration 1586, Loss: 37.18288803100586, L1: 10.423964500427246, L3: 26.758922576904297\n",
      "Current prediction:  61.70332336425781 \n",
      "\n",
      "Iteration 1587, Loss: 37.88392639160156, L1: 10.793837547302246, L3: 27.090087890625\n",
      "Current prediction:  61.65148162841797 \n",
      "\n",
      "Iteration 1588, Loss: 36.855289459228516, L1: 10.425458908081055, L3: 26.42983055114746\n",
      "Current prediction:  61.53558349609375 \n",
      "\n",
      "Iteration 1589, Loss: 38.299373626708984, L1: 11.01940631866455, L3: 27.279966354370117\n",
      "Current prediction:  61.37104415893555 \n",
      "\n",
      "Iteration 1590, Loss: 37.70777893066406, L1: 10.43198013305664, L3: 27.275800704956055\n",
      "Current prediction:  61.403865814208984 \n",
      "\n",
      "Iteration 1591, Loss: 36.805625915527344, L1: 10.36609172821045, L3: 26.43953514099121\n",
      "Current prediction:  61.47403335571289 \n",
      "\n",
      "Iteration 1592, Loss: 37.89337921142578, L1: 10.358814239501953, L3: 27.534563064575195\n",
      "Current prediction:  61.54222106933594 \n",
      "\n",
      "Iteration 1593, Loss: 37.34756851196289, L1: 10.342717170715332, L3: 27.004850387573242\n",
      "Current prediction:  61.568519592285156 \n",
      "\n",
      "Iteration 1594, Loss: 36.85758972167969, L1: 10.290872573852539, L3: 26.566715240478516\n",
      "Current prediction:  61.43864440917969 \n",
      "\n",
      "Iteration 1595, Loss: 37.93147659301758, L1: 10.146842002868652, L3: 27.78463363647461\n",
      "Current prediction:  61.50031661987305 \n",
      "\n",
      "Iteration 1596, Loss: 36.449195861816406, L1: 10.005908012390137, L3: 26.443286895751953\n",
      "Current prediction:  61.387855529785156 \n",
      "\n",
      "Iteration 1597, Loss: 39.061309814453125, L1: 12.820751190185547, L3: 26.240558624267578\n",
      "Current prediction:  61.20970916748047 \n",
      "\n",
      "Iteration 1598, Loss: 36.252952575683594, L1: 9.980850219726562, L3: 26.2721004486084\n",
      "Current prediction:  61.00471496582031 \n",
      "\n",
      "Iteration 1599, Loss: 37.93397903442383, L1: 10.356834411621094, L3: 27.577144622802734\n",
      "Current prediction:  60.83102035522461 \n",
      "\n",
      "Iteration 1600, Loss: 37.78714370727539, L1: 10.87349796295166, L3: 26.913646697998047\n",
      "Current prediction:  60.37870407104492 \n",
      "\n",
      "Iteration 1601, Loss: 38.74192810058594, L1: 11.812445640563965, L3: 26.929481506347656\n",
      "Current prediction:  58.97475814819336 \n",
      "\n",
      "Iteration 1602, Loss: 41.72212219238281, L1: 13.10313606262207, L3: 28.61898422241211\n",
      "Current prediction:  59.49589157104492 \n",
      "\n",
      "Iteration 1603, Loss: 44.00088882446289, L1: 15.061690330505371, L3: 28.939197540283203\n",
      "Current prediction:  60.271728515625 \n",
      "\n",
      "Iteration 1604, Loss: 40.43144989013672, L1: 13.480384826660156, L3: 26.95106315612793\n",
      "Current prediction:  60.40559005737305 \n",
      "\n",
      "Iteration 1605, Loss: 43.40325164794922, L1: 15.474824905395508, L3: 27.928424835205078\n",
      "Current prediction:  60.43996810913086 \n",
      "\n",
      "Iteration 1606, Loss: 40.82138442993164, L1: 12.557753562927246, L3: 28.26363182067871\n",
      "Current prediction:  60.47515106201172 \n",
      "\n",
      "Iteration 1607, Loss: 40.14888000488281, L1: 12.80573558807373, L3: 27.343143463134766\n",
      "Current prediction:  60.51438522338867 \n",
      "\n",
      "Iteration 1608, Loss: 40.24489212036133, L1: 13.030047416687012, L3: 27.21484375\n",
      "Current prediction:  60.557437896728516 \n",
      "\n",
      "Iteration 1609, Loss: 40.010093688964844, L1: 12.177507400512695, L3: 27.83258819580078\n",
      "Current prediction:  60.60357666015625 \n",
      "\n",
      "Iteration 1610, Loss: 41.25177001953125, L1: 11.880172729492188, L3: 29.371599197387695\n",
      "Current prediction:  58.65250778198242 \n",
      "\n",
      "Iteration 1611, Loss: 42.55009841918945, L1: 12.365325927734375, L3: 30.184772491455078\n",
      "Current prediction:  58.70586395263672 \n",
      "\n",
      "Iteration 1612, Loss: 43.19750213623047, L1: 12.945791244506836, L3: 30.251712799072266\n",
      "Current prediction:  58.76594543457031 \n",
      "\n",
      "Iteration 1613, Loss: 46.91599655151367, L1: 15.828612327575684, L3: 31.087385177612305\n",
      "Current prediction:  58.832069396972656 \n",
      "\n",
      "Iteration 1614, Loss: 43.3819694519043, L1: 14.037226676940918, L3: 29.344743728637695\n",
      "Current prediction:  58.903507232666016 \n",
      "\n",
      "Iteration 1615, Loss: 45.15406036376953, L1: 14.564212799072266, L3: 30.589847564697266\n",
      "Current prediction:  58.97953796386719 \n",
      "\n",
      "Iteration 1616, Loss: 45.39284133911133, L1: 14.607003211975098, L3: 30.785837173461914\n",
      "Current prediction:  59.060272216796875 \n",
      "\n",
      "Iteration 1617, Loss: 44.66227722167969, L1: 14.659842491149902, L3: 30.00243377685547\n",
      "Current prediction:  59.14414596557617 \n",
      "\n",
      "Iteration 1618, Loss: 44.70549392700195, L1: 15.11801528930664, L3: 29.587478637695312\n",
      "Current prediction:  59.2305908203125 \n",
      "\n",
      "Iteration 1619, Loss: 43.69200134277344, L1: 15.703634262084961, L3: 27.98836898803711\n",
      "Current prediction:  59.318790435791016 \n",
      "\n",
      "Iteration 1620, Loss: 45.10491180419922, L1: 15.822128295898438, L3: 29.28278160095215\n",
      "Current prediction:  59.407958984375 \n",
      "\n",
      "Iteration 1621, Loss: 43.35198974609375, L1: 14.206779479980469, L3: 29.14521026611328\n",
      "Current prediction:  59.49828338623047 \n",
      "\n",
      "Iteration 1622, Loss: 41.524200439453125, L1: 14.201379776000977, L3: 27.322818756103516\n",
      "Current prediction:  59.58893585205078 \n",
      "\n",
      "Iteration 1623, Loss: 41.67829895019531, L1: 13.483863830566406, L3: 28.194435119628906\n",
      "Current prediction:  59.67925262451172 \n",
      "\n",
      "Iteration 1624, Loss: 39.761962890625, L1: 12.272648811340332, L3: 27.48931312561035\n",
      "Current prediction:  59.76966094970703 \n",
      "\n",
      "Iteration 1625, Loss: 39.73915100097656, L1: 12.1659517288208, L3: 27.573200225830078\n",
      "Current prediction:  59.860267639160156 \n",
      "\n",
      "Iteration 1626, Loss: 39.45079040527344, L1: 12.109810829162598, L3: 27.340980529785156\n",
      "Current prediction:  59.950927734375 \n",
      "\n",
      "Iteration 1627, Loss: 40.94778060913086, L1: 11.978907585144043, L3: 28.9688720703125\n",
      "Current prediction:  60.04118728637695 \n",
      "\n",
      "Iteration 1628, Loss: 39.68052673339844, L1: 11.728584289550781, L3: 27.95194435119629\n",
      "Current prediction:  60.129981994628906 \n",
      "\n",
      "Iteration 1629, Loss: 39.82176208496094, L1: 11.753837585449219, L3: 28.067922592163086\n",
      "Current prediction:  60.217899322509766 \n",
      "\n",
      "Iteration 1630, Loss: 39.77239227294922, L1: 11.695712089538574, L3: 28.07668113708496\n",
      "Current prediction:  60.30394744873047 \n",
      "\n",
      "Iteration 1631, Loss: 38.04525375366211, L1: 11.663066864013672, L3: 26.382186889648438\n",
      "Current prediction:  60.38736343383789 \n",
      "\n",
      "Iteration 1632, Loss: 38.18520736694336, L1: 11.596293449401855, L3: 26.58891487121582\n",
      "Current prediction:  60.46885681152344 \n",
      "\n",
      "Iteration 1633, Loss: 39.04620361328125, L1: 11.598393440246582, L3: 27.44780921936035\n",
      "Current prediction:  60.547035217285156 \n",
      "\n",
      "Iteration 1634, Loss: 37.60161209106445, L1: 11.754615783691406, L3: 25.846996307373047\n",
      "Current prediction:  60.62180709838867 \n",
      "\n",
      "Iteration 1635, Loss: 38.33004379272461, L1: 11.872949600219727, L3: 26.457094192504883\n",
      "Current prediction:  60.69306182861328 \n",
      "\n",
      "Iteration 1636, Loss: 37.88232421875, L1: 11.863961219787598, L3: 26.01836395263672\n",
      "Current prediction:  60.76092529296875 \n",
      "\n",
      "Iteration 1637, Loss: 38.84095001220703, L1: 11.761177062988281, L3: 27.079774856567383\n",
      "Current prediction:  60.82475280761719 \n",
      "\n",
      "Iteration 1638, Loss: 36.73732376098633, L1: 12.182502746582031, L3: 24.554821014404297\n",
      "Current prediction:  60.88386154174805 \n",
      "\n",
      "Iteration 1639, Loss: 39.13831329345703, L1: 12.429832458496094, L3: 26.708478927612305\n",
      "Current prediction:  60.93836212158203 \n",
      "\n",
      "Iteration 1640, Loss: 37.31950759887695, L1: 11.596905708312988, L3: 25.72260093688965\n",
      "Current prediction:  60.98954772949219 \n",
      "\n",
      "Iteration 1641, Loss: 37.73019027709961, L1: 11.646418571472168, L3: 26.083772659301758\n",
      "Current prediction:  61.03691482543945 \n",
      "\n",
      "Iteration 1642, Loss: 37.25263977050781, L1: 11.762027740478516, L3: 25.490612030029297\n",
      "Current prediction:  61.08065414428711 \n",
      "\n",
      "Iteration 1643, Loss: 36.32067108154297, L1: 10.873159408569336, L3: 25.447513580322266\n",
      "Current prediction:  61.12102508544922 \n",
      "\n",
      "Iteration 1644, Loss: 36.9177360534668, L1: 11.058492660522461, L3: 25.859243392944336\n",
      "Current prediction:  61.15791702270508 \n",
      "\n",
      "Iteration 1645, Loss: 36.82538604736328, L1: 10.611222267150879, L3: 26.21416473388672\n",
      "Current prediction:  61.19148254394531 \n",
      "\n",
      "Iteration 1646, Loss: 36.533180236816406, L1: 10.769804954528809, L3: 25.763376235961914\n",
      "Current prediction:  61.221595764160156 \n",
      "\n",
      "Iteration 1647, Loss: 35.98675537109375, L1: 10.528883934020996, L3: 25.45787239074707\n",
      "Current prediction:  61.24901580810547 \n",
      "\n",
      "Iteration 1648, Loss: 36.9107780456543, L1: 10.690118789672852, L3: 26.220659255981445\n",
      "Current prediction:  61.27362060546875 \n",
      "\n",
      "Iteration 1649, Loss: 36.27117919921875, L1: 10.484989166259766, L3: 25.78618812561035\n",
      "Current prediction:  61.29582977294922 \n",
      "\n",
      "Iteration 1650, Loss: 36.056602478027344, L1: 10.466143608093262, L3: 25.5904598236084\n",
      "Current prediction:  61.315547943115234 \n",
      "\n",
      "Iteration 1651, Loss: 36.06367492675781, L1: 10.45416259765625, L3: 25.609514236450195\n",
      "Current prediction:  61.332977294921875 \n",
      "\n",
      "Iteration 1652, Loss: 36.954368591308594, L1: 10.434638977050781, L3: 26.519731521606445\n",
      "Current prediction:  61.34791564941406 \n",
      "\n",
      "Iteration 1653, Loss: 35.406455993652344, L1: 10.421963691711426, L3: 24.9844913482666\n",
      "Current prediction:  61.3604736328125 \n",
      "\n",
      "Iteration 1654, Loss: 36.484657287597656, L1: 10.411304473876953, L3: 26.07335090637207\n",
      "Current prediction:  61.370948791503906 \n",
      "\n",
      "Iteration 1655, Loss: 36.488651275634766, L1: 10.402419090270996, L3: 26.086231231689453\n",
      "Current prediction:  61.3787956237793 \n",
      "\n",
      "Iteration 1656, Loss: 37.01286315917969, L1: 10.395758628845215, L3: 26.617103576660156\n",
      "Current prediction:  61.38574981689453 \n",
      "\n",
      "Iteration 1657, Loss: 37.254234313964844, L1: 10.38985824584961, L3: 26.864377975463867\n",
      "Current prediction:  61.39026641845703 \n",
      "\n",
      "Iteration 1658, Loss: 36.884117126464844, L1: 10.386024475097656, L3: 26.49809455871582\n",
      "Current prediction:  61.39295959472656 \n",
      "\n",
      "Iteration 1659, Loss: 36.89297103881836, L1: 10.383737564086914, L3: 26.509233474731445\n",
      "Current prediction:  61.39480972290039 \n",
      "\n",
      "Iteration 1660, Loss: 36.73637390136719, L1: 10.382163047790527, L3: 26.354209899902344\n",
      "Current prediction:  61.395652770996094 \n",
      "\n",
      "Iteration 1661, Loss: 36.25387191772461, L1: 10.381454467773438, L3: 25.872417449951172\n",
      "Current prediction:  61.395328521728516 \n",
      "\n",
      "Iteration 1662, Loss: 36.526161193847656, L1: 10.381730079650879, L3: 26.144432067871094\n",
      "Current prediction:  61.39460372924805 \n",
      "\n",
      "Iteration 1663, Loss: 37.9937629699707, L1: 10.382340431213379, L3: 27.611421585083008\n",
      "Current prediction:  61.3920783996582 \n",
      "\n",
      "Iteration 1664, Loss: 37.47861862182617, L1: 10.38448715209961, L3: 27.094131469726562\n",
      "Current prediction:  61.3882942199707 \n",
      "\n",
      "Iteration 1665, Loss: 37.3935432434082, L1: 10.387700080871582, L3: 27.005844116210938\n",
      "Current prediction:  61.38383102416992 \n",
      "\n",
      "Iteration 1666, Loss: 37.619773864746094, L1: 10.391487121582031, L3: 27.228286743164062\n",
      "Current prediction:  61.37916564941406 \n",
      "\n",
      "Iteration 1667, Loss: 36.493526458740234, L1: 10.39544677734375, L3: 26.098079681396484\n",
      "Current prediction:  61.37397384643555 \n",
      "\n",
      "Iteration 1668, Loss: 36.26923370361328, L1: 10.399848937988281, L3: 25.869386672973633\n",
      "Current prediction:  61.36847686767578 \n",
      "\n",
      "Iteration 1669, Loss: 37.65470886230469, L1: 10.963933944702148, L3: 26.69077491760254\n",
      "Current prediction:  61.36216735839844 \n",
      "\n",
      "Iteration 1670, Loss: 37.27809143066406, L1: 10.409868240356445, L3: 26.86822509765625\n",
      "Current prediction:  61.35616683959961 \n",
      "\n",
      "Iteration 1671, Loss: 36.23222732543945, L1: 10.414960861206055, L3: 25.8172664642334\n",
      "Current prediction:  61.34956741333008 \n",
      "\n",
      "Iteration 1672, Loss: 36.365882873535156, L1: 10.420560836791992, L3: 25.945323944091797\n",
      "Current prediction:  61.34312057495117 \n",
      "\n",
      "Iteration 1673, Loss: 35.774593353271484, L1: 10.42603588104248, L3: 25.34855842590332\n",
      "Current prediction:  61.336734771728516 \n",
      "\n",
      "Iteration 1674, Loss: 36.37078857421875, L1: 10.43144702911377, L3: 25.939342498779297\n",
      "Current prediction:  61.33001708984375 \n",
      "\n",
      "Iteration 1675, Loss: 36.32298278808594, L1: 10.437151908874512, L3: 25.88582992553711\n",
      "Current prediction:  61.3235969543457 \n",
      "\n",
      "Iteration 1676, Loss: 37.15879821777344, L1: 10.442596435546875, L3: 26.716203689575195\n",
      "Current prediction:  61.31703186035156 \n",
      "\n",
      "Iteration 1677, Loss: 36.716304779052734, L1: 10.448159217834473, L3: 26.268144607543945\n",
      "Current prediction:  61.31026840209961 \n",
      "\n",
      "Iteration 1678, Loss: 36.806663513183594, L1: 10.45390796661377, L3: 26.35275650024414\n",
      "Current prediction:  61.304317474365234 \n",
      "\n",
      "Iteration 1679, Loss: 37.15336227416992, L1: 10.458950996398926, L3: 26.694412231445312\n",
      "Current prediction:  61.29821014404297 \n",
      "\n",
      "Iteration 1680, Loss: 37.485721588134766, L1: 10.464133262634277, L3: 27.021587371826172\n",
      "Current prediction:  61.29145050048828 \n",
      "\n",
      "Iteration 1681, Loss: 36.707481384277344, L1: 10.469868659973145, L3: 26.237611770629883\n",
      "Current prediction:  61.28504180908203 \n",
      "\n",
      "Iteration 1682, Loss: 36.37836456298828, L1: 10.475303649902344, L3: 25.90306282043457\n",
      "Current prediction:  61.2789192199707 \n",
      "\n",
      "Iteration 1683, Loss: 36.69252014160156, L1: 10.479168891906738, L3: 26.21335220336914\n",
      "Current prediction:  61.27311325073242 \n",
      "\n",
      "Iteration 1684, Loss: 36.07639694213867, L1: 10.485424995422363, L3: 25.590970993041992\n",
      "Current prediction:  61.26747131347656 \n",
      "\n",
      "Iteration 1685, Loss: 36.198577880859375, L1: 10.490206718444824, L3: 25.708372116088867\n",
      "Current prediction:  61.26148986816406 \n",
      "\n",
      "Iteration 1686, Loss: 36.73163986206055, L1: 10.495284080505371, L3: 26.236356735229492\n",
      "Current prediction:  61.25581359863281 \n",
      "\n",
      "Iteration 1687, Loss: 36.12454605102539, L1: 10.500094413757324, L3: 25.624452590942383\n",
      "Current prediction:  61.2505989074707 \n",
      "\n",
      "Iteration 1688, Loss: 36.46971893310547, L1: 10.504517555236816, L3: 25.96520233154297\n",
      "Current prediction:  61.245582580566406 \n",
      "\n",
      "Iteration 1689, Loss: 37.842811584472656, L1: 10.508771896362305, L3: 27.33403778076172\n",
      "Current prediction:  61.2412109375 \n",
      "\n",
      "Iteration 1690, Loss: 36.568477630615234, L1: 10.512481689453125, L3: 26.05599594116211\n",
      "Current prediction:  61.23686599731445 \n",
      "\n",
      "Iteration 1691, Loss: 35.712158203125, L1: 10.51616096496582, L3: 25.19599723815918\n",
      "Current prediction:  61.23268508911133 \n",
      "\n",
      "Iteration 1692, Loss: 36.69316864013672, L1: 10.519712448120117, L3: 26.17345428466797\n",
      "Current prediction:  61.22902297973633 \n",
      "\n",
      "Iteration 1693, Loss: 36.15617370605469, L1: 10.522838592529297, L3: 25.63333511352539\n",
      "Current prediction:  61.22581481933594 \n",
      "\n",
      "Iteration 1694, Loss: 35.84352111816406, L1: 10.52553653717041, L3: 25.317983627319336\n",
      "Current prediction:  61.2231559753418 \n",
      "\n",
      "Iteration 1695, Loss: 36.17607498168945, L1: 10.527792930603027, L3: 25.648283004760742\n",
      "Current prediction:  61.22064208984375 \n",
      "\n",
      "Iteration 1696, Loss: 36.6625862121582, L1: 10.529923439025879, L3: 26.132661819458008\n",
      "Current prediction:  61.218971252441406 \n",
      "\n",
      "Iteration 1697, Loss: 36.24193572998047, L1: 10.531338691711426, L3: 25.71059799194336\n",
      "Current prediction:  61.217350006103516 \n",
      "\n",
      "Iteration 1698, Loss: 35.67914581298828, L1: 10.532487869262695, L3: 25.146656036376953\n",
      "Current prediction:  61.21644973754883 \n",
      "\n",
      "Iteration 1699, Loss: 36.2566032409668, L1: 10.533476829528809, L3: 25.723127365112305\n",
      "Current prediction:  61.21553421020508 \n",
      "\n",
      "Iteration 1700, Loss: 36.13992691040039, L1: 10.534259796142578, L3: 25.605667114257812\n",
      "Current prediction:  61.21457290649414 \n",
      "\n",
      "Iteration 1701, Loss: 37.182037353515625, L1: 10.535069465637207, L3: 26.646968841552734\n",
      "Current prediction:  61.212825775146484 \n",
      "\n",
      "Iteration 1702, Loss: 36.039085388183594, L1: 10.53654956817627, L3: 25.50253677368164\n",
      "Current prediction:  61.211490631103516 \n",
      "\n",
      "Iteration 1703, Loss: 36.59279251098633, L1: 10.537680625915527, L3: 26.055110931396484\n",
      "Current prediction:  61.210418701171875 \n",
      "\n",
      "Iteration 1704, Loss: 37.68218994140625, L1: 10.538589477539062, L3: 27.143600463867188\n",
      "Current prediction:  61.20933151245117 \n",
      "\n",
      "Iteration 1705, Loss: 36.541900634765625, L1: 10.539512634277344, L3: 26.00238800048828\n",
      "Current prediction:  61.20848083496094 \n",
      "\n",
      "Iteration 1706, Loss: 36.66611099243164, L1: 10.540233612060547, L3: 26.125877380371094\n",
      "Current prediction:  61.207862854003906 \n",
      "\n",
      "Iteration 1707, Loss: 36.29623794555664, L1: 10.54076099395752, L3: 25.755477905273438\n",
      "Current prediction:  61.20667266845703 \n",
      "\n",
      "Iteration 1708, Loss: 36.948795318603516, L1: 10.541769981384277, L3: 26.407024383544922\n",
      "Current prediction:  61.20545196533203 \n",
      "\n",
      "Iteration 1709, Loss: 36.89480209350586, L1: 10.542805671691895, L3: 26.35199737548828\n",
      "Current prediction:  61.2038688659668 \n",
      "\n",
      "Iteration 1710, Loss: 36.23036193847656, L1: 10.544143676757812, L3: 25.686216354370117\n",
      "Current prediction:  61.202762603759766 \n",
      "\n",
      "Iteration 1711, Loss: 36.19341278076172, L1: 10.543111801147461, L3: 25.650299072265625\n",
      "Current prediction:  61.201358795166016 \n",
      "\n",
      "Iteration 1712, Loss: 37.20966720581055, L1: 10.546274185180664, L3: 26.663393020629883\n",
      "Current prediction:  61.20010757446289 \n",
      "\n",
      "Iteration 1713, Loss: 37.18389129638672, L1: 10.547332763671875, L3: 26.63655662536621\n",
      "Current prediction:  61.19875717163086 \n",
      "\n",
      "Iteration 1714, Loss: 37.13618087768555, L1: 10.548476219177246, L3: 26.587703704833984\n",
      "Current prediction:  61.19789505004883 \n",
      "\n",
      "Iteration 1715, Loss: 36.30820846557617, L1: 10.549210548400879, L3: 25.758996963500977\n",
      "Current prediction:  61.19743728637695 \n",
      "\n",
      "Iteration 1716, Loss: 35.767215728759766, L1: 10.549601554870605, L3: 25.217613220214844\n",
      "Current prediction:  61.19712829589844 \n",
      "\n",
      "Iteration 1717, Loss: 37.02006530761719, L1: 10.5498628616333, L3: 26.470203399658203\n",
      "Current prediction:  61.19641876220703 \n",
      "\n",
      "Iteration 1718, Loss: 35.74431610107422, L1: 10.550468444824219, L3: 25.193849563598633\n",
      "Current prediction:  61.19621276855469 \n",
      "\n",
      "Iteration 1719, Loss: 37.229434967041016, L1: 10.55063247680664, L3: 26.678802490234375\n",
      "Current prediction:  61.19607925415039 \n",
      "\n",
      "Iteration 1720, Loss: 37.64605712890625, L1: 10.550749778747559, L3: 27.095306396484375\n",
      "Current prediction:  61.19639205932617 \n",
      "\n",
      "Iteration 1721, Loss: 36.65742492675781, L1: 10.55048656463623, L3: 26.106937408447266\n",
      "Current prediction:  61.19607925415039 \n",
      "\n",
      "Iteration 1722, Loss: 36.841644287109375, L1: 10.550749778747559, L3: 26.2908935546875\n",
      "Current prediction:  61.19669723510742 \n",
      "\n",
      "Iteration 1723, Loss: 37.42111587524414, L1: 10.552999496459961, L3: 26.86811637878418\n",
      "Current prediction:  61.19771194458008 \n",
      "\n",
      "Iteration 1724, Loss: 37.764122009277344, L1: 10.549362182617188, L3: 27.214759826660156\n",
      "Current prediction:  61.197757720947266 \n",
      "\n",
      "Iteration 1725, Loss: 36.70817565917969, L1: 10.562789916992188, L3: 26.145383834838867\n",
      "Current prediction:  61.19794845581055 \n",
      "\n",
      "Iteration 1726, Loss: 36.27523422241211, L1: 10.549165725708008, L3: 25.7260684967041\n",
      "Current prediction:  61.198028564453125 \n",
      "\n",
      "Iteration 1727, Loss: 37.88958740234375, L1: 10.549098014831543, L3: 27.34048843383789\n",
      "Current prediction:  61.19748306274414 \n",
      "\n",
      "Iteration 1728, Loss: 36.73237609863281, L1: 10.549562454223633, L3: 26.182815551757812\n",
      "Current prediction:  61.19709777832031 \n",
      "\n",
      "Iteration 1729, Loss: 36.67591094970703, L1: 10.549880981445312, L3: 26.12602996826172\n",
      "Current prediction:  61.196571350097656 \n",
      "\n",
      "Iteration 1730, Loss: 37.17760467529297, L1: 10.550331115722656, L3: 26.627275466918945\n",
      "Current prediction:  61.1955451965332 \n",
      "\n",
      "Iteration 1731, Loss: 36.4796257019043, L1: 10.551199913024902, L3: 25.928424835205078\n",
      "Current prediction:  61.19530487060547 \n",
      "\n",
      "Iteration 1732, Loss: 37.16031265258789, L1: 10.551407814025879, L3: 26.608905792236328\n",
      "Current prediction:  61.19426345825195 \n",
      "\n",
      "Iteration 1733, Loss: 36.54671096801758, L1: 10.552289009094238, L3: 25.994421005249023\n",
      "Current prediction:  61.19393539428711 \n",
      "\n",
      "Iteration 1734, Loss: 36.85047912597656, L1: 10.552568435668945, L3: 26.29791259765625\n",
      "Current prediction:  61.19389724731445 \n",
      "\n",
      "Iteration 1735, Loss: 36.36256408691406, L1: 10.55259895324707, L3: 25.809967041015625\n",
      "Current prediction:  61.193931579589844 \n",
      "\n",
      "Iteration 1736, Loss: 37.26219940185547, L1: 10.552572250366211, L3: 26.709625244140625\n",
      "Current prediction:  61.19388198852539 \n",
      "\n",
      "Iteration 1737, Loss: 36.43891143798828, L1: 10.552614212036133, L3: 25.88629722595215\n",
      "Current prediction:  61.19425582885742 \n",
      "\n",
      "Iteration 1738, Loss: 37.01927947998047, L1: 10.552292823791504, L3: 26.46698760986328\n",
      "Current prediction:  61.19443130493164 \n",
      "\n",
      "Iteration 1739, Loss: 36.21834945678711, L1: 10.552153587341309, L3: 25.666194915771484\n",
      "Current prediction:  61.19553756713867 \n",
      "\n",
      "Iteration 1740, Loss: 36.25074768066406, L1: 10.551206588745117, L3: 25.699539184570312\n",
      "Current prediction:  61.196659088134766 \n",
      "\n",
      "Iteration 1741, Loss: 36.31892395019531, L1: 10.550260543823242, L3: 25.76866340637207\n",
      "Current prediction:  61.197998046875 \n",
      "\n",
      "Iteration 1742, Loss: 37.67390060424805, L1: 10.549124717712402, L3: 27.124774932861328\n",
      "Current prediction:  61.19895553588867 \n",
      "\n",
      "Iteration 1743, Loss: 36.303070068359375, L1: 10.548311233520508, L3: 25.754758834838867\n",
      "Current prediction:  61.19961166381836 \n",
      "\n",
      "Iteration 1744, Loss: 36.5722770690918, L1: 10.547759056091309, L3: 26.024518966674805\n",
      "Current prediction:  61.199729919433594 \n",
      "\n",
      "Iteration 1745, Loss: 36.75917053222656, L1: 10.54765510559082, L3: 26.211515426635742\n",
      "Current prediction:  61.19987487792969 \n",
      "\n",
      "Iteration 1746, Loss: 35.974246978759766, L1: 10.547532081604004, L3: 25.426715850830078\n",
      "Current prediction:  61.199546813964844 \n",
      "\n",
      "Iteration 1747, Loss: 36.99467849731445, L1: 10.547808647155762, L3: 26.446868896484375\n",
      "Current prediction:  61.19935607910156 \n",
      "\n",
      "Iteration 1748, Loss: 37.259803771972656, L1: 10.54797077178955, L3: 26.71183204650879\n",
      "Current prediction:  61.19914627075195 \n",
      "\n",
      "Iteration 1749, Loss: 38.035762786865234, L1: 10.548150062561035, L3: 27.487611770629883\n",
      "Current prediction:  61.19868087768555 \n",
      "\n",
      "Iteration 1750, Loss: 36.277000427246094, L1: 10.548544883728027, L3: 25.72845458984375\n",
      "Current prediction:  61.198516845703125 \n",
      "\n",
      "Iteration 1751, Loss: 36.13453674316406, L1: 10.548683166503906, L3: 25.585853576660156\n",
      "Current prediction:  61.198978424072266 \n",
      "\n",
      "Iteration 1752, Loss: 36.59333038330078, L1: 10.548290252685547, L3: 26.045042037963867\n",
      "Current prediction:  61.19953536987305 \n",
      "\n",
      "Iteration 1753, Loss: 36.54408645629883, L1: 10.547821998596191, L3: 25.996265411376953\n",
      "Current prediction:  61.19969940185547 \n",
      "\n",
      "Iteration 1754, Loss: 36.206382751464844, L1: 10.547677040100098, L3: 25.65870475769043\n",
      "Current prediction:  61.19968795776367 \n",
      "\n",
      "Iteration 1755, Loss: 37.51820373535156, L1: 10.54769229888916, L3: 26.97051239013672\n",
      "Current prediction:  61.1995849609375 \n",
      "\n",
      "Iteration 1756, Loss: 36.35314178466797, L1: 10.54777717590332, L3: 25.80536651611328\n",
      "Current prediction:  61.19956588745117 \n",
      "\n",
      "Iteration 1757, Loss: 35.91680145263672, L1: 10.5477933883667, L3: 25.369007110595703\n",
      "Current prediction:  61.19949722290039 \n",
      "\n",
      "Iteration 1758, Loss: 36.69623565673828, L1: 10.54785442352295, L3: 26.14838218688965\n",
      "Current prediction:  61.200008392333984 \n",
      "\n",
      "Iteration 1759, Loss: 36.57219696044922, L1: 10.547418594360352, L3: 26.024778366088867\n",
      "Current prediction:  61.20075225830078 \n",
      "\n",
      "Iteration 1760, Loss: 37.14068603515625, L1: 10.546789169311523, L3: 26.593896865844727\n",
      "Current prediction:  61.20033645629883 \n",
      "\n",
      "Iteration 1761, Loss: 36.233070373535156, L1: 10.547197341918945, L3: 25.685871124267578\n",
      "Current prediction:  61.199947357177734 \n",
      "\n",
      "Iteration 1762, Loss: 37.138851165771484, L1: 10.547469139099121, L3: 26.591381072998047\n",
      "Current prediction:  61.19929504394531 \n",
      "\n",
      "Iteration 1763, Loss: 36.42718505859375, L1: 10.54802417755127, L3: 25.879159927368164\n",
      "Current prediction:  61.1989631652832 \n",
      "\n",
      "Iteration 1764, Loss: 35.80473709106445, L1: 10.548309326171875, L3: 25.256427764892578\n",
      "Current prediction:  61.19887924194336 \n",
      "\n",
      "Iteration 1765, Loss: 36.70038604736328, L1: 10.54837417602539, L3: 26.152013778686523\n",
      "Current prediction:  61.1988410949707 \n",
      "\n",
      "Iteration 1766, Loss: 36.3917236328125, L1: 10.548404693603516, L3: 25.843318939208984\n",
      "Current prediction:  61.198917388916016 \n",
      "\n",
      "Iteration 1767, Loss: 36.74327087402344, L1: 10.548341751098633, L3: 26.194929122924805\n",
      "Current prediction:  61.19905090332031 \n",
      "\n",
      "Iteration 1768, Loss: 37.88313674926758, L1: 10.54823112487793, L3: 27.33490562438965\n",
      "Current prediction:  61.19849395751953 \n",
      "\n",
      "Iteration 1769, Loss: 35.850250244140625, L1: 10.548702239990234, L3: 25.30154800415039\n",
      "Current prediction:  61.19837951660156 \n",
      "\n",
      "Iteration 1770, Loss: 36.68376922607422, L1: 10.548800468444824, L3: 26.13496971130371\n",
      "Current prediction:  61.198421478271484 \n",
      "\n",
      "Iteration 1771, Loss: 36.84913635253906, L1: 10.548768043518066, L3: 26.300369262695312\n",
      "Current prediction:  61.1983757019043 \n",
      "\n",
      "Iteration 1772, Loss: 36.51701354980469, L1: 10.54880142211914, L3: 25.968210220336914\n",
      "Current prediction:  61.198978424072266 \n",
      "\n",
      "Iteration 1773, Loss: 36.53154754638672, L1: 10.548294067382812, L3: 25.983253479003906\n",
      "Current prediction:  61.199623107910156 \n",
      "\n",
      "Iteration 1774, Loss: 37.18680191040039, L1: 10.547743797302246, L3: 26.639057159423828\n",
      "Current prediction:  61.20056915283203 \n",
      "\n",
      "Iteration 1775, Loss: 36.828861236572266, L1: 10.54694652557373, L3: 26.28191566467285\n",
      "Current prediction:  61.20176315307617 \n",
      "\n",
      "Iteration 1776, Loss: 37.48814010620117, L1: 10.545930862426758, L3: 26.942209243774414\n",
      "Current prediction:  61.20245361328125 \n",
      "\n",
      "Iteration 1777, Loss: 36.3988037109375, L1: 10.545344352722168, L3: 25.85346031188965\n",
      "Current prediction:  61.20307540893555 \n",
      "\n",
      "Iteration 1778, Loss: 36.115108489990234, L1: 10.544815063476562, L3: 25.570293426513672\n",
      "Current prediction:  61.20378494262695 \n",
      "\n",
      "Iteration 1779, Loss: 37.74802017211914, L1: 10.544219970703125, L3: 27.203800201416016\n",
      "Current prediction:  61.204017639160156 \n",
      "\n",
      "Iteration 1780, Loss: 36.70911407470703, L1: 10.544020652770996, L3: 26.16509437561035\n",
      "Current prediction:  61.20442199707031 \n",
      "\n",
      "Iteration 1781, Loss: 36.859249114990234, L1: 10.543675422668457, L3: 26.31557273864746\n",
      "Current prediction:  61.204490661621094 \n",
      "\n",
      "Iteration 1782, Loss: 37.44179916381836, L1: 10.543615341186523, L3: 26.898183822631836\n",
      "Current prediction:  61.2039909362793 \n",
      "\n",
      "Iteration 1783, Loss: 35.60889434814453, L1: 10.544042587280273, L3: 25.064851760864258\n",
      "Current prediction:  61.20341491699219 \n",
      "\n",
      "Iteration 1784, Loss: 36.75979995727539, L1: 10.54453182220459, L3: 26.215269088745117\n",
      "Current prediction:  61.20233154296875 \n",
      "\n",
      "Iteration 1785, Loss: 36.428749084472656, L1: 10.545448303222656, L3: 25.88330078125\n",
      "Current prediction:  61.2021598815918 \n",
      "\n",
      "Iteration 1786, Loss: 36.69217300415039, L1: 10.545594215393066, L3: 26.14657974243164\n",
      "Current prediction:  61.20197677612305 \n",
      "\n",
      "Iteration 1787, Loss: 36.400299072265625, L1: 10.54574966430664, L3: 25.85454750061035\n",
      "Current prediction:  61.201751708984375 \n",
      "\n",
      "Iteration 1788, Loss: 37.54132080078125, L1: 10.545937538146973, L3: 26.995384216308594\n",
      "Current prediction:  61.20085144042969 \n",
      "\n",
      "Iteration 1789, Loss: 36.216156005859375, L1: 10.546707153320312, L3: 25.669450759887695\n",
      "Current prediction:  61.20085144042969 \n",
      "\n",
      "Iteration 1790, Loss: 35.93389129638672, L1: 10.546708106994629, L3: 25.387182235717773\n",
      "Current prediction:  61.20064926147461 \n",
      "\n",
      "Iteration 1791, Loss: 36.629608154296875, L1: 10.546875, L3: 26.082735061645508\n",
      "Current prediction:  61.1998291015625 \n",
      "\n",
      "Iteration 1792, Loss: 37.25584030151367, L1: 10.54757022857666, L3: 26.708269119262695\n",
      "Current prediction:  61.19870376586914 \n",
      "\n",
      "Iteration 1793, Loss: 36.28225326538086, L1: 10.548530578613281, L3: 25.733722686767578\n",
      "Current prediction:  61.19794845581055 \n",
      "\n",
      "Iteration 1794, Loss: 37.35758972167969, L1: 10.549171447753906, L3: 26.80841636657715\n",
      "Current prediction:  61.19744873046875 \n",
      "\n",
      "Iteration 1795, Loss: 36.70780563354492, L1: 10.549588203430176, L3: 26.15821647644043\n",
      "Current prediction:  61.19783401489258 \n",
      "\n",
      "Iteration 1796, Loss: 36.57756042480469, L1: 10.549262046813965, L3: 26.028297424316406\n",
      "Current prediction:  61.198265075683594 \n",
      "\n",
      "Iteration 1797, Loss: 37.048309326171875, L1: 10.548898696899414, L3: 26.499412536621094\n",
      "Current prediction:  61.1984748840332 \n",
      "\n",
      "Iteration 1798, Loss: 35.999053955078125, L1: 10.54871940612793, L3: 25.450332641601562\n",
      "Current prediction:  61.19779968261719 \n",
      "\n",
      "Iteration 1799, Loss: 36.075225830078125, L1: 10.54928970336914, L3: 25.525936126708984\n",
      "Current prediction:  61.19770812988281 \n",
      "\n",
      "Iteration 1800, Loss: 36.31725311279297, L1: 10.549365997314453, L3: 25.767885208129883\n",
      "Current prediction:  61.19745635986328 \n",
      "\n",
      "Iteration 1801, Loss: 36.63392639160156, L1: 10.549586296081543, L3: 26.084339141845703\n",
      "Current prediction:  61.19746398925781 \n",
      "\n",
      "Iteration 1802, Loss: 36.32293701171875, L1: 10.549576759338379, L3: 25.773359298706055\n",
      "Current prediction:  61.19736862182617 \n",
      "\n",
      "Iteration 1803, Loss: 37.322532653808594, L1: 10.54965591430664, L3: 26.77287483215332\n",
      "Current prediction:  61.197147369384766 \n",
      "\n",
      "Iteration 1804, Loss: 36.926700592041016, L1: 10.549845695495605, L3: 26.376853942871094\n",
      "Current prediction:  61.19630432128906 \n",
      "\n",
      "Iteration 1805, Loss: 35.980743408203125, L1: 10.550562858581543, L3: 25.4301815032959\n",
      "Current prediction:  61.19639587402344 \n",
      "\n",
      "Iteration 1806, Loss: 36.334564208984375, L1: 10.550483703613281, L3: 25.78407859802246\n",
      "Current prediction:  61.19618606567383 \n",
      "\n",
      "Iteration 1807, Loss: 36.817562103271484, L1: 10.5506591796875, L3: 26.266902923583984\n",
      "Current prediction:  61.194915771484375 \n",
      "\n",
      "Iteration 1808, Loss: 35.891685485839844, L1: 10.551737785339355, L3: 25.339946746826172\n",
      "Current prediction:  61.193641662597656 \n",
      "\n",
      "Iteration 1809, Loss: 36.699684143066406, L1: 10.552817344665527, L3: 26.146865844726562\n",
      "Current prediction:  61.192626953125 \n",
      "\n",
      "Iteration 1810, Loss: 36.558448791503906, L1: 10.553674697875977, L3: 26.00477409362793\n",
      "Current prediction:  61.19148254394531 \n",
      "\n",
      "Iteration 1811, Loss: 36.51385498046875, L1: 10.554644584655762, L3: 25.959209442138672\n",
      "Current prediction:  61.19082260131836 \n",
      "\n",
      "Iteration 1812, Loss: 36.10810852050781, L1: 10.555206298828125, L3: 25.552902221679688\n",
      "Current prediction:  61.19053649902344 \n",
      "\n",
      "Iteration 1813, Loss: 36.488407135009766, L1: 10.555451393127441, L3: 25.93295669555664\n",
      "Current prediction:  61.190040588378906 \n",
      "\n",
      "Iteration 1814, Loss: 35.54206466674805, L1: 10.555869102478027, L3: 24.986194610595703\n",
      "Current prediction:  61.1903076171875 \n",
      "\n",
      "Iteration 1815, Loss: 37.016414642333984, L1: 10.555644035339355, L3: 26.460769653320312\n",
      "Current prediction:  61.1901741027832 \n",
      "\n",
      "Iteration 1816, Loss: 36.60378646850586, L1: 10.555756568908691, L3: 26.048030853271484\n",
      "Current prediction:  61.19055938720703 \n",
      "\n",
      "Iteration 1817, Loss: 36.74435043334961, L1: 10.55543041229248, L3: 26.188919067382812\n",
      "Current prediction:  61.191078186035156 \n",
      "\n",
      "Iteration 1818, Loss: 36.29154586791992, L1: 10.554986000061035, L3: 25.73655891418457\n",
      "Current prediction:  61.19158935546875 \n",
      "\n",
      "Iteration 1819, Loss: 35.57358169555664, L1: 10.554560661315918, L3: 25.019020080566406\n",
      "Current prediction:  61.19203186035156 \n",
      "\n",
      "Iteration 1820, Loss: 35.97992706298828, L1: 10.554182052612305, L3: 25.42574691772461\n",
      "Current prediction:  61.192588806152344 \n",
      "\n",
      "Iteration 1821, Loss: 36.267967224121094, L1: 10.553705215454102, L3: 25.714262008666992\n",
      "Current prediction:  61.193084716796875 \n",
      "\n",
      "Iteration 1822, Loss: 37.76409149169922, L1: 10.553288459777832, L3: 27.210803985595703\n",
      "Current prediction:  61.193267822265625 \n",
      "\n",
      "Iteration 1823, Loss: 36.67523193359375, L1: 10.553133964538574, L3: 26.12209701538086\n",
      "Current prediction:  61.1934814453125 \n",
      "\n",
      "Iteration 1824, Loss: 36.59461212158203, L1: 10.552956581115723, L3: 26.041656494140625\n",
      "Current prediction:  61.193363189697266 \n",
      "\n",
      "Iteration 1825, Loss: 35.934234619140625, L1: 10.553056716918945, L3: 25.381179809570312\n",
      "Current prediction:  61.19329071044922 \n",
      "\n",
      "Iteration 1826, Loss: 35.66344451904297, L1: 10.553123474121094, L3: 25.110319137573242\n",
      "Current prediction:  61.193824768066406 \n",
      "\n",
      "Iteration 1827, Loss: 37.702945709228516, L1: 10.55266284942627, L3: 27.150283813476562\n",
      "Current prediction:  61.1933708190918 \n",
      "\n",
      "Iteration 1828, Loss: 36.73377990722656, L1: 10.553043365478516, L3: 26.18073844909668\n",
      "Current prediction:  61.19312286376953 \n",
      "\n",
      "Iteration 1829, Loss: 36.79951858520508, L1: 10.553254127502441, L3: 26.246265411376953\n",
      "Current prediction:  61.19282913208008 \n",
      "\n",
      "Iteration 1830, Loss: 37.068763732910156, L1: 10.553508758544922, L3: 26.5152530670166\n",
      "Current prediction:  61.192726135253906 \n",
      "\n",
      "Iteration 1831, Loss: 37.20212936401367, L1: 10.553594589233398, L3: 26.648534774780273\n",
      "Current prediction:  61.192298889160156 \n",
      "\n",
      "Iteration 1832, Loss: 35.9734992980957, L1: 10.553959846496582, L3: 25.419538497924805\n",
      "Current prediction:  61.19200134277344 \n",
      "\n",
      "Iteration 1833, Loss: 36.23872375488281, L1: 10.55420970916748, L3: 25.68451499938965\n",
      "Current prediction:  61.191287994384766 \n",
      "\n",
      "Iteration 1834, Loss: 36.53093719482422, L1: 10.55492115020752, L3: 25.976016998291016\n",
      "Current prediction:  61.19130325317383 \n",
      "\n",
      "Iteration 1835, Loss: 36.42439651489258, L1: 10.554800033569336, L3: 25.869596481323242\n",
      "Current prediction:  61.191917419433594 \n",
      "\n",
      "Iteration 1836, Loss: 36.45064163208008, L1: 10.554280281066895, L3: 25.896360397338867\n",
      "Current prediction:  61.19294357299805 \n",
      "\n",
      "Iteration 1837, Loss: 36.364830017089844, L1: 10.553423881530762, L3: 25.8114070892334\n",
      "Current prediction:  61.19430923461914 \n",
      "\n",
      "Iteration 1838, Loss: 36.31597900390625, L1: 10.552250862121582, L3: 25.76372718811035\n",
      "Current prediction:  61.1957893371582 \n",
      "\n",
      "Iteration 1839, Loss: 36.37348556518555, L1: 10.550997734069824, L3: 25.822486877441406\n",
      "Current prediction:  61.19706344604492 \n",
      "\n",
      "Iteration 1840, Loss: 36.662044525146484, L1: 10.549915313720703, L3: 26.11212921142578\n",
      "Current prediction:  61.19779968261719 \n",
      "\n",
      "Iteration 1841, Loss: 37.157432556152344, L1: 10.549288749694824, L3: 26.608142852783203\n",
      "Current prediction:  61.198097229003906 \n",
      "\n",
      "Iteration 1842, Loss: 36.279052734375, L1: 10.54903793334961, L3: 25.73001480102539\n",
      "Current prediction:  61.19844055175781 \n",
      "\n",
      "Iteration 1843, Loss: 36.65499496459961, L1: 10.548749923706055, L3: 26.106245040893555\n",
      "Current prediction:  61.19833755493164 \n",
      "\n",
      "Iteration 1844, Loss: 36.87199783325195, L1: 10.548834800720215, L3: 26.323162078857422\n",
      "Current prediction:  61.1985969543457 \n",
      "\n",
      "Iteration 1845, Loss: 36.14527893066406, L1: 10.548614501953125, L3: 25.596662521362305\n",
      "Current prediction:  61.19900131225586 \n",
      "\n",
      "Iteration 1846, Loss: 36.552032470703125, L1: 10.548271179199219, L3: 26.00376319885254\n",
      "Current prediction:  61.19900131225586 \n",
      "\n",
      "Iteration 1847, Loss: 37.82527542114258, L1: 10.54931640625, L3: 27.275959014892578\n",
      "Current prediction:  61.19892501831055 \n",
      "\n",
      "Iteration 1848, Loss: 36.55665588378906, L1: 10.548338890075684, L3: 26.008316040039062\n",
      "Current prediction:  61.19905090332031 \n",
      "\n",
      "Iteration 1849, Loss: 37.147762298583984, L1: 10.548291206359863, L3: 26.599472045898438\n",
      "Current prediction:  61.19960021972656 \n",
      "\n",
      "Iteration 1850, Loss: 36.66187286376953, L1: 10.622920036315918, L3: 26.03895378112793\n",
      "Current prediction:  61.20041275024414 \n",
      "\n",
      "Iteration 1851, Loss: 36.4617919921875, L1: 10.611140251159668, L3: 25.850650787353516\n",
      "Current prediction:  61.20119094848633 \n",
      "\n",
      "Iteration 1852, Loss: 35.803627014160156, L1: 10.571516036987305, L3: 25.23210906982422\n",
      "Current prediction:  61.202083587646484 \n",
      "\n",
      "Iteration 1853, Loss: 36.92774200439453, L1: 10.61652660369873, L3: 26.311216354370117\n",
      "Current prediction:  61.19709396362305 \n",
      "\n",
      "Iteration 1854, Loss: 36.65064239501953, L1: 10.660843849182129, L3: 25.989797592163086\n",
      "Current prediction:  60.88380813598633 \n",
      "\n",
      "Iteration 1855, Loss: 37.31957244873047, L1: 10.720556259155273, L3: 26.599016189575195\n",
      "Current prediction:  60.88820266723633 \n",
      "\n",
      "Iteration 1856, Loss: 36.235904693603516, L1: 10.771456718444824, L3: 25.464448928833008\n",
      "Current prediction:  60.89360046386719 \n",
      "\n",
      "Iteration 1857, Loss: 37.173133850097656, L1: 10.835234642028809, L3: 26.337900161743164\n",
      "Current prediction:  60.89986038208008 \n",
      "\n",
      "Iteration 1858, Loss: 36.66682052612305, L1: 10.879776954650879, L3: 25.787044525146484\n",
      "Current prediction:  60.90752410888672 \n",
      "\n",
      "Iteration 1859, Loss: 37.25415802001953, L1: 10.864365577697754, L3: 26.38979148864746\n",
      "Current prediction:  60.9160270690918 \n",
      "\n",
      "Iteration 1860, Loss: 36.26885223388672, L1: 10.799440383911133, L3: 25.469409942626953\n",
      "Current prediction:  60.9260139465332 \n",
      "\n",
      "Iteration 1861, Loss: 36.05805969238281, L1: 10.83092975616455, L3: 25.227130889892578\n",
      "Current prediction:  60.93696975708008 \n",
      "\n",
      "Iteration 1862, Loss: 35.5656623840332, L1: 10.80656909942627, L3: 24.759092330932617\n",
      "Current prediction:  60.9489860534668 \n",
      "\n",
      "Iteration 1863, Loss: 37.46421432495117, L1: 10.809464454650879, L3: 26.65475082397461\n",
      "Current prediction:  60.96139907836914 \n",
      "\n",
      "Iteration 1864, Loss: 36.178131103515625, L1: 10.817415237426758, L3: 25.360713958740234\n",
      "Current prediction:  60.97444152832031 \n",
      "\n",
      "Iteration 1865, Loss: 36.82514953613281, L1: 10.784781455993652, L3: 26.040369033813477\n",
      "Current prediction:  60.98739242553711 \n",
      "\n",
      "Iteration 1866, Loss: 36.66606903076172, L1: 10.784838676452637, L3: 25.8812313079834\n",
      "Current prediction:  61.00080490112305 \n",
      "\n",
      "Iteration 1867, Loss: 36.76351547241211, L1: 10.792698860168457, L3: 25.970815658569336\n",
      "Current prediction:  61.014076232910156 \n",
      "\n",
      "Iteration 1868, Loss: 36.249908447265625, L1: 10.726691246032715, L3: 25.523216247558594\n",
      "Current prediction:  61.02695846557617 \n",
      "\n",
      "Iteration 1869, Loss: 36.75681686401367, L1: 10.642373085021973, L3: 26.114442825317383\n",
      "Current prediction:  61.03938674926758 \n",
      "\n",
      "Iteration 1870, Loss: 36.013916015625, L1: 10.555639266967773, L3: 25.458274841308594\n",
      "Current prediction:  61.371055603027344 \n",
      "\n",
      "Iteration 1871, Loss: 37.016357421875, L1: 10.481060028076172, L3: 26.535297393798828\n",
      "Current prediction:  61.38026428222656 \n",
      "\n",
      "Iteration 1872, Loss: 36.04767608642578, L1: 10.453912734985352, L3: 25.59376335144043\n",
      "Current prediction:  61.38801193237305 \n",
      "\n",
      "Iteration 1873, Loss: 36.888816833496094, L1: 10.468412399291992, L3: 26.42040252685547\n",
      "Current prediction:  61.3945426940918 \n",
      "\n",
      "Iteration 1874, Loss: 36.74641799926758, L1: 10.455117225646973, L3: 26.29129981994629\n",
      "Current prediction:  61.3994255065918 \n",
      "\n",
      "Iteration 1875, Loss: 36.73897933959961, L1: 10.442448616027832, L3: 26.296531677246094\n",
      "Current prediction:  61.403602600097656 \n",
      "\n",
      "Iteration 1876, Loss: 36.694183349609375, L1: 10.486095428466797, L3: 26.20808982849121\n",
      "Current prediction:  61.40711975097656 \n",
      "\n",
      "Iteration 1877, Loss: 36.93568420410156, L1: 10.390527725219727, L3: 26.545156478881836\n",
      "Current prediction:  61.40850830078125 \n",
      "\n",
      "Iteration 1878, Loss: 35.86203384399414, L1: 10.45018482208252, L3: 25.411849975585938\n",
      "Current prediction:  61.40868377685547 \n",
      "\n",
      "Iteration 1879, Loss: 36.49805450439453, L1: 10.370270729064941, L3: 26.127784729003906\n",
      "Current prediction:  61.408119201660156 \n",
      "\n",
      "Iteration 1880, Loss: 37.15959167480469, L1: 10.368890762329102, L3: 26.79070281982422\n",
      "Current prediction:  61.40681457519531 \n",
      "\n",
      "Iteration 1881, Loss: 35.56464767456055, L1: 10.371834754943848, L3: 25.192813873291016\n",
      "Current prediction:  61.40492248535156 \n",
      "\n",
      "Iteration 1882, Loss: 36.622657775878906, L1: 10.373584747314453, L3: 26.24907112121582\n",
      "Current prediction:  61.401824951171875 \n",
      "\n",
      "Iteration 1883, Loss: 36.913875579833984, L1: 10.376213073730469, L3: 26.537662506103516\n",
      "Current prediction:  61.39781951904297 \n",
      "\n",
      "Iteration 1884, Loss: 36.05657958984375, L1: 10.379615783691406, L3: 25.676965713500977\n",
      "Current prediction:  61.39265823364258 \n",
      "\n",
      "Iteration 1885, Loss: 36.30502700805664, L1: 10.383993148803711, L3: 25.92103385925293\n",
      "Current prediction:  61.38728713989258 \n",
      "\n",
      "Iteration 1886, Loss: 36.076866149902344, L1: 10.3885498046875, L3: 25.688316345214844\n",
      "Current prediction:  61.381385803222656 \n",
      "\n",
      "Iteration 1887, Loss: 37.00637435913086, L1: 10.393555641174316, L3: 26.61281967163086\n",
      "Current prediction:  61.37362289428711 \n",
      "\n",
      "Iteration 1888, Loss: 36.974327087402344, L1: 10.400148391723633, L3: 26.574180603027344\n",
      "Current prediction:  61.365718841552734 \n",
      "\n",
      "Iteration 1889, Loss: 36.51205062866211, L1: 10.406861305236816, L3: 26.105188369750977\n",
      "Current prediction:  61.35795211791992 \n",
      "\n",
      "Iteration 1890, Loss: 35.73426818847656, L1: 10.413446426391602, L3: 25.320819854736328\n",
      "Current prediction:  61.349788665771484 \n",
      "\n",
      "Iteration 1891, Loss: 36.63685989379883, L1: 10.41884994506836, L3: 26.21800994873047\n",
      "Current prediction:  61.34109115600586 \n",
      "\n",
      "Iteration 1892, Loss: 37.23617172241211, L1: 10.427754402160645, L3: 26.80841827392578\n",
      "Current prediction:  61.332218170166016 \n",
      "\n",
      "Iteration 1893, Loss: 36.83675003051758, L1: 10.435281753540039, L3: 26.40146827697754\n",
      "Current prediction:  61.322547912597656 \n",
      "\n",
      "Iteration 1894, Loss: 35.9454345703125, L1: 10.443489074707031, L3: 25.501943588256836\n",
      "Current prediction:  61.31391906738281 \n",
      "\n",
      "Iteration 1895, Loss: 37.39793395996094, L1: 10.450806617736816, L3: 26.947128295898438\n",
      "Current prediction:  61.30443572998047 \n",
      "\n",
      "Iteration 1896, Loss: 37.11648178100586, L1: 10.458856582641602, L3: 26.657625198364258\n",
      "Current prediction:  61.29524230957031 \n",
      "\n",
      "Iteration 1897, Loss: 36.192752838134766, L1: 10.466656684875488, L3: 25.726097106933594\n",
      "Current prediction:  61.28724670410156 \n",
      "\n",
      "Iteration 1898, Loss: 37.30870056152344, L1: 10.475119590759277, L3: 26.833581924438477\n",
      "Current prediction:  61.278724670410156 \n",
      "\n",
      "Iteration 1899, Loss: 36.133514404296875, L1: 10.480661392211914, L3: 25.65285301208496\n",
      "Current prediction:  61.27146911621094 \n",
      "\n",
      "Iteration 1900, Loss: 36.08467102050781, L1: 10.486818313598633, L3: 25.597850799560547\n",
      "Current prediction:  61.26435470581055 \n",
      "\n",
      "Iteration 1901, Loss: 37.27862548828125, L1: 10.492854118347168, L3: 26.7857723236084\n",
      "Current prediction:  61.25764465332031 \n",
      "\n",
      "Iteration 1902, Loss: 37.742835998535156, L1: 10.498543739318848, L3: 27.244291305541992\n",
      "Current prediction:  61.250972747802734 \n",
      "\n",
      "Iteration 1903, Loss: 36.352664947509766, L1: 10.504204750061035, L3: 25.848459243774414\n",
      "Current prediction:  61.24467086791992 \n",
      "\n",
      "Iteration 1904, Loss: 36.274635314941406, L1: 10.509544372558594, L3: 25.765090942382812\n",
      "Current prediction:  61.23823547363281 \n",
      "\n",
      "Iteration 1905, Loss: 36.35923767089844, L1: 10.571873664855957, L3: 25.787364959716797\n",
      "Current prediction:  61.232051849365234 \n",
      "\n",
      "Iteration 1906, Loss: 37.00633239746094, L1: 10.52051830291748, L3: 26.48581314086914\n",
      "Current prediction:  61.22652053833008 \n",
      "\n",
      "Iteration 1907, Loss: 36.68377685546875, L1: 10.524935722351074, L3: 26.15884017944336\n",
      "Current prediction:  61.22127151489258 \n",
      "\n",
      "Iteration 1908, Loss: 37.006858825683594, L1: 10.52935791015625, L3: 26.477502822875977\n",
      "Current prediction:  61.2160758972168 \n",
      "\n",
      "Iteration 1909, Loss: 36.868873596191406, L1: 10.570911407470703, L3: 26.297962188720703\n",
      "Current prediction:  61.21161651611328 \n",
      "\n",
      "Iteration 1910, Loss: 36.860347747802734, L1: 10.545003890991211, L3: 26.315343856811523\n",
      "Current prediction:  61.207130432128906 \n",
      "\n",
      "Iteration 1911, Loss: 37.06334686279297, L1: 10.64254093170166, L3: 26.420806884765625\n",
      "Current prediction:  61.204036712646484 \n",
      "\n",
      "Iteration 1912, Loss: 36.63676452636719, L1: 10.602720260620117, L3: 26.034046173095703\n",
      "Current prediction:  61.200218200683594 \n",
      "\n",
      "Iteration 1913, Loss: 36.70777130126953, L1: 10.733210563659668, L3: 25.97456169128418\n",
      "Current prediction:  61.19792175292969 \n",
      "\n",
      "Iteration 1914, Loss: 36.306884765625, L1: 10.881365776062012, L3: 25.425518035888672\n",
      "Current prediction:  61.19593811035156 \n",
      "\n",
      "Iteration 1915, Loss: 36.909393310546875, L1: 10.764179229736328, L3: 26.145212173461914\n",
      "Current prediction:  61.19365692138672 \n",
      "\n",
      "Iteration 1916, Loss: 36.735443115234375, L1: 10.806020736694336, L3: 25.929424285888672\n",
      "Current prediction:  61.19202423095703 \n",
      "\n",
      "Iteration 1917, Loss: 37.26944351196289, L1: 10.674896240234375, L3: 26.594547271728516\n",
      "Current prediction:  61.19110107421875 \n",
      "\n",
      "Iteration 1918, Loss: 36.46376037597656, L1: 10.745351791381836, L3: 25.718408584594727\n",
      "Current prediction:  60.605464935302734 \n",
      "\n",
      "Iteration 1919, Loss: 36.5290641784668, L1: 10.873276710510254, L3: 25.655786514282227\n",
      "Current prediction:  60.60886001586914 \n",
      "\n",
      "Iteration 1920, Loss: 36.75673294067383, L1: 11.00115966796875, L3: 25.755573272705078\n",
      "Current prediction:  60.614654541015625 \n",
      "\n",
      "Iteration 1921, Loss: 37.0433349609375, L1: 11.16382122039795, L3: 25.879514694213867\n",
      "Current prediction:  60.62326431274414 \n",
      "\n",
      "Iteration 1922, Loss: 37.471126556396484, L1: 11.186866760253906, L3: 26.284259796142578\n",
      "Current prediction:  60.63430404663086 \n",
      "\n",
      "Iteration 1923, Loss: 36.815948486328125, L1: 11.360527038574219, L3: 25.455421447753906\n",
      "Current prediction:  60.64714050292969 \n",
      "\n",
      "Iteration 1924, Loss: 36.16347885131836, L1: 11.097891807556152, L3: 25.065587997436523\n",
      "Current prediction:  60.662353515625 \n",
      "\n",
      "Iteration 1925, Loss: 37.99045944213867, L1: 11.152799606323242, L3: 26.83765983581543\n",
      "Current prediction:  60.6787109375 \n",
      "\n",
      "Iteration 1926, Loss: 37.523406982421875, L1: 11.078727722167969, L3: 26.44468116760254\n",
      "Current prediction:  60.69634246826172 \n",
      "\n",
      "Iteration 1927, Loss: 36.530487060546875, L1: 10.896597862243652, L3: 25.63389015197754\n",
      "Current prediction:  60.714195251464844 \n",
      "\n",
      "Iteration 1928, Loss: 36.53860092163086, L1: 10.783099174499512, L3: 25.75550079345703\n",
      "Current prediction:  61.309444427490234 \n",
      "\n",
      "Iteration 1929, Loss: 36.68136215209961, L1: 10.679511070251465, L3: 26.00185203552246\n",
      "Current prediction:  61.33430862426758 \n",
      "\n",
      "Iteration 1930, Loss: 35.99317169189453, L1: 10.589889526367188, L3: 25.403282165527344\n",
      "Current prediction:  61.35040283203125 \n",
      "\n",
      "Iteration 1931, Loss: 35.804901123046875, L1: 10.55916690826416, L3: 25.24573516845703\n",
      "Current prediction:  61.36554718017578 \n",
      "\n",
      "Iteration 1932, Loss: 36.78418731689453, L1: 10.572190284729004, L3: 26.21199607849121\n",
      "Current prediction:  61.37916564941406 \n",
      "\n",
      "Iteration 1933, Loss: 36.74689483642578, L1: 10.537450790405273, L3: 26.20944595336914\n",
      "Current prediction:  61.390907287597656 \n",
      "\n",
      "Iteration 1934, Loss: 37.309234619140625, L1: 10.548721313476562, L3: 26.760515213012695\n",
      "Current prediction:  61.40117263793945 \n",
      "\n",
      "Iteration 1935, Loss: 37.272056579589844, L1: 10.521354675292969, L3: 26.750699996948242\n",
      "Current prediction:  61.40922927856445 \n",
      "\n",
      "Iteration 1936, Loss: 36.67569351196289, L1: 10.631213188171387, L3: 26.04448127746582\n",
      "Current prediction:  61.415653228759766 \n",
      "\n",
      "Iteration 1937, Loss: 42.990234375, L1: 16.69548988342285, L3: 26.29474449157715\n",
      "Current prediction:  61.420204162597656 \n",
      "\n",
      "Iteration 1938, Loss: 36.677276611328125, L1: 10.497262954711914, L3: 26.180011749267578\n",
      "Current prediction:  61.423789978027344 \n",
      "\n",
      "Iteration 1939, Loss: 36.097137451171875, L1: 10.561344146728516, L3: 25.535791397094727\n",
      "Current prediction:  61.427001953125 \n",
      "\n",
      "Iteration 1940, Loss: 35.47620391845703, L1: 10.514538764953613, L3: 24.9616641998291\n",
      "Current prediction:  61.43000030517578 \n",
      "\n",
      "Iteration 1941, Loss: 36.82929611206055, L1: 10.506112098693848, L3: 26.323184967041016\n",
      "Current prediction:  61.43187713623047 \n",
      "\n",
      "Iteration 1942, Loss: 36.35818099975586, L1: 10.55941104888916, L3: 25.798768997192383\n",
      "Current prediction:  60.87556457519531 \n",
      "\n",
      "Iteration 1943, Loss: 37.535274505615234, L1: 10.621966361999512, L3: 26.91330909729004\n",
      "Current prediction:  60.85126876831055 \n",
      "\n",
      "Iteration 1944, Loss: 36.37580871582031, L1: 10.67774486541748, L3: 25.69806480407715\n",
      "Current prediction:  60.852394104003906 \n",
      "\n",
      "Iteration 1945, Loss: 37.13344192504883, L1: 10.72086238861084, L3: 26.412578582763672\n",
      "Current prediction:  60.854408264160156 \n",
      "\n",
      "Iteration 1946, Loss: 36.4432373046875, L1: 10.735919952392578, L3: 25.707319259643555\n",
      "Current prediction:  60.857330322265625 \n",
      "\n",
      "Iteration 1947, Loss: 37.03258514404297, L1: 10.811208724975586, L3: 26.221378326416016\n",
      "Current prediction:  60.86124038696289 \n",
      "\n",
      "Iteration 1948, Loss: 36.84158706665039, L1: 10.857854843139648, L3: 25.983732223510742\n",
      "Current prediction:  60.86627960205078 \n",
      "\n",
      "Iteration 1949, Loss: 36.69963455200195, L1: 10.852328300476074, L3: 25.847305297851562\n",
      "Current prediction:  60.8719482421875 \n",
      "\n",
      "Iteration 1950, Loss: 37.19502258300781, L1: 10.88331127166748, L3: 26.31171226501465\n",
      "Current prediction:  60.87793731689453 \n",
      "\n",
      "Iteration 1951, Loss: 37.622650146484375, L1: 10.988702774047852, L3: 26.633949279785156\n",
      "Current prediction:  60.88499450683594 \n",
      "\n",
      "Iteration 1952, Loss: 37.739009857177734, L1: 11.093426704406738, L3: 26.64558219909668\n",
      "Current prediction:  60.89259719848633 \n",
      "\n",
      "Iteration 1953, Loss: 37.29523468017578, L1: 10.956354141235352, L3: 26.33888053894043\n",
      "Current prediction:  60.90086364746094 \n",
      "\n",
      "Iteration 1954, Loss: 37.627113342285156, L1: 10.97452449798584, L3: 26.652589797973633\n",
      "Current prediction:  60.909996032714844 \n",
      "\n",
      "Iteration 1955, Loss: 37.430572509765625, L1: 11.007848739624023, L3: 26.42272186279297\n",
      "Current prediction:  60.920169830322266 \n",
      "\n",
      "Iteration 1956, Loss: 37.23674011230469, L1: 10.914091110229492, L3: 26.322647094726562\n",
      "Current prediction:  60.93113708496094 \n",
      "\n",
      "Iteration 1957, Loss: 36.71091842651367, L1: 10.810483932495117, L3: 25.900434494018555\n",
      "Current prediction:  60.942359924316406 \n",
      "\n",
      "Iteration 1958, Loss: 35.73832702636719, L1: 10.725251197814941, L3: 25.013076782226562\n",
      "Current prediction:  60.95349884033203 \n",
      "\n",
      "Iteration 1959, Loss: 37.97515106201172, L1: 10.638144493103027, L3: 27.337005615234375\n",
      "Current prediction:  60.96315383911133 \n",
      "\n",
      "Iteration 1960, Loss: 36.13610076904297, L1: 10.558403968811035, L3: 25.57769775390625\n",
      "Current prediction:  61.210105895996094 \n",
      "\n",
      "Iteration 1961, Loss: 37.63134765625, L1: 10.492168426513672, L3: 27.13918113708496\n",
      "Current prediction:  61.559505462646484 \n",
      "\n",
      "Iteration 1962, Loss: 36.953453063964844, L1: 10.443460464477539, L3: 26.509994506835938\n",
      "Current prediction:  61.564117431640625 \n",
      "\n",
      "Iteration 1963, Loss: 36.81609344482422, L1: 10.401134490966797, L3: 26.414958953857422\n",
      "Current prediction:  61.56746292114258 \n",
      "\n",
      "Iteration 1964, Loss: 36.920902252197266, L1: 10.355804443359375, L3: 26.56509780883789\n",
      "Current prediction:  61.56865310668945 \n",
      "\n",
      "Iteration 1965, Loss: 37.02508544921875, L1: 10.392271041870117, L3: 26.632816314697266\n",
      "Current prediction:  61.56776809692383 \n",
      "\n",
      "Iteration 1966, Loss: 35.98699188232422, L1: 10.351304054260254, L3: 25.63568878173828\n",
      "Current prediction:  61.56555938720703 \n",
      "\n",
      "Iteration 1967, Loss: 37.77273941040039, L1: 10.360560417175293, L3: 27.41217803955078\n",
      "Current prediction:  61.56179428100586 \n",
      "\n",
      "Iteration 1968, Loss: 36.56132507324219, L1: 10.33703899383545, L3: 26.224287033081055\n",
      "Current prediction:  61.5574836730957 \n",
      "\n",
      "Iteration 1969, Loss: 36.956600189208984, L1: 10.4044828414917, L3: 26.55211639404297\n",
      "Current prediction:  61.55205535888672 \n",
      "\n",
      "Iteration 1970, Loss: 37.59052276611328, L1: 10.442523956298828, L3: 27.148000717163086\n",
      "Current prediction:  61.54470443725586 \n",
      "\n",
      "Iteration 1971, Loss: 36.3883056640625, L1: 10.444290161132812, L3: 25.944013595581055\n",
      "Current prediction:  61.536685943603516 \n",
      "\n",
      "Iteration 1972, Loss: 36.43439483642578, L1: 10.461338996887207, L3: 25.97305679321289\n",
      "Current prediction:  61.527931213378906 \n",
      "\n",
      "Iteration 1973, Loss: 36.666603088378906, L1: 10.384306907653809, L3: 26.282297134399414\n",
      "Current prediction:  61.51835250854492 \n",
      "\n",
      "Iteration 1974, Loss: 37.63503646850586, L1: 10.47070598602295, L3: 27.164331436157227\n",
      "Current prediction:  61.50790786743164 \n",
      "\n",
      "Iteration 1975, Loss: 37.13538360595703, L1: 10.434632301330566, L3: 26.70075225830078\n",
      "Current prediction:  61.49705123901367 \n",
      "\n",
      "Iteration 1976, Loss: 36.5770149230957, L1: 10.436792373657227, L3: 26.140222549438477\n",
      "Current prediction:  61.48640441894531 \n",
      "\n",
      "Iteration 1977, Loss: 37.338504791259766, L1: 10.488909721374512, L3: 26.849594116210938\n",
      "Current prediction:  60.99100875854492 \n",
      "\n",
      "Iteration 1978, Loss: 36.14165496826172, L1: 10.571102142333984, L3: 25.5705509185791\n",
      "Current prediction:  60.88874435424805 \n",
      "\n",
      "Iteration 1979, Loss: 36.705909729003906, L1: 10.62200927734375, L3: 26.083898544311523\n",
      "Current prediction:  60.88120651245117 \n",
      "\n",
      "Iteration 1980, Loss: 38.20628356933594, L1: 10.679576873779297, L3: 27.526708602905273\n",
      "Current prediction:  60.874874114990234 \n",
      "\n",
      "Iteration 1981, Loss: 37.97230911254883, L1: 10.718735694885254, L3: 27.253572463989258\n",
      "Current prediction:  60.8703727722168 \n",
      "\n",
      "Iteration 1982, Loss: 35.51216125488281, L1: 10.790384292602539, L3: 24.72177505493164\n",
      "Current prediction:  60.86805725097656 \n",
      "\n",
      "Iteration 1983, Loss: 36.65033721923828, L1: 10.797680854797363, L3: 25.8526554107666\n",
      "Current prediction:  60.867095947265625 \n",
      "\n",
      "Iteration 1984, Loss: 37.31719207763672, L1: 10.820390701293945, L3: 26.496803283691406\n",
      "Current prediction:  60.867313385009766 \n",
      "\n",
      "Iteration 1985, Loss: 37.25090026855469, L1: 10.884159088134766, L3: 26.366741180419922\n",
      "Current prediction:  60.86899185180664 \n",
      "\n",
      "Iteration 1986, Loss: 36.36555480957031, L1: 10.866860389709473, L3: 25.498693466186523\n",
      "Current prediction:  60.87257385253906 \n",
      "\n",
      "Iteration 1987, Loss: 36.52022933959961, L1: 10.840753555297852, L3: 25.679475784301758\n",
      "Current prediction:  60.87727737426758 \n",
      "\n",
      "Iteration 1988, Loss: 36.063804626464844, L1: 10.774653434753418, L3: 25.289152145385742\n",
      "Current prediction:  60.88229751586914 \n",
      "\n",
      "Iteration 1989, Loss: 37.756832122802734, L1: 11.594206809997559, L3: 26.16262435913086\n",
      "Current prediction:  60.888145446777344 \n",
      "\n",
      "Iteration 1990, Loss: 37.03878402709961, L1: 10.633685111999512, L3: 26.405099868774414\n",
      "Current prediction:  60.89474105834961 \n",
      "\n",
      "Iteration 1991, Loss: 36.78227996826172, L1: 10.580330848693848, L3: 26.201950073242188\n",
      "Current prediction:  61.41685485839844 \n",
      "\n",
      "Iteration 1992, Loss: 36.67695999145508, L1: 10.540236473083496, L3: 26.136722564697266\n",
      "Current prediction:  61.4779167175293 \n",
      "\n",
      "Iteration 1993, Loss: 36.43857955932617, L1: 10.50063705444336, L3: 25.937942504882812\n",
      "Current prediction:  61.481178283691406 \n",
      "\n",
      "Iteration 1994, Loss: 36.119712829589844, L1: 10.467426300048828, L3: 25.652284622192383\n",
      "Current prediction:  61.483642578125 \n",
      "\n",
      "Iteration 1995, Loss: 37.4168701171875, L1: 11.713605880737305, L3: 25.703266143798828\n",
      "Current prediction:  61.48508834838867 \n",
      "\n",
      "Iteration 1996, Loss: 35.56840515136719, L1: 10.415305137634277, L3: 25.153100967407227\n",
      "Current prediction:  61.48612976074219 \n",
      "\n",
      "Iteration 1997, Loss: 37.676300048828125, L1: 10.41562271118164, L3: 27.260677337646484\n",
      "Current prediction:  61.48588562011719 \n",
      "\n",
      "Iteration 1998, Loss: 35.72483444213867, L1: 10.429898262023926, L3: 25.29493522644043\n",
      "Current prediction:  61.48447799682617 \n",
      "\n",
      "â†³ LR reduced to 1.0e-03 at iteration 2000 \n",
      "\n",
      "Iteration 1999, Loss: 36.342281341552734, L1: 10.357022285461426, L3: 25.985260009765625\n",
      "Current prediction:  61.481666564941406 \n",
      "\n",
      "Iteration 2000, Loss: 36.818275451660156, L1: 10.442651748657227, L3: 26.375621795654297\n",
      "Current prediction:  61.47799301147461 \n",
      "\n",
      "Iteration 2001, Loss: 37.37327575683594, L1: 10.504180908203125, L3: 26.86909294128418\n",
      "Current prediction:  61.473323822021484 \n",
      "\n",
      "Iteration 2002, Loss: 35.29570388793945, L1: 10.503484725952148, L3: 24.792219161987305\n",
      "Current prediction:  61.46864700317383 \n",
      "\n",
      "Iteration 2003, Loss: 36.842247009277344, L1: 10.454566955566406, L3: 26.387678146362305\n",
      "Current prediction:  61.463409423828125 \n",
      "\n",
      "Iteration 2004, Loss: 37.1044807434082, L1: 10.723714828491211, L3: 26.380765914916992\n",
      "Current prediction:  61.45722579956055 \n",
      "\n",
      "Iteration 2005, Loss: 36.445892333984375, L1: 10.458338737487793, L3: 25.987552642822266\n",
      "Current prediction:  61.45071792602539 \n",
      "\n",
      "Iteration 2006, Loss: 36.61756134033203, L1: 10.455268859863281, L3: 26.16229248046875\n",
      "Current prediction:  61.44415283203125 \n",
      "\n",
      "Iteration 2007, Loss: 36.636863708496094, L1: 10.39736557006836, L3: 26.239500045776367\n",
      "Current prediction:  61.4374885559082 \n",
      "\n",
      "Iteration 2008, Loss: 37.61137008666992, L1: 10.500188827514648, L3: 27.111181259155273\n",
      "Current prediction:  61.42971420288086 \n",
      "\n",
      "Iteration 2009, Loss: 36.420162200927734, L1: 10.488408088684082, L3: 25.931753158569336\n",
      "Current prediction:  61.422115325927734 \n",
      "\n",
      "Iteration 2010, Loss: 37.09064865112305, L1: 10.545672416687012, L3: 26.54497528076172\n",
      "Current prediction:  61.41421127319336 \n",
      "\n",
      "Iteration 2011, Loss: 37.18695831298828, L1: 10.51559066772461, L3: 26.67136573791504\n",
      "Current prediction:  61.40599060058594 \n",
      "\n",
      "Iteration 2012, Loss: 37.246925354003906, L1: 10.600019454956055, L3: 26.64690399169922\n",
      "Current prediction:  61.398658752441406 \n",
      "\n",
      "Iteration 2013, Loss: 36.22437286376953, L1: 10.559226036071777, L3: 25.66514778137207\n",
      "Current prediction:  61.39110565185547 \n",
      "\n",
      "Iteration 2014, Loss: 37.36031723022461, L1: 10.51794719696045, L3: 26.842369079589844\n",
      "Current prediction:  61.38307189941406 \n",
      "\n",
      "Iteration 2015, Loss: 35.55791473388672, L1: 10.611687660217285, L3: 24.946226119995117\n",
      "Current prediction:  61.374671936035156 \n",
      "\n",
      "Iteration 2016, Loss: 36.563941955566406, L1: 10.608345985412598, L3: 25.955596923828125\n",
      "Current prediction:  61.36544418334961 \n",
      "\n",
      "Iteration 2017, Loss: 36.96693420410156, L1: 10.649127960205078, L3: 26.317806243896484\n",
      "Current prediction:  61.356014251708984 \n",
      "\n",
      "Iteration 2018, Loss: 36.871253967285156, L1: 10.600769996643066, L3: 26.270484924316406\n",
      "Current prediction:  61.346351623535156 \n",
      "\n",
      "Iteration 2019, Loss: 37.16907501220703, L1: 10.58580493927002, L3: 26.583271026611328\n",
      "Current prediction:  61.337093353271484 \n",
      "\n",
      "Iteration 2020, Loss: 36.47526931762695, L1: 10.634955406188965, L3: 25.840314865112305\n",
      "Current prediction:  61.328487396240234 \n",
      "\n",
      "Iteration 2021, Loss: 37.61051559448242, L1: 10.534658432006836, L3: 27.075857162475586\n",
      "Current prediction:  61.3191032409668 \n",
      "\n",
      "Iteration 2022, Loss: 36.598350524902344, L1: 10.57890510559082, L3: 26.01944351196289\n",
      "Current prediction:  61.31028747558594 \n",
      "\n",
      "Iteration 2023, Loss: 36.503414154052734, L1: 10.692217826843262, L3: 25.81119728088379\n",
      "Current prediction:  61.301578521728516 \n",
      "\n",
      "Iteration 2024, Loss: 36.800811767578125, L1: 10.602581024169922, L3: 26.198232650756836\n",
      "Current prediction:  61.29356002807617 \n",
      "\n",
      "Iteration 2025, Loss: 36.68099594116211, L1: 10.63156795501709, L3: 26.049428939819336\n",
      "Current prediction:  61.28631591796875 \n",
      "\n",
      "Iteration 2026, Loss: 36.65574264526367, L1: 10.6207914352417, L3: 26.034950256347656\n",
      "Current prediction:  61.279781341552734 \n",
      "\n",
      "Iteration 2027, Loss: 36.30640411376953, L1: 10.616395950317383, L3: 25.690006256103516\n",
      "Current prediction:  61.27487564086914 \n",
      "\n",
      "Iteration 2028, Loss: 36.336639404296875, L1: 10.660053253173828, L3: 25.67658805847168\n",
      "Current prediction:  60.70530319213867 \n",
      "\n",
      "Iteration 2029, Loss: 36.962059020996094, L1: 10.741869926452637, L3: 26.220190048217773\n",
      "Current prediction:  60.698123931884766 \n",
      "\n",
      "Iteration 2030, Loss: 36.777748107910156, L1: 10.822490692138672, L3: 25.95525550842285\n",
      "Current prediction:  60.6987190246582 \n",
      "\n",
      "Iteration 2031, Loss: 37.176822662353516, L1: 10.890158653259277, L3: 26.286663055419922\n",
      "Current prediction:  60.701778411865234 \n",
      "\n",
      "Iteration 2032, Loss: 36.93244171142578, L1: 10.998586654663086, L3: 25.933856964111328\n",
      "Current prediction:  60.706993103027344 \n",
      "\n",
      "Iteration 2033, Loss: 37.3616943359375, L1: 11.113676071166992, L3: 26.248018264770508\n",
      "Current prediction:  60.71377944946289 \n",
      "\n",
      "Iteration 2034, Loss: 37.264442443847656, L1: 11.069561004638672, L3: 26.194881439208984\n",
      "Current prediction:  60.72248840332031 \n",
      "\n",
      "Iteration 2035, Loss: 37.33995056152344, L1: 11.175411224365234, L3: 26.16453742980957\n",
      "Current prediction:  60.73320007324219 \n",
      "\n",
      "Iteration 2036, Loss: 36.90938186645508, L1: 11.062670707702637, L3: 25.846712112426758\n",
      "Current prediction:  60.74544906616211 \n",
      "\n",
      "Iteration 2037, Loss: 36.42530059814453, L1: 11.01358413696289, L3: 25.41171646118164\n",
      "Current prediction:  60.76036834716797 \n",
      "\n",
      "Iteration 2038, Loss: 36.367347717285156, L1: 10.964020729064941, L3: 25.40332794189453\n",
      "Current prediction:  60.776432037353516 \n",
      "\n",
      "Iteration 2039, Loss: 35.97235870361328, L1: 10.906051635742188, L3: 25.06630516052246\n",
      "Current prediction:  60.793861389160156 \n",
      "\n",
      "Iteration 2040, Loss: 36.77814483642578, L1: 10.891324996948242, L3: 25.88681983947754\n",
      "Current prediction:  60.81201934814453 \n",
      "\n",
      "Iteration 2041, Loss: 36.76106262207031, L1: 10.99445629119873, L3: 25.766605377197266\n",
      "Current prediction:  60.83070373535156 \n",
      "\n",
      "Iteration 2042, Loss: 37.000762939453125, L1: 10.860166549682617, L3: 26.140596389770508\n",
      "Current prediction:  60.85018539428711 \n",
      "\n",
      "Iteration 2043, Loss: 36.31986999511719, L1: 10.842689514160156, L3: 25.47718048095703\n",
      "Current prediction:  60.87030029296875 \n",
      "\n",
      "Iteration 2044, Loss: 36.78559494018555, L1: 10.826675415039062, L3: 25.958919525146484\n",
      "Current prediction:  60.88998031616211 \n",
      "\n",
      "Iteration 2045, Loss: 35.68317794799805, L1: 10.81002140045166, L3: 24.873157501220703\n",
      "Current prediction:  60.90971755981445 \n",
      "\n",
      "Iteration 2046, Loss: 36.848228454589844, L1: 10.793318748474121, L3: 26.054908752441406\n",
      "Current prediction:  60.92894744873047 \n",
      "\n",
      "Iteration 2047, Loss: 37.177345275878906, L1: 10.777045249938965, L3: 26.400299072265625\n",
      "Current prediction:  60.94816207885742 \n",
      "\n",
      "Iteration 2048, Loss: 36.47542190551758, L1: 10.760787010192871, L3: 25.71463394165039\n",
      "Current prediction:  60.967533111572266 \n",
      "\n",
      "Iteration 2049, Loss: 37.06908416748047, L1: 10.74438762664795, L3: 26.324697494506836\n",
      "Current prediction:  60.985801696777344 \n",
      "\n",
      "Iteration 2050, Loss: 35.45350646972656, L1: 10.728914260864258, L3: 24.724590301513672\n",
      "Current prediction:  61.0042724609375 \n",
      "\n",
      "Iteration 2051, Loss: 37.90192413330078, L1: 10.713279724121094, L3: 27.188644409179688\n",
      "Current prediction:  61.022701263427734 \n",
      "\n",
      "Iteration 2052, Loss: 36.12530517578125, L1: 10.697673797607422, L3: 25.427629470825195\n",
      "Current prediction:  61.03988265991211 \n",
      "\n",
      "Iteration 2053, Loss: 36.225894927978516, L1: 10.683114051818848, L3: 25.542781829833984\n",
      "Current prediction:  61.05669021606445 \n",
      "\n",
      "Iteration 2054, Loss: 35.97821807861328, L1: 10.668882369995117, L3: 25.30933380126953\n",
      "Current prediction:  61.073097229003906 \n",
      "\n",
      "Iteration 2055, Loss: 37.143333435058594, L1: 10.65498161315918, L3: 26.488353729248047\n",
      "Current prediction:  61.08855056762695 \n",
      "\n",
      "Iteration 2056, Loss: 36.945926666259766, L1: 10.642065048217773, L3: 26.303861618041992\n",
      "Current prediction:  61.102134704589844 \n",
      "\n",
      "Iteration 2057, Loss: 36.67731475830078, L1: 10.630374908447266, L3: 26.046937942504883\n",
      "Current prediction:  61.11545944213867 \n",
      "\n",
      "Iteration 2058, Loss: 36.66193389892578, L1: 10.619091033935547, L3: 26.042842864990234\n",
      "Current prediction:  61.12853240966797 \n",
      "\n",
      "Iteration 2059, Loss: 36.55134201049805, L1: 10.608012199401855, L3: 25.943328857421875\n",
      "Current prediction:  61.13990020751953 \n",
      "\n",
      "Iteration 2060, Loss: 36.661285400390625, L1: 10.598371505737305, L3: 26.06291389465332\n",
      "Current prediction:  61.15052032470703 \n",
      "\n",
      "Iteration 2061, Loss: 36.50956726074219, L1: 10.67902946472168, L3: 25.83053970336914\n",
      "Current prediction:  61.16047286987305 \n",
      "\n",
      "Iteration 2062, Loss: 36.5262565612793, L1: 10.750109672546387, L3: 25.776147842407227\n",
      "Current prediction:  61.17064666748047 \n",
      "\n",
      "Iteration 2063, Loss: 37.063865661621094, L1: 10.572312355041504, L3: 26.491552352905273\n",
      "Current prediction:  61.18051528930664 \n",
      "\n",
      "Iteration 2064, Loss: 36.62489318847656, L1: 10.563949584960938, L3: 26.060945510864258\n",
      "Current prediction:  61.18873977661133 \n",
      "\n",
      "Iteration 2065, Loss: 36.72880172729492, L1: 10.556975364685059, L3: 26.17182731628418\n",
      "Current prediction:  61.196556091308594 \n",
      "\n",
      "Iteration 2066, Loss: 36.63812255859375, L1: 10.5503511428833, L3: 26.087770462036133\n",
      "Current prediction:  61.20399475097656 \n",
      "\n",
      "Iteration 2067, Loss: 36.02659225463867, L1: 10.544038772583008, L3: 25.482553482055664\n",
      "Current prediction:  61.21098709106445 \n",
      "\n",
      "Iteration 2068, Loss: 36.077064514160156, L1: 10.53810977935791, L3: 25.53895378112793\n",
      "Current prediction:  61.2171630859375 \n",
      "\n",
      "Iteration 2069, Loss: 36.997039794921875, L1: 10.532876968383789, L3: 26.464162826538086\n",
      "Current prediction:  61.22245407104492 \n",
      "\n",
      "Iteration 2070, Loss: 37.530517578125, L1: 10.528383255004883, L3: 27.002134323120117\n",
      "Current prediction:  61.225589752197266 \n",
      "\n",
      "Iteration 2071, Loss: 35.977256774902344, L1: 10.525725364685059, L3: 25.4515323638916\n",
      "Current prediction:  61.22811508178711 \n",
      "\n",
      "Iteration 2072, Loss: 36.374107360839844, L1: 10.523587226867676, L3: 25.85051918029785\n",
      "Current prediction:  61.22978591918945 \n",
      "\n",
      "Iteration 2073, Loss: 36.1412239074707, L1: 10.522173881530762, L3: 25.619050979614258\n",
      "Current prediction:  61.23112487792969 \n",
      "\n",
      "Iteration 2074, Loss: 36.9251823425293, L1: 10.521027565002441, L3: 26.404155731201172\n",
      "Current prediction:  61.232093811035156 \n",
      "\n",
      "Iteration 2075, Loss: 37.137962341308594, L1: 10.520211219787598, L3: 26.61775016784668\n",
      "Current prediction:  61.23259353637695 \n",
      "\n",
      "Iteration 2076, Loss: 37.088008880615234, L1: 10.519783973693848, L3: 26.568225860595703\n",
      "Current prediction:  61.23249435424805 \n",
      "\n",
      "Iteration 2077, Loss: 36.36174011230469, L1: 10.51987075805664, L3: 25.84187126159668\n",
      "Current prediction:  61.23127365112305 \n",
      "\n",
      "Iteration 2078, Loss: 35.84206008911133, L1: 10.520914077758789, L3: 25.32114601135254\n",
      "Current prediction:  61.2305908203125 \n",
      "\n",
      "Iteration 2079, Loss: 36.71426773071289, L1: 10.521486282348633, L3: 26.192781448364258\n",
      "Current prediction:  61.230323791503906 \n",
      "\n",
      "Iteration 2080, Loss: 36.15111541748047, L1: 10.521713256835938, L3: 25.629404067993164\n",
      "Current prediction:  61.229713439941406 \n",
      "\n",
      "Iteration 2081, Loss: 36.054344177246094, L1: 10.522232055664062, L3: 25.532114028930664\n",
      "Current prediction:  61.22886276245117 \n",
      "\n",
      "Iteration 2082, Loss: 37.00718307495117, L1: 10.52293586730957, L3: 26.4842472076416\n",
      "Current prediction:  61.228050231933594 \n",
      "\n",
      "Iteration 2083, Loss: 36.44330978393555, L1: 10.523638725280762, L3: 25.91967010498047\n",
      "Current prediction:  61.22720718383789 \n",
      "\n",
      "Iteration 2084, Loss: 37.278385162353516, L1: 10.524362564086914, L3: 26.7540225982666\n",
      "Current prediction:  61.226139068603516 \n",
      "\n",
      "Iteration 2085, Loss: 36.41944122314453, L1: 10.525261878967285, L3: 25.89417839050293\n",
      "Current prediction:  61.224788665771484 \n",
      "\n",
      "Iteration 2086, Loss: 36.123626708984375, L1: 10.526405334472656, L3: 25.597219467163086\n",
      "Current prediction:  61.2233772277832 \n",
      "\n",
      "Iteration 2087, Loss: 36.95573425292969, L1: 10.527599334716797, L3: 26.428136825561523\n",
      "Current prediction:  61.221466064453125 \n",
      "\n",
      "Iteration 2088, Loss: 36.52774429321289, L1: 10.529226303100586, L3: 25.998517990112305\n",
      "Current prediction:  61.22001266479492 \n",
      "\n",
      "Iteration 2089, Loss: 37.26445770263672, L1: 10.530457496643066, L3: 26.733999252319336\n",
      "Current prediction:  61.21894073486328 \n",
      "\n",
      "Iteration 2090, Loss: 36.779396057128906, L1: 10.53136920928955, L3: 26.248027801513672\n",
      "Current prediction:  61.218631744384766 \n",
      "\n",
      "Iteration 2091, Loss: 37.0457878112793, L1: 10.531628608703613, L3: 26.51416015625\n",
      "Current prediction:  61.2185173034668 \n",
      "\n",
      "Iteration 2092, Loss: 35.930015563964844, L1: 10.53172492980957, L3: 25.398292541503906\n",
      "Current prediction:  61.21867752075195 \n",
      "\n",
      "Iteration 2093, Loss: 36.608787536621094, L1: 10.531575202941895, L3: 26.077213287353516\n",
      "Current prediction:  61.21906661987305 \n",
      "\n",
      "Iteration 2094, Loss: 36.29077911376953, L1: 10.531255722045898, L3: 25.759521484375\n",
      "Current prediction:  61.21915817260742 \n",
      "\n",
      "Iteration 2095, Loss: 39.453369140625, L1: 14.107723236083984, L3: 25.345643997192383\n",
      "Current prediction:  61.218997955322266 \n",
      "\n",
      "Iteration 2096, Loss: 36.177101135253906, L1: 10.531315803527832, L3: 25.645784378051758\n",
      "Current prediction:  61.21815490722656 \n",
      "\n",
      "Iteration 2097, Loss: 36.5572624206543, L1: 10.532035827636719, L3: 26.025226593017578\n",
      "Current prediction:  61.21791076660156 \n",
      "\n",
      "Iteration 2098, Loss: 37.23261260986328, L1: 10.532241821289062, L3: 26.700368881225586\n",
      "Current prediction:  61.218040466308594 \n",
      "\n",
      "Iteration 2099, Loss: 37.02035903930664, L1: 10.532132148742676, L3: 26.48822784423828\n",
      "Current prediction:  61.217506408691406 \n",
      "\n",
      "Iteration 2100, Loss: 37.33416748046875, L1: 10.532587051391602, L3: 26.801578521728516\n",
      "Current prediction:  61.21718978881836 \n",
      "\n",
      "Iteration 2101, Loss: 36.986812591552734, L1: 10.532853126525879, L3: 26.45395851135254\n",
      "Current prediction:  61.21707534790039 \n",
      "\n",
      "Iteration 2102, Loss: 36.651649475097656, L1: 10.532951354980469, L3: 26.118696212768555\n",
      "Current prediction:  61.21774673461914 \n",
      "\n",
      "Iteration 2103, Loss: 36.9762077331543, L1: 10.532380104064941, L3: 26.443828582763672\n",
      "Current prediction:  61.21759796142578 \n",
      "\n",
      "Iteration 2104, Loss: 36.585731506347656, L1: 10.532503128051758, L3: 26.0532283782959\n",
      "Current prediction:  61.217857360839844 \n",
      "\n",
      "Iteration 2105, Loss: 35.91560363769531, L1: 10.535507202148438, L3: 25.380096435546875\n",
      "Current prediction:  61.21754837036133 \n",
      "\n",
      "Iteration 2106, Loss: 36.90587615966797, L1: 10.532548904418945, L3: 26.37332534790039\n",
      "Current prediction:  61.2170295715332 \n",
      "\n",
      "Iteration 2107, Loss: 36.64707946777344, L1: 10.532989501953125, L3: 26.114091873168945\n",
      "Current prediction:  61.21566390991211 \n",
      "\n",
      "Iteration 2108, Loss: 36.8392219543457, L1: 10.534148216247559, L3: 26.30507469177246\n",
      "Current prediction:  61.21387481689453 \n",
      "\n",
      "Iteration 2109, Loss: 36.506465911865234, L1: 10.535660743713379, L3: 25.97080421447754\n",
      "Current prediction:  61.212646484375 \n",
      "\n",
      "Iteration 2110, Loss: 36.606353759765625, L1: 10.536706924438477, L3: 26.06964683532715\n",
      "Current prediction:  61.211456298828125 \n",
      "\n",
      "Iteration 2111, Loss: 36.871402740478516, L1: 10.537717819213867, L3: 26.33368492126465\n",
      "Current prediction:  61.20968246459961 \n",
      "\n",
      "Iteration 2112, Loss: 36.79330062866211, L1: 10.539219856262207, L3: 26.25408172607422\n",
      "Current prediction:  61.207923889160156 \n",
      "\n",
      "Iteration 2113, Loss: 36.846553802490234, L1: 10.540711402893066, L3: 26.30584144592285\n",
      "Current prediction:  61.20661926269531 \n",
      "\n",
      "Iteration 2114, Loss: 36.422393798828125, L1: 10.541813850402832, L3: 25.88058090209961\n",
      "Current prediction:  61.205169677734375 \n",
      "\n",
      "Iteration 2115, Loss: 36.80630874633789, L1: 10.543044090270996, L3: 26.26326560974121\n",
      "Current prediction:  61.20362854003906 \n",
      "\n",
      "Iteration 2116, Loss: 36.338218688964844, L1: 10.544349670410156, L3: 25.79387092590332\n",
      "Current prediction:  61.20234298706055 \n",
      "\n",
      "Iteration 2117, Loss: 36.899410247802734, L1: 10.545438766479492, L3: 26.353971481323242\n",
      "Current prediction:  61.201229095458984 \n",
      "\n",
      "Iteration 2118, Loss: 35.855018615722656, L1: 10.54638671875, L3: 25.308631896972656\n",
      "Current prediction:  61.201412200927734 \n",
      "\n",
      "Iteration 2119, Loss: 36.38958740234375, L1: 10.546232223510742, L3: 25.843353271484375\n",
      "Current prediction:  61.201759338378906 \n",
      "\n",
      "Iteration 2120, Loss: 37.14715576171875, L1: 10.54593563079834, L3: 26.601221084594727\n",
      "Current prediction:  61.20173645019531 \n",
      "\n",
      "Iteration 2121, Loss: 36.82788848876953, L1: 10.545958518981934, L3: 26.28192901611328\n",
      "Current prediction:  61.20176696777344 \n",
      "\n",
      "Iteration 2122, Loss: 36.5693244934082, L1: 10.545924186706543, L3: 26.023399353027344\n",
      "Current prediction:  61.20259475708008 \n",
      "\n",
      "Iteration 2123, Loss: 36.1345100402832, L1: 10.545228004455566, L3: 25.58928108215332\n",
      "Current prediction:  61.20336151123047 \n",
      "\n",
      "Iteration 2124, Loss: 35.799896240234375, L1: 10.544578552246094, L3: 25.25531768798828\n",
      "Current prediction:  61.20405197143555 \n",
      "\n",
      "Iteration 2125, Loss: 36.96509552001953, L1: 10.543992042541504, L3: 26.42110252380371\n",
      "Current prediction:  61.20455551147461 \n",
      "\n",
      "Iteration 2126, Loss: 36.98354721069336, L1: 10.543562889099121, L3: 26.439985275268555\n",
      "Current prediction:  61.20421600341797 \n",
      "\n",
      "Iteration 2127, Loss: 36.578819274902344, L1: 10.543851852416992, L3: 26.034969329833984\n",
      "Current prediction:  61.203346252441406 \n",
      "\n",
      "Iteration 2128, Loss: 36.516536712646484, L1: 10.54458999633789, L3: 25.971946716308594\n",
      "Current prediction:  61.20277786254883 \n",
      "\n",
      "Iteration 2129, Loss: 37.231903076171875, L1: 10.545073509216309, L3: 26.686830520629883\n",
      "Current prediction:  61.20188522338867 \n",
      "\n",
      "Iteration 2130, Loss: 36.501564025878906, L1: 10.545831680297852, L3: 25.955732345581055\n",
      "Current prediction:  61.201171875 \n",
      "\n",
      "Iteration 2131, Loss: 36.75932693481445, L1: 10.546431541442871, L3: 26.212894439697266\n",
      "Current prediction:  61.20064926147461 \n",
      "\n",
      "Iteration 2132, Loss: 36.11222457885742, L1: 10.54687786102295, L3: 25.56534767150879\n",
      "Current prediction:  61.199493408203125 \n",
      "\n",
      "Iteration 2133, Loss: 36.71750259399414, L1: 10.547858238220215, L3: 26.169645309448242\n",
      "Current prediction:  61.19871520996094 \n",
      "\n",
      "Iteration 2134, Loss: 37.73011016845703, L1: 10.548514366149902, L3: 27.181596755981445\n",
      "Current prediction:  61.197044372558594 \n",
      "\n",
      "Iteration 2135, Loss: 36.57127380371094, L1: 10.549931526184082, L3: 26.02134132385254\n",
      "Current prediction:  61.19579315185547 \n",
      "\n",
      "Iteration 2136, Loss: 36.81361389160156, L1: 10.550993919372559, L3: 26.26262092590332\n",
      "Current prediction:  61.19337463378906 \n",
      "\n",
      "Iteration 2137, Loss: 36.6815185546875, L1: 10.553038597106934, L3: 26.128480911254883\n",
      "Current prediction:  61.19070053100586 \n",
      "\n",
      "Iteration 2138, Loss: 36.62361145019531, L1: 10.555314064025879, L3: 26.06829833984375\n",
      "Current prediction:  61.18913650512695 \n",
      "\n",
      "Iteration 2139, Loss: 37.13128662109375, L1: 10.556640625, L3: 26.574644088745117\n",
      "Current prediction:  61.186927795410156 \n",
      "\n",
      "Iteration 2140, Loss: 37.14042663574219, L1: 10.558449745178223, L3: 26.58197784423828\n",
      "Current prediction:  61.18421173095703 \n",
      "\n",
      "Iteration 2141, Loss: 36.441009521484375, L1: 10.560812950134277, L3: 25.880197525024414\n",
      "Current prediction:  61.182395935058594 \n",
      "\n",
      "Iteration 2142, Loss: 36.51879119873047, L1: 10.562355041503906, L3: 25.956438064575195\n",
      "Current prediction:  61.18087387084961 \n",
      "\n",
      "Iteration 2143, Loss: 36.30778884887695, L1: 10.563643455505371, L3: 25.7441463470459\n",
      "Current prediction:  61.18009567260742 \n",
      "\n",
      "Iteration 2144, Loss: 36.7138671875, L1: 10.564306259155273, L3: 26.14956283569336\n",
      "Current prediction:  61.17946243286133 \n",
      "\n",
      "Iteration 2145, Loss: 36.558349609375, L1: 10.564836502075195, L3: 25.993515014648438\n",
      "Current prediction:  61.17892837524414 \n",
      "\n",
      "Iteration 2146, Loss: 37.25455093383789, L1: 10.565293312072754, L3: 26.689258575439453\n",
      "Current prediction:  61.17939758300781 \n",
      "\n",
      "Iteration 2147, Loss: 36.84654998779297, L1: 10.564894676208496, L3: 26.28165626525879\n",
      "Current prediction:  61.18029022216797 \n",
      "\n",
      "Iteration 2148, Loss: 37.11993408203125, L1: 10.564135551452637, L3: 26.55579948425293\n",
      "Current prediction:  61.18183135986328 \n",
      "\n",
      "Iteration 2149, Loss: 37.32855224609375, L1: 10.562835693359375, L3: 26.765716552734375\n",
      "Current prediction:  61.183048248291016 \n",
      "\n",
      "Iteration 2150, Loss: 36.55205535888672, L1: 10.561800003051758, L3: 25.990253448486328\n",
      "Current prediction:  61.18378829956055 \n",
      "\n",
      "Iteration 2151, Loss: 37.14173889160156, L1: 10.560144424438477, L3: 26.58159637451172\n",
      "Current prediction:  61.184226989746094 \n",
      "\n",
      "Iteration 2152, Loss: 36.27716827392578, L1: 10.560795783996582, L3: 25.716371536254883\n",
      "Current prediction:  61.184722900390625 \n",
      "\n",
      "Iteration 2153, Loss: 35.99618148803711, L1: 10.560378074645996, L3: 25.43580436706543\n",
      "Current prediction:  61.18558120727539 \n",
      "\n",
      "Iteration 2154, Loss: 36.08065414428711, L1: 10.559650421142578, L3: 25.52100372314453\n",
      "Current prediction:  61.18651580810547 \n",
      "\n",
      "Iteration 2155, Loss: 36.28787612915039, L1: 10.558856964111328, L3: 25.729019165039062\n",
      "Current prediction:  61.187076568603516 \n",
      "\n",
      "Iteration 2156, Loss: 36.40663528442383, L1: 10.558380126953125, L3: 25.848255157470703\n",
      "Current prediction:  61.18729019165039 \n",
      "\n",
      "Iteration 2157, Loss: 36.90692901611328, L1: 10.558206558227539, L3: 26.348722457885742\n",
      "Current prediction:  61.18859100341797 \n",
      "\n",
      "Iteration 2158, Loss: 37.854801177978516, L1: 10.557099342346191, L3: 27.297700881958008\n",
      "Current prediction:  61.189090728759766 \n",
      "\n",
      "Iteration 2159, Loss: 39.0366325378418, L1: 11.797612190246582, L3: 27.23902130126953\n",
      "Current prediction:  61.18912887573242 \n",
      "\n",
      "Iteration 2160, Loss: 35.68268585205078, L1: 10.55665397644043, L3: 25.12603187561035\n",
      "Current prediction:  61.18898010253906 \n",
      "\n",
      "Iteration 2161, Loss: 36.57776641845703, L1: 10.556769371032715, L3: 26.02099609375\n",
      "Current prediction:  61.18881607055664 \n",
      "\n",
      "Iteration 2162, Loss: 36.082454681396484, L1: 10.556909561157227, L3: 25.525545120239258\n",
      "Current prediction:  61.18894577026367 \n",
      "\n",
      "Iteration 2163, Loss: 36.36846923828125, L1: 10.556804656982422, L3: 25.811662673950195\n",
      "Current prediction:  61.18880081176758 \n",
      "\n",
      "Iteration 2164, Loss: 37.470706939697266, L1: 10.556923866271973, L3: 26.913782119750977\n",
      "Current prediction:  61.188499450683594 \n",
      "\n",
      "Iteration 2165, Loss: 36.13447570800781, L1: 10.557178497314453, L3: 25.57729721069336\n",
      "Current prediction:  61.18939208984375 \n",
      "\n",
      "Iteration 2166, Loss: 37.33307647705078, L1: 10.55642032623291, L3: 26.776657104492188\n",
      "Current prediction:  61.190284729003906 \n",
      "\n",
      "Iteration 2167, Loss: 36.92919158935547, L1: 10.5556640625, L3: 26.37352752685547\n",
      "Current prediction:  61.191471099853516 \n",
      "\n",
      "Iteration 2168, Loss: 36.4803352355957, L1: 10.554656028747559, L3: 25.92568016052246\n",
      "Current prediction:  61.19307327270508 \n",
      "\n",
      "Iteration 2169, Loss: 36.512290954589844, L1: 10.553299903869629, L3: 25.9589900970459\n",
      "Current prediction:  61.194618225097656 \n",
      "\n",
      "Iteration 2170, Loss: 36.893157958984375, L1: 10.55198860168457, L3: 26.341171264648438\n",
      "Current prediction:  61.1965217590332 \n",
      "\n",
      "Iteration 2171, Loss: 36.363525390625, L1: 10.550373077392578, L3: 25.81315040588379\n",
      "Current prediction:  61.19820785522461 \n",
      "\n",
      "Iteration 2172, Loss: 36.76142120361328, L1: 10.5489501953125, L3: 26.21247100830078\n",
      "Current prediction:  61.20002365112305 \n",
      "\n",
      "Iteration 2173, Loss: 36.60078430175781, L1: 10.547406196594238, L3: 26.05337905883789\n",
      "Current prediction:  61.20088195800781 \n",
      "\n",
      "Iteration 2174, Loss: 37.12037658691406, L1: 10.546682357788086, L3: 26.573694229125977\n",
      "Current prediction:  61.201290130615234 \n",
      "\n",
      "Iteration 2175, Loss: 36.80804443359375, L1: 10.546330451965332, L3: 26.261714935302734\n",
      "Current prediction:  61.201988220214844 \n",
      "\n",
      "Iteration 2176, Loss: 36.196083068847656, L1: 10.545737266540527, L3: 25.650346755981445\n",
      "Current prediction:  61.20334243774414 \n",
      "\n",
      "Iteration 2177, Loss: 36.72685241699219, L1: 10.544588088989258, L3: 26.182262420654297\n",
      "Current prediction:  61.204750061035156 \n",
      "\n",
      "Iteration 2178, Loss: 36.116065979003906, L1: 10.543400764465332, L3: 25.57266616821289\n",
      "Current prediction:  61.20664596557617 \n",
      "\n",
      "Iteration 2179, Loss: 36.53719711303711, L1: 10.541787147521973, L3: 25.99540901184082\n",
      "Current prediction:  61.207733154296875 \n",
      "\n",
      "Iteration 2180, Loss: 36.752445220947266, L1: 10.540873527526855, L3: 26.211570739746094\n",
      "Current prediction:  61.20750427246094 \n",
      "\n",
      "Iteration 2181, Loss: 37.010581970214844, L1: 10.541065216064453, L3: 26.469514846801758\n",
      "Current prediction:  61.20583724975586 \n",
      "\n",
      "Iteration 2182, Loss: 36.262508392333984, L1: 10.542474746704102, L3: 25.720033645629883\n",
      "Current prediction:  61.20410919189453 \n",
      "\n",
      "Iteration 2183, Loss: 36.01799392700195, L1: 10.54394245147705, L3: 25.47405242919922\n",
      "Current prediction:  61.20205307006836 \n",
      "\n",
      "Iteration 2184, Loss: 36.9109001159668, L1: 10.545685768127441, L3: 26.365215301513672\n",
      "Current prediction:  61.19998550415039 \n",
      "\n",
      "Iteration 2185, Loss: 36.84029006958008, L1: 10.547438621520996, L3: 26.2928524017334\n",
      "Current prediction:  61.198760986328125 \n",
      "\n",
      "Iteration 2186, Loss: 35.25468444824219, L1: 10.548476219177246, L3: 24.706209182739258\n",
      "Current prediction:  61.1995849609375 \n",
      "\n",
      "Iteration 2187, Loss: 37.33845901489258, L1: 10.547778129577637, L3: 26.790681838989258\n",
      "Current prediction:  61.200557708740234 \n",
      "\n",
      "Iteration 2188, Loss: 36.79888916015625, L1: 10.546950340270996, L3: 26.251937866210938\n",
      "Current prediction:  61.20180892944336 \n",
      "\n",
      "Iteration 2189, Loss: 37.666744232177734, L1: 10.545891761779785, L3: 27.120853424072266\n",
      "Current prediction:  61.2024040222168 \n",
      "\n",
      "Iteration 2190, Loss: 36.84405517578125, L1: 10.545393943786621, L3: 26.298660278320312\n",
      "Current prediction:  61.20301055908203 \n",
      "\n",
      "Iteration 2191, Loss: 36.17841339111328, L1: 10.544876098632812, L3: 25.6335391998291\n",
      "Current prediction:  61.2026252746582 \n",
      "\n",
      "Iteration 2192, Loss: 35.36646270751953, L1: 10.54520034790039, L3: 24.821264266967773\n",
      "Current prediction:  61.20280838012695 \n",
      "\n",
      "Iteration 2193, Loss: 37.23972702026367, L1: 10.545042991638184, L3: 26.694684982299805\n",
      "Current prediction:  61.20248031616211 \n",
      "\n",
      "Iteration 2194, Loss: 36.684364318847656, L1: 10.545324325561523, L3: 26.139039993286133\n",
      "Current prediction:  61.20159149169922 \n",
      "\n",
      "Iteration 2195, Loss: 36.2696533203125, L1: 10.546076774597168, L3: 25.723575592041016\n",
      "Current prediction:  61.201171875 \n",
      "\n",
      "Iteration 2196, Loss: 37.379974365234375, L1: 10.546436309814453, L3: 26.833539962768555\n",
      "Current prediction:  61.20151901245117 \n",
      "\n",
      "Iteration 2197, Loss: 37.0753059387207, L1: 10.546135902404785, L3: 26.529170989990234\n",
      "Current prediction:  61.201480865478516 \n",
      "\n",
      "Iteration 2198, Loss: 36.554443359375, L1: 10.546173095703125, L3: 26.008268356323242\n",
      "Current prediction:  61.201087951660156 \n",
      "\n",
      "Iteration 2199, Loss: 35.93938064575195, L1: 10.546501159667969, L3: 25.392879486083984\n",
      "Current prediction:  61.201656341552734 \n",
      "\n",
      "Iteration 2200, Loss: 37.00767135620117, L1: 10.54602336883545, L3: 26.461647033691406\n",
      "Current prediction:  61.203285217285156 \n",
      "\n",
      "Iteration 2201, Loss: 38.03948974609375, L1: 10.544642448425293, L3: 27.49484634399414\n",
      "Current prediction:  61.20534896850586 \n",
      "\n",
      "Iteration 2202, Loss: 36.86487579345703, L1: 10.542954444885254, L3: 26.321922302246094\n",
      "Current prediction:  61.207122802734375 \n",
      "\n",
      "Iteration 2203, Loss: 36.59394836425781, L1: 10.541387557983398, L3: 26.052560806274414\n",
      "Current prediction:  61.208744049072266 \n",
      "\n",
      "Iteration 2204, Loss: 36.350582122802734, L1: 10.54001235961914, L3: 25.810569763183594\n",
      "Current prediction:  61.21047592163086 \n",
      "\n",
      "Iteration 2205, Loss: 36.232879638671875, L1: 10.538538932800293, L3: 25.694339752197266\n",
      "Current prediction:  61.21168518066406 \n",
      "\n",
      "Iteration 2206, Loss: 36.98657989501953, L1: 10.537519454956055, L3: 26.449058532714844\n",
      "Current prediction:  61.21255874633789 \n",
      "\n",
      "Iteration 2207, Loss: 36.27091598510742, L1: 10.536774635314941, L3: 25.734142303466797\n",
      "Current prediction:  61.21348571777344 \n",
      "\n",
      "Iteration 2208, Loss: 36.43545913696289, L1: 10.535993576049805, L3: 25.899465560913086\n",
      "Current prediction:  61.213626861572266 \n",
      "\n",
      "Iteration 2209, Loss: 37.127586364746094, L1: 10.535873413085938, L3: 26.59171485900879\n",
      "Current prediction:  61.214385986328125 \n",
      "\n",
      "Iteration 2210, Loss: 36.99934387207031, L1: 10.535226821899414, L3: 26.46411895751953\n",
      "Current prediction:  61.21482849121094 \n",
      "\n",
      "Iteration 2211, Loss: 37.13442611694336, L1: 10.534847259521484, L3: 26.599578857421875\n",
      "Current prediction:  61.215728759765625 \n",
      "\n",
      "Iteration 2212, Loss: 36.52054214477539, L1: 10.53409481048584, L3: 25.986448287963867\n",
      "Current prediction:  61.215145111083984 \n",
      "\n",
      "Iteration 2213, Loss: 35.786128997802734, L1: 10.534585952758789, L3: 25.251543045043945\n",
      "Current prediction:  61.21501541137695 \n",
      "\n",
      "Iteration 2214, Loss: 36.39289093017578, L1: 10.534689903259277, L3: 25.858200073242188\n",
      "Current prediction:  61.21486282348633 \n",
      "\n",
      "Iteration 2215, Loss: 35.536685943603516, L1: 10.534825325012207, L3: 25.001861572265625\n",
      "Current prediction:  61.21442794799805 \n",
      "\n",
      "Iteration 2216, Loss: 37.151790618896484, L1: 10.535194396972656, L3: 26.616596221923828\n",
      "Current prediction:  61.21390151977539 \n",
      "\n",
      "Iteration 2217, Loss: 35.766761779785156, L1: 10.53564167022705, L3: 25.231121063232422\n",
      "Current prediction:  61.214168548583984 \n",
      "\n",
      "Iteration 2218, Loss: 35.57737350463867, L1: 10.53541374206543, L3: 25.041959762573242\n",
      "Current prediction:  61.21391296386719 \n",
      "\n",
      "Iteration 2219, Loss: 36.7540283203125, L1: 10.535629272460938, L3: 26.218399047851562\n",
      "Current prediction:  61.21381378173828 \n",
      "\n",
      "Iteration 2220, Loss: 36.59447479248047, L1: 10.535713195800781, L3: 26.05876350402832\n",
      "Current prediction:  61.213539123535156 \n",
      "\n",
      "Iteration 2221, Loss: 36.642822265625, L1: 10.535948753356934, L3: 26.106874465942383\n",
      "Current prediction:  61.212791442871094 \n",
      "\n",
      "Iteration 2222, Loss: 36.02174377441406, L1: 10.536581039428711, L3: 25.485164642333984\n",
      "Current prediction:  61.21228790283203 \n",
      "\n",
      "Iteration 2223, Loss: 36.4457893371582, L1: 10.537012100219727, L3: 25.908777236938477\n",
      "Current prediction:  61.211219787597656 \n",
      "\n",
      "Iteration 2224, Loss: 36.80952453613281, L1: 10.537919044494629, L3: 26.2716064453125\n",
      "Current prediction:  61.21039962768555 \n",
      "\n",
      "Iteration 2225, Loss: 36.98579025268555, L1: 10.538605690002441, L3: 26.44718360900879\n",
      "Current prediction:  61.209510803222656 \n",
      "\n",
      "Iteration 2226, Loss: 35.812896728515625, L1: 10.539363861083984, L3: 25.273534774780273\n",
      "Current prediction:  61.20924377441406 \n",
      "\n",
      "Iteration 2227, Loss: 36.69664764404297, L1: 10.53958797454834, L3: 26.157058715820312\n",
      "Current prediction:  61.20827102661133 \n",
      "\n",
      "Iteration 2228, Loss: 36.32378005981445, L1: 10.540411949157715, L3: 25.783367156982422\n",
      "Current prediction:  61.208152770996094 \n",
      "\n",
      "Iteration 2229, Loss: 37.14503479003906, L1: 10.540512084960938, L3: 26.604524612426758\n",
      "Current prediction:  61.20729064941406 \n",
      "\n",
      "Iteration 2230, Loss: 36.31941223144531, L1: 10.541244506835938, L3: 25.778169631958008\n",
      "Current prediction:  61.2062873840332 \n",
      "\n",
      "Iteration 2231, Loss: 36.70557403564453, L1: 10.542097091674805, L3: 26.16347885131836\n",
      "Current prediction:  61.205535888671875 \n",
      "\n",
      "Iteration 2232, Loss: 35.030235290527344, L1: 10.54273796081543, L3: 24.487497329711914\n",
      "Current prediction:  61.20619583129883 \n",
      "\n",
      "Iteration 2233, Loss: 37.47665023803711, L1: 10.542170524597168, L3: 26.934478759765625\n",
      "Current prediction:  61.206825256347656 \n",
      "\n",
      "Iteration 2234, Loss: 35.964988708496094, L1: 10.541643142700195, L3: 25.42334747314453\n",
      "Current prediction:  61.207584381103516 \n",
      "\n",
      "Iteration 2235, Loss: 36.74896240234375, L1: 10.540994644165039, L3: 26.207969665527344\n",
      "Current prediction:  61.208457946777344 \n",
      "\n",
      "Iteration 2236, Loss: 37.075103759765625, L1: 10.540257453918457, L3: 26.534847259521484\n",
      "Current prediction:  61.20876693725586 \n",
      "\n",
      "Iteration 2237, Loss: 35.511436462402344, L1: 10.539993286132812, L3: 24.97144317626953\n",
      "Current prediction:  61.20984649658203 \n",
      "\n",
      "Iteration 2238, Loss: 37.49996566772461, L1: 10.539080619812012, L3: 26.96088409423828\n",
      "Current prediction:  61.20969772338867 \n",
      "\n",
      "Iteration 2239, Loss: 37.03911590576172, L1: 10.539206504821777, L3: 26.499908447265625\n",
      "Current prediction:  61.20957946777344 \n",
      "\n",
      "Iteration 2240, Loss: 36.4759635925293, L1: 10.539305686950684, L3: 25.936656951904297\n",
      "Current prediction:  61.2092170715332 \n",
      "\n",
      "Iteration 2241, Loss: 36.95372772216797, L1: 10.539610862731934, L3: 26.41411781311035\n",
      "Current prediction:  61.208133697509766 \n",
      "\n",
      "Iteration 2242, Loss: 37.61618423461914, L1: 10.540526390075684, L3: 27.07565689086914\n",
      "Current prediction:  61.20670700073242 \n",
      "\n",
      "Iteration 2243, Loss: 36.20775604248047, L1: 10.541743278503418, L3: 25.666011810302734\n",
      "Current prediction:  61.20545196533203 \n",
      "\n",
      "Iteration 2244, Loss: 36.67677307128906, L1: 10.542803764343262, L3: 26.133970260620117\n",
      "Current prediction:  61.20341873168945 \n",
      "\n",
      "Iteration 2245, Loss: 36.560935974121094, L1: 10.544530868530273, L3: 26.01640510559082\n",
      "Current prediction:  61.201019287109375 \n",
      "\n",
      "Iteration 2246, Loss: 36.549339294433594, L1: 10.546562194824219, L3: 26.002775192260742\n",
      "Current prediction:  61.19856262207031 \n",
      "\n",
      "Iteration 2247, Loss: 36.51874923706055, L1: 10.547868728637695, L3: 25.97088050842285\n",
      "Current prediction:  61.19656753540039 \n",
      "\n",
      "Iteration 2248, Loss: 36.77351379394531, L1: 10.550337791442871, L3: 26.223175048828125\n",
      "Current prediction:  61.19444274902344 \n",
      "\n",
      "Iteration 2249, Loss: 36.44672393798828, L1: 10.55213737487793, L3: 25.89458465576172\n",
      "Current prediction:  61.193416595458984 \n",
      "\n",
      "Iteration 2250, Loss: 37.14788055419922, L1: 10.553010940551758, L3: 26.594871520996094\n",
      "Current prediction:  61.19234848022461 \n",
      "\n",
      "Iteration 2251, Loss: 37.466796875, L1: 10.553912162780762, L3: 26.912883758544922\n",
      "Current prediction:  61.191123962402344 \n",
      "\n",
      "Iteration 2252, Loss: 35.599735260009766, L1: 10.554954528808594, L3: 25.044780731201172\n",
      "Current prediction:  61.190032958984375 \n",
      "\n",
      "Iteration 2253, Loss: 36.40744400024414, L1: 10.555877685546875, L3: 25.851566314697266\n",
      "Current prediction:  61.18922805786133 \n",
      "\n",
      "Iteration 2254, Loss: 37.09235763549805, L1: 10.556560516357422, L3: 26.535797119140625\n",
      "Current prediction:  61.18825149536133 \n",
      "\n",
      "Iteration 2255, Loss: 37.148521423339844, L1: 10.557388305664062, L3: 26.59113121032715\n",
      "Current prediction:  61.18693542480469 \n",
      "\n",
      "Iteration 2256, Loss: 38.19194412231445, L1: 10.55850601196289, L3: 27.633438110351562\n",
      "Current prediction:  61.18524169921875 \n",
      "\n",
      "Iteration 2257, Loss: 36.90617370605469, L1: 10.559937477111816, L3: 26.346237182617188\n",
      "Current prediction:  61.18415832519531 \n",
      "\n",
      "Iteration 2258, Loss: 37.98564529418945, L1: 10.560856819152832, L3: 27.424787521362305\n",
      "Current prediction:  61.18294906616211 \n",
      "\n",
      "Iteration 2259, Loss: 36.12489318847656, L1: 10.561883926391602, L3: 25.563007354736328\n",
      "Current prediction:  61.18185806274414 \n",
      "\n",
      "Iteration 2260, Loss: 36.62531280517578, L1: 10.562808990478516, L3: 26.062503814697266\n",
      "Current prediction:  61.180782318115234 \n",
      "\n",
      "Iteration 2261, Loss: 36.578861236572266, L1: 10.563722610473633, L3: 26.015138626098633\n",
      "Current prediction:  61.179988861083984 \n",
      "\n",
      "Iteration 2262, Loss: 36.37099838256836, L1: 10.564393997192383, L3: 25.806604385375977\n",
      "Current prediction:  61.1799201965332 \n",
      "\n",
      "Iteration 2263, Loss: 36.7486686706543, L1: 10.564449310302734, L3: 26.184219360351562\n",
      "Current prediction:  61.18055725097656 \n",
      "\n",
      "Iteration 2264, Loss: 36.344791412353516, L1: 10.563904762268066, L3: 25.780885696411133\n",
      "Current prediction:  61.180789947509766 \n",
      "\n",
      "Iteration 2265, Loss: 36.15565490722656, L1: 10.563712120056152, L3: 25.591941833496094\n",
      "Current prediction:  61.18144226074219 \n",
      "\n",
      "Iteration 2266, Loss: 37.43657684326172, L1: 10.563158988952637, L3: 26.8734188079834\n",
      "Current prediction:  61.18169403076172 \n",
      "\n",
      "Iteration 2267, Loss: 36.357391357421875, L1: 10.562950134277344, L3: 25.79444122314453\n",
      "Current prediction:  61.18247604370117 \n",
      "\n",
      "Iteration 2268, Loss: 35.8607177734375, L1: 10.562284469604492, L3: 25.298431396484375\n",
      "Current prediction:  61.18349075317383 \n",
      "\n",
      "Iteration 2269, Loss: 37.19239807128906, L1: 10.56142807006836, L3: 26.630971908569336\n",
      "Current prediction:  61.18401336669922 \n",
      "\n",
      "Iteration 2270, Loss: 35.979698181152344, L1: 10.5609769821167, L3: 25.418720245361328\n",
      "Current prediction:  61.183685302734375 \n",
      "\n",
      "Iteration 2271, Loss: 36.658504486083984, L1: 10.561262130737305, L3: 26.09724235534668\n",
      "Current prediction:  61.18214797973633 \n",
      "\n",
      "Iteration 2272, Loss: 37.22901153564453, L1: 10.562561988830566, L3: 26.66645050048828\n",
      "Current prediction:  61.18079376220703 \n",
      "\n",
      "Iteration 2273, Loss: 35.370479583740234, L1: 10.56371784210205, L3: 24.8067626953125\n",
      "Current prediction:  61.17973709106445 \n",
      "\n",
      "Iteration 2274, Loss: 36.81782913208008, L1: 10.564603805541992, L3: 26.253225326538086\n",
      "Current prediction:  61.17886734008789 \n",
      "\n",
      "Iteration 2275, Loss: 36.1063232421875, L1: 10.565339088439941, L3: 25.540985107421875\n",
      "Current prediction:  61.17792510986328 \n",
      "\n",
      "Iteration 2276, Loss: 36.00736999511719, L1: 10.566137313842773, L3: 25.44123077392578\n",
      "Current prediction:  61.17757797241211 \n",
      "\n",
      "Iteration 2277, Loss: 37.308837890625, L1: 10.566436767578125, L3: 26.742403030395508\n",
      "Current prediction:  61.17758560180664 \n",
      "\n",
      "Iteration 2278, Loss: 36.338375091552734, L1: 10.566429138183594, L3: 25.77194595336914\n",
      "Current prediction:  61.17778396606445 \n",
      "\n",
      "Iteration 2279, Loss: 37.79859161376953, L1: 10.566259384155273, L3: 27.232330322265625\n",
      "Current prediction:  61.17774200439453 \n",
      "\n",
      "Iteration 2280, Loss: 36.83241271972656, L1: 10.566295623779297, L3: 26.266115188598633\n",
      "Current prediction:  61.17793655395508 \n",
      "\n",
      "Iteration 2281, Loss: 36.35981750488281, L1: 10.566132545471191, L3: 25.793684005737305\n",
      "Current prediction:  61.17901611328125 \n",
      "\n",
      "Iteration 2282, Loss: 37.27262496948242, L1: 10.565216064453125, L3: 26.707408905029297\n",
      "Current prediction:  61.17966079711914 \n",
      "\n",
      "Iteration 2283, Loss: 36.24492263793945, L1: 10.56467342376709, L3: 25.68025016784668\n",
      "Current prediction:  61.17985916137695 \n",
      "\n",
      "Iteration 2284, Loss: 36.437320709228516, L1: 10.56450366973877, L3: 25.872817993164062\n",
      "Current prediction:  61.180450439453125 \n",
      "\n",
      "Iteration 2285, Loss: 36.768653869628906, L1: 10.56400203704834, L3: 26.20465087890625\n",
      "Current prediction:  61.18129348754883 \n",
      "\n",
      "Iteration 2286, Loss: 36.613243103027344, L1: 10.563285827636719, L3: 26.049955368041992\n",
      "Current prediction:  61.181495666503906 \n",
      "\n",
      "Iteration 2287, Loss: 37.993263244628906, L1: 10.563114166259766, L3: 27.43014907836914\n",
      "Current prediction:  61.181053161621094 \n",
      "\n",
      "Iteration 2288, Loss: 36.74759292602539, L1: 10.563490867614746, L3: 26.184101104736328\n",
      "Current prediction:  61.18077850341797 \n",
      "\n",
      "Iteration 2289, Loss: 36.552555084228516, L1: 10.563720703125, L3: 25.988834381103516\n",
      "Current prediction:  61.18085479736328 \n",
      "\n",
      "Iteration 2290, Loss: 37.65716552734375, L1: 10.563660621643066, L3: 27.093505859375\n",
      "Current prediction:  61.180118560791016 \n",
      "\n",
      "Iteration 2291, Loss: 36.36927795410156, L1: 10.564282417297363, L3: 25.804994583129883\n",
      "Current prediction:  61.1798210144043 \n",
      "\n",
      "Iteration 2292, Loss: 35.78585433959961, L1: 10.564537048339844, L3: 25.221317291259766\n",
      "Current prediction:  61.17988204956055 \n",
      "\n",
      "Iteration 2293, Loss: 36.32443618774414, L1: 10.564485549926758, L3: 25.759950637817383\n",
      "Current prediction:  61.18065643310547 \n",
      "\n",
      "Iteration 2294, Loss: 35.91595458984375, L1: 10.563825607299805, L3: 25.352130889892578\n",
      "Current prediction:  61.181236267089844 \n",
      "\n",
      "Iteration 2295, Loss: 34.489044189453125, L1: 10.563331604003906, L3: 23.92571258544922\n",
      "Current prediction:  61.183040618896484 \n",
      "\n",
      "Iteration 2296, Loss: 36.50933837890625, L1: 10.561805725097656, L3: 25.94753074645996\n",
      "Current prediction:  61.185123443603516 \n",
      "\n",
      "Iteration 2297, Loss: 36.408851623535156, L1: 10.560035705566406, L3: 25.848817825317383\n",
      "Current prediction:  61.187740325927734 \n",
      "\n",
      "Iteration 2298, Loss: 36.51087951660156, L1: 10.557821273803711, L3: 25.95305633544922\n",
      "Current prediction:  61.1904182434082 \n",
      "\n",
      "Iteration 2299, Loss: 37.15534210205078, L1: 10.555549621582031, L3: 26.599794387817383\n",
      "Current prediction:  61.19215774536133 \n",
      "\n",
      "Iteration 2300, Loss: 36.50040054321289, L1: 10.554075241088867, L3: 25.946325302124023\n",
      "Current prediction:  61.194332122802734 \n",
      "\n",
      "Iteration 2301, Loss: 37.28487777709961, L1: 10.55223560333252, L3: 26.732643127441406\n",
      "Current prediction:  61.195858001708984 \n",
      "\n",
      "Iteration 2302, Loss: 37.295650482177734, L1: 10.550934791564941, L3: 26.744714736938477\n",
      "Current prediction:  61.19694900512695 \n",
      "\n",
      "Iteration 2303, Loss: 35.73344802856445, L1: 10.550017356872559, L3: 25.18343162536621\n",
      "Current prediction:  61.198272705078125 \n",
      "\n",
      "Iteration 2304, Loss: 36.15258026123047, L1: 10.548896789550781, L3: 25.603681564331055\n",
      "Current prediction:  61.19987106323242 \n",
      "\n",
      "Iteration 2305, Loss: 37.17580032348633, L1: 10.547531127929688, L3: 26.62826919555664\n",
      "Current prediction:  61.20160675048828 \n",
      "\n",
      "Iteration 2306, Loss: 36.16785430908203, L1: 10.546060562133789, L3: 25.621795654296875\n",
      "Current prediction:  61.20387649536133 \n",
      "\n",
      "Iteration 2307, Loss: 35.9537467956543, L1: 10.544137954711914, L3: 25.409608840942383\n",
      "Current prediction:  61.20518493652344 \n",
      "\n",
      "Iteration 2308, Loss: 36.994136810302734, L1: 10.543027877807617, L3: 26.451108932495117\n",
      "Current prediction:  61.20692443847656 \n",
      "\n",
      "Iteration 2309, Loss: 36.83038330078125, L1: 10.541555404663086, L3: 26.288827896118164\n",
      "Current prediction:  61.207576751708984 \n",
      "\n",
      "Iteration 2310, Loss: 37.89081954956055, L1: 10.541003227233887, L3: 27.349815368652344\n",
      "Current prediction:  61.20725631713867 \n",
      "\n",
      "Iteration 2311, Loss: 36.24278259277344, L1: 10.541275024414062, L3: 25.701505661010742\n",
      "Current prediction:  61.20703125 \n",
      "\n",
      "Iteration 2312, Loss: 36.45002746582031, L1: 10.541465759277344, L3: 25.90856170654297\n",
      "Current prediction:  61.20732116699219 \n",
      "\n",
      "Iteration 2313, Loss: 37.144683837890625, L1: 10.541219711303711, L3: 26.60346221923828\n",
      "Current prediction:  61.20737838745117 \n",
      "\n",
      "Iteration 2314, Loss: 36.0626335144043, L1: 10.541169166564941, L3: 25.52146339416504\n",
      "Current prediction:  61.2069206237793 \n",
      "\n",
      "Iteration 2315, Loss: 36.12434005737305, L1: 10.541556358337402, L3: 25.582782745361328\n",
      "Current prediction:  61.20669937133789 \n",
      "\n",
      "Iteration 2316, Loss: 36.171600341796875, L1: 10.541749000549316, L3: 25.629850387573242\n",
      "Current prediction:  61.20600128173828 \n",
      "\n",
      "Iteration 2317, Loss: 36.924224853515625, L1: 10.542339324951172, L3: 26.38188362121582\n",
      "Current prediction:  61.20506286621094 \n",
      "\n",
      "Iteration 2318, Loss: 36.693870544433594, L1: 10.543135643005371, L3: 26.150733947753906\n",
      "Current prediction:  61.20408248901367 \n",
      "\n",
      "Iteration 2319, Loss: 37.03738784790039, L1: 10.54396915435791, L3: 26.493419647216797\n",
      "Current prediction:  61.203346252441406 \n",
      "\n",
      "Iteration 2320, Loss: 36.931007385253906, L1: 10.54458999633789, L3: 26.38641929626465\n",
      "Current prediction:  61.20250701904297 \n",
      "\n",
      "Iteration 2321, Loss: 36.68898391723633, L1: 10.545306205749512, L3: 26.143678665161133\n",
      "Current prediction:  61.20189666748047 \n",
      "\n",
      "Iteration 2322, Loss: 36.27570343017578, L1: 10.545820236206055, L3: 25.72988510131836\n",
      "Current prediction:  61.20152282714844 \n",
      "\n",
      "Iteration 2323, Loss: 37.22886276245117, L1: 10.546135902404785, L3: 26.68272590637207\n",
      "Current prediction:  61.20155715942383 \n",
      "\n",
      "Iteration 2324, Loss: 36.977962493896484, L1: 10.546107292175293, L3: 26.431856155395508\n",
      "Current prediction:  61.20151138305664 \n",
      "\n",
      "Iteration 2325, Loss: 35.881996154785156, L1: 10.546144485473633, L3: 25.33584976196289\n",
      "Current prediction:  61.20240020751953 \n",
      "\n",
      "Iteration 2326, Loss: 37.175472259521484, L1: 10.545390129089355, L3: 26.630081176757812\n",
      "Current prediction:  61.202735900878906 \n",
      "\n",
      "Iteration 2327, Loss: 36.62840270996094, L1: 10.545106887817383, L3: 26.083297729492188\n",
      "Current prediction:  61.20245361328125 \n",
      "\n",
      "Iteration 2328, Loss: 36.99932098388672, L1: 10.545344352722168, L3: 26.453975677490234\n",
      "Current prediction:  61.20322036743164 \n",
      "\n",
      "Iteration 2329, Loss: 36.629852294921875, L1: 10.544692039489746, L3: 26.085159301757812\n",
      "Current prediction:  61.204219818115234 \n",
      "\n",
      "Iteration 2330, Loss: 36.65757751464844, L1: 10.54384994506836, L3: 26.11372947692871\n",
      "Current prediction:  61.204593658447266 \n",
      "\n",
      "Iteration 2331, Loss: 36.28712463378906, L1: 10.543532371520996, L3: 25.743593215942383\n",
      "Current prediction:  61.20551300048828 \n",
      "\n",
      "Iteration 2332, Loss: 37.14798355102539, L1: 10.542755126953125, L3: 26.605228424072266\n",
      "Current prediction:  61.20640563964844 \n",
      "\n",
      "Iteration 2333, Loss: 37.11466979980469, L1: 10.54199504852295, L3: 26.572675704956055\n",
      "Current prediction:  61.20738983154297 \n",
      "\n",
      "Iteration 2334, Loss: 35.87656021118164, L1: 10.541155815124512, L3: 25.335405349731445\n",
      "Current prediction:  61.20943832397461 \n",
      "\n",
      "Iteration 2335, Loss: 36.759986877441406, L1: 10.539424896240234, L3: 26.220561981201172\n",
      "Current prediction:  61.21177673339844 \n",
      "\n",
      "Iteration 2336, Loss: 36.566734313964844, L1: 10.537444114685059, L3: 26.0292911529541\n",
      "Current prediction:  61.213111877441406 \n",
      "\n",
      "Iteration 2337, Loss: 37.42101287841797, L1: 10.53630256652832, L3: 26.88471031188965\n",
      "Current prediction:  61.21489715576172 \n",
      "\n",
      "Iteration 2338, Loss: 37.057960510253906, L1: 10.534791946411133, L3: 26.523168563842773\n",
      "Current prediction:  61.21565628051758 \n",
      "\n",
      "Iteration 2339, Loss: 36.24559783935547, L1: 10.53415298461914, L3: 25.711442947387695\n",
      "Current prediction:  61.21673583984375 \n",
      "\n",
      "Iteration 2340, Loss: 37.468082427978516, L1: 10.533231735229492, L3: 26.934850692749023\n",
      "Current prediction:  61.216705322265625 \n",
      "\n",
      "Iteration 2341, Loss: 36.578697204589844, L1: 10.53326416015625, L3: 26.045433044433594\n",
      "Current prediction:  61.21577072143555 \n",
      "\n",
      "Iteration 2342, Loss: 37.092247009277344, L1: 10.534055709838867, L3: 26.55819320678711\n",
      "Current prediction:  61.21464920043945 \n",
      "\n",
      "Iteration 2343, Loss: 36.16987609863281, L1: 10.535003662109375, L3: 25.63487434387207\n",
      "Current prediction:  61.21495056152344 \n",
      "\n",
      "Iteration 2344, Loss: 36.04744338989258, L1: 10.534748077392578, L3: 25.5126953125\n",
      "Current prediction:  61.21577072143555 \n",
      "\n",
      "Iteration 2345, Loss: 37.13770294189453, L1: 10.534055709838867, L3: 26.603649139404297\n",
      "Current prediction:  61.216102600097656 \n",
      "\n",
      "Iteration 2346, Loss: 36.94009780883789, L1: 10.533774375915527, L3: 26.40632438659668\n",
      "Current prediction:  61.21568298339844 \n",
      "\n",
      "Iteration 2347, Loss: 36.23011779785156, L1: 10.538226127624512, L3: 25.691892623901367\n",
      "Current prediction:  61.21554183959961 \n",
      "\n",
      "Iteration 2348, Loss: 34.69380187988281, L1: 10.53425121307373, L3: 24.1595516204834\n",
      "Current prediction:  61.21560287475586 \n",
      "\n",
      "Iteration 2349, Loss: 36.705223083496094, L1: 10.534195899963379, L3: 26.17102813720703\n",
      "Current prediction:  61.2156982421875 \n",
      "\n",
      "Iteration 2350, Loss: 35.7828369140625, L1: 10.53411865234375, L3: 25.24871826171875\n",
      "Current prediction:  61.21565628051758 \n",
      "\n",
      "Iteration 2351, Loss: 36.45201110839844, L1: 10.534151077270508, L3: 25.917861938476562\n",
      "Current prediction:  61.21550369262695 \n",
      "\n",
      "Iteration 2352, Loss: 36.47898864746094, L1: 10.534286499023438, L3: 25.944704055786133\n",
      "Current prediction:  61.21566390991211 \n",
      "\n",
      "Iteration 2353, Loss: 36.159908294677734, L1: 10.534148216247559, L3: 25.625761032104492\n",
      "Current prediction:  61.216590881347656 \n",
      "\n",
      "Iteration 2354, Loss: 36.37461471557617, L1: 10.533357620239258, L3: 25.841257095336914\n",
      "Current prediction:  61.218040466308594 \n",
      "\n",
      "Iteration 2355, Loss: 37.46722412109375, L1: 10.532125473022461, L3: 26.935100555419922\n",
      "Current prediction:  61.219730377197266 \n",
      "\n",
      "Iteration 2356, Loss: 37.649147033691406, L1: 10.530698776245117, L3: 27.118450164794922\n",
      "Current prediction:  61.22158432006836 \n",
      "\n",
      "Iteration 2357, Loss: 37.68940734863281, L1: 10.529123306274414, L3: 27.1602840423584\n",
      "Current prediction:  61.222774505615234 \n",
      "\n",
      "Iteration 2358, Loss: 36.89220428466797, L1: 10.528114318847656, L3: 26.364089965820312\n",
      "Current prediction:  61.2235221862793 \n",
      "\n",
      "Iteration 2359, Loss: 36.50324249267578, L1: 10.527480125427246, L3: 25.97576332092285\n",
      "Current prediction:  61.22370147705078 \n",
      "\n",
      "Iteration 2360, Loss: 36.443180084228516, L1: 10.527329444885254, L3: 25.915849685668945\n",
      "Current prediction:  61.223602294921875 \n",
      "\n",
      "Iteration 2361, Loss: 36.502071380615234, L1: 10.527412414550781, L3: 25.974658966064453\n",
      "Current prediction:  61.223358154296875 \n",
      "\n",
      "Iteration 2362, Loss: 36.78742218017578, L1: 10.527042388916016, L3: 26.2603816986084\n",
      "Current prediction:  61.22282791137695 \n",
      "\n",
      "Iteration 2363, Loss: 36.073448181152344, L1: 10.528068542480469, L3: 25.545379638671875\n",
      "Current prediction:  61.222084045410156 \n",
      "\n",
      "Iteration 2364, Loss: 36.47510528564453, L1: 10.52869987487793, L3: 25.94640350341797\n",
      "Current prediction:  61.222198486328125 \n",
      "\n",
      "Iteration 2365, Loss: 38.001338958740234, L1: 10.528600692749023, L3: 27.47273826599121\n",
      "Current prediction:  61.220619201660156 \n",
      "\n",
      "Iteration 2366, Loss: 36.04865264892578, L1: 10.52994441986084, L3: 25.518707275390625\n",
      "Current prediction:  61.21871566772461 \n",
      "\n",
      "Iteration 2367, Loss: 37.30574417114258, L1: 10.531556129455566, L3: 26.774187088012695\n",
      "Current prediction:  61.21731185913086 \n",
      "\n",
      "Iteration 2368, Loss: 36.16581344604492, L1: 10.532744407653809, L3: 25.633068084716797\n",
      "Current prediction:  61.215667724609375 \n",
      "\n",
      "Iteration 2369, Loss: 37.2994499206543, L1: 10.534148216247559, L3: 26.765302658081055\n",
      "Current prediction:  61.21401596069336 \n",
      "\n",
      "Iteration 2370, Loss: 36.31664276123047, L1: 10.535547256469727, L3: 25.781095504760742\n",
      "Current prediction:  61.21318817138672 \n",
      "\n",
      "Iteration 2371, Loss: 36.691471099853516, L1: 10.536245346069336, L3: 26.15522575378418\n",
      "Current prediction:  61.212547302246094 \n",
      "\n",
      "Iteration 2372, Loss: 36.8556022644043, L1: 10.536787033081055, L3: 26.318815231323242\n",
      "Current prediction:  61.21144104003906 \n",
      "\n",
      "Iteration 2373, Loss: 36.12047576904297, L1: 10.537725448608398, L3: 25.582752227783203\n",
      "Current prediction:  61.21017837524414 \n",
      "\n",
      "Iteration 2374, Loss: 36.734561920166016, L1: 10.538795471191406, L3: 26.19576644897461\n",
      "Current prediction:  61.208213806152344 \n",
      "\n",
      "Iteration 2375, Loss: 37.14675521850586, L1: 10.5404634475708, L3: 26.606290817260742\n",
      "Current prediction:  61.20584487915039 \n",
      "\n",
      "Iteration 2376, Loss: 37.25209045410156, L1: 10.54246997833252, L3: 26.709619522094727\n",
      "Current prediction:  61.20319366455078 \n",
      "\n",
      "Iteration 2377, Loss: 36.25694274902344, L1: 10.544720649719238, L3: 25.712221145629883\n",
      "Current prediction:  61.20037078857422 \n",
      "\n",
      "Iteration 2378, Loss: 37.35562515258789, L1: 10.547110557556152, L3: 26.808515548706055\n",
      "Current prediction:  61.19812774658203 \n",
      "\n",
      "Iteration 2379, Loss: 36.17637634277344, L1: 10.549018859863281, L3: 25.62735939025879\n",
      "Current prediction:  61.195960998535156 \n",
      "\n",
      "Iteration 2380, Loss: 37.36384201049805, L1: 10.55085277557373, L3: 26.81298828125\n",
      "Current prediction:  61.193729400634766 \n",
      "\n",
      "Iteration 2381, Loss: 36.43818664550781, L1: 10.552740097045898, L3: 25.885448455810547\n",
      "Current prediction:  61.19129180908203 \n",
      "\n",
      "Iteration 2382, Loss: 36.402198791503906, L1: 10.554810523986816, L3: 25.847387313842773\n",
      "Current prediction:  61.18965530395508 \n",
      "\n",
      "Iteration 2383, Loss: 36.63863754272461, L1: 10.554953575134277, L3: 26.08368492126465\n",
      "Current prediction:  61.189395904541016 \n",
      "\n",
      "Iteration 2384, Loss: 35.79955291748047, L1: 10.556418418884277, L3: 25.243133544921875\n",
      "Current prediction:  61.18988037109375 \n",
      "\n",
      "Iteration 2385, Loss: 36.02861785888672, L1: 10.556007385253906, L3: 25.47260856628418\n",
      "Current prediction:  61.19083023071289 \n",
      "\n",
      "Iteration 2386, Loss: 36.69058609008789, L1: 10.55519962310791, L3: 26.135385513305664\n",
      "Current prediction:  61.191688537597656 \n",
      "\n",
      "Iteration 2387, Loss: 35.697959899902344, L1: 10.554473876953125, L3: 25.143484115600586\n",
      "Current prediction:  61.19213104248047 \n",
      "\n",
      "Iteration 2388, Loss: 35.484596252441406, L1: 10.554100036621094, L3: 24.93049430847168\n",
      "Current prediction:  61.192405700683594 \n",
      "\n",
      "Iteration 2389, Loss: 36.8094367980957, L1: 10.553862571716309, L3: 26.25557518005371\n",
      "Current prediction:  61.19349670410156 \n",
      "\n",
      "Iteration 2390, Loss: 36.45671844482422, L1: 10.552942276000977, L3: 25.90377426147461\n",
      "Current prediction:  61.194034576416016 \n",
      "\n",
      "Iteration 2391, Loss: 37.5958366394043, L1: 10.552489280700684, L3: 27.043346405029297\n",
      "Current prediction:  61.19504165649414 \n",
      "\n",
      "Iteration 2392, Loss: 36.03156661987305, L1: 10.55163288116455, L3: 25.47993278503418\n",
      "Current prediction:  61.19649887084961 \n",
      "\n",
      "Iteration 2393, Loss: 37.202980041503906, L1: 10.550394058227539, L3: 26.652587890625\n",
      "Current prediction:  61.197349548339844 \n",
      "\n",
      "Iteration 2394, Loss: 37.51958465576172, L1: 10.549678802490234, L3: 26.969905853271484\n",
      "Current prediction:  61.19671630859375 \n",
      "\n",
      "Iteration 2395, Loss: 37.01636505126953, L1: 10.550212860107422, L3: 26.466150283813477\n",
      "Current prediction:  61.196041107177734 \n",
      "\n",
      "Iteration 2396, Loss: 36.29351806640625, L1: 10.550783157348633, L3: 25.74273681640625\n",
      "Current prediction:  61.195892333984375 \n",
      "\n",
      "Iteration 2397, Loss: 36.00503158569336, L1: 10.550908088684082, L3: 25.45412254333496\n",
      "Current prediction:  61.19580841064453 \n",
      "\n",
      "Iteration 2398, Loss: 36.167808532714844, L1: 10.550980567932129, L3: 25.61682891845703\n",
      "Current prediction:  61.19651794433594 \n",
      "\n",
      "Iteration 2399, Loss: 37.02576446533203, L1: 10.55038070678711, L3: 26.47538185119629\n",
      "Current prediction:  61.19706726074219 \n",
      "\n",
      "Iteration 2400, Loss: 35.81978225708008, L1: 10.549911499023438, L3: 25.26987075805664\n",
      "Current prediction:  61.199005126953125 \n",
      "\n",
      "Iteration 2401, Loss: 37.82234191894531, L1: 10.548267364501953, L3: 27.27407455444336\n",
      "Current prediction:  61.200965881347656 \n",
      "\n",
      "Iteration 2402, Loss: 37.10584259033203, L1: 10.546606063842773, L3: 26.55923843383789\n",
      "Current prediction:  61.20249938964844 \n",
      "\n",
      "Iteration 2403, Loss: 37.041419982910156, L1: 10.545310974121094, L3: 26.49610710144043\n",
      "Current prediction:  61.20362091064453 \n",
      "\n",
      "Iteration 2404, Loss: 35.81986999511719, L1: 10.544353485107422, L3: 25.275516510009766\n",
      "Current prediction:  61.20513916015625 \n",
      "\n",
      "Iteration 2405, Loss: 37.17859649658203, L1: 10.54306697845459, L3: 26.635530471801758\n",
      "Current prediction:  61.206321716308594 \n",
      "\n",
      "Iteration 2406, Loss: 36.48935317993164, L1: 10.54206657409668, L3: 25.94728660583496\n",
      "Current prediction:  61.2072639465332 \n",
      "\n",
      "Iteration 2407, Loss: 35.78651428222656, L1: 10.541266441345215, L3: 25.24524688720703\n",
      "Current prediction:  61.20730972290039 \n",
      "\n",
      "Iteration 2408, Loss: 34.83367156982422, L1: 10.541229248046875, L3: 24.29244041442871\n",
      "Current prediction:  61.20799255371094 \n",
      "\n",
      "Iteration 2409, Loss: 36.240177154541016, L1: 10.54065227508545, L3: 25.699525833129883\n",
      "Current prediction:  61.20866012573242 \n",
      "\n",
      "Iteration 2410, Loss: 36.62760925292969, L1: 10.540084838867188, L3: 26.087522506713867\n",
      "Current prediction:  61.20895004272461 \n",
      "\n",
      "Iteration 2411, Loss: 36.171478271484375, L1: 10.539837837219238, L3: 25.631641387939453\n",
      "Current prediction:  61.20941925048828 \n",
      "\n",
      "Iteration 2412, Loss: 36.76568603515625, L1: 10.539440155029297, L3: 26.226247787475586\n",
      "Current prediction:  61.209808349609375 \n",
      "\n",
      "Iteration 2413, Loss: 35.65614700317383, L1: 10.53911018371582, L3: 25.117036819458008\n",
      "Current prediction:  61.21070861816406 \n",
      "\n",
      "Iteration 2414, Loss: 36.06766891479492, L1: 10.538349151611328, L3: 25.529319763183594\n",
      "Current prediction:  61.21042251586914 \n",
      "\n",
      "Iteration 2415, Loss: 36.131046295166016, L1: 10.53858757019043, L3: 25.592458724975586\n",
      "Current prediction:  61.21034240722656 \n",
      "\n",
      "Iteration 2416, Loss: 36.69126892089844, L1: 10.538656234741211, L3: 26.152612686157227\n",
      "Current prediction:  61.21174621582031 \n",
      "\n",
      "Iteration 2417, Loss: 36.3203125, L1: 10.537469863891602, L3: 25.78284454345703\n",
      "Current prediction:  61.213138580322266 \n",
      "\n",
      "Iteration 2418, Loss: 36.89176940917969, L1: 10.536287307739258, L3: 26.35548210144043\n",
      "Current prediction:  61.213871002197266 \n",
      "\n",
      "Iteration 2419, Loss: 37.14884567260742, L1: 10.535660743713379, L3: 26.613183975219727\n",
      "Current prediction:  61.21474075317383 \n",
      "\n",
      "Iteration 2420, Loss: 37.34398651123047, L1: 10.53492546081543, L3: 26.809059143066406\n",
      "Current prediction:  61.214012145996094 \n",
      "\n",
      "Iteration 2421, Loss: 35.697547912597656, L1: 10.53554916381836, L3: 25.16200065612793\n",
      "Current prediction:  61.21379852294922 \n",
      "\n",
      "Iteration 2422, Loss: 36.91547393798828, L1: 10.535726547241211, L3: 26.379749298095703\n",
      "Current prediction:  61.213134765625 \n",
      "\n",
      "Iteration 2423, Loss: 36.15174865722656, L1: 10.536290168762207, L3: 25.615459442138672\n",
      "Current prediction:  61.21247482299805 \n",
      "\n",
      "Iteration 2424, Loss: 36.594512939453125, L1: 10.53685474395752, L3: 26.057659149169922\n",
      "Current prediction:  61.212738037109375 \n",
      "\n",
      "Iteration 2425, Loss: 35.66044616699219, L1: 10.536628723144531, L3: 25.123817443847656\n",
      "Current prediction:  61.212711334228516 \n",
      "\n",
      "Iteration 2426, Loss: 37.40202713012695, L1: 10.536650657653809, L3: 26.865375518798828\n",
      "Current prediction:  61.21181869506836 \n",
      "\n",
      "Iteration 2427, Loss: 37.347408294677734, L1: 10.537405014038086, L3: 26.81000328063965\n",
      "Current prediction:  61.210853576660156 \n",
      "\n",
      "Iteration 2428, Loss: 36.57170104980469, L1: 10.538225173950195, L3: 26.03347396850586\n",
      "Current prediction:  61.20962142944336 \n",
      "\n",
      "Iteration 2429, Loss: 37.42255783081055, L1: 10.539273262023926, L3: 26.883283615112305\n",
      "Current prediction:  61.20733642578125 \n",
      "\n",
      "Iteration 2430, Loss: 37.16176986694336, L1: 10.5412015914917, L3: 26.620569229125977\n",
      "Current prediction:  61.20449447631836 \n",
      "\n",
      "Iteration 2431, Loss: 36.45839309692383, L1: 10.54361629486084, L3: 25.914775848388672\n",
      "Current prediction:  61.20264434814453 \n",
      "\n",
      "Iteration 2432, Loss: 36.27066421508789, L1: 10.545182228088379, L3: 25.725481033325195\n",
      "Current prediction:  61.19993209838867 \n",
      "\n",
      "Iteration 2433, Loss: 35.96182632446289, L1: 10.547484397888184, L3: 25.414342880249023\n",
      "Current prediction:  61.19686508178711 \n",
      "\n",
      "Iteration 2434, Loss: 36.672760009765625, L1: 10.550086975097656, L3: 26.122671127319336\n",
      "Current prediction:  61.19392776489258 \n",
      "\n",
      "Iteration 2435, Loss: 36.83522033691406, L1: 10.552574157714844, L3: 26.282644271850586\n",
      "Current prediction:  61.19062423706055 \n",
      "\n",
      "Iteration 2436, Loss: 37.57431411743164, L1: 10.555377006530762, L3: 27.018938064575195\n",
      "Current prediction:  61.18705368041992 \n",
      "\n",
      "Iteration 2437, Loss: 36.026947021484375, L1: 10.558401107788086, L3: 25.468544006347656\n",
      "Current prediction:  61.184776306152344 \n",
      "\n",
      "Iteration 2438, Loss: 36.37955093383789, L1: 10.560332298278809, L3: 25.819217681884766\n",
      "Current prediction:  61.18339920043945 \n",
      "\n",
      "Iteration 2439, Loss: 36.226619720458984, L1: 10.56149959564209, L3: 25.665119171142578\n",
      "Current prediction:  61.18336868286133 \n",
      "\n",
      "Iteration 2440, Loss: 35.91974639892578, L1: 10.561521530151367, L3: 25.358224868774414\n",
      "Current prediction:  61.18398666381836 \n",
      "\n",
      "Iteration 2441, Loss: 35.9393424987793, L1: 10.561004638671875, L3: 25.378337860107422\n",
      "Current prediction:  61.18428421020508 \n",
      "\n",
      "Iteration 2442, Loss: 36.49119186401367, L1: 10.560750007629395, L3: 25.93044090270996\n",
      "Current prediction:  61.184852600097656 \n",
      "\n",
      "Iteration 2443, Loss: 37.208595275878906, L1: 10.560269355773926, L3: 26.648324966430664\n",
      "Current prediction:  61.184165954589844 \n",
      "\n",
      "Iteration 2444, Loss: 37.2918815612793, L1: 10.560851097106934, L3: 26.73103141784668\n",
      "Current prediction:  61.182979583740234 \n",
      "\n",
      "Iteration 2445, Loss: 36.432125091552734, L1: 10.561857223510742, L3: 25.870267868041992\n",
      "Current prediction:  61.182945251464844 \n",
      "\n",
      "Iteration 2446, Loss: 36.81510925292969, L1: 10.561885833740234, L3: 26.253225326538086\n",
      "Current prediction:  61.182735443115234 \n",
      "\n",
      "Iteration 2447, Loss: 36.41775131225586, L1: 10.56206226348877, L3: 25.855690002441406\n",
      "Current prediction:  61.182796478271484 \n",
      "\n",
      "Iteration 2448, Loss: 37.00728988647461, L1: 10.562018394470215, L3: 26.44527244567871\n",
      "Current prediction:  61.183406829833984 \n",
      "\n",
      "Iteration 2449, Loss: 36.35041809082031, L1: 10.561493873596191, L3: 25.788925170898438\n",
      "Current prediction:  61.184322357177734 \n",
      "\n",
      "Iteration 2450, Loss: 36.523231506347656, L1: 10.560717582702637, L3: 25.962512969970703\n",
      "Current prediction:  61.185245513916016 \n",
      "\n",
      "Iteration 2451, Loss: 37.27360153198242, L1: 10.559935569763184, L3: 26.713665008544922\n",
      "Current prediction:  61.18595504760742 \n",
      "\n",
      "Iteration 2452, Loss: 37.224857330322266, L1: 10.559331893920898, L3: 26.665525436401367\n",
      "Current prediction:  61.187721252441406 \n",
      "\n",
      "Iteration 2453, Loss: 36.334476470947266, L1: 10.55783748626709, L3: 25.77663803100586\n",
      "Current prediction:  61.18883514404297 \n",
      "\n",
      "Iteration 2454, Loss: 37.077789306640625, L1: 10.556893348693848, L3: 26.520896911621094\n",
      "Current prediction:  61.18994903564453 \n",
      "\n",
      "Iteration 2455, Loss: 36.880104064941406, L1: 10.555950164794922, L3: 26.324155807495117\n",
      "Current prediction:  61.1900634765625 \n",
      "\n",
      "Iteration 2456, Loss: 36.43037414550781, L1: 10.555854797363281, L3: 25.874521255493164\n",
      "Current prediction:  61.190738677978516 \n",
      "\n",
      "Iteration 2457, Loss: 36.72157669067383, L1: 10.555275917053223, L3: 26.166301727294922\n",
      "Current prediction:  61.192081451416016 \n",
      "\n",
      "Iteration 2458, Loss: 36.528846740722656, L1: 10.554139137268066, L3: 25.974708557128906\n",
      "Current prediction:  61.19428253173828 \n",
      "\n",
      "Iteration 2459, Loss: 36.355018615722656, L1: 10.552275657653809, L3: 25.802743911743164\n",
      "Current prediction:  61.196468353271484 \n",
      "\n",
      "Iteration 2460, Loss: 35.95647048950195, L1: 10.550415992736816, L3: 25.406055450439453\n",
      "Current prediction:  61.199073791503906 \n",
      "\n",
      "Iteration 2461, Loss: 36.8037109375, L1: 10.5482177734375, L3: 26.255491256713867\n",
      "Current prediction:  61.20138931274414 \n",
      "\n",
      "Iteration 2462, Loss: 37.03634262084961, L1: 10.546250343322754, L3: 26.49009132385254\n",
      "Current prediction:  61.204345703125 \n",
      "\n",
      "Iteration 2463, Loss: 36.16168212890625, L1: 10.543739318847656, L3: 25.617942810058594\n",
      "Current prediction:  61.20658874511719 \n",
      "\n",
      "Iteration 2464, Loss: 37.27749252319336, L1: 10.541838645935059, L3: 26.735652923583984\n",
      "Current prediction:  61.20937728881836 \n",
      "\n",
      "Iteration 2465, Loss: 36.31681442260742, L1: 10.539475440979004, L3: 25.777339935302734\n",
      "Current prediction:  61.211727142333984 \n",
      "\n",
      "Iteration 2466, Loss: 36.87812423706055, L1: 10.53748893737793, L3: 26.340635299682617\n",
      "Current prediction:  61.213340759277344 \n",
      "\n",
      "Iteration 2467, Loss: 35.812713623046875, L1: 10.536117553710938, L3: 25.27659797668457\n",
      "Current prediction:  61.21523666381836 \n",
      "\n",
      "Iteration 2468, Loss: 37.560211181640625, L1: 10.534503936767578, L3: 27.025705337524414\n",
      "Current prediction:  61.21646499633789 \n",
      "\n",
      "Iteration 2469, Loss: 37.358062744140625, L1: 10.533469200134277, L3: 26.824594497680664\n",
      "Current prediction:  61.216129302978516 \n",
      "\n",
      "Iteration 2470, Loss: 36.55145263671875, L1: 10.5337495803833, L3: 26.017702102661133\n",
      "Current prediction:  61.215938568115234 \n",
      "\n",
      "Iteration 2471, Loss: 37.10873031616211, L1: 10.533913612365723, L3: 26.57481575012207\n",
      "Current prediction:  61.214805603027344 \n",
      "\n",
      "Iteration 2472, Loss: 37.242855072021484, L1: 10.53487491607666, L3: 26.70798110961914\n",
      "Current prediction:  61.21267318725586 \n",
      "\n",
      "Iteration 2473, Loss: 36.686004638671875, L1: 10.536681175231934, L3: 26.149324417114258\n",
      "Current prediction:  61.211631774902344 \n",
      "\n",
      "Iteration 2474, Loss: 36.68438720703125, L1: 10.537568092346191, L3: 26.146820068359375\n",
      "Current prediction:  61.21055221557617 \n",
      "\n",
      "Iteration 2475, Loss: 36.50104904174805, L1: 10.538481712341309, L3: 25.962568283081055\n",
      "Current prediction:  61.21022033691406 \n",
      "\n",
      "Iteration 2476, Loss: 36.834800720214844, L1: 10.537240982055664, L3: 26.297561645507812\n",
      "Current prediction:  61.21054458618164 \n",
      "\n",
      "Iteration 2477, Loss: 36.503639221191406, L1: 10.538482666015625, L3: 25.96515464782715\n",
      "Current prediction:  61.21121597290039 \n",
      "\n",
      "Iteration 2478, Loss: 36.19077682495117, L1: 10.537919998168945, L3: 25.652856826782227\n",
      "Current prediction:  61.21234893798828 \n",
      "\n",
      "Iteration 2479, Loss: 36.046207427978516, L1: 10.536955833435059, L3: 25.509252548217773\n",
      "Current prediction:  61.213050842285156 \n",
      "\n",
      "Iteration 2480, Loss: 36.40357208251953, L1: 10.536362648010254, L3: 25.86720848083496\n",
      "Current prediction:  61.21554946899414 \n",
      "\n",
      "Iteration 2481, Loss: 36.09036636352539, L1: 10.534238815307617, L3: 25.556127548217773\n",
      "Current prediction:  61.21791076660156 \n",
      "\n",
      "Iteration 2482, Loss: 36.707862854003906, L1: 10.53223705291748, L3: 26.17562484741211\n",
      "Current prediction:  61.219581604003906 \n",
      "\n",
      "Iteration 2483, Loss: 36.257564544677734, L1: 10.530823707580566, L3: 25.726741790771484\n",
      "Current prediction:  61.2217903137207 \n",
      "\n",
      "Iteration 2484, Loss: 37.91295623779297, L1: 10.528947830200195, L3: 27.384010314941406\n",
      "Current prediction:  61.22224426269531 \n",
      "\n",
      "Iteration 2485, Loss: 35.57178497314453, L1: 10.528564453125, L3: 25.04322052001953\n",
      "Current prediction:  61.223785400390625 \n",
      "\n",
      "Iteration 2486, Loss: 35.995243072509766, L1: 10.527255058288574, L3: 25.467987060546875\n",
      "Current prediction:  61.22549819946289 \n",
      "\n",
      "Iteration 2487, Loss: 37.146392822265625, L1: 10.525806427001953, L3: 26.620588302612305\n",
      "Current prediction:  61.22659683227539 \n",
      "\n",
      "Iteration 2488, Loss: 36.950927734375, L1: 10.524872779846191, L3: 26.426055908203125\n",
      "Current prediction:  61.22722244262695 \n",
      "\n",
      "Iteration 2489, Loss: 37.06773376464844, L1: 10.524347305297852, L3: 26.543384552001953\n",
      "Current prediction:  61.22856140136719 \n",
      "\n",
      "Iteration 2490, Loss: 36.84500503540039, L1: 10.523208618164062, L3: 26.321796417236328\n",
      "Current prediction:  61.22995376586914 \n",
      "\n",
      "Iteration 2491, Loss: 36.92231369018555, L1: 10.522028923034668, L3: 26.400283813476562\n",
      "Current prediction:  61.2311897277832 \n",
      "\n",
      "Iteration 2492, Loss: 37.0423583984375, L1: 10.52098560333252, L3: 26.521371841430664\n",
      "Current prediction:  61.23201370239258 \n",
      "\n",
      "Iteration 2493, Loss: 36.899253845214844, L1: 10.520280838012695, L3: 26.378971099853516\n",
      "Current prediction:  61.232845306396484 \n",
      "\n",
      "Iteration 2494, Loss: 36.04989242553711, L1: 10.519577026367188, L3: 25.530315399169922\n",
      "Current prediction:  61.23413848876953 \n",
      "\n",
      "Iteration 2495, Loss: 35.93498992919922, L1: 10.518476486206055, L3: 25.416515350341797\n",
      "Current prediction:  61.23540115356445 \n",
      "\n",
      "Iteration 2496, Loss: 36.60209274291992, L1: 10.517411231994629, L3: 26.08468246459961\n",
      "Current prediction:  61.23625183105469 \n",
      "\n",
      "Iteration 2497, Loss: 36.768585205078125, L1: 10.516690254211426, L3: 26.251893997192383\n",
      "Current prediction:  61.23902130126953 \n",
      "\n",
      "Iteration 2498, Loss: 35.40843200683594, L1: 10.514335632324219, L3: 24.89409637451172\n",
      "Current prediction:  61.24161148071289 \n",
      "\n",
      "Iteration 2499, Loss: 36.75475311279297, L1: 10.512139320373535, L3: 26.24261474609375\n",
      "Current prediction:  61.243751525878906 \n",
      "\n",
      "Iteration 2500, Loss: 36.17117691040039, L1: 10.510331153869629, L3: 25.660846710205078\n",
      "Current prediction:  61.245967864990234 \n",
      "\n",
      "Iteration 2501, Loss: 36.78370666503906, L1: 10.508445739746094, L3: 26.275259017944336\n",
      "Current prediction:  61.248146057128906 \n",
      "\n",
      "Iteration 2502, Loss: 36.91851806640625, L1: 10.506600379943848, L3: 26.41191864013672\n",
      "Current prediction:  61.25010681152344 \n",
      "\n",
      "Iteration 2503, Loss: 37.0247802734375, L1: 10.504934310913086, L3: 26.51984405517578\n",
      "Current prediction:  61.251319885253906 \n",
      "\n",
      "Iteration 2504, Loss: 36.63469696044922, L1: 10.503908157348633, L3: 26.130786895751953\n",
      "Current prediction:  61.251220703125 \n",
      "\n",
      "Iteration 2505, Loss: 36.20117950439453, L1: 10.503994941711426, L3: 25.697185516357422\n",
      "Current prediction:  61.249969482421875 \n",
      "\n",
      "Iteration 2506, Loss: 37.00463104248047, L1: 10.505049705505371, L3: 26.49958038330078\n",
      "Current prediction:  61.24858093261719 \n",
      "\n",
      "Iteration 2507, Loss: 37.18784713745117, L1: 10.50622844696045, L3: 26.68161964416504\n",
      "Current prediction:  61.2481803894043 \n",
      "\n",
      "Iteration 2508, Loss: 37.57305145263672, L1: 10.506568908691406, L3: 27.06648063659668\n",
      "Current prediction:  61.24620819091797 \n",
      "\n",
      "Iteration 2509, Loss: 36.552574157714844, L1: 10.508238792419434, L3: 26.044334411621094\n",
      "Current prediction:  61.243900299072266 \n",
      "\n",
      "Iteration 2510, Loss: 36.0806884765625, L1: 10.51020336151123, L3: 25.570484161376953\n",
      "Current prediction:  61.24188995361328 \n",
      "\n",
      "Iteration 2511, Loss: 37.089454650878906, L1: 10.511902809143066, L3: 26.577550888061523\n",
      "Current prediction:  61.23904037475586 \n",
      "\n",
      "Iteration 2512, Loss: 35.72189712524414, L1: 10.51431941986084, L3: 25.207578659057617\n",
      "Current prediction:  61.235679626464844 \n",
      "\n",
      "Iteration 2513, Loss: 36.531837463378906, L1: 10.517173767089844, L3: 26.01466178894043\n",
      "Current prediction:  61.231876373291016 \n",
      "\n",
      "Iteration 2514, Loss: 36.68023681640625, L1: 10.520401000976562, L3: 26.15983772277832\n",
      "Current prediction:  61.229225158691406 \n",
      "\n",
      "Iteration 2515, Loss: 37.19083786010742, L1: 10.5226469039917, L3: 26.668190002441406\n",
      "Current prediction:  61.22629165649414 \n",
      "\n",
      "Iteration 2516, Loss: 36.772430419921875, L1: 10.525132179260254, L3: 26.247297286987305\n",
      "Current prediction:  61.223995208740234 \n",
      "\n",
      "Iteration 2517, Loss: 36.52976989746094, L1: 10.527078628540039, L3: 26.00269317626953\n",
      "Current prediction:  61.222225189208984 \n",
      "\n",
      "Iteration 2518, Loss: 36.64794921875, L1: 10.528587341308594, L3: 26.119361877441406\n",
      "Current prediction:  61.22132873535156 \n",
      "\n",
      "Iteration 2519, Loss: 36.404537200927734, L1: 10.529342651367188, L3: 25.875194549560547\n",
      "Current prediction:  61.22040939331055 \n",
      "\n",
      "Iteration 2520, Loss: 36.442325592041016, L1: 10.530123710632324, L3: 25.912202835083008\n",
      "Current prediction:  61.219520568847656 \n",
      "\n",
      "Iteration 2521, Loss: 37.2027473449707, L1: 10.530875205993652, L3: 26.671871185302734\n",
      "Current prediction:  61.21929168701172 \n",
      "\n",
      "Iteration 2522, Loss: 37.560237884521484, L1: 10.531062126159668, L3: 27.0291748046875\n",
      "Current prediction:  61.219486236572266 \n",
      "\n",
      "Iteration 2523, Loss: 36.732540130615234, L1: 10.530902862548828, L3: 26.201637268066406\n",
      "Current prediction:  61.21988296508789 \n",
      "\n",
      "Iteration 2524, Loss: 35.44677734375, L1: 10.530569076538086, L3: 24.91620635986328\n",
      "Current prediction:  61.22123718261719 \n",
      "\n",
      "Iteration 2525, Loss: 36.41767120361328, L1: 10.52942180633545, L3: 25.888248443603516\n",
      "Current prediction:  61.22217559814453 \n",
      "\n",
      "Iteration 2526, Loss: 35.75600814819336, L1: 10.528624534606934, L3: 25.22738265991211\n",
      "Current prediction:  61.223392486572266 \n",
      "\n",
      "Iteration 2527, Loss: 36.0455322265625, L1: 10.527593612670898, L3: 25.51793670654297\n",
      "Current prediction:  61.22422790527344 \n",
      "\n",
      "Iteration 2528, Loss: 37.17572784423828, L1: 10.526885986328125, L3: 26.648839950561523\n",
      "Current prediction:  61.22463607788086 \n",
      "\n",
      "Iteration 2529, Loss: 36.0245475769043, L1: 10.526540756225586, L3: 25.49800682067871\n",
      "Current prediction:  61.22478485107422 \n",
      "\n",
      "Iteration 2530, Loss: 36.97697067260742, L1: 10.526411056518555, L3: 26.450559616088867\n",
      "Current prediction:  61.2243766784668 \n",
      "\n",
      "Iteration 2531, Loss: 36.87035369873047, L1: 10.526754379272461, L3: 26.343599319458008\n",
      "Current prediction:  61.22490692138672 \n",
      "\n",
      "Iteration 2532, Loss: 36.881919860839844, L1: 10.52630615234375, L3: 26.355615615844727\n",
      "Current prediction:  61.22547912597656 \n",
      "\n",
      "Iteration 2533, Loss: 37.3806266784668, L1: 10.525818824768066, L3: 26.854808807373047\n",
      "Current prediction:  61.22629165649414 \n",
      "\n",
      "Iteration 2534, Loss: 37.37675857543945, L1: 10.52513599395752, L3: 26.851621627807617\n",
      "Current prediction:  61.22626876831055 \n",
      "\n",
      "Iteration 2535, Loss: 37.247596740722656, L1: 10.525148391723633, L3: 26.722450256347656\n",
      "Current prediction:  61.22568130493164 \n",
      "\n",
      "Iteration 2536, Loss: 35.665809631347656, L1: 10.525651931762695, L3: 25.140159606933594\n",
      "Current prediction:  61.2247200012207 \n",
      "\n",
      "Iteration 2537, Loss: 35.109947204589844, L1: 10.526460647583008, L3: 24.583486557006836\n",
      "Current prediction:  61.223968505859375 \n",
      "\n",
      "Iteration 2538, Loss: 36.585323333740234, L1: 10.527099609375, L3: 26.058223724365234\n",
      "Current prediction:  61.22376251220703 \n",
      "\n",
      "Iteration 2539, Loss: 36.75384521484375, L1: 10.527275085449219, L3: 26.2265682220459\n",
      "Current prediction:  61.224212646484375 \n",
      "\n",
      "Iteration 2540, Loss: 36.480751037597656, L1: 10.526894569396973, L3: 25.953855514526367\n",
      "Current prediction:  61.224754333496094 \n",
      "\n",
      "Iteration 2541, Loss: 37.014610290527344, L1: 10.526435852050781, L3: 26.48817253112793\n",
      "Current prediction:  61.22465515136719 \n",
      "\n",
      "Iteration 2542, Loss: 36.43010711669922, L1: 10.526520729064941, L3: 25.903587341308594\n",
      "Current prediction:  61.22392272949219 \n",
      "\n",
      "Iteration 2543, Loss: 36.796234130859375, L1: 10.527144432067871, L3: 26.26909065246582\n",
      "Current prediction:  61.22300338745117 \n",
      "\n",
      "Iteration 2544, Loss: 36.78593063354492, L1: 10.527923583984375, L3: 26.258007049560547\n",
      "Current prediction:  61.22180938720703 \n",
      "\n",
      "Iteration 2545, Loss: 36.7113037109375, L1: 10.528931617736816, L3: 26.182373046875\n",
      "Current prediction:  61.22048568725586 \n",
      "\n",
      "Iteration 2546, Loss: 37.25666046142578, L1: 10.530057907104492, L3: 26.72660255432129\n",
      "Current prediction:  61.21843338012695 \n",
      "\n",
      "Iteration 2547, Loss: 36.57886505126953, L1: 10.531795501708984, L3: 26.047069549560547\n",
      "Current prediction:  61.217044830322266 \n",
      "\n",
      "Iteration 2548, Loss: 35.91487503051758, L1: 10.532974243164062, L3: 25.381900787353516\n",
      "Current prediction:  61.21601486206055 \n",
      "\n",
      "Iteration 2549, Loss: 37.31987380981445, L1: 10.536199569702148, L3: 26.783674240112305\n",
      "Current prediction:  61.214019775390625 \n",
      "\n",
      "Iteration 2550, Loss: 37.10606384277344, L1: 10.535537719726562, L3: 26.570524215698242\n",
      "Current prediction:  61.21159744262695 \n",
      "\n",
      "Iteration 2551, Loss: 36.39714050292969, L1: 10.537596702575684, L3: 25.859542846679688\n",
      "Current prediction:  61.20957565307617 \n",
      "\n",
      "Iteration 2552, Loss: 35.7413215637207, L1: 10.539306640625, L3: 25.202014923095703\n",
      "Current prediction:  61.20817184448242 \n",
      "\n",
      "Iteration 2553, Loss: 36.74829864501953, L1: 10.540497779846191, L3: 26.207801818847656\n",
      "Current prediction:  61.20554733276367 \n",
      "\n",
      "Iteration 2554, Loss: 36.88050842285156, L1: 10.542724609375, L3: 26.337783813476562\n",
      "Current prediction:  61.20317077636719 \n",
      "\n",
      "Iteration 2555, Loss: 36.31686019897461, L1: 10.54473876953125, L3: 25.77212142944336\n",
      "Current prediction:  61.202152252197266 \n",
      "\n",
      "Iteration 2556, Loss: 36.60930252075195, L1: 10.545599937438965, L3: 26.063701629638672\n",
      "Current prediction:  61.2011604309082 \n",
      "\n",
      "Iteration 2557, Loss: 37.20794677734375, L1: 10.5464448928833, L3: 26.661502838134766\n",
      "Current prediction:  61.200103759765625 \n",
      "\n",
      "Iteration 2558, Loss: 36.80815887451172, L1: 10.547332763671875, L3: 26.26082420349121\n",
      "Current prediction:  61.19963073730469 \n",
      "\n",
      "Iteration 2559, Loss: 36.04999923706055, L1: 10.547741889953613, L3: 25.50225830078125\n",
      "Current prediction:  61.19925308227539 \n",
      "\n",
      "Iteration 2560, Loss: 36.915870666503906, L1: 10.548057556152344, L3: 26.367813110351562\n",
      "Current prediction:  61.198543548583984 \n",
      "\n",
      "Iteration 2561, Loss: 36.7896728515625, L1: 10.548662185668945, L3: 26.241012573242188\n",
      "Current prediction:  61.1971435546875 \n",
      "\n",
      "Iteration 2562, Loss: 38.11421203613281, L1: 10.549850463867188, L3: 27.564361572265625\n",
      "Current prediction:  61.19588851928711 \n",
      "\n",
      "Iteration 2563, Loss: 36.81651306152344, L1: 10.55091381072998, L3: 26.265600204467773\n",
      "Current prediction:  61.195255279541016 \n",
      "\n",
      "Iteration 2564, Loss: 36.318023681640625, L1: 10.551448822021484, L3: 25.766576766967773\n",
      "Current prediction:  61.19425964355469 \n",
      "\n",
      "Iteration 2565, Loss: 37.85002136230469, L1: 10.552295684814453, L3: 27.297725677490234\n",
      "Current prediction:  61.19259262084961 \n",
      "\n",
      "Iteration 2566, Loss: 37.98365020751953, L1: 10.55370807647705, L3: 27.429941177368164\n",
      "Current prediction:  61.18979263305664 \n",
      "\n",
      "Iteration 2567, Loss: 36.91364288330078, L1: 10.556079864501953, L3: 26.357563018798828\n",
      "Current prediction:  61.18667984008789 \n",
      "\n",
      "Iteration 2568, Loss: 36.126953125, L1: 10.558719635009766, L3: 25.568233489990234\n",
      "Current prediction:  61.18560028076172 \n",
      "\n",
      "Iteration 2569, Loss: 37.128604888916016, L1: 10.559633255004883, L3: 26.568971633911133\n",
      "Current prediction:  61.18374252319336 \n",
      "\n",
      "Iteration 2570, Loss: 36.86125946044922, L1: 10.561210632324219, L3: 26.300050735473633\n",
      "Current prediction:  61.182518005371094 \n",
      "\n",
      "Iteration 2571, Loss: 35.939762115478516, L1: 10.562249183654785, L3: 25.377511978149414\n",
      "Current prediction:  61.1816291809082 \n",
      "\n",
      "Iteration 2572, Loss: 37.06219482421875, L1: 10.563000679016113, L3: 26.49919319152832\n",
      "Current prediction:  61.18058776855469 \n",
      "\n",
      "Iteration 2573, Loss: 36.42879867553711, L1: 10.563887596130371, L3: 25.864910125732422\n",
      "Current prediction:  61.1802864074707 \n",
      "\n",
      "Iteration 2574, Loss: 36.11642074584961, L1: 10.564138412475586, L3: 25.552282333374023\n",
      "Current prediction:  61.181175231933594 \n",
      "\n",
      "Iteration 2575, Loss: 36.66191864013672, L1: 10.563386917114258, L3: 26.098533630371094\n",
      "Current prediction:  61.1817512512207 \n",
      "\n",
      "Iteration 2576, Loss: 36.092464447021484, L1: 10.562899589538574, L3: 25.529565811157227\n",
      "Current prediction:  61.18250274658203 \n",
      "\n",
      "Iteration 2577, Loss: 36.5612907409668, L1: 10.562263488769531, L3: 25.999027252197266\n",
      "Current prediction:  61.184139251708984 \n",
      "\n",
      "Iteration 2578, Loss: 36.43086624145508, L1: 10.56087589263916, L3: 25.869991302490234\n",
      "Current prediction:  61.184593200683594 \n",
      "\n",
      "Iteration 2579, Loss: 36.26299285888672, L1: 10.560489654541016, L3: 25.702505111694336\n",
      "Current prediction:  61.185760498046875 \n",
      "\n",
      "Iteration 2580, Loss: 37.19745635986328, L1: 10.559500694274902, L3: 26.637956619262695\n",
      "Current prediction:  61.18746566772461 \n",
      "\n",
      "Iteration 2581, Loss: 36.55694580078125, L1: 10.558053970336914, L3: 25.998889923095703\n",
      "Current prediction:  61.189754486083984 \n",
      "\n",
      "Iteration 2582, Loss: 37.299312591552734, L1: 10.556107521057129, L3: 26.743206024169922\n",
      "Current prediction:  61.19169616699219 \n",
      "\n",
      "Iteration 2583, Loss: 36.21244812011719, L1: 10.55446720123291, L3: 25.657981872558594\n",
      "Current prediction:  61.1935920715332 \n",
      "\n",
      "Iteration 2584, Loss: 36.94538116455078, L1: 10.5528564453125, L3: 26.39252471923828\n",
      "Current prediction:  61.195274353027344 \n",
      "\n",
      "Iteration 2585, Loss: 36.99260711669922, L1: 10.551435470581055, L3: 26.441171646118164\n",
      "Current prediction:  61.195716857910156 \n",
      "\n",
      "Iteration 2586, Loss: 37.509647369384766, L1: 10.551055908203125, L3: 26.95859146118164\n",
      "Current prediction:  61.19541931152344 \n",
      "\n",
      "Iteration 2587, Loss: 37.191627502441406, L1: 10.843729019165039, L3: 26.347900390625\n",
      "Current prediction:  61.195472717285156 \n",
      "\n",
      "Iteration 2588, Loss: 36.35798645019531, L1: 10.551267623901367, L3: 25.806718826293945\n",
      "Current prediction:  61.19599914550781 \n",
      "\n",
      "Iteration 2589, Loss: 37.60307312011719, L1: 10.87333869934082, L3: 26.729734420776367\n",
      "Current prediction:  61.19649887084961 \n",
      "\n",
      "Iteration 2590, Loss: 35.575565338134766, L1: 10.550394058227539, L3: 25.025171279907227\n",
      "Current prediction:  61.19783020019531 \n",
      "\n",
      "Iteration 2591, Loss: 36.83397674560547, L1: 10.54926872253418, L3: 26.284709930419922\n",
      "Current prediction:  61.199058532714844 \n",
      "\n",
      "Iteration 2592, Loss: 36.89752197265625, L1: 10.548225402832031, L3: 26.34929656982422\n",
      "Current prediction:  61.20063781738281 \n",
      "\n",
      "Iteration 2593, Loss: 37.058349609375, L1: 10.546882629394531, L3: 26.511465072631836\n",
      "Current prediction:  61.2019157409668 \n",
      "\n",
      "Iteration 2594, Loss: 36.29949188232422, L1: 10.54580020904541, L3: 25.753690719604492\n",
      "Current prediction:  61.202857971191406 \n",
      "\n",
      "Iteration 2595, Loss: 36.69903564453125, L1: 10.545003890991211, L3: 26.15403175354004\n",
      "Current prediction:  61.20387268066406 \n",
      "\n",
      "Iteration 2596, Loss: 36.76206588745117, L1: 10.544140815734863, L3: 26.217924118041992\n",
      "Current prediction:  61.20460891723633 \n",
      "\n",
      "Iteration 2597, Loss: 37.30516052246094, L1: 10.543519020080566, L3: 26.761640548706055\n",
      "Current prediction:  61.20527267456055 \n",
      "\n",
      "Iteration 2598, Loss: 36.11396408081055, L1: 10.542954444885254, L3: 25.571008682250977\n",
      "Current prediction:  61.206520080566406 \n",
      "\n",
      "Iteration 2599, Loss: 37.05763244628906, L1: 10.541899681091309, L3: 26.515731811523438\n",
      "Current prediction:  61.207557678222656 \n",
      "\n",
      "Iteration 2600, Loss: 37.15791320800781, L1: 10.541017532348633, L3: 26.616897583007812\n",
      "Current prediction:  61.2080192565918 \n",
      "\n",
      "Iteration 2601, Loss: 37.14012145996094, L1: 10.540632247924805, L3: 26.5994873046875\n",
      "Current prediction:  61.20824432373047 \n",
      "\n",
      "Iteration 2602, Loss: 36.961299896240234, L1: 10.540436744689941, L3: 26.420862197875977\n",
      "Current prediction:  61.20888900756836 \n",
      "\n",
      "Iteration 2603, Loss: 36.63469696044922, L1: 10.53989028930664, L3: 26.09480857849121\n",
      "Current prediction:  61.20991516113281 \n",
      "\n",
      "Iteration 2604, Loss: 36.91001510620117, L1: 10.539020538330078, L3: 26.370994567871094\n",
      "Current prediction:  61.21133804321289 \n",
      "\n",
      "Iteration 2605, Loss: 36.58411407470703, L1: 10.537814140319824, L3: 26.046300888061523\n",
      "Current prediction:  61.21187973022461 \n",
      "\n",
      "Iteration 2606, Loss: 36.82256317138672, L1: 10.537351608276367, L3: 26.28521156311035\n",
      "Current prediction:  61.212825775146484 \n",
      "\n",
      "Iteration 2607, Loss: 35.80458068847656, L1: 10.536555290222168, L3: 25.26802635192871\n",
      "Current prediction:  61.21380615234375 \n",
      "\n",
      "Iteration 2608, Loss: 36.392662048339844, L1: 10.535722732543945, L3: 25.8569393157959\n",
      "Current prediction:  61.2165412902832 \n",
      "\n",
      "Iteration 2609, Loss: 36.01844024658203, L1: 10.533402442932129, L3: 25.485036849975586\n",
      "Current prediction:  61.21919250488281 \n",
      "\n",
      "Iteration 2610, Loss: 36.326759338378906, L1: 10.531150817871094, L3: 25.79560661315918\n",
      "Current prediction:  61.22170639038086 \n",
      "\n",
      "Iteration 2611, Loss: 36.723548889160156, L1: 10.529023170471191, L3: 26.19452476501465\n",
      "Current prediction:  61.22472381591797 \n",
      "\n",
      "Iteration 2612, Loss: 36.485660552978516, L1: 10.526459693908691, L3: 25.95920181274414\n",
      "Current prediction:  61.226924896240234 \n",
      "\n",
      "Iteration 2613, Loss: 36.44834518432617, L1: 10.524598121643066, L3: 25.92374610900879\n",
      "Current prediction:  61.228572845458984 \n",
      "\n",
      "Iteration 2614, Loss: 35.98074722290039, L1: 10.523200988769531, L3: 25.45754623413086\n",
      "Current prediction:  61.23078536987305 \n",
      "\n",
      "Iteration 2615, Loss: 36.28144836425781, L1: 10.521322250366211, L3: 25.76012420654297\n",
      "Current prediction:  61.23297119140625 \n",
      "\n",
      "Iteration 2616, Loss: 36.521419525146484, L1: 10.519469261169434, L3: 26.001949310302734\n",
      "Current prediction:  61.234859466552734 \n",
      "\n",
      "Iteration 2617, Loss: 36.83925247192383, L1: 10.517868995666504, L3: 26.32138442993164\n",
      "Current prediction:  61.23579025268555 \n",
      "\n",
      "Iteration 2618, Loss: 36.0731086730957, L1: 10.51708698272705, L3: 25.55602264404297\n",
      "Current prediction:  61.23649978637695 \n",
      "\n",
      "Iteration 2619, Loss: 36.571205139160156, L1: 10.516477584838867, L3: 26.054725646972656\n",
      "Current prediction:  61.23600387573242 \n",
      "\n",
      "Iteration 2620, Loss: 36.882564544677734, L1: 10.516894340515137, L3: 26.36566925048828\n",
      "Current prediction:  61.23542785644531 \n",
      "\n",
      "Iteration 2621, Loss: 36.93944549560547, L1: 10.517385482788086, L3: 26.422061920166016\n",
      "Current prediction:  61.234500885009766 \n",
      "\n",
      "Iteration 2622, Loss: 36.996131896972656, L1: 10.51817512512207, L3: 26.47795867919922\n",
      "Current prediction:  61.23286437988281 \n",
      "\n",
      "Iteration 2623, Loss: 36.34402847290039, L1: 10.519556999206543, L3: 25.824472427368164\n",
      "Current prediction:  61.231807708740234 \n",
      "\n",
      "Iteration 2624, Loss: 37.15199279785156, L1: 10.520454406738281, L3: 26.631540298461914\n",
      "Current prediction:  61.2318229675293 \n",
      "\n",
      "Iteration 2625, Loss: 36.77544021606445, L1: 10.520440101623535, L3: 26.2549991607666\n",
      "Current prediction:  61.232486724853516 \n",
      "\n",
      "Iteration 2626, Loss: 36.315704345703125, L1: 10.519882202148438, L3: 25.795822143554688\n",
      "Current prediction:  61.232967376708984 \n",
      "\n",
      "Iteration 2627, Loss: 37.06598663330078, L1: 10.519472122192383, L3: 26.54651641845703\n",
      "Current prediction:  61.232513427734375 \n",
      "\n",
      "Iteration 2628, Loss: 36.109893798828125, L1: 10.519857406616211, L3: 25.590038299560547\n",
      "Current prediction:  61.23208236694336 \n",
      "\n",
      "Iteration 2629, Loss: 36.89295196533203, L1: 10.520219802856445, L3: 26.372732162475586\n",
      "Current prediction:  61.23175811767578 \n",
      "\n",
      "Iteration 2630, Loss: 36.60836410522461, L1: 10.520495414733887, L3: 26.08786964416504\n",
      "Current prediction:  61.23210906982422 \n",
      "\n",
      "Iteration 2631, Loss: 36.064056396484375, L1: 10.520200729370117, L3: 25.54385757446289\n",
      "Current prediction:  61.23373031616211 \n",
      "\n",
      "Iteration 2632, Loss: 37.373939514160156, L1: 10.518828392028809, L3: 26.85511016845703\n",
      "Current prediction:  61.23542785644531 \n",
      "\n",
      "Iteration 2633, Loss: 35.896427154541016, L1: 10.517387390136719, L3: 25.379039764404297\n",
      "Current prediction:  61.23768997192383 \n",
      "\n",
      "Iteration 2634, Loss: 36.0763053894043, L1: 10.538189888000488, L3: 25.538116455078125\n",
      "Current prediction:  61.239280700683594 \n",
      "\n",
      "Iteration 2635, Loss: 36.91761016845703, L1: 10.514118194580078, L3: 26.403493881225586\n",
      "Current prediction:  61.239967346191406 \n",
      "\n",
      "Iteration 2636, Loss: 37.20151901245117, L1: 10.513537406921387, L3: 26.68798065185547\n",
      "Current prediction:  61.23970413208008 \n",
      "\n",
      "Iteration 2637, Loss: 37.86283874511719, L1: 10.51375961303711, L3: 27.349077224731445\n",
      "Current prediction:  61.2390022277832 \n",
      "\n",
      "Iteration 2638, Loss: 37.450721740722656, L1: 10.51435661315918, L3: 26.936363220214844\n",
      "Current prediction:  61.23763656616211 \n",
      "\n",
      "Iteration 2639, Loss: 36.19562530517578, L1: 10.515514373779297, L3: 25.680112838745117\n",
      "Current prediction:  61.236900329589844 \n",
      "\n",
      "Iteration 2640, Loss: 36.299598693847656, L1: 10.516140937805176, L3: 25.783456802368164\n",
      "Current prediction:  61.236541748046875 \n",
      "\n",
      "Iteration 2641, Loss: 36.6776123046875, L1: 10.516465187072754, L3: 26.16114616394043\n",
      "Current prediction:  61.23570251464844 \n",
      "\n",
      "Iteration 2642, Loss: 36.69020080566406, L1: 10.514554977416992, L3: 26.175643920898438\n",
      "Current prediction:  61.23551940917969 \n",
      "\n",
      "Iteration 2643, Loss: 36.08611297607422, L1: 10.529884338378906, L3: 25.556230545043945\n",
      "Current prediction:  61.23552322387695 \n",
      "\n",
      "Iteration 2644, Loss: 37.38505172729492, L1: 10.51672649383545, L3: 26.868324279785156\n",
      "Current prediction:  61.23434066772461 \n",
      "\n",
      "Iteration 2645, Loss: 36.42338180541992, L1: 10.51769733428955, L3: 25.905683517456055\n",
      "Current prediction:  61.23302459716797 \n",
      "\n",
      "Iteration 2646, Loss: 37.83338165283203, L1: 10.521925926208496, L3: 27.31145668029785\n",
      "Current prediction:  61.23052978515625 \n",
      "\n",
      "Iteration 2647, Loss: 36.35712814331055, L1: 10.522767066955566, L3: 25.834362030029297\n",
      "Current prediction:  61.22697830200195 \n",
      "\n",
      "Iteration 2648, Loss: 37.39295196533203, L1: 10.533087730407715, L3: 26.85986328125\n",
      "Current prediction:  61.22337341308594 \n",
      "\n",
      "Iteration 2649, Loss: 36.3900032043457, L1: 10.529290199279785, L3: 25.8607120513916\n",
      "Current prediction:  61.2200927734375 \n",
      "\n",
      "Iteration 2650, Loss: 35.959468841552734, L1: 10.533575057983398, L3: 25.425893783569336\n",
      "Current prediction:  61.21650695800781 \n",
      "\n",
      "Iteration 2651, Loss: 36.07072830200195, L1: 10.54219913482666, L3: 25.528528213500977\n",
      "Current prediction:  61.2136344909668 \n",
      "\n",
      "Iteration 2652, Loss: 36.1417350769043, L1: 10.535038948059082, L3: 25.60669708251953\n",
      "Current prediction:  61.211708068847656 \n",
      "\n",
      "Iteration 2653, Loss: 35.85487365722656, L1: 10.54292106628418, L3: 25.31195068359375\n",
      "Current prediction:  61.2094612121582 \n",
      "\n",
      "Iteration 2654, Loss: 36.1314582824707, L1: 10.550954818725586, L3: 25.580503463745117\n",
      "Current prediction:  61.2080078125 \n",
      "\n",
      "Iteration 2655, Loss: 36.81536865234375, L1: 10.560904502868652, L3: 26.25446319580078\n",
      "Current prediction:  61.20698547363281 \n",
      "\n",
      "Iteration 2656, Loss: 35.70135498046875, L1: 10.571295738220215, L3: 25.13006019592285\n",
      "Current prediction:  61.207942962646484 \n",
      "\n",
      "Iteration 2657, Loss: 36.11481475830078, L1: 10.577127456665039, L3: 25.537689208984375\n",
      "Current prediction:  61.189998626708984 \n",
      "\n",
      "Iteration 2658, Loss: 36.6178092956543, L1: 10.58320140838623, L3: 26.03460693359375\n",
      "Current prediction:  61.10230255126953 \n",
      "\n",
      "Iteration 2659, Loss: 35.73392105102539, L1: 10.587156295776367, L3: 25.146764755249023\n",
      "Current prediction:  61.10185623168945 \n",
      "\n",
      "Iteration 2660, Loss: 36.596961975097656, L1: 10.589685440063477, L3: 26.007278442382812\n",
      "Current prediction:  61.10547637939453 \n",
      "\n",
      "Iteration 2661, Loss: 36.81278610229492, L1: 10.590904235839844, L3: 26.221881866455078\n",
      "Current prediction:  61.10850524902344 \n",
      "\n",
      "Iteration 2662, Loss: 36.14878845214844, L1: 10.593060493469238, L3: 25.555728912353516\n",
      "Current prediction:  61.11174774169922 \n",
      "\n",
      "Iteration 2663, Loss: 36.27674865722656, L1: 10.590554237365723, L3: 25.686193466186523\n",
      "Current prediction:  61.11502456665039 \n",
      "\n",
      "Iteration 2664, Loss: 36.50212097167969, L1: 10.586920738220215, L3: 25.915199279785156\n",
      "Current prediction:  61.117958068847656 \n",
      "\n",
      "Iteration 2665, Loss: 36.575958251953125, L1: 10.588935852050781, L3: 25.98702049255371\n",
      "Current prediction:  61.121612548828125 \n",
      "\n",
      "Iteration 2666, Loss: 36.054359436035156, L1: 10.587960243225098, L3: 25.466398239135742\n",
      "Current prediction:  61.12592315673828 \n",
      "\n",
      "Iteration 2667, Loss: 37.04507064819336, L1: 10.588129997253418, L3: 26.456939697265625\n",
      "Current prediction:  61.130558013916016 \n",
      "\n",
      "Iteration 2668, Loss: 36.443763732910156, L1: 10.584095001220703, L3: 25.85966682434082\n",
      "Current prediction:  61.13502502441406 \n",
      "\n",
      "Iteration 2669, Loss: 35.97610092163086, L1: 10.578782081604004, L3: 25.397319793701172\n",
      "Current prediction:  61.1397705078125 \n",
      "\n",
      "Iteration 2670, Loss: 35.95728302001953, L1: 10.57436752319336, L3: 25.382915496826172\n",
      "Current prediction:  61.14606857299805 \n",
      "\n",
      "Iteration 2671, Loss: 37.07887649536133, L1: 10.571846961975098, L3: 26.507028579711914\n",
      "Current prediction:  61.15199661254883 \n",
      "\n",
      "Iteration 2672, Loss: 38.03740692138672, L1: 10.567192077636719, L3: 27.470212936401367\n",
      "Current prediction:  61.157798767089844 \n",
      "\n",
      "Iteration 2673, Loss: 36.371097564697266, L1: 10.555249214172363, L3: 25.81584930419922\n",
      "Current prediction:  61.163063049316406 \n",
      "\n",
      "Iteration 2674, Loss: 37.36223220825195, L1: 10.549050331115723, L3: 26.813180923461914\n",
      "Current prediction:  61.16734313964844 \n",
      "\n",
      "Iteration 2675, Loss: 36.44487762451172, L1: 10.538375854492188, L3: 25.90650177001953\n",
      "Current prediction:  61.16989517211914 \n",
      "\n",
      "Iteration 2676, Loss: 37.458431243896484, L1: 10.53840160369873, L3: 26.92003059387207\n",
      "Current prediction:  61.172019958496094 \n",
      "\n",
      "Iteration 2677, Loss: 36.61207962036133, L1: 10.530786514282227, L3: 26.0812931060791\n",
      "Current prediction:  61.17273712158203 \n",
      "\n",
      "Iteration 2678, Loss: 36.212345123291016, L1: 10.52900218963623, L3: 25.6833438873291\n",
      "Current prediction:  61.17405700683594 \n",
      "\n",
      "Iteration 2679, Loss: 37.354698181152344, L1: 10.523767471313477, L3: 26.830930709838867\n",
      "Current prediction:  61.18389129638672 \n",
      "\n",
      "Iteration 2680, Loss: 37.280517578125, L1: 10.51949405670166, L3: 26.761024475097656\n",
      "Current prediction:  61.23650360107422 \n",
      "\n",
      "Iteration 2681, Loss: 37.205413818359375, L1: 10.517890930175781, L3: 26.68752098083496\n",
      "Current prediction:  61.28062438964844 \n",
      "\n",
      "Iteration 2682, Loss: 36.62651443481445, L1: 10.513077735900879, L3: 26.11343765258789\n",
      "Current prediction:  61.28803634643555 \n",
      "\n",
      "Iteration 2683, Loss: 36.119388580322266, L1: 10.50998306274414, L3: 25.609405517578125\n",
      "Current prediction:  61.287105560302734 \n",
      "\n",
      "Iteration 2684, Loss: 36.52153396606445, L1: 10.507407188415527, L3: 26.01412582397461\n",
      "Current prediction:  61.28475570678711 \n",
      "\n",
      "Iteration 2685, Loss: 35.97195053100586, L1: 10.51120662689209, L3: 25.460742950439453\n",
      "Current prediction:  61.281982421875 \n",
      "\n",
      "Iteration 2686, Loss: 36.40037536621094, L1: 10.505603790283203, L3: 25.8947696685791\n",
      "Current prediction:  61.279396057128906 \n",
      "\n",
      "Iteration 2687, Loss: 36.68962860107422, L1: 10.509437561035156, L3: 26.180192947387695\n",
      "Current prediction:  61.277103424072266 \n",
      "\n",
      "Iteration 2688, Loss: 35.80554962158203, L1: 10.510666847229004, L3: 25.294883728027344\n",
      "Current prediction:  61.27510070800781 \n",
      "\n",
      "Iteration 2689, Loss: 36.11459732055664, L1: 10.504770278930664, L3: 25.609827041625977\n",
      "Current prediction:  61.27329635620117 \n",
      "\n",
      "Iteration 2690, Loss: 37.26466751098633, L1: 10.504319190979004, L3: 26.76034927368164\n",
      "Current prediction:  61.271141052246094 \n",
      "\n",
      "Iteration 2691, Loss: 35.60062026977539, L1: 10.504473686218262, L3: 25.096147537231445\n",
      "Current prediction:  61.26887512207031 \n",
      "\n",
      "Iteration 2692, Loss: 36.1705207824707, L1: 10.503495216369629, L3: 25.66702651977539\n",
      "Current prediction:  61.26605987548828 \n",
      "\n",
      "Iteration 2693, Loss: 36.560028076171875, L1: 10.511773109436035, L3: 26.048254013061523\n",
      "Current prediction:  61.26411056518555 \n",
      "\n",
      "Iteration 2694, Loss: 36.45433044433594, L1: 10.50290298461914, L3: 25.951427459716797\n",
      "Current prediction:  61.26154327392578 \n",
      "\n",
      "Iteration 2695, Loss: 37.13268280029297, L1: 10.50309944152832, L3: 26.629581451416016\n",
      "Current prediction:  61.25914764404297 \n",
      "\n",
      "Iteration 2696, Loss: 35.527984619140625, L1: 10.520313262939453, L3: 25.00766944885254\n",
      "Current prediction:  61.25774002075195 \n",
      "\n",
      "Iteration 2697, Loss: 36.72870635986328, L1: 10.514350891113281, L3: 26.21435546875\n",
      "Current prediction:  61.25537872314453 \n",
      "\n",
      "Iteration 2698, Loss: 36.161102294921875, L1: 10.521142959594727, L3: 25.63995933532715\n",
      "Current prediction:  61.25305938720703 \n",
      "\n",
      "Iteration 2699, Loss: 36.52796936035156, L1: 10.52013111114502, L3: 26.00783920288086\n",
      "Current prediction:  61.24971389770508 \n",
      "\n",
      "Iteration 2700, Loss: 35.99610137939453, L1: 10.530598640441895, L3: 25.46550178527832\n",
      "Current prediction:  61.247520446777344 \n",
      "\n",
      "Iteration 2701, Loss: 37.84967041015625, L1: 10.525674819946289, L3: 27.323993682861328\n",
      "Current prediction:  61.24432373046875 \n",
      "\n",
      "Iteration 2702, Loss: 36.094390869140625, L1: 10.52770709991455, L3: 25.56668472290039\n",
      "Current prediction:  61.24201583862305 \n",
      "\n",
      "Iteration 2703, Loss: 36.88146209716797, L1: 10.52879810333252, L3: 26.352664947509766\n",
      "Current prediction:  61.23963928222656 \n",
      "\n",
      "Iteration 2704, Loss: 36.53459167480469, L1: 10.524550437927246, L3: 26.010042190551758\n",
      "Current prediction:  61.23798370361328 \n",
      "\n",
      "Iteration 2705, Loss: 37.31168746948242, L1: 10.528669357299805, L3: 26.783018112182617\n",
      "Current prediction:  61.23556900024414 \n",
      "\n",
      "Iteration 2706, Loss: 35.316532135009766, L1: 10.529834747314453, L3: 24.786697387695312\n",
      "Current prediction:  61.23408126831055 \n",
      "\n",
      "Iteration 2707, Loss: 36.67815017700195, L1: 10.532880783081055, L3: 26.1452693939209\n",
      "Current prediction:  61.23183059692383 \n",
      "\n",
      "Iteration 2708, Loss: 37.020511627197266, L1: 10.533660888671875, L3: 26.48685073852539\n",
      "Current prediction:  61.22856521606445 \n",
      "\n",
      "Iteration 2709, Loss: 37.512779235839844, L1: 10.534505844116211, L3: 26.978275299072266\n",
      "Current prediction:  61.22526168823242 \n",
      "\n",
      "Iteration 2710, Loss: 36.29780578613281, L1: 10.52932357788086, L3: 25.768482208251953\n",
      "Current prediction:  61.22280502319336 \n",
      "\n",
      "Iteration 2711, Loss: 35.20676803588867, L1: 10.5369873046875, L3: 24.669780731201172\n",
      "Current prediction:  61.2203254699707 \n",
      "\n",
      "Iteration 2712, Loss: 36.16048049926758, L1: 10.54006290435791, L3: 25.620418548583984\n",
      "Current prediction:  61.21794891357422 \n",
      "\n",
      "Iteration 2713, Loss: 36.944427490234375, L1: 10.544042587280273, L3: 26.400386810302734\n",
      "Current prediction:  61.215877532958984 \n",
      "\n",
      "Iteration 2714, Loss: 36.510658264160156, L1: 10.549871444702148, L3: 25.96078872680664\n",
      "Current prediction:  61.214237213134766 \n",
      "\n",
      "Iteration 2715, Loss: 36.720703125, L1: 10.552447319030762, L3: 26.168256759643555\n",
      "Current prediction:  61.21345138549805 \n",
      "\n",
      "Iteration 2716, Loss: 36.78865051269531, L1: 10.554778099060059, L3: 26.233871459960938\n",
      "Current prediction:  61.21292495727539 \n",
      "\n",
      "Iteration 2717, Loss: 36.21175003051758, L1: 10.554566383361816, L3: 25.657182693481445\n",
      "Current prediction:  61.213157653808594 \n",
      "\n",
      "Iteration 2718, Loss: 35.97864532470703, L1: 10.550965309143066, L3: 25.42768096923828\n",
      "Current prediction:  61.213966369628906 \n",
      "\n",
      "Iteration 2719, Loss: 37.577796936035156, L1: 10.551403999328613, L3: 27.02639389038086\n",
      "Current prediction:  61.21443176269531 \n",
      "\n",
      "Iteration 2720, Loss: 35.87293243408203, L1: 10.547187805175781, L3: 25.32574462890625\n",
      "Current prediction:  61.215904235839844 \n",
      "\n",
      "Iteration 2721, Loss: 36.29388427734375, L1: 10.549736976623535, L3: 25.74414825439453\n",
      "Current prediction:  61.216190338134766 \n",
      "\n",
      "Iteration 2722, Loss: 37.182861328125, L1: 10.547379493713379, L3: 26.635480880737305\n",
      "Current prediction:  61.2159538269043 \n",
      "\n",
      "Iteration 2723, Loss: 36.98529815673828, L1: 10.545417785644531, L3: 26.43988037109375\n",
      "Current prediction:  61.214683532714844 \n",
      "\n",
      "Iteration 2724, Loss: 35.472557067871094, L1: 10.542336463928223, L3: 24.930221557617188\n",
      "Current prediction:  61.21490478515625 \n",
      "\n",
      "Iteration 2725, Loss: 37.077388763427734, L1: 10.549637794494629, L3: 26.52775001525879\n",
      "Current prediction:  61.214229583740234 \n",
      "\n",
      "Iteration 2726, Loss: 35.95827865600586, L1: 10.549160957336426, L3: 25.409116744995117\n",
      "Current prediction:  61.2140007019043 \n",
      "\n",
      "Iteration 2727, Loss: 36.27196502685547, L1: 10.55111312866211, L3: 25.720849990844727\n",
      "Current prediction:  61.212947845458984 \n",
      "\n",
      "Iteration 2728, Loss: 35.91892623901367, L1: 10.540966033935547, L3: 25.377960205078125\n",
      "Current prediction:  61.212562561035156 \n",
      "\n",
      "Iteration 2729, Loss: 35.724212646484375, L1: 10.551725387573242, L3: 25.172487258911133\n",
      "Current prediction:  61.21235275268555 \n",
      "\n",
      "Iteration 2730, Loss: 37.00517272949219, L1: 10.5404634475708, L3: 26.464710235595703\n",
      "Current prediction:  61.212074279785156 \n",
      "\n",
      "Iteration 2731, Loss: 36.24242401123047, L1: 10.540556907653809, L3: 25.701868057250977\n",
      "Current prediction:  61.211544036865234 \n",
      "\n",
      "Iteration 2732, Loss: 35.91466522216797, L1: 10.535346031188965, L3: 25.37932014465332\n",
      "Current prediction:  61.21108627319336 \n",
      "\n",
      "Iteration 2733, Loss: 36.15802001953125, L1: 10.546638488769531, L3: 25.611379623413086\n",
      "Current prediction:  61.21076202392578 \n",
      "\n",
      "Iteration 2734, Loss: 36.96110153198242, L1: 10.536067008972168, L3: 26.425033569335938\n",
      "Current prediction:  61.210655212402344 \n",
      "\n",
      "Iteration 2735, Loss: 36.7716064453125, L1: 10.55318832397461, L3: 26.21841812133789\n",
      "Current prediction:  61.20960235595703 \n",
      "\n",
      "Iteration 2736, Loss: 36.46405792236328, L1: 10.53908634185791, L3: 25.924970626831055\n",
      "Current prediction:  61.209625244140625 \n",
      "\n",
      "Iteration 2737, Loss: 36.25748062133789, L1: 10.543862342834473, L3: 25.713619232177734\n",
      "Current prediction:  61.210113525390625 \n",
      "\n",
      "Iteration 2738, Loss: 35.74558639526367, L1: 10.538835525512695, L3: 25.206750869750977\n",
      "Current prediction:  61.211509704589844 \n",
      "\n",
      "Iteration 2739, Loss: 35.85673904418945, L1: 10.537968635559082, L3: 25.318769454956055\n",
      "Current prediction:  61.213401794433594 \n",
      "\n",
      "Iteration 2740, Loss: 36.595436096191406, L1: 10.547771453857422, L3: 26.047664642333984\n",
      "Current prediction:  61.21510314941406 \n",
      "\n",
      "Iteration 2741, Loss: 36.23331069946289, L1: 10.539246559143066, L3: 25.694063186645508\n",
      "Current prediction:  61.21730041503906 \n",
      "\n",
      "Iteration 2742, Loss: 37.37462615966797, L1: 10.530058860778809, L3: 26.844566345214844\n",
      "Current prediction:  61.218589782714844 \n",
      "\n",
      "Iteration 2743, Loss: 36.1292839050293, L1: 10.535311698913574, L3: 25.59397315979004\n",
      "Current prediction:  61.2198600769043 \n",
      "\n",
      "Iteration 2744, Loss: 36.3386344909668, L1: 10.534709930419922, L3: 25.803924560546875\n",
      "Current prediction:  61.220794677734375 \n",
      "\n",
      "Iteration 2745, Loss: 36.836204528808594, L1: 10.542844772338867, L3: 26.29336166381836\n",
      "Current prediction:  61.222164154052734 \n",
      "\n",
      "Iteration 2746, Loss: 37.020172119140625, L1: 10.548955917358398, L3: 26.471216201782227\n",
      "Current prediction:  61.22359848022461 \n",
      "\n",
      "Iteration 2747, Loss: 37.078369140625, L1: 10.534995079040527, L3: 26.54337501525879\n",
      "Current prediction:  61.22494888305664 \n",
      "\n",
      "Iteration 2748, Loss: 37.41260528564453, L1: 10.526116371154785, L3: 26.886489868164062\n",
      "Current prediction:  61.22700881958008 \n",
      "\n",
      "Iteration 2749, Loss: 35.714271545410156, L1: 10.529167175292969, L3: 25.18510627746582\n",
      "Current prediction:  61.22769546508789 \n",
      "\n",
      "Iteration 2750, Loss: 36.25310134887695, L1: 10.521202087402344, L3: 25.73189926147461\n",
      "Current prediction:  61.22728729248047 \n",
      "\n",
      "Iteration 2751, Loss: 37.367408752441406, L1: 10.52791690826416, L3: 26.83949089050293\n",
      "Current prediction:  61.2259521484375 \n",
      "\n",
      "Iteration 2752, Loss: 36.86154556274414, L1: 10.529208183288574, L3: 26.332338333129883\n",
      "Current prediction:  61.22391128540039 \n",
      "\n",
      "Iteration 2753, Loss: 36.167545318603516, L1: 10.527153968811035, L3: 25.640392303466797\n",
      "Current prediction:  61.223270416259766 \n",
      "\n",
      "Iteration 2754, Loss: 37.275978088378906, L1: 10.52771282196045, L3: 26.748266220092773\n",
      "Current prediction:  61.22090530395508 \n",
      "\n",
      "Iteration 2755, Loss: 37.15583038330078, L1: 10.529701232910156, L3: 26.626131057739258\n",
      "Current prediction:  61.21829605102539 \n",
      "\n",
      "Iteration 2756, Loss: 35.71978759765625, L1: 10.531913757324219, L3: 25.1878719329834\n",
      "Current prediction:  61.21702194213867 \n",
      "\n",
      "Iteration 2757, Loss: 36.05115509033203, L1: 10.532987594604492, L3: 25.518169403076172\n",
      "Current prediction:  61.21607208251953 \n",
      "\n",
      "Iteration 2758, Loss: 36.4119873046875, L1: 10.53380012512207, L3: 25.878185272216797\n",
      "Current prediction:  61.21634292602539 \n",
      "\n",
      "Iteration 2759, Loss: 37.165462493896484, L1: 10.533570289611816, L3: 26.631893157958984\n",
      "Current prediction:  61.216087341308594 \n",
      "\n",
      "Iteration 2760, Loss: 36.804359436035156, L1: 10.533787727355957, L3: 26.270572662353516\n",
      "Current prediction:  61.21520233154297 \n",
      "\n",
      "Iteration 2761, Loss: 35.89341354370117, L1: 10.534534454345703, L3: 25.35887908935547\n",
      "Current prediction:  61.21467971801758 \n",
      "\n",
      "Iteration 2762, Loss: 36.83672332763672, L1: 10.534974098205566, L3: 26.30175018310547\n",
      "Current prediction:  61.21556091308594 \n",
      "\n",
      "Iteration 2763, Loss: 36.65996551513672, L1: 10.534213066101074, L3: 26.12575340270996\n",
      "Current prediction:  61.2146110534668 \n",
      "\n",
      "Iteration 2764, Loss: 36.32786560058594, L1: 10.535036087036133, L3: 25.792827606201172\n",
      "Current prediction:  61.214263916015625 \n",
      "\n",
      "Iteration 2765, Loss: 36.91392517089844, L1: 10.535331726074219, L3: 26.378591537475586\n",
      "Current prediction:  61.212928771972656 \n",
      "\n",
      "Iteration 2766, Loss: 36.18092727661133, L1: 10.536467552185059, L3: 25.644458770751953\n",
      "Current prediction:  61.21273422241211 \n",
      "\n",
      "Iteration 2767, Loss: 36.41954803466797, L1: 10.536632537841797, L3: 25.882915496826172\n",
      "Current prediction:  61.21271514892578 \n",
      "\n",
      "Iteration 2768, Loss: 36.34435272216797, L1: 10.53664493560791, L3: 25.807708740234375\n",
      "Current prediction:  61.21253204345703 \n",
      "\n",
      "Iteration 2769, Loss: 36.73979949951172, L1: 10.536800384521484, L3: 26.2029972076416\n",
      "Current prediction:  61.21367645263672 \n",
      "\n",
      "Iteration 2770, Loss: 36.22357177734375, L1: 10.535832405090332, L3: 25.6877384185791\n",
      "Current prediction:  61.21540069580078 \n",
      "\n",
      "Iteration 2771, Loss: 35.57872009277344, L1: 10.534368515014648, L3: 25.044353485107422\n",
      "Current prediction:  61.216949462890625 \n",
      "\n",
      "Iteration 2772, Loss: 38.072113037109375, L1: 10.533055305480957, L3: 27.5390567779541\n",
      "Current prediction:  61.21676254272461 \n",
      "\n",
      "Iteration 2773, Loss: 36.30097961425781, L1: 10.533218383789062, L3: 25.76776123046875\n",
      "Current prediction:  61.21533203125 \n",
      "\n",
      "Iteration 2774, Loss: 36.688438415527344, L1: 10.534423828125, L3: 26.154016494750977\n",
      "Current prediction:  61.213348388671875 \n",
      "\n",
      "Iteration 2775, Loss: 36.27516174316406, L1: 10.536105155944824, L3: 25.739057540893555\n",
      "Current prediction:  61.21281051635742 \n",
      "\n",
      "Iteration 2776, Loss: 37.054412841796875, L1: 10.536561965942383, L3: 26.517850875854492\n",
      "Current prediction:  61.21281814575195 \n",
      "\n",
      "Iteration 2777, Loss: 36.1126823425293, L1: 10.536558151245117, L3: 25.57612419128418\n",
      "Current prediction:  61.21165084838867 \n",
      "\n",
      "Iteration 2778, Loss: 36.93894577026367, L1: 10.537550926208496, L3: 26.40139389038086\n",
      "Current prediction:  61.21215057373047 \n",
      "\n",
      "Iteration 2779, Loss: 36.0413818359375, L1: 10.537124633789062, L3: 25.504255294799805\n",
      "Current prediction:  61.21316146850586 \n",
      "\n",
      "Iteration 2780, Loss: 36.70872116088867, L1: 10.536267280578613, L3: 26.172452926635742\n",
      "Current prediction:  61.21379852294922 \n",
      "\n",
      "Iteration 2781, Loss: 37.676910400390625, L1: 10.535728454589844, L3: 27.14118194580078\n",
      "Current prediction:  61.2149543762207 \n",
      "\n",
      "Iteration 2782, Loss: 36.31997299194336, L1: 10.534749031066895, L3: 25.78522491455078\n",
      "Current prediction:  61.215187072753906 \n",
      "\n",
      "Iteration 2783, Loss: 36.7328987121582, L1: 10.534547805786133, L3: 26.19835090637207\n",
      "Current prediction:  61.21571731567383 \n",
      "\n",
      "Iteration 2784, Loss: 36.79642105102539, L1: 10.534100532531738, L3: 26.26232147216797\n",
      "Current prediction:  61.216026306152344 \n",
      "\n",
      "Iteration 2785, Loss: 36.23768615722656, L1: 10.53384017944336, L3: 25.703845977783203\n",
      "Current prediction:  61.216888427734375 \n",
      "\n",
      "Iteration 2786, Loss: 36.617698669433594, L1: 10.533109664916992, L3: 26.084590911865234\n",
      "Current prediction:  61.21699905395508 \n",
      "\n",
      "Iteration 2787, Loss: 36.22186279296875, L1: 10.533011436462402, L3: 25.68885040283203\n",
      "Current prediction:  61.21645736694336 \n",
      "\n",
      "Iteration 2788, Loss: 36.29834747314453, L1: 10.533473014831543, L3: 25.764873504638672\n",
      "Current prediction:  61.216121673583984 \n",
      "\n",
      "Iteration 2789, Loss: 36.37605285644531, L1: 10.533757209777832, L3: 25.842294692993164\n",
      "Current prediction:  61.21675109863281 \n",
      "\n",
      "Iteration 2790, Loss: 36.77986526489258, L1: 10.533223152160645, L3: 26.24664306640625\n",
      "Current prediction:  61.2165412902832 \n",
      "\n",
      "Iteration 2791, Loss: 36.85379409790039, L1: 10.533400535583496, L3: 26.32039451599121\n",
      "Current prediction:  61.2166633605957 \n",
      "\n",
      "Iteration 2792, Loss: 36.30855941772461, L1: 10.53330135345459, L3: 25.775257110595703\n",
      "Current prediction:  61.21685791015625 \n",
      "\n",
      "Iteration 2793, Loss: 37.48957061767578, L1: 10.533130645751953, L3: 26.956439971923828\n",
      "Current prediction:  61.21517562866211 \n",
      "\n",
      "Iteration 2794, Loss: 37.48637771606445, L1: 10.53455638885498, L3: 26.95182228088379\n",
      "Current prediction:  61.212493896484375 \n",
      "\n",
      "Iteration 2795, Loss: 38.028533935546875, L1: 10.53681755065918, L3: 27.491718292236328\n",
      "Current prediction:  61.2098274230957 \n",
      "\n",
      "Iteration 2796, Loss: 36.99679183959961, L1: 10.539093971252441, L3: 26.45769691467285\n",
      "Current prediction:  61.20741271972656 \n",
      "\n",
      "Iteration 2797, Loss: 36.469417572021484, L1: 10.541138648986816, L3: 25.92827796936035\n",
      "Current prediction:  61.20547866821289 \n",
      "\n",
      "Iteration 2798, Loss: 36.715545654296875, L1: 10.542781829833984, L3: 26.17276382446289\n",
      "Current prediction:  61.204959869384766 \n",
      "\n",
      "Iteration 2799, Loss: 36.748931884765625, L1: 10.543220520019531, L3: 26.20570945739746\n",
      "Current prediction:  61.20452117919922 \n",
      "\n",
      "Iteration 2800, Loss: 37.40281677246094, L1: 10.543596267700195, L3: 26.859222412109375\n",
      "Current prediction:  61.202789306640625 \n",
      "\n",
      "Iteration 2801, Loss: 37.33171844482422, L1: 10.545056343078613, L3: 26.786663055419922\n",
      "Current prediction:  61.200538635253906 \n",
      "\n",
      "Iteration 2802, Loss: 37.47276306152344, L1: 10.546975135803223, L3: 26.9257869720459\n",
      "Current prediction:  61.19776153564453 \n",
      "\n",
      "Iteration 2803, Loss: 35.81789779663086, L1: 10.549324989318848, L3: 25.268573760986328\n",
      "Current prediction:  61.19440460205078 \n",
      "\n",
      "Iteration 2804, Loss: 36.873905181884766, L1: 10.552170753479004, L3: 26.321733474731445\n",
      "Current prediction:  61.19056701660156 \n",
      "\n",
      "Iteration 2805, Loss: 37.00779724121094, L1: 10.555424690246582, L3: 26.45237159729004\n",
      "Current prediction:  61.18709182739258 \n",
      "\n",
      "Iteration 2806, Loss: 36.65158462524414, L1: 10.558363914489746, L3: 26.09322166442871\n",
      "Current prediction:  61.18370819091797 \n",
      "\n",
      "Iteration 2807, Loss: 36.19830322265625, L1: 10.561233520507812, L3: 25.637069702148438\n",
      "Current prediction:  61.18136978149414 \n",
      "\n",
      "Iteration 2808, Loss: 37.0206413269043, L1: 10.563223838806152, L3: 26.45741844177246\n",
      "Current prediction:  61.17887496948242 \n",
      "\n",
      "Iteration 2809, Loss: 35.38078308105469, L1: 10.565338134765625, L3: 24.815444946289062\n",
      "Current prediction:  61.17793655395508 \n",
      "\n",
      "Iteration 2810, Loss: 35.6393928527832, L1: 10.566131591796875, L3: 25.073261260986328\n",
      "Current prediction:  61.17710494995117 \n",
      "\n",
      "Iteration 2811, Loss: 36.41436004638672, L1: 10.566840171813965, L3: 25.847518920898438\n",
      "Current prediction:  61.176212310791016 \n",
      "\n",
      "Iteration 2812, Loss: 35.48012924194336, L1: 10.567593574523926, L3: 24.912534713745117\n",
      "Current prediction:  61.176727294921875 \n",
      "\n",
      "Iteration 2813, Loss: 36.33274841308594, L1: 10.567155838012695, L3: 25.76559066772461\n",
      "Current prediction:  61.177433013916016 \n",
      "\n",
      "Iteration 2814, Loss: 37.088741302490234, L1: 10.56656265258789, L3: 26.522178649902344\n",
      "Current prediction:  61.17847442626953 \n",
      "\n",
      "Iteration 2815, Loss: 36.699302673339844, L1: 10.565674781799316, L3: 26.133628845214844\n",
      "Current prediction:  61.17952346801758 \n",
      "\n",
      "Iteration 2816, Loss: 35.312740325927734, L1: 10.564785957336426, L3: 24.747953414916992\n",
      "Current prediction:  61.18241500854492 \n",
      "\n",
      "Iteration 2817, Loss: 36.95635223388672, L1: 10.562335968017578, L3: 26.39401626586914\n",
      "Current prediction:  61.1845817565918 \n",
      "\n",
      "Iteration 2818, Loss: 36.970184326171875, L1: 10.560501098632812, L3: 26.40968132019043\n",
      "Current prediction:  61.18661880493164 \n",
      "\n",
      "Iteration 2819, Loss: 36.08414077758789, L1: 10.558772087097168, L3: 25.52536964416504\n",
      "Current prediction:  61.18967819213867 \n",
      "\n",
      "Iteration 2820, Loss: 36.992088317871094, L1: 10.55617618560791, L3: 26.4359130859375\n",
      "Current prediction:  61.19269943237305 \n",
      "\n",
      "Iteration 2821, Loss: 36.803951263427734, L1: 10.553617477416992, L3: 26.250333786010742\n",
      "Current prediction:  61.19523239135742 \n",
      "\n",
      "Iteration 2822, Loss: 37.37385559082031, L1: 10.551469802856445, L3: 26.8223876953125\n",
      "Current prediction:  61.196998596191406 \n",
      "\n",
      "Iteration 2823, Loss: 37.73482894897461, L1: 10.549967765808105, L3: 27.18486213684082\n",
      "Current prediction:  61.1976203918457 \n",
      "\n",
      "Iteration 2824, Loss: 35.52243423461914, L1: 10.549443244934082, L3: 24.972990036010742\n",
      "Current prediction:  61.197410583496094 \n",
      "\n",
      "Iteration 2825, Loss: 36.567562103271484, L1: 10.549620628356934, L3: 26.017940521240234\n",
      "Current prediction:  61.19613265991211 \n",
      "\n",
      "Iteration 2826, Loss: 37.64865493774414, L1: 10.550705909729004, L3: 27.097949981689453\n",
      "Current prediction:  61.19490051269531 \n",
      "\n",
      "Iteration 2827, Loss: 36.951560974121094, L1: 10.551750183105469, L3: 26.399812698364258\n",
      "Current prediction:  61.19255828857422 \n",
      "\n",
      "Iteration 2828, Loss: 36.33647918701172, L1: 10.55373764038086, L3: 25.78274154663086\n",
      "Current prediction:  61.190528869628906 \n",
      "\n",
      "Iteration 2829, Loss: 36.172523498535156, L1: 10.555456161499023, L3: 25.6170654296875\n",
      "Current prediction:  61.1895637512207 \n",
      "\n",
      "Iteration 2830, Loss: 37.76983642578125, L1: 10.5562744140625, L3: 27.21356201171875\n",
      "Current prediction:  61.18843078613281 \n",
      "\n",
      "Iteration 2831, Loss: 36.13413619995117, L1: 10.557236671447754, L3: 25.576900482177734\n",
      "Current prediction:  61.18782043457031 \n",
      "\n",
      "Iteration 2832, Loss: 36.45756149291992, L1: 10.557755470275879, L3: 25.89980697631836\n",
      "Current prediction:  61.18596267700195 \n",
      "\n",
      "Iteration 2833, Loss: 37.62269592285156, L1: 10.559331893920898, L3: 27.06336212158203\n",
      "Current prediction:  61.18354034423828 \n",
      "\n",
      "Iteration 2834, Loss: 36.52149200439453, L1: 10.561376571655273, L3: 25.960115432739258\n",
      "Current prediction:  61.18262481689453 \n",
      "\n",
      "Iteration 2835, Loss: 37.38233947753906, L1: 10.562153816223145, L3: 26.820186614990234\n",
      "Current prediction:  61.18095397949219 \n",
      "\n",
      "Iteration 2836, Loss: 37.00091552734375, L1: 10.563573837280273, L3: 26.43734359741211\n",
      "Current prediction:  61.17973327636719 \n",
      "\n",
      "Iteration 2837, Loss: 36.77924346923828, L1: 10.564605712890625, L3: 26.214637756347656\n",
      "Current prediction:  61.17793655395508 \n",
      "\n",
      "Iteration 2838, Loss: 36.63751220703125, L1: 10.566244125366211, L3: 26.071269989013672\n",
      "Current prediction:  61.17721939086914 \n",
      "\n",
      "Iteration 2839, Loss: 36.79042053222656, L1: 10.566740989685059, L3: 26.223678588867188\n",
      "Current prediction:  61.1771125793457 \n",
      "\n",
      "Iteration 2840, Loss: 37.134735107421875, L1: 10.566831588745117, L3: 26.56790542602539\n",
      "Current prediction:  61.1777458190918 \n",
      "\n",
      "Iteration 2841, Loss: 36.90058898925781, L1: 10.566293716430664, L3: 26.33429718017578\n",
      "Current prediction:  61.177879333496094 \n",
      "\n",
      "Iteration 2842, Loss: 36.97613525390625, L1: 10.566180229187012, L3: 26.409955978393555\n",
      "Current prediction:  61.17787170410156 \n",
      "\n",
      "Iteration 2843, Loss: 37.056365966796875, L1: 10.56618595123291, L3: 26.49017906188965\n",
      "Current prediction:  61.17881774902344 \n",
      "\n",
      "Iteration 2844, Loss: 37.83131790161133, L1: 10.565388679504395, L3: 27.265928268432617\n",
      "Current prediction:  61.17873001098633 \n",
      "\n",
      "Iteration 2845, Loss: 36.065792083740234, L1: 10.565457344055176, L3: 25.500333786010742\n",
      "Current prediction:  61.178199768066406 \n",
      "\n",
      "Iteration 2846, Loss: 35.56940460205078, L1: 10.565911293029785, L3: 25.00349235534668\n",
      "Current prediction:  61.17805862426758 \n",
      "\n",
      "Iteration 2847, Loss: 36.57566452026367, L1: 10.566024780273438, L3: 26.009639739990234\n",
      "Current prediction:  61.17842483520508 \n",
      "\n",
      "Iteration 2848, Loss: 36.527984619140625, L1: 10.565713882446289, L3: 25.962270736694336\n",
      "Current prediction:  61.17803192138672 \n",
      "\n",
      "Iteration 2849, Loss: 35.80384826660156, L1: 10.566049575805664, L3: 25.237796783447266\n",
      "Current prediction:  61.17963790893555 \n",
      "\n",
      "Iteration 2850, Loss: 37.43032455444336, L1: 10.56469440460205, L3: 26.865631103515625\n",
      "Current prediction:  61.18162536621094 \n",
      "\n",
      "Iteration 2851, Loss: 37.287776947021484, L1: 10.563007354736328, L3: 26.724769592285156\n",
      "Current prediction:  61.18351364135742 \n",
      "\n",
      "Iteration 2852, Loss: 36.95066452026367, L1: 10.5614013671875, L3: 26.389263153076172\n",
      "Current prediction:  61.184913635253906 \n",
      "\n",
      "Iteration 2853, Loss: 36.585601806640625, L1: 10.560219764709473, L3: 26.025381088256836\n",
      "Current prediction:  61.18614196777344 \n",
      "\n",
      "Iteration 2854, Loss: 36.568931579589844, L1: 10.559179306030273, L3: 26.00975227355957\n",
      "Current prediction:  61.18721389770508 \n",
      "\n",
      "Iteration 2855, Loss: 37.29269790649414, L1: 10.558267593383789, L3: 26.73443031311035\n",
      "Current prediction:  61.18843078613281 \n",
      "\n",
      "Iteration 2856, Loss: 36.443023681640625, L1: 10.557236671447754, L3: 25.885786056518555\n",
      "Current prediction:  61.188568115234375 \n",
      "\n",
      "Iteration 2857, Loss: 36.321617126464844, L1: 10.557121276855469, L3: 25.764495849609375\n",
      "Current prediction:  61.19034957885742 \n",
      "\n",
      "Iteration 2858, Loss: 35.985939025878906, L1: 10.555606842041016, L3: 25.430330276489258\n",
      "Current prediction:  61.19253921508789 \n",
      "\n",
      "Iteration 2859, Loss: 36.72125244140625, L1: 10.553750038146973, L3: 26.16750144958496\n",
      "Current prediction:  61.19448471069336 \n",
      "\n",
      "Iteration 2860, Loss: 36.580718994140625, L1: 10.552104949951172, L3: 26.028614044189453\n",
      "Current prediction:  61.19596862792969 \n",
      "\n",
      "Iteration 2861, Loss: 37.21617889404297, L1: 10.550843238830566, L3: 26.66533660888672\n",
      "Current prediction:  61.19778823852539 \n",
      "\n",
      "Iteration 2862, Loss: 35.997276306152344, L1: 10.549301147460938, L3: 25.447973251342773\n",
      "Current prediction:  61.20005416870117 \n",
      "\n",
      "Iteration 2863, Loss: 35.892581939697266, L1: 10.547379493713379, L3: 25.34520149230957\n",
      "Current prediction:  61.20376968383789 \n",
      "\n",
      "Iteration 2864, Loss: 36.45750427246094, L1: 10.544229507446289, L3: 25.91327476501465\n",
      "Current prediction:  61.20623016357422 \n",
      "\n",
      "Iteration 2865, Loss: 36.006752014160156, L1: 10.542144775390625, L3: 25.464609146118164\n",
      "Current prediction:  61.208709716796875 \n",
      "\n",
      "Iteration 2866, Loss: 36.575782775878906, L1: 10.540045738220215, L3: 26.035737991333008\n",
      "Current prediction:  61.21178436279297 \n",
      "\n",
      "Iteration 2867, Loss: 37.36773681640625, L1: 10.537437438964844, L3: 26.830297470092773\n",
      "Current prediction:  61.21452331542969 \n",
      "\n",
      "Iteration 2868, Loss: 36.84357452392578, L1: 10.535112380981445, L3: 26.308462142944336\n",
      "Current prediction:  61.216331481933594 \n",
      "\n",
      "Iteration 2869, Loss: 36.44293975830078, L1: 10.533580780029297, L3: 25.90935707092285\n",
      "Current prediction:  61.217281341552734 \n",
      "\n",
      "Iteration 2870, Loss: 36.025352478027344, L1: 10.532773971557617, L3: 25.49258041381836\n",
      "Current prediction:  61.218902587890625 \n",
      "\n",
      "Iteration 2871, Loss: 36.570003509521484, L1: 10.531394004821777, L3: 26.03860855102539\n",
      "Current prediction:  61.220184326171875 \n",
      "\n",
      "Iteration 2872, Loss: 36.345703125, L1: 10.530314445495605, L3: 25.81538963317871\n",
      "Current prediction:  61.22220993041992 \n",
      "\n",
      "Iteration 2873, Loss: 36.53279495239258, L1: 10.528594970703125, L3: 26.004199981689453\n",
      "Current prediction:  61.22437286376953 \n",
      "\n",
      "Iteration 2874, Loss: 36.98103332519531, L1: 10.526762962341309, L3: 26.454269409179688\n",
      "Current prediction:  61.22627258300781 \n",
      "\n",
      "Iteration 2875, Loss: 35.831661224365234, L1: 10.525148391723633, L3: 25.3065128326416\n",
      "Current prediction:  61.228553771972656 \n",
      "\n",
      "Iteration 2876, Loss: 36.44076156616211, L1: 10.523215293884277, L3: 25.91754722595215\n",
      "Current prediction:  61.230560302734375 \n",
      "\n",
      "Iteration 2877, Loss: 37.2258186340332, L1: 10.521513938903809, L3: 26.704303741455078\n",
      "Current prediction:  61.23173141479492 \n",
      "\n",
      "Iteration 2878, Loss: 36.76316452026367, L1: 10.52051830291748, L3: 26.242647171020508\n",
      "Current prediction:  61.231937408447266 \n",
      "\n",
      "Iteration 2879, Loss: 36.68408203125, L1: 10.520342826843262, L3: 26.163738250732422\n",
      "Current prediction:  61.231842041015625 \n",
      "\n",
      "Iteration 2880, Loss: 36.68974304199219, L1: 10.520424842834473, L3: 26.16931915283203\n",
      "Current prediction:  61.232086181640625 \n",
      "\n",
      "Iteration 2881, Loss: 35.81939697265625, L1: 10.520219802856445, L3: 25.299177169799805\n",
      "Current prediction:  61.23173904418945 \n",
      "\n",
      "Iteration 2882, Loss: 36.53301239013672, L1: 10.520509719848633, L3: 26.012502670288086\n",
      "Current prediction:  61.229557037353516 \n",
      "\n",
      "Iteration 2883, Loss: 36.59595489501953, L1: 10.522363662719727, L3: 26.073591232299805\n",
      "Current prediction:  61.227638244628906 \n",
      "\n",
      "Iteration 2884, Loss: 36.427310943603516, L1: 10.523993492126465, L3: 25.903318405151367\n",
      "Current prediction:  61.22571563720703 \n",
      "\n",
      "Iteration 2885, Loss: 36.74224853515625, L1: 10.525619506835938, L3: 26.216629028320312\n",
      "Current prediction:  61.22536849975586 \n",
      "\n",
      "Iteration 2886, Loss: 36.73148727416992, L1: 10.525921821594238, L3: 26.20556640625\n",
      "Current prediction:  61.22544479370117 \n",
      "\n",
      "Iteration 2887, Loss: 35.78346633911133, L1: 10.52585506439209, L3: 25.257612228393555\n",
      "Current prediction:  61.22513961791992 \n",
      "\n",
      "Iteration 2888, Loss: 36.536556243896484, L1: 10.526110649108887, L3: 26.01044464111328\n",
      "Current prediction:  61.2255744934082 \n",
      "\n",
      "Iteration 2889, Loss: 35.98055648803711, L1: 10.525739669799805, L3: 25.454816818237305\n",
      "Current prediction:  61.22614288330078 \n",
      "\n",
      "Iteration 2890, Loss: 35.096580505371094, L1: 10.525259017944336, L3: 24.571319580078125\n",
      "Current prediction:  61.22553634643555 \n",
      "\n",
      "Iteration 2891, Loss: 35.629295349121094, L1: 10.525774955749512, L3: 25.103519439697266\n",
      "Current prediction:  61.22514724731445 \n",
      "\n",
      "Iteration 2892, Loss: 35.94664764404297, L1: 10.526105880737305, L3: 25.42053985595703\n",
      "Current prediction:  61.22490310668945 \n",
      "\n",
      "Iteration 2893, Loss: 35.619972229003906, L1: 10.5263090133667, L3: 25.09366226196289\n",
      "Current prediction:  61.22632598876953 \n",
      "\n",
      "Iteration 2894, Loss: 36.927764892578125, L1: 10.525103569030762, L3: 26.40266227722168\n",
      "Current prediction:  61.22825622558594 \n",
      "\n",
      "Iteration 2895, Loss: 36.50428009033203, L1: 10.523468971252441, L3: 25.980810165405273\n",
      "Current prediction:  61.229461669921875 \n",
      "\n",
      "Iteration 2896, Loss: 36.816829681396484, L1: 10.522444725036621, L3: 26.294384002685547\n",
      "Current prediction:  61.23033142089844 \n",
      "\n",
      "Iteration 2897, Loss: 35.788700103759766, L1: 10.52170467376709, L3: 25.266996383666992\n",
      "Current prediction:  61.231964111328125 \n",
      "\n",
      "Iteration 2898, Loss: 37.30632019042969, L1: 10.520318984985352, L3: 26.785999298095703\n",
      "Current prediction:  61.23344421386719 \n",
      "\n",
      "Iteration 2899, Loss: 36.51261901855469, L1: 10.51906681060791, L3: 25.99355125427246\n",
      "Current prediction:  61.234249114990234 \n",
      "\n",
      "Iteration 2900, Loss: 37.5834846496582, L1: 10.51838207244873, L3: 27.06510353088379\n",
      "Current prediction:  61.23421096801758 \n",
      "\n",
      "Iteration 2901, Loss: 37.526641845703125, L1: 10.518418312072754, L3: 27.008224487304688\n",
      "Current prediction:  61.233642578125 \n",
      "\n",
      "Iteration 2902, Loss: 36.6068115234375, L1: 10.518898010253906, L3: 26.087915420532227\n",
      "Current prediction:  61.232112884521484 \n",
      "\n",
      "Iteration 2903, Loss: 36.850807189941406, L1: 10.520195007324219, L3: 26.330612182617188\n",
      "Current prediction:  61.23088073730469 \n",
      "\n",
      "Iteration 2904, Loss: 35.623817443847656, L1: 10.521240234375, L3: 25.102577209472656\n",
      "Current prediction:  61.23033142089844 \n",
      "\n",
      "Iteration 2905, Loss: 36.701805114746094, L1: 10.521707534790039, L3: 26.180097579956055\n",
      "Current prediction:  61.22983932495117 \n",
      "\n",
      "Iteration 2906, Loss: 35.74485778808594, L1: 10.522126197814941, L3: 25.22273063659668\n",
      "Current prediction:  61.22889709472656 \n",
      "\n",
      "Iteration 2907, Loss: 36.623722076416016, L1: 10.522921562194824, L3: 26.100799560546875\n",
      "Current prediction:  61.22781753540039 \n",
      "\n",
      "Iteration 2908, Loss: 36.24584197998047, L1: 10.523838996887207, L3: 25.722003936767578\n",
      "Current prediction:  61.22700881958008 \n",
      "\n",
      "Iteration 2909, Loss: 35.56897735595703, L1: 10.524530410766602, L3: 25.04444694519043\n",
      "Current prediction:  61.225196838378906 \n",
      "\n",
      "Iteration 2910, Loss: 36.3590087890625, L1: 10.526058197021484, L3: 25.83295249938965\n",
      "Current prediction:  61.222774505615234 \n",
      "\n",
      "Iteration 2911, Loss: 36.65535354614258, L1: 10.528119087219238, L3: 26.127233505249023\n",
      "Current prediction:  61.2186279296875 \n",
      "\n",
      "Iteration 2912, Loss: 35.85535430908203, L1: 10.531632423400879, L3: 25.32372283935547\n",
      "Current prediction:  61.214603424072266 \n",
      "\n",
      "Iteration 2913, Loss: 36.60938262939453, L1: 10.53504467010498, L3: 26.074338912963867\n",
      "Current prediction:  61.2110710144043 \n",
      "\n",
      "Iteration 2914, Loss: 36.87380599975586, L1: 10.538037300109863, L3: 26.335769653320312\n",
      "Current prediction:  61.206417083740234 \n",
      "\n",
      "Iteration 2915, Loss: 35.95668029785156, L1: 10.541985511779785, L3: 25.41469383239746\n",
      "Current prediction:  61.20234298706055 \n",
      "\n",
      "Iteration 2916, Loss: 36.04392623901367, L1: 10.545438766479492, L3: 25.49848747253418\n",
      "Current prediction:  61.19895553588867 \n",
      "\n",
      "Iteration 2917, Loss: 37.003440856933594, L1: 10.548316955566406, L3: 26.455121994018555\n",
      "Current prediction:  61.196044921875 \n",
      "\n",
      "Iteration 2918, Loss: 37.236900329589844, L1: 10.55078125, L3: 26.686120986938477\n",
      "Current prediction:  61.192440032958984 \n",
      "\n",
      "Iteration 2919, Loss: 36.51969909667969, L1: 10.553834915161133, L3: 25.965866088867188\n",
      "Current prediction:  61.18872833251953 \n",
      "\n",
      "Iteration 2920, Loss: 37.32136535644531, L1: 10.556985855102539, L3: 26.76437759399414\n",
      "Current prediction:  61.18483352661133 \n",
      "\n",
      "Iteration 2921, Loss: 35.733062744140625, L1: 10.560287475585938, L3: 25.17277717590332\n",
      "Current prediction:  61.18228530883789 \n",
      "\n",
      "Iteration 2922, Loss: 36.524837493896484, L1: 10.562437057495117, L3: 25.962400436401367\n",
      "Current prediction:  61.18132019042969 \n",
      "\n",
      "Iteration 2923, Loss: 36.53691101074219, L1: 10.563262939453125, L3: 25.97364616394043\n",
      "Current prediction:  61.18039321899414 \n",
      "\n",
      "Iteration 2924, Loss: 36.12049865722656, L1: 10.564055442810059, L3: 25.556442260742188\n",
      "Current prediction:  61.17988967895508 \n",
      "\n",
      "Iteration 2925, Loss: 36.86484146118164, L1: 10.564481735229492, L3: 26.30035972595215\n",
      "Current prediction:  61.179931640625 \n",
      "\n",
      "Iteration 2926, Loss: 36.327980041503906, L1: 10.564438819885254, L3: 25.76354217529297\n",
      "Current prediction:  61.179866790771484 \n",
      "\n",
      "Iteration 2927, Loss: 36.79477310180664, L1: 10.564494132995605, L3: 26.23027801513672\n",
      "Current prediction:  61.180030822753906 \n",
      "\n",
      "Iteration 2928, Loss: 36.231353759765625, L1: 10.564355850219727, L3: 25.666996002197266\n",
      "Current prediction:  61.18141174316406 \n",
      "\n",
      "Iteration 2929, Loss: 37.08135986328125, L1: 10.563186645507812, L3: 26.51817512512207\n",
      "Current prediction:  61.18388748168945 \n",
      "\n",
      "Iteration 2930, Loss: 36.19610595703125, L1: 10.561089515686035, L3: 25.6350154876709\n",
      "Current prediction:  61.18661117553711 \n",
      "\n",
      "Iteration 2931, Loss: 37.53722381591797, L1: 10.55877685546875, L3: 26.97844696044922\n",
      "Current prediction:  61.187461853027344 \n",
      "\n",
      "Iteration 2932, Loss: 36.34498596191406, L1: 10.558060646057129, L3: 25.786924362182617\n",
      "Current prediction:  61.1889533996582 \n",
      "\n",
      "Iteration 2933, Loss: 36.63676071166992, L1: 10.55678939819336, L3: 26.079971313476562\n",
      "Current prediction:  61.190460205078125 \n",
      "\n",
      "Iteration 2934, Loss: 36.97194290161133, L1: 10.555512428283691, L3: 26.416431427001953\n",
      "Current prediction:  61.19253921508789 \n",
      "\n",
      "Iteration 2935, Loss: 36.56977844238281, L1: 10.553749084472656, L3: 26.01603126525879\n",
      "Current prediction:  61.193790435791016 \n",
      "\n",
      "Iteration 2936, Loss: 36.79436111450195, L1: 10.552693367004395, L3: 26.241668701171875\n",
      "Current prediction:  61.19590759277344 \n",
      "\n",
      "Iteration 2937, Loss: 36.73152542114258, L1: 10.550899505615234, L3: 26.180625915527344\n",
      "Current prediction:  61.198631286621094 \n",
      "\n",
      "Iteration 2938, Loss: 36.64892578125, L1: 10.548588752746582, L3: 26.1003360748291\n",
      "Current prediction:  61.20064926147461 \n",
      "\n",
      "Iteration 2939, Loss: 36.88070297241211, L1: 10.546875, L3: 26.33382797241211\n",
      "Current prediction:  61.20148468017578 \n",
      "\n",
      "Iteration 2940, Loss: 35.9465217590332, L1: 10.546172142028809, L3: 25.40035057067871\n",
      "Current prediction:  61.201934814453125 \n",
      "\n",
      "Iteration 2941, Loss: 36.576454162597656, L1: 10.545787811279297, L3: 26.030668258666992\n",
      "Current prediction:  61.2020378112793 \n",
      "\n",
      "Iteration 2942, Loss: 36.66187286376953, L1: 10.545701026916504, L3: 26.116172790527344\n",
      "Current prediction:  61.20210266113281 \n",
      "\n",
      "Iteration 2943, Loss: 36.95878601074219, L1: 10.545642852783203, L3: 26.413143157958984\n",
      "Current prediction:  61.2008171081543 \n",
      "\n",
      "Iteration 2944, Loss: 38.52337646484375, L1: 10.546737670898438, L3: 27.97663688659668\n",
      "Current prediction:  61.19953155517578 \n",
      "\n",
      "Iteration 2945, Loss: 37.90721130371094, L1: 10.547823905944824, L3: 27.35938835144043\n",
      "Current prediction:  61.19782638549805 \n",
      "\n",
      "Iteration 2946, Loss: 36.337623596191406, L1: 10.549266815185547, L3: 25.788358688354492\n",
      "Current prediction:  61.196075439453125 \n",
      "\n",
      "Iteration 2947, Loss: 35.60863494873047, L1: 10.55075740814209, L3: 25.057878494262695\n",
      "Current prediction:  61.19611740112305 \n",
      "\n",
      "Iteration 2948, Loss: 36.942100524902344, L1: 10.550718307495117, L3: 26.39138412475586\n",
      "Current prediction:  61.19734191894531 \n",
      "\n",
      "Iteration 2949, Loss: 35.94538116455078, L1: 10.549681663513184, L3: 25.395700454711914\n",
      "Current prediction:  61.198585510253906 \n",
      "\n",
      "Iteration 2950, Loss: 35.74266815185547, L1: 10.548624038696289, L3: 25.194046020507812\n",
      "Current prediction:  61.19934844970703 \n",
      "\n",
      "Iteration 2951, Loss: 37.091190338134766, L1: 10.547980308532715, L3: 26.543209075927734\n",
      "Current prediction:  61.19853591918945 \n",
      "\n",
      "Iteration 2952, Loss: 36.4384651184082, L1: 10.548670768737793, L3: 25.889793395996094\n",
      "Current prediction:  61.199623107910156 \n",
      "\n",
      "Iteration 2953, Loss: 37.34001922607422, L1: 10.547745704650879, L3: 26.792272567749023\n",
      "Current prediction:  61.201602935791016 \n",
      "\n",
      "Iteration 2954, Loss: 36.96818161010742, L1: 10.54606819152832, L3: 26.4221134185791\n",
      "Current prediction:  61.202880859375 \n",
      "\n",
      "Iteration 2955, Loss: 35.625213623046875, L1: 10.544981956481934, L3: 25.080232620239258\n",
      "Current prediction:  61.2048454284668 \n",
      "\n",
      "Iteration 2956, Loss: 37.60569763183594, L1: 10.543317794799805, L3: 27.0623779296875\n",
      "Current prediction:  61.20668029785156 \n",
      "\n",
      "Iteration 2957, Loss: 37.88328552246094, L1: 10.541762351989746, L3: 27.341524124145508\n",
      "Current prediction:  61.20827865600586 \n",
      "\n",
      "Iteration 2958, Loss: 36.248199462890625, L1: 10.5404052734375, L3: 25.707796096801758\n",
      "Current prediction:  61.20899200439453 \n",
      "\n",
      "Iteration 2959, Loss: 36.85432434082031, L1: 10.539800643920898, L3: 26.31452178955078\n",
      "Current prediction:  61.20923614501953 \n",
      "\n",
      "Iteration 2960, Loss: 35.97697448730469, L1: 10.539596557617188, L3: 25.4373779296875\n",
      "Current prediction:  61.21006393432617 \n",
      "\n",
      "Iteration 2961, Loss: 35.979496002197266, L1: 10.538893699645996, L3: 25.440603256225586\n",
      "Current prediction:  61.212242126464844 \n",
      "\n",
      "Iteration 2962, Loss: 36.596336364746094, L1: 10.537046432495117, L3: 26.059289932250977\n",
      "Current prediction:  61.21268844604492 \n",
      "\n",
      "Iteration 2963, Loss: 36.127891540527344, L1: 10.536664009094238, L3: 25.59122657775879\n",
      "Current prediction:  61.213809967041016 \n",
      "\n",
      "Iteration 2964, Loss: 36.40024948120117, L1: 10.535715103149414, L3: 25.864534378051758\n",
      "Current prediction:  61.21455383300781 \n",
      "\n",
      "Iteration 2965, Loss: 37.2544059753418, L1: 10.535088539123535, L3: 26.719316482543945\n",
      "Current prediction:  61.21521759033203 \n",
      "\n",
      "Iteration 2966, Loss: 34.826839447021484, L1: 10.534523963928223, L3: 24.292314529418945\n",
      "Current prediction:  61.21648406982422 \n",
      "\n",
      "Iteration 2967, Loss: 37.77815628051758, L1: 10.533452033996582, L3: 27.24470329284668\n",
      "Current prediction:  61.21615982055664 \n",
      "\n",
      "Iteration 2968, Loss: 37.004112243652344, L1: 10.533723831176758, L3: 26.470388412475586\n",
      "Current prediction:  61.215152740478516 \n",
      "\n",
      "Iteration 2969, Loss: 36.64932632446289, L1: 10.534578323364258, L3: 26.114748001098633\n",
      "Current prediction:  61.212764739990234 \n",
      "\n",
      "Iteration 2970, Loss: 37.55302429199219, L1: 10.536599159240723, L3: 27.01642417907715\n",
      "Current prediction:  61.21058654785156 \n",
      "\n",
      "Iteration 2971, Loss: 36.90375518798828, L1: 10.5384521484375, L3: 26.36530303955078\n",
      "Current prediction:  61.20960998535156 \n",
      "\n",
      "Iteration 2972, Loss: 36.41768264770508, L1: 10.539281845092773, L3: 25.878400802612305\n",
      "Current prediction:  61.20917892456055 \n",
      "\n",
      "Iteration 2973, Loss: 36.08914566040039, L1: 10.539648056030273, L3: 25.549497604370117\n",
      "Current prediction:  61.20855712890625 \n",
      "\n",
      "Iteration 2974, Loss: 36.374717712402344, L1: 10.540175437927246, L3: 25.83454132080078\n",
      "Current prediction:  61.20708084106445 \n",
      "\n",
      "Iteration 2975, Loss: 36.99474334716797, L1: 10.541421890258789, L3: 26.453319549560547\n",
      "Current prediction:  61.20389175415039 \n",
      "\n",
      "Iteration 2976, Loss: 35.480037689208984, L1: 10.544127464294434, L3: 24.935911178588867\n",
      "Current prediction:  61.20122146606445 \n",
      "\n",
      "Iteration 2977, Loss: 34.94068908691406, L1: 10.546392440795898, L3: 24.394298553466797\n",
      "Current prediction:  61.20039749145508 \n",
      "\n",
      "Iteration 2978, Loss: 36.799224853515625, L1: 10.547091484069824, L3: 26.252134323120117\n",
      "Current prediction:  61.19853591918945 \n",
      "\n",
      "Iteration 2979, Loss: 36.237789154052734, L1: 10.548667907714844, L3: 25.68912124633789\n",
      "Current prediction:  61.196659088134766 \n",
      "\n",
      "Iteration 2980, Loss: 36.264892578125, L1: 10.55025577545166, L3: 25.714637756347656\n",
      "Current prediction:  61.19452667236328 \n",
      "\n",
      "Iteration 2981, Loss: 36.66326904296875, L1: 10.552066802978516, L3: 26.111202239990234\n",
      "Current prediction:  61.193138122558594 \n",
      "\n",
      "Iteration 2982, Loss: 36.390541076660156, L1: 10.553244590759277, L3: 25.837295532226562\n",
      "Current prediction:  61.19249725341797 \n",
      "\n",
      "Iteration 2983, Loss: 37.70471954345703, L1: 10.553789138793945, L3: 27.150928497314453\n",
      "Current prediction:  61.19198226928711 \n",
      "\n",
      "Iteration 2984, Loss: 36.43014144897461, L1: 10.554226875305176, L3: 25.87591552734375\n",
      "Current prediction:  61.192108154296875 \n",
      "\n",
      "Iteration 2985, Loss: 36.693641662597656, L1: 10.554121017456055, L3: 26.139522552490234\n",
      "Current prediction:  61.19208526611328 \n",
      "\n",
      "Iteration 2986, Loss: 35.70072937011719, L1: 10.554137229919434, L3: 25.14659309387207\n",
      "Current prediction:  61.19401168823242 \n",
      "\n",
      "Iteration 2987, Loss: 36.247249603271484, L1: 10.552506446838379, L3: 25.694744110107422\n",
      "Current prediction:  61.195762634277344 \n",
      "\n",
      "Iteration 2988, Loss: 36.799293518066406, L1: 10.551017761230469, L3: 26.248275756835938\n",
      "Current prediction:  61.197017669677734 \n",
      "\n",
      "Iteration 2989, Loss: 37.26017379760742, L1: 10.54995059967041, L3: 26.710224151611328\n",
      "Current prediction:  61.19817352294922 \n",
      "\n",
      "Iteration 2990, Loss: 36.39889907836914, L1: 10.54897689819336, L3: 25.84992218017578\n",
      "Current prediction:  61.199981689453125 \n",
      "\n",
      "Iteration 2991, Loss: 36.458900451660156, L1: 10.547443389892578, L3: 25.911457061767578\n",
      "Current prediction:  61.201602935791016 \n",
      "\n",
      "Iteration 2992, Loss: 36.308448791503906, L1: 10.54606819152832, L3: 25.76238250732422\n",
      "Current prediction:  61.20237350463867 \n",
      "\n",
      "Iteration 2993, Loss: 36.98217010498047, L1: 10.545414924621582, L3: 26.436756134033203\n",
      "Current prediction:  61.20252990722656 \n",
      "\n",
      "Iteration 2994, Loss: 36.79147720336914, L1: 10.545281410217285, L3: 26.246196746826172\n",
      "Current prediction:  61.20309066772461 \n",
      "\n",
      "Iteration 2995, Loss: 38.532588958740234, L1: 10.544804573059082, L3: 27.987783432006836\n",
      "Current prediction:  61.20243453979492 \n",
      "\n",
      "Iteration 2996, Loss: 36.891151428222656, L1: 10.545361518859863, L3: 26.34579086303711\n",
      "Current prediction:  61.20148468017578 \n",
      "\n",
      "Iteration 2997, Loss: 36.152687072753906, L1: 10.544097900390625, L3: 25.60858726501465\n",
      "Current prediction:  61.199493408203125 \n",
      "\n",
      "Iteration 2998, Loss: 36.981727600097656, L1: 10.547859191894531, L3: 26.433866500854492\n",
      "Current prediction:  61.19648742675781 \n",
      "\n",
      "â†³ LR reduced to 1.0e-03 at iteration 3000 \n",
      "\n",
      "Iteration 2999, Loss: 37.48365783691406, L1: 10.550407409667969, L3: 26.933252334594727\n",
      "Current prediction:  61.191654205322266 \n",
      "\n",
      "Iteration 3000, Loss: 36.86666488647461, L1: 10.554502487182617, L3: 26.312162399291992\n",
      "Current prediction:  61.186405181884766 \n",
      "\n",
      "Iteration 3001, Loss: 36.345069885253906, L1: 10.558953285217285, L3: 25.786115646362305\n",
      "Current prediction:  61.18181228637695 \n",
      "\n",
      "Iteration 3002, Loss: 36.3306884765625, L1: 10.562849998474121, L3: 25.767837524414062\n",
      "Current prediction:  61.17721176147461 \n",
      "\n",
      "Iteration 3003, Loss: 36.668148040771484, L1: 10.566746711730957, L3: 26.10140037536621\n",
      "Current prediction:  61.17313766479492 \n",
      "\n",
      "Iteration 3004, Loss: 36.506385803222656, L1: 10.570197105407715, L3: 25.936189651489258\n",
      "Current prediction:  61.17019271850586 \n",
      "\n",
      "Iteration 3005, Loss: 37.297828674316406, L1: 10.5726957321167, L3: 26.72513198852539\n",
      "Current prediction:  61.166908264160156 \n",
      "\n",
      "Iteration 3006, Loss: 36.48911666870117, L1: 10.575482368469238, L3: 25.91363525390625\n",
      "Current prediction:  61.165283203125 \n",
      "\n",
      "Iteration 3007, Loss: 36.1671142578125, L1: 10.576865196228027, L3: 25.590248107910156\n",
      "Current prediction:  61.16386795043945 \n",
      "\n",
      "Iteration 3008, Loss: 36.8809928894043, L1: 10.578057289123535, L3: 26.302934646606445\n",
      "Current prediction:  61.162391662597656 \n",
      "\n",
      "Iteration 3009, Loss: 36.17164993286133, L1: 10.579313278198242, L3: 25.592336654663086\n",
      "Current prediction:  61.16259765625 \n",
      "\n",
      "Iteration 3010, Loss: 36.20039749145508, L1: 10.57913589477539, L3: 25.621261596679688\n",
      "Current prediction:  61.16207504272461 \n",
      "\n",
      "Iteration 3011, Loss: 35.80979537963867, L1: 10.57957935333252, L3: 25.23021697998047\n",
      "Current prediction:  61.16263961791992 \n",
      "\n",
      "Iteration 3012, Loss: 36.49972152709961, L1: 10.579100608825684, L3: 25.920621871948242\n",
      "Current prediction:  61.162357330322266 \n",
      "\n",
      "Iteration 3013, Loss: 35.69175720214844, L1: 10.579344749450684, L3: 25.112411499023438\n",
      "Current prediction:  61.163124084472656 \n",
      "\n",
      "Iteration 3014, Loss: 36.64876937866211, L1: 10.57868766784668, L3: 26.07008171081543\n",
      "Current prediction:  61.164859771728516 \n",
      "\n",
      "Iteration 3015, Loss: 37.40989303588867, L1: 10.577217102050781, L3: 26.83267593383789\n",
      "Current prediction:  61.167205810546875 \n",
      "\n",
      "Iteration 3016, Loss: 36.20444107055664, L1: 10.575231552124023, L3: 25.629209518432617\n",
      "Current prediction:  61.169246673583984 \n",
      "\n",
      "Iteration 3017, Loss: 36.67034149169922, L1: 10.57349967956543, L3: 26.096843719482422\n",
      "Current prediction:  61.171783447265625 \n",
      "\n",
      "Iteration 3018, Loss: 36.65731430053711, L1: 10.571352005004883, L3: 26.085962295532227\n",
      "Current prediction:  61.174800872802734 \n",
      "\n",
      "Iteration 3019, Loss: 37.65821838378906, L1: 10.568787574768066, L3: 27.089431762695312\n",
      "Current prediction:  61.1765022277832 \n",
      "\n",
      "Iteration 3020, Loss: 36.54352569580078, L1: 10.567350387573242, L3: 25.97617530822754\n",
      "Current prediction:  61.17897415161133 \n",
      "\n",
      "Iteration 3021, Loss: 35.972877502441406, L1: 10.565254211425781, L3: 25.407623291015625\n",
      "Current prediction:  61.18313980102539 \n",
      "\n",
      "Iteration 3022, Loss: 36.65730285644531, L1: 10.561716079711914, L3: 26.095584869384766\n",
      "Current prediction:  61.18754196166992 \n",
      "\n",
      "Iteration 3023, Loss: 36.749759674072266, L1: 10.557990074157715, L3: 26.191770553588867\n",
      "Current prediction:  61.19143295288086 \n",
      "\n",
      "Iteration 3024, Loss: 36.403194427490234, L1: 10.5546875, L3: 25.848506927490234\n",
      "Current prediction:  61.194637298583984 \n",
      "\n",
      "Iteration 3025, Loss: 36.69458770751953, L1: 10.551971435546875, L3: 26.14261817932129\n",
      "Current prediction:  61.197845458984375 \n",
      "\n",
      "Iteration 3026, Loss: 37.043128967285156, L1: 10.549251556396484, L3: 26.49387550354004\n",
      "Current prediction:  61.20049285888672 \n",
      "\n",
      "Iteration 3027, Loss: 37.980472564697266, L1: 10.547011375427246, L3: 27.433462142944336\n",
      "Current prediction:  61.20270919799805 \n",
      "\n",
      "Iteration 3028, Loss: 36.01254653930664, L1: 10.545126914978027, L3: 25.467418670654297\n",
      "Current prediction:  61.205833435058594 \n",
      "\n",
      "Iteration 3029, Loss: 37.391395568847656, L1: 10.54248046875, L3: 26.848915100097656\n",
      "Current prediction:  61.20832443237305 \n",
      "\n",
      "Iteration 3030, Loss: 36.35828399658203, L1: 10.540365219116211, L3: 25.817916870117188\n",
      "Current prediction:  61.20937728881836 \n",
      "\n",
      "Iteration 3031, Loss: 35.99455642700195, L1: 10.53947639465332, L3: 25.455080032348633\n",
      "Current prediction:  61.21004104614258 \n",
      "\n",
      "Iteration 3032, Loss: 36.267581939697266, L1: 10.538910865783691, L3: 25.728670120239258\n",
      "Current prediction:  61.21101760864258 \n",
      "\n",
      "Iteration 3033, Loss: 36.985450744628906, L1: 10.5380859375, L3: 26.447364807128906\n",
      "Current prediction:  61.21110153198242 \n",
      "\n",
      "Iteration 3034, Loss: 36.865623474121094, L1: 10.538012504577637, L3: 26.32761001586914\n",
      "Current prediction:  61.21095275878906 \n",
      "\n",
      "Iteration 3035, Loss: 36.80203628540039, L1: 10.538139343261719, L3: 26.263896942138672\n",
      "Current prediction:  61.2116584777832 \n",
      "\n",
      "Iteration 3036, Loss: 36.46398162841797, L1: 10.5375394821167, L3: 25.926441192626953\n",
      "Current prediction:  61.21159744262695 \n",
      "\n",
      "Iteration 3037, Loss: 37.71092224121094, L1: 10.537596702575684, L3: 27.173324584960938\n",
      "Current prediction:  61.21115493774414 \n",
      "\n",
      "Iteration 3038, Loss: 37.33882141113281, L1: 10.537969589233398, L3: 26.800853729248047\n",
      "Current prediction:  61.210018157958984 \n",
      "\n",
      "Iteration 3039, Loss: 36.009315490722656, L1: 10.53892993927002, L3: 25.470386505126953\n",
      "Current prediction:  61.2086296081543 \n",
      "\n",
      "Iteration 3040, Loss: 36.533302307128906, L1: 10.540109634399414, L3: 25.993194580078125\n",
      "Current prediction:  61.20866775512695 \n",
      "\n",
      "Iteration 3041, Loss: 35.350257873535156, L1: 10.540075302124023, L3: 24.810184478759766\n",
      "Current prediction:  61.20928955078125 \n",
      "\n",
      "Iteration 3042, Loss: 36.90106201171875, L1: 10.539549827575684, L3: 26.361513137817383\n",
      "Current prediction:  61.210994720458984 \n",
      "\n",
      "Iteration 3043, Loss: 37.16307830810547, L1: 10.538101196289062, L3: 26.624975204467773\n",
      "Current prediction:  61.21278762817383 \n",
      "\n",
      "Iteration 3044, Loss: 36.46864700317383, L1: 10.536580085754395, L3: 25.932065963745117\n",
      "Current prediction:  61.214210510253906 \n",
      "\n",
      "Iteration 3045, Loss: 37.60969543457031, L1: 10.535379409790039, L3: 27.074317932128906\n",
      "Current prediction:  61.214595794677734 \n",
      "\n",
      "Iteration 3046, Loss: 37.48193359375, L1: 10.535050392150879, L3: 26.946884155273438\n",
      "Current prediction:  61.213069915771484 \n",
      "\n",
      "Iteration 3047, Loss: 37.43539047241211, L1: 10.536346435546875, L3: 26.899044036865234\n",
      "Current prediction:  61.211280822753906 \n",
      "\n",
      "Iteration 3048, Loss: 36.89722442626953, L1: 10.537861824035645, L3: 26.35936164855957\n",
      "Current prediction:  61.209991455078125 \n",
      "\n",
      "Iteration 3049, Loss: 35.999820709228516, L1: 10.538932800292969, L3: 25.460887908935547\n",
      "Current prediction:  61.210601806640625 \n",
      "\n",
      "Iteration 3050, Loss: 35.869964599609375, L1: 10.538436889648438, L3: 25.331527709960938\n",
      "Current prediction:  61.21183776855469 \n",
      "\n",
      "Iteration 3051, Loss: 37.61832809448242, L1: 10.537388801574707, L3: 27.0809383392334\n",
      "Current prediction:  61.212589263916016 \n",
      "\n",
      "Iteration 3052, Loss: 36.5820198059082, L1: 10.536757469177246, L3: 26.04526138305664\n",
      "Current prediction:  61.213748931884766 \n",
      "\n",
      "Iteration 3053, Loss: 37.940269470214844, L1: 10.535768508911133, L3: 27.404502868652344\n",
      "Current prediction:  61.213722229003906 \n",
      "\n",
      "Iteration 3054, Loss: 36.67247009277344, L1: 10.53579330444336, L3: 26.13667869567871\n",
      "Current prediction:  61.21360397338867 \n",
      "\n",
      "Iteration 3055, Loss: 36.43455505371094, L1: 10.535888671875, L3: 25.898664474487305\n",
      "Current prediction:  61.21332550048828 \n",
      "\n",
      "Iteration 3056, Loss: 35.95695495605469, L1: 10.536130905151367, L3: 25.42082405090332\n",
      "Current prediction:  61.21326446533203 \n",
      "\n",
      "Iteration 3057, Loss: 36.944034576416016, L1: 10.536172866821289, L3: 26.407861709594727\n",
      "Current prediction:  61.21214294433594 \n",
      "\n",
      "Iteration 3058, Loss: 36.20038604736328, L1: 10.537131309509277, L3: 25.663253784179688\n",
      "Current prediction:  61.2116813659668 \n",
      "\n",
      "Iteration 3059, Loss: 36.41051483154297, L1: 10.537522315979004, L3: 25.87299346923828\n",
      "Current prediction:  61.21159362792969 \n",
      "\n",
      "Iteration 3060, Loss: 36.65167236328125, L1: 10.537601470947266, L3: 26.11406898498535\n",
      "Current prediction:  61.21137237548828 \n",
      "\n",
      "Iteration 3061, Loss: 37.350677490234375, L1: 10.53778076171875, L3: 26.812896728515625\n",
      "Current prediction:  61.21217727661133 \n",
      "\n",
      "Iteration 3062, Loss: 36.22801208496094, L1: 10.537105560302734, L3: 25.690906524658203\n",
      "Current prediction:  61.21234893798828 \n",
      "\n",
      "Iteration 3063, Loss: 37.273277282714844, L1: 10.536956787109375, L3: 26.736318588256836\n",
      "Current prediction:  61.21199035644531 \n",
      "\n",
      "Iteration 3064, Loss: 36.99505615234375, L1: 10.537261009216309, L3: 26.457796096801758\n",
      "Current prediction:  61.212860107421875 \n",
      "\n",
      "Iteration 3065, Loss: 36.676124572753906, L1: 10.536523818969727, L3: 26.139602661132812\n",
      "Current prediction:  61.21248245239258 \n",
      "\n",
      "Iteration 3066, Loss: 36.26914596557617, L1: 10.536839485168457, L3: 25.73230743408203\n",
      "Current prediction:  61.21318054199219 \n",
      "\n",
      "Iteration 3067, Loss: 36.170955657958984, L1: 10.536255836486816, L3: 25.634700775146484\n",
      "Current prediction:  61.213741302490234 \n",
      "\n",
      "Iteration 3068, Loss: 37.135047912597656, L1: 10.53577995300293, L3: 26.599266052246094\n",
      "Current prediction:  61.21462631225586 \n",
      "\n",
      "Iteration 3069, Loss: 36.35785675048828, L1: 10.535027503967285, L3: 25.82282829284668\n",
      "Current prediction:  61.21535110473633 \n",
      "\n",
      "Iteration 3070, Loss: 36.74616622924805, L1: 10.534409523010254, L3: 26.211755752563477\n",
      "Current prediction:  61.21542739868164 \n",
      "\n",
      "Iteration 3071, Loss: 37.045677185058594, L1: 10.534341812133789, L3: 26.511333465576172\n",
      "Current prediction:  61.2154655456543 \n",
      "\n",
      "Iteration 3072, Loss: 36.47550582885742, L1: 10.53431224822998, L3: 25.941194534301758\n",
      "Current prediction:  61.215240478515625 \n",
      "\n",
      "Iteration 3073, Loss: 36.6750373840332, L1: 10.53396224975586, L3: 26.141075134277344\n",
      "Current prediction:  61.21297073364258 \n",
      "\n",
      "Iteration 3074, Loss: 36.686767578125, L1: 10.536430358886719, L3: 26.150339126586914\n",
      "Current prediction:  61.21173095703125 \n",
      "\n",
      "Iteration 3075, Loss: 37.362525939941406, L1: 10.537484169006348, L3: 26.825040817260742\n",
      "Current prediction:  61.20960235595703 \n",
      "\n",
      "Iteration 3076, Loss: 36.47395324707031, L1: 10.539284706115723, L3: 25.934669494628906\n",
      "Current prediction:  61.207237243652344 \n",
      "\n",
      "Iteration 3077, Loss: 36.34064483642578, L1: 10.541287422180176, L3: 25.79935646057129\n",
      "Current prediction:  61.20516586303711 \n",
      "\n",
      "Iteration 3078, Loss: 36.443660736083984, L1: 10.543047904968262, L3: 25.900611877441406\n",
      "Current prediction:  61.20352554321289 \n",
      "\n",
      "Iteration 3079, Loss: 36.13566970825195, L1: 10.544438362121582, L3: 25.591230392456055\n",
      "Current prediction:  61.20111846923828 \n",
      "\n",
      "Iteration 3080, Loss: 35.871849060058594, L1: 10.546476364135742, L3: 25.32537269592285\n",
      "Current prediction:  61.20094680786133 \n",
      "\n",
      "Iteration 3081, Loss: 36.024871826171875, L1: 10.54662036895752, L3: 25.478252410888672\n",
      "Current prediction:  61.201541900634766 \n",
      "\n",
      "Iteration 3082, Loss: 36.76567077636719, L1: 10.54611873626709, L3: 26.219552993774414\n",
      "Current prediction:  61.20125198364258 \n",
      "\n",
      "Iteration 3083, Loss: 35.21656799316406, L1: 10.546367645263672, L3: 24.67020034790039\n",
      "Current prediction:  61.20208740234375 \n",
      "\n",
      "Iteration 3084, Loss: 37.671592712402344, L1: 10.545660972595215, L3: 27.125930786132812\n",
      "Current prediction:  61.20164108276367 \n",
      "\n",
      "Iteration 3085, Loss: 36.75231170654297, L1: 10.546031951904297, L3: 26.206281661987305\n",
      "Current prediction:  61.201072692871094 \n",
      "\n",
      "Iteration 3086, Loss: 36.0580940246582, L1: 10.546514511108398, L3: 25.511579513549805\n",
      "Current prediction:  61.20175552368164 \n",
      "\n",
      "Iteration 3087, Loss: 36.331703186035156, L1: 10.545941352844238, L3: 25.7857608795166\n",
      "Current prediction:  61.20200729370117 \n",
      "\n",
      "Iteration 3088, Loss: 35.81231689453125, L1: 10.545726776123047, L3: 25.266592025756836\n",
      "Current prediction:  61.2020263671875 \n",
      "\n",
      "Iteration 3089, Loss: 36.52561569213867, L1: 10.545706748962402, L3: 25.979909896850586\n",
      "Current prediction:  61.20078659057617 \n",
      "\n",
      "Iteration 3090, Loss: 35.419837951660156, L1: 10.546759605407715, L3: 24.873079299926758\n",
      "Current prediction:  61.19969177246094 \n",
      "\n",
      "Iteration 3091, Loss: 36.779823303222656, L1: 10.547687530517578, L3: 26.232133865356445\n",
      "Current prediction:  61.19775390625 \n",
      "\n",
      "Iteration 3092, Loss: 37.159324645996094, L1: 10.549331665039062, L3: 26.60999298095703\n",
      "Current prediction:  61.19649887084961 \n",
      "\n",
      "Iteration 3093, Loss: 37.397377014160156, L1: 10.550394058227539, L3: 26.846981048583984\n",
      "Current prediction:  61.19456481933594 \n",
      "\n",
      "Iteration 3094, Loss: 36.021636962890625, L1: 10.55203914642334, L3: 25.46959686279297\n",
      "Current prediction:  61.192115783691406 \n",
      "\n",
      "Iteration 3095, Loss: 37.30657196044922, L1: 10.554107666015625, L3: 26.75246238708496\n",
      "Current prediction:  61.190147399902344 \n",
      "\n",
      "Iteration 3096, Loss: 36.03575134277344, L1: 10.555773735046387, L3: 25.479978561401367\n",
      "Current prediction:  61.18828201293945 \n",
      "\n",
      "Iteration 3097, Loss: 35.71416473388672, L1: 10.557360649108887, L3: 25.156803131103516\n",
      "Current prediction:  61.18690872192383 \n",
      "\n",
      "Iteration 3098, Loss: 36.24012756347656, L1: 10.558525085449219, L3: 25.68160057067871\n",
      "Current prediction:  61.185943603515625 \n",
      "\n",
      "Iteration 3099, Loss: 35.523258209228516, L1: 10.559342384338379, L3: 24.96391487121582\n",
      "Current prediction:  61.1845703125 \n",
      "\n",
      "Iteration 3100, Loss: 37.0942497253418, L1: 10.560505867004395, L3: 26.533742904663086\n",
      "Current prediction:  61.182796478271484 \n",
      "\n",
      "Iteration 3101, Loss: 35.68734359741211, L1: 10.562013626098633, L3: 25.125329971313477\n",
      "Current prediction:  61.1806640625 \n",
      "\n",
      "Iteration 3102, Loss: 37.223297119140625, L1: 10.563819885253906, L3: 26.659475326538086\n",
      "Current prediction:  61.17898941040039 \n",
      "\n",
      "Iteration 3103, Loss: 36.17284393310547, L1: 10.565237045288086, L3: 25.607606887817383\n",
      "Current prediction:  61.17780303955078 \n",
      "\n",
      "Iteration 3104, Loss: 36.831180572509766, L1: 10.566248893737793, L3: 26.264930725097656\n",
      "Current prediction:  61.17807388305664 \n",
      "\n",
      "Iteration 3105, Loss: 37.40412139892578, L1: 10.566017150878906, L3: 26.838106155395508\n",
      "Current prediction:  61.17958068847656 \n",
      "\n",
      "Iteration 3106, Loss: 36.827049255371094, L1: 10.564737319946289, L3: 26.262311935424805\n",
      "Current prediction:  61.1816520690918 \n",
      "\n",
      "Iteration 3107, Loss: 36.656715393066406, L1: 10.562986373901367, L3: 26.09372901916504\n",
      "Current prediction:  61.18312072753906 \n",
      "\n",
      "Iteration 3108, Loss: 37.515506744384766, L1: 10.561737060546875, L3: 26.95376968383789\n",
      "Current prediction:  61.183780670166016 \n",
      "\n",
      "Iteration 3109, Loss: 35.95301055908203, L1: 10.56118106842041, L3: 25.391830444335938\n",
      "Current prediction:  61.18409729003906 \n",
      "\n",
      "Iteration 3110, Loss: 36.77876281738281, L1: 10.560907363891602, L3: 26.21785545349121\n",
      "Current prediction:  61.18393325805664 \n",
      "\n",
      "Iteration 3111, Loss: 36.216678619384766, L1: 10.561049461364746, L3: 25.655630111694336\n",
      "Current prediction:  61.18433380126953 \n",
      "\n",
      "Iteration 3112, Loss: 36.555015563964844, L1: 10.560712814331055, L3: 25.994304656982422\n",
      "Current prediction:  61.184173583984375 \n",
      "\n",
      "Iteration 3113, Loss: 36.5128173828125, L1: 10.560847282409668, L3: 25.95197105407715\n",
      "Current prediction:  61.1850471496582 \n",
      "\n",
      "Iteration 3114, Loss: 36.895729064941406, L1: 10.560103416442871, L3: 26.33562469482422\n",
      "Current prediction:  61.18629455566406 \n",
      "\n",
      "Iteration 3115, Loss: 36.24048614501953, L1: 10.559050559997559, L3: 25.681434631347656\n",
      "Current prediction:  61.1895637512207 \n",
      "\n",
      "Iteration 3116, Loss: 36.27531051635742, L1: 10.556275367736816, L3: 25.71903419494629\n",
      "Current prediction:  61.19245910644531 \n",
      "\n",
      "Iteration 3117, Loss: 36.52599334716797, L1: 10.553818702697754, L3: 25.97217559814453\n",
      "Current prediction:  61.19482421875 \n",
      "\n",
      "Iteration 3118, Loss: 37.385643005371094, L1: 10.551813125610352, L3: 26.833831787109375\n",
      "Current prediction:  61.197017669677734 \n",
      "\n",
      "Iteration 3119, Loss: 35.79283905029297, L1: 10.549952507019043, L3: 25.242887496948242\n",
      "Current prediction:  61.19922637939453 \n",
      "\n",
      "Iteration 3120, Loss: 36.949764251708984, L1: 10.548081398010254, L3: 26.401683807373047\n",
      "Current prediction:  61.20068359375 \n",
      "\n",
      "Iteration 3121, Loss: 36.80083465576172, L1: 10.546850204467773, L3: 26.253984451293945\n",
      "Current prediction:  61.202674865722656 \n",
      "\n",
      "Iteration 3122, Loss: 36.00822830200195, L1: 10.545161247253418, L3: 25.46306800842285\n",
      "Current prediction:  61.20449447631836 \n",
      "\n",
      "Iteration 3123, Loss: 36.602203369140625, L1: 10.543614387512207, L3: 26.058589935302734\n",
      "Current prediction:  61.206031799316406 \n",
      "\n",
      "Iteration 3124, Loss: 36.67235565185547, L1: 10.542313575744629, L3: 26.130043029785156\n",
      "Current prediction:  61.20803451538086 \n",
      "\n",
      "Iteration 3125, Loss: 37.025901794433594, L1: 10.54061222076416, L3: 26.48529052734375\n",
      "Current prediction:  61.20997619628906 \n",
      "\n",
      "Iteration 3126, Loss: 36.7326545715332, L1: 10.53896713256836, L3: 26.193687438964844\n",
      "Current prediction:  61.21104431152344 \n",
      "\n",
      "Iteration 3127, Loss: 36.3852653503418, L1: 10.586549758911133, L3: 25.798715591430664\n",
      "Current prediction:  61.21238708496094 \n",
      "\n",
      "Iteration 3128, Loss: 35.786590576171875, L1: 10.536920547485352, L3: 25.249671936035156\n",
      "Current prediction:  61.21414566040039 \n",
      "\n",
      "Iteration 3129, Loss: 36.99102020263672, L1: 10.535430908203125, L3: 26.455591201782227\n",
      "Current prediction:  61.214595794677734 \n",
      "\n",
      "Iteration 3130, Loss: 36.72652053833008, L1: 10.535051345825195, L3: 26.191469192504883\n",
      "Current prediction:  61.214271545410156 \n",
      "\n",
      "Iteration 3131, Loss: 36.73261642456055, L1: 10.535327911376953, L3: 26.197288513183594\n",
      "Current prediction:  61.2125244140625 \n",
      "\n",
      "Iteration 3132, Loss: 35.84175109863281, L1: 10.536812782287598, L3: 25.3049373626709\n",
      "Current prediction:  61.213130950927734 \n",
      "\n",
      "Iteration 3133, Loss: 36.996299743652344, L1: 10.536295890808105, L3: 26.460002899169922\n",
      "Current prediction:  61.21522521972656 \n",
      "\n",
      "Iteration 3134, Loss: 35.984169006347656, L1: 10.534517288208008, L3: 25.449649810791016\n",
      "Current prediction:  61.21688461303711 \n",
      "\n",
      "Iteration 3135, Loss: 37.86104202270508, L1: 10.533108711242676, L3: 27.32793426513672\n",
      "Current prediction:  61.21749496459961 \n",
      "\n",
      "Iteration 3136, Loss: 36.16114807128906, L1: 10.5325927734375, L3: 25.62855339050293\n",
      "Current prediction:  61.21770095825195 \n",
      "\n",
      "Iteration 3137, Loss: 36.718910217285156, L1: 10.532419204711914, L3: 26.186492919921875\n",
      "Current prediction:  61.218109130859375 \n",
      "\n",
      "Iteration 3138, Loss: 36.12966537475586, L1: 10.532074928283691, L3: 25.59758949279785\n",
      "Current prediction:  61.21809768676758 \n",
      "\n",
      "Iteration 3139, Loss: 36.09515380859375, L1: 10.532081604003906, L3: 25.563074111938477\n",
      "Current prediction:  61.2179069519043 \n",
      "\n",
      "Iteration 3140, Loss: 37.05529022216797, L1: 10.532243728637695, L3: 26.52304458618164\n",
      "Current prediction:  61.219234466552734 \n",
      "\n",
      "Iteration 3141, Loss: 35.50055694580078, L1: 10.531119346618652, L3: 24.969436645507812\n",
      "Current prediction:  61.221282958984375 \n",
      "\n",
      "Iteration 3142, Loss: 37.15985107421875, L1: 10.529380798339844, L3: 26.630468368530273\n",
      "Current prediction:  61.22258758544922 \n",
      "\n",
      "Iteration 3143, Loss: 36.663787841796875, L1: 10.528274536132812, L3: 26.135515213012695\n",
      "Current prediction:  61.22512435913086 \n",
      "\n",
      "Iteration 3144, Loss: 36.40814971923828, L1: 10.526123046875, L3: 25.882028579711914\n",
      "Current prediction:  61.22816848754883 \n",
      "\n",
      "Iteration 3145, Loss: 37.21152114868164, L1: 10.523540496826172, L3: 26.68798065185547\n",
      "Current prediction:  61.22994613647461 \n",
      "\n",
      "Iteration 3146, Loss: 36.08757019042969, L1: 10.522032737731934, L3: 25.56553840637207\n",
      "Current prediction:  61.23283767700195 \n",
      "\n",
      "Iteration 3147, Loss: 36.38893127441406, L1: 10.5195894241333, L3: 25.869340896606445\n",
      "Current prediction:  61.23533248901367 \n",
      "\n",
      "Iteration 3148, Loss: 37.28540802001953, L1: 10.517462730407715, L3: 26.767946243286133\n",
      "Current prediction:  61.23704147338867 \n",
      "\n",
      "Iteration 3149, Loss: 36.87224578857422, L1: 10.516012191772461, L3: 26.356231689453125\n",
      "Current prediction:  61.236942291259766 \n",
      "\n",
      "Iteration 3150, Loss: 36.55588912963867, L1: 10.51609992980957, L3: 26.0397891998291\n",
      "Current prediction:  61.23683166503906 \n",
      "\n",
      "Iteration 3151, Loss: 36.861087799072266, L1: 10.516194343566895, L3: 26.344894409179688\n",
      "Current prediction:  61.23763656616211 \n",
      "\n",
      "Iteration 3152, Loss: 36.275516510009766, L1: 10.515512466430664, L3: 25.7600040435791\n",
      "Current prediction:  61.23936462402344 \n",
      "\n",
      "Iteration 3153, Loss: 35.48556900024414, L1: 10.51404857635498, L3: 24.971519470214844\n",
      "Current prediction:  61.241310119628906 \n",
      "\n",
      "Iteration 3154, Loss: 37.36573791503906, L1: 10.512399673461914, L3: 26.85334014892578\n",
      "Current prediction:  61.24258804321289 \n",
      "\n",
      "Iteration 3155, Loss: 36.272369384765625, L1: 10.51131820678711, L3: 25.761049270629883\n",
      "Current prediction:  61.24256134033203 \n",
      "\n",
      "Iteration 3156, Loss: 35.80667495727539, L1: 10.511343002319336, L3: 25.295331954956055\n",
      "Current prediction:  61.24325180053711 \n",
      "\n",
      "Iteration 3157, Loss: 36.555511474609375, L1: 10.510748863220215, L3: 26.044763565063477\n",
      "Current prediction:  61.24443817138672 \n",
      "\n",
      "Iteration 3158, Loss: 36.13969802856445, L1: 10.509743690490723, L3: 25.629955291748047\n",
      "Current prediction:  61.24570083618164 \n",
      "\n",
      "Iteration 3159, Loss: 36.12895965576172, L1: 10.508674621582031, L3: 25.620285034179688\n",
      "Current prediction:  61.24613952636719 \n",
      "\n",
      "Iteration 3160, Loss: 36.77442932128906, L1: 10.515549659729004, L3: 26.258880615234375\n",
      "Current prediction:  61.24601745605469 \n",
      "\n",
      "Iteration 3161, Loss: 38.892215728759766, L1: 11.507288932800293, L3: 27.384925842285156\n",
      "Current prediction:  61.244834899902344 \n",
      "\n",
      "Iteration 3162, Loss: 37.0716667175293, L1: 10.23164176940918, L3: 26.840024948120117\n",
      "Current prediction:  62.37252426147461 \n",
      "\n",
      "Iteration 3163, Loss: 38.21000671386719, L1: 9.99809455871582, L3: 28.211910247802734\n",
      "Current prediction:  62.352294921875 \n",
      "\n",
      "Iteration 3164, Loss: 40.32639694213867, L1: 10.967391014099121, L3: 29.359006881713867\n",
      "Current prediction:  62.31889343261719 \n",
      "\n",
      "Iteration 3165, Loss: 38.25551986694336, L1: 9.593598365783691, L3: 28.661922454833984\n",
      "Current prediction:  62.27338409423828 \n",
      "\n",
      "Iteration 3166, Loss: 37.032203674316406, L1: 9.634775161743164, L3: 27.397430419921875\n",
      "Current prediction:  62.21965408325195 \n",
      "\n",
      "Iteration 3167, Loss: 38.55527877807617, L1: 9.681198120117188, L3: 28.874080657958984\n",
      "Current prediction:  62.15689468383789 \n",
      "\n",
      "Iteration 3168, Loss: 36.859710693359375, L1: 9.734567642211914, L3: 27.12514305114746\n",
      "Current prediction:  62.08802032470703 \n",
      "\n",
      "Iteration 3169, Loss: 37.92007064819336, L1: 9.793133735656738, L3: 28.126937866210938\n",
      "Current prediction:  62.013153076171875 \n",
      "\n",
      "Iteration 3170, Loss: 38.16590881347656, L1: 9.856794357299805, L3: 28.309112548828125\n",
      "Current prediction:  61.93458557128906 \n",
      "\n",
      "Iteration 3171, Loss: 37.07134246826172, L1: 9.923593521118164, L3: 27.147750854492188\n",
      "Current prediction:  61.85475540161133 \n",
      "\n",
      "Iteration 3172, Loss: 37.0373649597168, L1: 9.991455078125, L3: 27.045909881591797\n",
      "Current prediction:  61.77579116821289 \n",
      "\n",
      "Iteration 3173, Loss: 37.0265998840332, L1: 10.058574676513672, L3: 26.96802520751953\n",
      "Current prediction:  61.6981315612793 \n",
      "\n",
      "Iteration 3174, Loss: 38.155487060546875, L1: 10.124567031860352, L3: 28.030921936035156\n",
      "Current prediction:  61.618228912353516 \n",
      "\n",
      "Iteration 3175, Loss: 37.58108139038086, L1: 10.19245719909668, L3: 27.38862419128418\n",
      "Current prediction:  61.54026794433594 \n",
      "\n",
      "Iteration 3176, Loss: 36.88945007324219, L1: 10.258679389953613, L3: 26.63077163696289\n",
      "Current prediction:  61.46782684326172 \n",
      "\n",
      "Iteration 3177, Loss: 37.505184173583984, L1: 10.320183753967285, L3: 27.184999465942383\n",
      "Current prediction:  61.3989143371582 \n",
      "\n",
      "Iteration 3178, Loss: 36.82572937011719, L1: 10.37868595123291, L3: 26.447044372558594\n",
      "Current prediction:  61.33393859863281 \n",
      "\n",
      "Iteration 3179, Loss: 36.718353271484375, L1: 10.43382453918457, L3: 26.284530639648438\n",
      "Current prediction:  61.27222442626953 \n",
      "\n",
      "Iteration 3180, Loss: 37.89796829223633, L1: 12.069795608520508, L3: 25.82817268371582\n",
      "Current prediction:  61.21602249145508 \n",
      "\n",
      "Iteration 3181, Loss: 36.5902099609375, L1: 10.533843040466309, L3: 26.056365966796875\n",
      "Current prediction:  61.16472244262695 \n",
      "\n",
      "Iteration 3182, Loss: 37.739017486572266, L1: 11.026238441467285, L3: 26.712778091430664\n",
      "Current prediction:  61.117042541503906 \n",
      "\n",
      "Iteration 3183, Loss: 37.303314208984375, L1: 11.785603523254395, L3: 25.517709732055664\n",
      "Current prediction:  61.07551956176758 \n",
      "\n",
      "Iteration 3184, Loss: 37.17891311645508, L1: 11.283719062805176, L3: 25.895193099975586\n",
      "Current prediction:  61.041465759277344 \n",
      "\n",
      "Iteration 3185, Loss: 37.7116584777832, L1: 11.145610809326172, L3: 26.56604766845703\n",
      "Current prediction:  61.01704788208008 \n",
      "\n",
      "Iteration 3186, Loss: 37.468345642089844, L1: 11.04882526397705, L3: 26.41952133178711\n",
      "Current prediction:  60.985721588134766 \n",
      "\n",
      "Iteration 3187, Loss: 38.66108703613281, L1: 11.185478210449219, L3: 27.475608825683594\n",
      "Current prediction:  59.69329833984375 \n",
      "\n",
      "Iteration 3188, Loss: 38.95354461669922, L1: 11.332188606262207, L3: 27.621356964111328\n",
      "Current prediction:  59.70493698120117 \n",
      "\n",
      "Iteration 3189, Loss: 37.89179992675781, L1: 11.497753143310547, L3: 26.394046783447266\n",
      "Current prediction:  59.72797775268555 \n",
      "\n",
      "Iteration 3190, Loss: 37.993282318115234, L1: 11.649683952331543, L3: 26.343599319458008\n",
      "Current prediction:  59.76356506347656 \n",
      "\n",
      "Iteration 3191, Loss: 38.75428009033203, L1: 11.779583930969238, L3: 26.97469711303711\n",
      "Current prediction:  59.809906005859375 \n",
      "\n",
      "Iteration 3192, Loss: 38.02841567993164, L1: 11.80233097076416, L3: 26.226083755493164\n",
      "Current prediction:  59.86529541015625 \n",
      "\n",
      "Iteration 3193, Loss: 38.22359848022461, L1: 12.016194343566895, L3: 26.2074031829834\n",
      "Current prediction:  59.92979049682617 \n",
      "\n",
      "Iteration 3194, Loss: 38.9529914855957, L1: 12.17348575592041, L3: 26.77950668334961\n",
      "Current prediction:  60.000980377197266 \n",
      "\n",
      "Iteration 3195, Loss: 37.992919921875, L1: 11.982674598693848, L3: 26.01024627685547\n",
      "Current prediction:  60.07895278930664 \n",
      "\n",
      "Iteration 3196, Loss: 38.2199821472168, L1: 12.015539169311523, L3: 26.204442977905273\n",
      "Current prediction:  60.16106033325195 \n",
      "\n",
      "Iteration 3197, Loss: 38.84199142456055, L1: 11.930670738220215, L3: 26.91132164001465\n",
      "Current prediction:  60.24386215209961 \n",
      "\n",
      "Iteration 3198, Loss: 38.920318603515625, L1: 11.78617000579834, L3: 27.13414764404297\n",
      "Current prediction:  60.32767868041992 \n",
      "\n",
      "Iteration 3199, Loss: 37.8287353515625, L1: 11.83210277557373, L3: 25.996633529663086\n",
      "Current prediction:  60.411502838134766 \n",
      "\n",
      "Iteration 3200, Loss: 37.89690399169922, L1: 11.472969055175781, L3: 26.42393684387207\n",
      "Current prediction:  60.494415283203125 \n",
      "\n",
      "Iteration 3201, Loss: 36.86252975463867, L1: 11.1920804977417, L3: 25.670448303222656\n",
      "Current prediction:  60.5740966796875 \n",
      "\n",
      "Iteration 3202, Loss: 37.0054817199707, L1: 10.990401268005371, L3: 26.015079498291016\n",
      "Current prediction:  60.6484260559082 \n",
      "\n",
      "Iteration 3203, Loss: 38.07537841796875, L1: 10.815301895141602, L3: 27.26007843017578\n",
      "Current prediction:  60.71567153930664 \n",
      "\n",
      "Iteration 3204, Loss: 37.243045806884766, L1: 10.670422554016113, L3: 26.57262420654297\n",
      "Current prediction:  60.776954650878906 \n",
      "\n",
      "Iteration 3205, Loss: 37.831478118896484, L1: 10.50964069366455, L3: 27.32183837890625\n",
      "Current prediction:  60.831790924072266 \n",
      "\n",
      "Iteration 3206, Loss: 37.21918487548828, L1: 10.405961036682129, L3: 26.81322479248047\n",
      "Current prediction:  61.01120376586914 \n",
      "\n",
      "Iteration 3207, Loss: 37.765724182128906, L1: 10.305241584777832, L3: 27.46048355102539\n",
      "Current prediction:  61.96393585205078 \n",
      "\n",
      "Iteration 3208, Loss: 36.55482864379883, L1: 10.231677055358887, L3: 26.323150634765625\n",
      "Current prediction:  62.15488815307617 \n",
      "\n",
      "Iteration 3209, Loss: 37.80848693847656, L1: 10.182323455810547, L3: 27.626161575317383\n",
      "Current prediction:  62.17722702026367 \n",
      "\n",
      "Iteration 3210, Loss: 38.107757568359375, L1: 10.120196342468262, L3: 27.98756217956543\n",
      "Current prediction:  62.187042236328125 \n",
      "\n",
      "Iteration 3211, Loss: 37.23586654663086, L1: 10.079654693603516, L3: 27.156211853027344\n",
      "Current prediction:  62.18836212158203 \n",
      "\n",
      "Iteration 3212, Loss: 37.63103103637695, L1: 10.097053527832031, L3: 27.533977508544922\n",
      "Current prediction:  62.18362808227539 \n",
      "\n",
      "Iteration 3213, Loss: 37.44203186035156, L1: 10.07396411895752, L3: 27.36806869506836\n",
      "Current prediction:  62.172752380371094 \n",
      "\n",
      "Iteration 3214, Loss: 36.75514221191406, L1: 10.053937911987305, L3: 26.701202392578125\n",
      "Current prediction:  62.15655517578125 \n",
      "\n",
      "Iteration 3215, Loss: 37.24513244628906, L1: 10.03187084197998, L3: 27.2132625579834\n",
      "Current prediction:  62.13544845581055 \n",
      "\n",
      "Iteration 3216, Loss: 37.36328125, L1: 10.06635856628418, L3: 27.29692268371582\n",
      "Current prediction:  62.109230041503906 \n",
      "\n",
      "Iteration 3217, Loss: 38.109073638916016, L1: 10.03601360321045, L3: 28.07305908203125\n",
      "Current prediction:  62.07849884033203 \n",
      "\n",
      "Iteration 3218, Loss: 37.6811637878418, L1: 10.123373031616211, L3: 27.557790756225586\n",
      "Current prediction:  62.04372787475586 \n",
      "\n",
      "Iteration 3219, Loss: 35.645938873291016, L1: 10.125184059143066, L3: 25.520755767822266\n",
      "Current prediction:  62.00837707519531 \n",
      "\n",
      "Iteration 3220, Loss: 37.201568603515625, L1: 10.179058074951172, L3: 27.02250862121582\n",
      "Current prediction:  61.97037887573242 \n",
      "\n",
      "Iteration 3221, Loss: 37.360904693603516, L1: 10.137039184570312, L3: 27.223865509033203\n",
      "Current prediction:  61.930938720703125 \n",
      "\n",
      "Iteration 3222, Loss: 36.666770935058594, L1: 10.258856773376465, L3: 26.407913208007812\n",
      "Current prediction:  61.89169692993164 \n",
      "\n",
      "Iteration 3223, Loss: 37.315208435058594, L1: 10.345937728881836, L3: 26.969268798828125\n",
      "Current prediction:  61.8509635925293 \n",
      "\n",
      "Iteration 3224, Loss: 37.20092010498047, L1: 10.296998977661133, L3: 26.903921127319336\n",
      "Current prediction:  61.81015396118164 \n",
      "\n",
      "Iteration 3225, Loss: 36.58063507080078, L1: 10.277992248535156, L3: 26.302640914916992\n",
      "Current prediction:  61.76995849609375 \n",
      "\n",
      "Iteration 3226, Loss: 36.49797821044922, L1: 10.408347129821777, L3: 26.089632034301758\n",
      "Current prediction:  61.73060989379883 \n",
      "\n",
      "Iteration 3227, Loss: 36.848663330078125, L1: 10.418785095214844, L3: 26.42987823486328\n",
      "Current prediction:  61.69218444824219 \n",
      "\n",
      "Iteration 3228, Loss: 37.286617279052734, L1: 10.412033081054688, L3: 26.874584197998047\n",
      "Current prediction:  61.65521240234375 \n",
      "\n",
      "Iteration 3229, Loss: 37.64070129394531, L1: 10.472743034362793, L3: 27.167959213256836\n",
      "Current prediction:  61.62015914916992 \n",
      "\n",
      "Iteration 3230, Loss: 36.868621826171875, L1: 10.504128456115723, L3: 26.36449432373047\n",
      "Current prediction:  61.58835983276367 \n",
      "\n",
      "Iteration 3231, Loss: 36.51370620727539, L1: 10.493136405944824, L3: 26.020570755004883\n",
      "Current prediction:  61.559452056884766 \n",
      "\n",
      "Iteration 3232, Loss: 35.77690887451172, L1: 10.520240783691406, L3: 25.256668090820312\n",
      "Current prediction:  61.53214645385742 \n",
      "\n",
      "Iteration 3233, Loss: 36.43177795410156, L1: 10.503028869628906, L3: 25.92875099182129\n",
      "Current prediction:  61.50739288330078 \n",
      "\n",
      "Iteration 3234, Loss: 37.15605545043945, L1: 10.618603706359863, L3: 26.537450790405273\n",
      "Current prediction:  61.485923767089844 \n",
      "\n",
      "Iteration 3235, Loss: 36.876739501953125, L1: 10.614541053771973, L3: 26.26219940185547\n",
      "Current prediction:  61.467464447021484 \n",
      "\n",
      "Iteration 3236, Loss: 36.88811111450195, L1: 10.621094703674316, L3: 26.26701545715332\n",
      "Current prediction:  61.45167541503906 \n",
      "\n",
      "Iteration 3237, Loss: 36.42436218261719, L1: 10.60672378540039, L3: 25.817638397216797\n",
      "Current prediction:  61.43864822387695 \n",
      "\n",
      "Iteration 3238, Loss: 37.51945495605469, L1: 10.610557556152344, L3: 26.908897399902344\n",
      "Current prediction:  61.42690658569336 \n",
      "\n",
      "Iteration 3239, Loss: 37.650718688964844, L1: 10.679132461547852, L3: 26.971588134765625\n",
      "Current prediction:  61.41667175292969 \n",
      "\n",
      "Iteration 3240, Loss: 36.656951904296875, L1: 10.697725296020508, L3: 25.959224700927734\n",
      "Current prediction:  61.40585708618164 \n",
      "\n",
      "Iteration 3241, Loss: 36.33939743041992, L1: 10.743131637573242, L3: 25.59626579284668\n",
      "Current prediction:  61.39712142944336 \n",
      "\n",
      "Iteration 3242, Loss: 35.29279708862305, L1: 10.846102714538574, L3: 24.446693420410156\n",
      "Current prediction:  61.38942337036133 \n",
      "\n",
      "Iteration 3243, Loss: 37.433319091796875, L1: 11.037460327148438, L3: 26.395856857299805\n",
      "Current prediction:  61.38075637817383 \n",
      "\n",
      "Iteration 3244, Loss: 36.32931137084961, L1: 10.937355041503906, L3: 25.391956329345703\n",
      "Current prediction:  61.37141799926758 \n",
      "\n",
      "Iteration 3245, Loss: 36.304588317871094, L1: 10.853388786315918, L3: 25.451200485229492\n",
      "Current prediction:  61.36248016357422 \n",
      "\n",
      "Iteration 3246, Loss: 38.06329345703125, L1: 11.343835830688477, L3: 26.71945571899414\n",
      "Current prediction:  61.35418701171875 \n",
      "\n",
      "Iteration 3247, Loss: 37.02118682861328, L1: 11.050616264343262, L3: 25.970569610595703\n",
      "Current prediction:  61.34303665161133 \n",
      "\n",
      "Iteration 3248, Loss: 38.200260162353516, L1: 11.597260475158691, L3: 26.60300064086914\n",
      "Current prediction:  61.329471588134766 \n",
      "\n",
      "Iteration 3249, Loss: 36.424598693847656, L1: 10.797304153442383, L3: 25.627294540405273\n",
      "Current prediction:  61.31699752807617 \n",
      "\n",
      "Iteration 3250, Loss: 37.96741485595703, L1: 11.115358352661133, L3: 26.8520565032959\n",
      "Current prediction:  61.30482482910156 \n",
      "\n",
      "Iteration 3251, Loss: 36.94979476928711, L1: 10.57990550994873, L3: 26.369888305664062\n",
      "Current prediction:  61.293113708496094 \n",
      "\n",
      "Iteration 3252, Loss: 37.12149429321289, L1: 10.866035461425781, L3: 26.25545883178711\n",
      "Current prediction:  61.28281021118164 \n",
      "\n",
      "Iteration 3253, Loss: 37.91368103027344, L1: 10.711812973022461, L3: 27.201866149902344\n",
      "Current prediction:  61.272239685058594 \n",
      "\n",
      "Iteration 3254, Loss: 36.586769104003906, L1: 10.570111274719238, L3: 26.01665687561035\n",
      "Current prediction:  61.26234817504883 \n",
      "\n",
      "Iteration 3255, Loss: 36.853057861328125, L1: 10.669008255004883, L3: 26.18404769897461\n",
      "Current prediction:  61.25385284423828 \n",
      "\n",
      "Iteration 3256, Loss: 37.21424865722656, L1: 10.50175952911377, L3: 26.712488174438477\n",
      "Current prediction:  61.24660873413086 \n",
      "\n",
      "Iteration 3257, Loss: 35.789344787597656, L1: 10.507905006408691, L3: 25.28143882751465\n",
      "Current prediction:  61.238685607910156 \n",
      "\n",
      "Iteration 3258, Loss: 36.825401306152344, L1: 10.514630317687988, L3: 26.310771942138672\n",
      "Current prediction:  61.23014450073242 \n",
      "\n",
      "Iteration 3259, Loss: 36.27968215942383, L1: 10.52186393737793, L3: 25.7578182220459\n",
      "Current prediction:  61.222923278808594 \n",
      "\n",
      "Iteration 3260, Loss: 36.347415924072266, L1: 10.527990341186523, L3: 25.819425582885742\n",
      "Current prediction:  61.215614318847656 \n",
      "\n",
      "Iteration 3261, Loss: 35.919769287109375, L1: 10.534367561340332, L3: 25.38540267944336\n",
      "Current prediction:  61.208831787109375 \n",
      "\n",
      "Iteration 3262, Loss: 37.23533630371094, L1: 10.539937973022461, L3: 26.695396423339844\n",
      "Current prediction:  61.202880859375 \n",
      "\n",
      "Iteration 3263, Loss: 37.218017578125, L1: 10.544981002807617, L3: 26.673036575317383\n",
      "Current prediction:  61.19635772705078 \n",
      "\n",
      "Iteration 3264, Loss: 36.67289733886719, L1: 10.550512313842773, L3: 26.122385025024414\n",
      "Current prediction:  61.191322326660156 \n",
      "\n",
      "Iteration 3265, Loss: 37.23301696777344, L1: 10.55478572845459, L3: 26.678232192993164\n",
      "Current prediction:  61.18693542480469 \n",
      "\n",
      "Iteration 3266, Loss: 35.032752990722656, L1: 10.558505058288574, L3: 24.4742488861084\n",
      "Current prediction:  61.184906005859375 \n",
      "\n",
      "Iteration 3267, Loss: 36.65019226074219, L1: 10.560220718383789, L3: 26.08997344970703\n",
      "Current prediction:  61.181724548339844 \n",
      "\n",
      "Iteration 3268, Loss: 35.94482421875, L1: 10.562914848327637, L3: 25.381908416748047\n",
      "Current prediction:  61.1786003112793 \n",
      "\n",
      "Iteration 3269, Loss: 36.351287841796875, L1: 10.565567970275879, L3: 25.78571891784668\n",
      "Current prediction:  61.17538070678711 \n",
      "\n",
      "Iteration 3270, Loss: 37.470542907714844, L1: 10.568300247192383, L3: 26.902244567871094\n",
      "Current prediction:  61.17219543457031 \n",
      "\n",
      "Iteration 3271, Loss: 35.641319274902344, L1: 10.571000099182129, L3: 25.0703182220459\n",
      "Current prediction:  61.17020034790039 \n",
      "\n",
      "Iteration 3272, Loss: 35.605247497558594, L1: 10.572690963745117, L3: 25.032554626464844\n",
      "Current prediction:  61.16957473754883 \n",
      "\n",
      "Iteration 3273, Loss: 35.75143814086914, L1: 10.57321834564209, L3: 25.178218841552734\n",
      "Current prediction:  61.16929244995117 \n",
      "\n",
      "Iteration 3274, Loss: 36.605247497558594, L1: 10.573455810546875, L3: 26.031789779663086\n",
      "Current prediction:  61.16908264160156 \n",
      "\n",
      "Iteration 3275, Loss: 35.5444450378418, L1: 10.573637962341309, L3: 24.970808029174805\n",
      "Current prediction:  61.16847229003906 \n",
      "\n",
      "Iteration 3276, Loss: 37.61895751953125, L1: 10.574155807495117, L3: 27.044803619384766\n",
      "Current prediction:  61.1685791015625 \n",
      "\n",
      "Iteration 3277, Loss: 37.916839599609375, L1: 10.574064254760742, L3: 27.342775344848633\n",
      "Current prediction:  61.168731689453125 \n",
      "\n",
      "Iteration 3278, Loss: 37.102264404296875, L1: 10.573935508728027, L3: 26.52832794189453\n",
      "Current prediction:  61.1708869934082 \n",
      "\n",
      "Iteration 3279, Loss: 36.235076904296875, L1: 10.57210922241211, L3: 25.662965774536133\n",
      "Current prediction:  61.172401428222656 \n",
      "\n",
      "Iteration 3280, Loss: 37.52119445800781, L1: 10.570823669433594, L3: 26.950368881225586\n",
      "Current prediction:  61.173728942871094 \n",
      "\n",
      "Iteration 3281, Loss: 36.236412048339844, L1: 10.569701194763184, L3: 25.666711807250977\n",
      "Current prediction:  61.17536926269531 \n",
      "\n",
      "Iteration 3282, Loss: 37.02501678466797, L1: 10.568309783935547, L3: 26.456707000732422\n",
      "Current prediction:  61.178043365478516 \n",
      "\n",
      "Iteration 3283, Loss: 36.67658996582031, L1: 10.566039085388184, L3: 26.110549926757812\n",
      "Current prediction:  61.18162536621094 \n",
      "\n",
      "Iteration 3284, Loss: 36.486228942871094, L1: 10.56299877166748, L3: 25.92323112487793\n",
      "Current prediction:  61.1854362487793 \n",
      "\n",
      "Iteration 3285, Loss: 36.971527099609375, L1: 10.559775352478027, L3: 26.411752700805664\n",
      "Current prediction:  61.19102478027344 \n",
      "\n",
      "Iteration 3286, Loss: 37.238433837890625, L1: 10.555037498474121, L3: 26.68339729309082\n",
      "Current prediction:  61.19510269165039 \n",
      "\n",
      "Iteration 3287, Loss: 37.02277755737305, L1: 10.551575660705566, L3: 26.471202850341797\n",
      "Current prediction:  61.199371337890625 \n",
      "\n",
      "Iteration 3288, Loss: 36.520938873291016, L1: 10.547957420349121, L3: 25.972980499267578\n",
      "Current prediction:  61.20334243774414 \n",
      "\n",
      "Iteration 3289, Loss: 36.13389587402344, L1: 10.544053077697754, L3: 25.589841842651367\n",
      "Current prediction:  61.20663070678711 \n",
      "\n",
      "Iteration 3290, Loss: 37.154048919677734, L1: 10.541800498962402, L3: 26.61224937438965\n",
      "Current prediction:  61.209529876708984 \n",
      "\n",
      "Iteration 3291, Loss: 35.984336853027344, L1: 10.53934383392334, L3: 25.44499397277832\n",
      "Current prediction:  61.21354675292969 \n",
      "\n",
      "Iteration 3292, Loss: 36.4221076965332, L1: 10.53593921661377, L3: 25.88616943359375\n",
      "Current prediction:  61.21782684326172 \n",
      "\n",
      "Iteration 3293, Loss: 36.18571472167969, L1: 10.532310485839844, L3: 25.65340232849121\n",
      "Current prediction:  61.222625732421875 \n",
      "\n",
      "Iteration 3294, Loss: 36.7330322265625, L1: 10.528244972229004, L3: 26.20478630065918\n",
      "Current prediction:  61.22767639160156 \n",
      "\n",
      "Iteration 3295, Loss: 35.9855842590332, L1: 10.523961067199707, L3: 25.46162223815918\n",
      "Current prediction:  61.23031997680664 \n",
      "\n",
      "Iteration 3296, Loss: 36.230506896972656, L1: 10.521717071533203, L3: 25.708789825439453\n",
      "Current prediction:  61.2327766418457 \n",
      "\n",
      "Iteration 3297, Loss: 35.76551818847656, L1: 10.519637107849121, L3: 25.245880126953125\n",
      "Current prediction:  61.234405517578125 \n",
      "\n",
      "Iteration 3298, Loss: 37.380455017089844, L1: 10.518251419067383, L3: 26.86220359802246\n",
      "Current prediction:  61.23469543457031 \n",
      "\n",
      "Iteration 3299, Loss: 36.459251403808594, L1: 10.518007278442383, L3: 25.941246032714844\n",
      "Current prediction:  61.2332878112793 \n",
      "\n",
      "Iteration 3300, Loss: 35.848388671875, L1: 10.519201278686523, L3: 25.32918930053711\n",
      "Current prediction:  61.23293685913086 \n",
      "\n",
      "Iteration 3301, Loss: 36.59013748168945, L1: 10.519499778747559, L3: 26.070636749267578\n",
      "Current prediction:  61.23073959350586 \n",
      "\n",
      "Iteration 3302, Loss: 35.481239318847656, L1: 10.52136516571045, L3: 24.95987319946289\n",
      "Current prediction:  61.228580474853516 \n",
      "\n",
      "Iteration 3303, Loss: 36.40522003173828, L1: 10.523193359375, L3: 25.882028579711914\n",
      "Current prediction:  61.22703552246094 \n",
      "\n",
      "Iteration 3304, Loss: 36.01293182373047, L1: 10.524499893188477, L3: 25.488433837890625\n",
      "Current prediction:  61.22649002075195 \n",
      "\n",
      "Iteration 3305, Loss: 35.62541961669922, L1: 10.52496337890625, L3: 25.10045623779297\n",
      "Current prediction:  61.226837158203125 \n",
      "\n",
      "Iteration 3306, Loss: 36.37765884399414, L1: 10.524887084960938, L3: 25.852771759033203\n",
      "Current prediction:  61.22650909423828 \n",
      "\n",
      "Iteration 3307, Loss: 36.56938552856445, L1: 10.524950981140137, L3: 26.04443359375\n",
      "Current prediction:  61.22620391845703 \n",
      "\n",
      "Iteration 3308, Loss: 36.4471321105957, L1: 10.525209426879883, L3: 25.92192268371582\n",
      "Current prediction:  61.22749328613281 \n",
      "\n",
      "Iteration 3309, Loss: 36.549224853515625, L1: 10.524112701416016, L3: 26.025110244750977\n",
      "Current prediction:  61.228397369384766 \n",
      "\n",
      "Iteration 3310, Loss: 36.972293853759766, L1: 10.523348808288574, L3: 26.448945999145508\n",
      "Current prediction:  61.228206634521484 \n",
      "\n",
      "Iteration 3311, Loss: 36.05335235595703, L1: 10.523508071899414, L3: 25.529844284057617\n",
      "Current prediction:  61.227420806884766 \n",
      "\n",
      "Iteration 3312, Loss: 37.13365936279297, L1: 10.524179458618164, L3: 26.609477996826172\n",
      "Current prediction:  61.226043701171875 \n",
      "\n",
      "Iteration 3313, Loss: 36.36876678466797, L1: 10.525344848632812, L3: 25.843421936035156\n",
      "Current prediction:  61.22459411621094 \n",
      "\n",
      "Iteration 3314, Loss: 36.74437713623047, L1: 10.526575088500977, L3: 26.21780014038086\n",
      "Current prediction:  61.22408676147461 \n",
      "\n",
      "Iteration 3315, Loss: 36.17535400390625, L1: 10.527002334594727, L3: 25.648353576660156\n",
      "Current prediction:  61.22320556640625 \n",
      "\n",
      "Iteration 3316, Loss: 36.431060791015625, L1: 10.527750968933105, L3: 25.903308868408203\n",
      "Current prediction:  61.22275924682617 \n",
      "\n",
      "Iteration 3317, Loss: 37.109134674072266, L1: 10.528128623962402, L3: 26.58100700378418\n",
      "Current prediction:  61.22288131713867 \n",
      "\n",
      "Iteration 3318, Loss: 36.995765686035156, L1: 10.528024673461914, L3: 26.467741012573242\n",
      "Current prediction:  61.222042083740234 \n",
      "\n",
      "Iteration 3319, Loss: 36.256465911865234, L1: 10.52873706817627, L3: 25.72772789001465\n",
      "Current prediction:  61.221046447753906 \n",
      "\n",
      "Iteration 3320, Loss: 36.0294189453125, L1: 10.529585838317871, L3: 25.499834060668945\n",
      "Current prediction:  61.220367431640625 \n",
      "\n",
      "Iteration 3321, Loss: 36.36191177368164, L1: 10.53015422821045, L3: 25.831758499145508\n",
      "Current prediction:  61.221405029296875 \n",
      "\n",
      "Iteration 3322, Loss: 35.742591857910156, L1: 10.529273986816406, L3: 25.213319778442383\n",
      "Current prediction:  61.2224235534668 \n",
      "\n",
      "Iteration 3323, Loss: 36.873924255371094, L1: 10.528412818908691, L3: 26.34551239013672\n",
      "Current prediction:  61.22386169433594 \n",
      "\n",
      "Iteration 3324, Loss: 35.97356414794922, L1: 10.527193069458008, L3: 25.44637107849121\n",
      "Current prediction:  61.2255973815918 \n",
      "\n",
      "Iteration 3325, Loss: 36.351524353027344, L1: 10.525721549987793, L3: 25.825803756713867\n",
      "Current prediction:  61.22730255126953 \n",
      "\n",
      "Iteration 3326, Loss: 39.187896728515625, L1: 12.525336265563965, L3: 26.662559509277344\n",
      "Current prediction:  61.228267669677734 \n",
      "\n",
      "Iteration 3327, Loss: 35.532264709472656, L1: 10.523455619812012, L3: 25.00881004333496\n",
      "Current prediction:  61.22992706298828 \n",
      "\n",
      "Iteration 3328, Loss: 36.259559631347656, L1: 10.746075630187988, L3: 25.51348304748535\n",
      "Current prediction:  61.23158264160156 \n",
      "\n",
      "Iteration 3329, Loss: 36.720367431640625, L1: 10.70075798034668, L3: 26.019611358642578\n",
      "Current prediction:  60.28793716430664 \n",
      "\n",
      "Iteration 3330, Loss: 36.77469253540039, L1: 11.021641731262207, L3: 25.7530517578125\n",
      "Current prediction:  60.29585266113281 \n",
      "\n",
      "Iteration 3331, Loss: 38.307525634765625, L1: 11.505476951599121, L3: 26.80204963684082\n",
      "Current prediction:  60.3134880065918 \n",
      "\n",
      "Iteration 3332, Loss: 37.40771484375, L1: 11.625995635986328, L3: 25.781721115112305\n",
      "Current prediction:  60.34112548828125 \n",
      "\n",
      "Iteration 3333, Loss: 37.664024353027344, L1: 11.565313339233398, L3: 26.098709106445312\n",
      "Current prediction:  60.377952575683594 \n",
      "\n",
      "Iteration 3334, Loss: 37.09191131591797, L1: 11.242324829101562, L3: 25.849584579467773\n",
      "Current prediction:  60.42337417602539 \n",
      "\n",
      "Iteration 3335, Loss: 37.71085739135742, L1: 11.204056739807129, L3: 26.506799697875977\n",
      "Current prediction:  60.474769592285156 \n",
      "\n",
      "Iteration 3336, Loss: 37.657562255859375, L1: 11.160737037658691, L3: 26.496824264526367\n",
      "Current prediction:  60.53138732910156 \n",
      "\n",
      "Iteration 3337, Loss: 37.48737716674805, L1: 11.112993240356445, L3: 26.3743839263916\n",
      "Current prediction:  60.591854095458984 \n",
      "\n",
      "Iteration 3338, Loss: 37.46483612060547, L1: 11.061967849731445, L3: 26.40286636352539\n",
      "Current prediction:  60.65410614013672 \n",
      "\n",
      "Iteration 3339, Loss: 37.522518157958984, L1: 11.009413719177246, L3: 26.513105392456055\n",
      "Current prediction:  60.717899322509766 \n",
      "\n",
      "Iteration 3340, Loss: 35.74237060546875, L1: 10.955519676208496, L3: 24.786849975585938\n",
      "Current prediction:  60.78229904174805 \n",
      "\n",
      "Iteration 3341, Loss: 37.70260238647461, L1: 10.901093482971191, L3: 26.801509857177734\n",
      "Current prediction:  60.844635009765625 \n",
      "\n",
      "Iteration 3342, Loss: 37.36489486694336, L1: 10.848383903503418, L3: 26.516511917114258\n",
      "Current prediction:  60.906105041503906 \n",
      "\n",
      "Iteration 3343, Loss: 36.034873962402344, L1: 10.79637622833252, L3: 25.23849868774414\n",
      "Current prediction:  60.96490478515625 \n",
      "\n",
      "Iteration 3344, Loss: 35.19971466064453, L1: 10.746609687805176, L3: 24.453105926513672\n",
      "Current prediction:  61.022308349609375 \n",
      "\n",
      "Iteration 3345, Loss: 36.38154220581055, L1: 10.697999000549316, L3: 25.683542251586914\n",
      "Current prediction:  61.07627487182617 \n",
      "\n",
      "Iteration 3346, Loss: 36.77650451660156, L1: 10.6522855758667, L3: 26.124217987060547\n",
      "Current prediction:  61.12793731689453 \n",
      "\n",
      "Iteration 3347, Loss: 37.42427062988281, L1: 10.608512878417969, L3: 26.815759658813477\n",
      "Current prediction:  61.175724029541016 \n",
      "\n",
      "Iteration 3348, Loss: 36.355003356933594, L1: 10.568009376525879, L3: 25.7869930267334\n",
      "Current prediction:  61.22221374511719 \n",
      "\n",
      "Iteration 3349, Loss: 36.42300033569336, L1: 10.528594970703125, L3: 25.894405364990234\n",
      "Current prediction:  61.26341247558594 \n",
      "\n",
      "Iteration 3350, Loss: 35.73582077026367, L1: 10.493645668029785, L3: 25.242176055908203\n",
      "Current prediction:  61.300113677978516 \n",
      "\n",
      "Iteration 3351, Loss: 36.797298431396484, L1: 10.462520599365234, L3: 26.33477783203125\n",
      "Current prediction:  61.33137893676758 \n",
      "\n",
      "Iteration 3352, Loss: 36.95280075073242, L1: 10.435992240905762, L3: 26.516809463500977\n",
      "Current prediction:  61.35769271850586 \n",
      "\n",
      "Iteration 3353, Loss: 37.164817810058594, L1: 10.413666725158691, L3: 26.751150131225586\n",
      "Current prediction:  61.37837219238281 \n",
      "\n",
      "Iteration 3354, Loss: 36.60032653808594, L1: 10.3961181640625, L3: 26.20421028137207\n",
      "Current prediction:  61.39491271972656 \n",
      "\n",
      "Iteration 3355, Loss: 37.23789596557617, L1: 10.382080078125, L3: 26.855815887451172\n",
      "Current prediction:  61.406463623046875 \n",
      "\n",
      "Iteration 3356, Loss: 36.38538360595703, L1: 10.372273445129395, L3: 26.01310920715332\n",
      "Current prediction:  61.412986755371094 \n",
      "\n",
      "Iteration 3357, Loss: 36.22089385986328, L1: 10.366740226745605, L3: 25.854154586791992\n",
      "Current prediction:  61.41732406616211 \n",
      "\n",
      "Iteration 3358, Loss: 37.72235107421875, L1: 10.363058090209961, L3: 27.359291076660156\n",
      "Current prediction:  61.41640090942383 \n",
      "\n",
      "Iteration 3359, Loss: 36.818031311035156, L1: 10.36384391784668, L3: 26.454187393188477\n",
      "Current prediction:  61.413753509521484 \n",
      "\n",
      "Iteration 3360, Loss: 36.24349594116211, L1: 10.366087913513184, L3: 25.87740707397461\n",
      "Current prediction:  61.4081916809082 \n",
      "\n",
      "Iteration 3361, Loss: 37.27876281738281, L1: 10.37081241607666, L3: 26.907949447631836\n",
      "Current prediction:  61.399105072021484 \n",
      "\n",
      "Iteration 3362, Loss: 36.70720672607422, L1: 10.378524780273438, L3: 26.328683853149414\n",
      "Current prediction:  61.38694763183594 \n",
      "\n",
      "Iteration 3363, Loss: 35.39231872558594, L1: 10.388837814331055, L3: 25.00347900390625\n",
      "Current prediction:  61.37467956542969 \n",
      "\n",
      "Iteration 3364, Loss: 37.443843841552734, L1: 10.399249076843262, L3: 27.044593811035156\n",
      "Current prediction:  61.36068344116211 \n",
      "\n",
      "Iteration 3365, Loss: 36.104122161865234, L1: 10.411126136779785, L3: 25.692995071411133\n",
      "Current prediction:  61.34553527832031 \n",
      "\n",
      "Iteration 3366, Loss: 36.970733642578125, L1: 10.423981666564941, L3: 26.5467529296875\n",
      "Current prediction:  61.32991409301758 \n",
      "\n",
      "Iteration 3367, Loss: 36.350914001464844, L1: 10.437238693237305, L3: 25.913677215576172\n",
      "Current prediction:  61.31418991088867 \n",
      "\n",
      "Iteration 3368, Loss: 36.81523132324219, L1: 10.450576782226562, L3: 26.364652633666992\n",
      "Current prediction:  61.29840087890625 \n",
      "\n",
      "Iteration 3369, Loss: 35.287254333496094, L1: 10.463994026184082, L3: 24.823261260986328\n",
      "Current prediction:  61.28471755981445 \n",
      "\n",
      "Iteration 3370, Loss: 36.59635543823242, L1: 10.475576400756836, L3: 26.120779037475586\n",
      "Current prediction:  61.2715950012207 \n",
      "\n",
      "Iteration 3371, Loss: 36.30857849121094, L1: 10.486710548400879, L3: 25.821868896484375\n",
      "Current prediction:  61.26089859008789 \n",
      "\n",
      "Iteration 3372, Loss: 37.31101608276367, L1: 10.495787620544434, L3: 26.815227508544922\n",
      "Current prediction:  61.24998474121094 \n",
      "\n",
      "Iteration 3373, Loss: 37.11570739746094, L1: 10.505038261413574, L3: 26.610668182373047\n",
      "Current prediction:  61.23899459838867 \n",
      "\n",
      "Iteration 3374, Loss: 36.967132568359375, L1: 10.514361381530762, L3: 26.45277214050293\n",
      "Current prediction:  61.229549407958984 \n",
      "\n",
      "Iteration 3375, Loss: 35.77360916137695, L1: 10.522370338439941, L3: 25.251239776611328\n",
      "Current prediction:  61.221858978271484 \n",
      "\n",
      "Iteration 3376, Loss: 36.376461029052734, L1: 10.528889656066895, L3: 25.847570419311523\n",
      "Current prediction:  61.21506118774414 \n",
      "\n",
      "Iteration 3377, Loss: 35.985599517822266, L1: 10.53465747833252, L3: 25.45094108581543\n",
      "Current prediction:  61.21009063720703 \n",
      "\n",
      "Iteration 3378, Loss: 36.34284210205078, L1: 10.538869857788086, L3: 25.803974151611328\n",
      "Current prediction:  61.207149505615234 \n",
      "\n",
      "Iteration 3379, Loss: 35.60858154296875, L1: 10.541367530822754, L3: 25.067214965820312\n",
      "Current prediction:  61.203285217285156 \n",
      "\n",
      "Iteration 3380, Loss: 36.23872756958008, L1: 10.54464054107666, L3: 25.6940860748291\n",
      "Current prediction:  61.20016860961914 \n",
      "\n",
      "Iteration 3381, Loss: 37.09687042236328, L1: 10.547286987304688, L3: 26.549583435058594\n",
      "Current prediction:  61.196537017822266 \n",
      "\n",
      "Iteration 3382, Loss: 36.49021530151367, L1: 10.550362586975098, L3: 25.93985366821289\n",
      "Current prediction:  61.194091796875 \n",
      "\n",
      "Iteration 3383, Loss: 36.85086441040039, L1: 10.552435874938965, L3: 26.29842758178711\n",
      "Current prediction:  61.19343566894531 \n",
      "\n",
      "Iteration 3384, Loss: 36.96495819091797, L1: 10.552990913391113, L3: 26.411968231201172\n",
      "Current prediction:  61.19216537475586 \n",
      "\n",
      "Iteration 3385, Loss: 36.63660430908203, L1: 10.554069519042969, L3: 26.082536697387695\n",
      "Current prediction:  61.19194412231445 \n",
      "\n",
      "Iteration 3386, Loss: 37.61797332763672, L1: 10.5542573928833, L3: 27.0637149810791\n",
      "Current prediction:  61.19023513793945 \n",
      "\n",
      "Iteration 3387, Loss: 37.198341369628906, L1: 10.555709838867188, L3: 26.64263153076172\n",
      "Current prediction:  61.188533782958984 \n",
      "\n",
      "Iteration 3388, Loss: 36.5347785949707, L1: 10.557145118713379, L3: 25.97763442993164\n",
      "Current prediction:  61.18818283081055 \n",
      "\n",
      "Iteration 3389, Loss: 36.52287292480469, L1: 10.557443618774414, L3: 25.965431213378906\n",
      "Current prediction:  61.18720626831055 \n",
      "\n",
      "Iteration 3390, Loss: 36.27442932128906, L1: 10.558268547058105, L3: 25.71615982055664\n",
      "Current prediction:  61.185585021972656 \n",
      "\n",
      "Iteration 3391, Loss: 36.750343322753906, L1: 10.559650421142578, L3: 26.19069480895996\n",
      "Current prediction:  61.18489456176758 \n",
      "\n",
      "Iteration 3392, Loss: 36.30829620361328, L1: 10.560235977172852, L3: 25.74806022644043\n",
      "Current prediction:  61.18309020996094 \n",
      "\n",
      "Iteration 3393, Loss: 36.96086883544922, L1: 10.561767578125, L3: 26.399099349975586\n",
      "Current prediction:  61.180789947509766 \n",
      "\n",
      "Iteration 3394, Loss: 35.99820327758789, L1: 10.563714027404785, L3: 25.43448829650879\n",
      "Current prediction:  61.177093505859375 \n",
      "\n",
      "Iteration 3395, Loss: 37.807281494140625, L1: 10.566844940185547, L3: 27.240434646606445\n",
      "Current prediction:  61.173709869384766 \n",
      "\n",
      "Iteration 3396, Loss: 36.18596649169922, L1: 10.569718360900879, L3: 25.616247177124023\n",
      "Current prediction:  61.17150115966797 \n",
      "\n",
      "Iteration 3397, Loss: 35.1063117980957, L1: 10.571587562561035, L3: 24.53472328186035\n",
      "Current prediction:  61.17270278930664 \n",
      "\n",
      "Iteration 3398, Loss: 37.043121337890625, L1: 10.570573806762695, L3: 26.47254753112793\n",
      "Current prediction:  61.174930572509766 \n",
      "\n",
      "Iteration 3399, Loss: 36.34526062011719, L1: 10.568681716918945, L3: 25.776580810546875\n",
      "Current prediction:  61.17737579345703 \n",
      "\n",
      "Iteration 3400, Loss: 37.604026794433594, L1: 10.566610336303711, L3: 27.037416458129883\n",
      "Current prediction:  61.178001403808594 \n",
      "\n",
      "Iteration 3401, Loss: 36.161033630371094, L1: 10.566071510314941, L3: 25.594961166381836\n",
      "Current prediction:  61.18150329589844 \n",
      "\n",
      "Iteration 3402, Loss: 36.60597229003906, L1: 10.563111305236816, L3: 26.04286003112793\n",
      "Current prediction:  61.18532943725586 \n",
      "\n",
      "Iteration 3403, Loss: 35.76017761230469, L1: 10.559867858886719, L3: 25.2003116607666\n",
      "Current prediction:  61.189674377441406 \n",
      "\n",
      "Iteration 3404, Loss: 36.494972229003906, L1: 10.556181907653809, L3: 25.938791275024414\n",
      "Current prediction:  61.19474411010742 \n",
      "\n",
      "Iteration 3405, Loss: 36.977813720703125, L1: 10.551883697509766, L3: 26.425931930541992\n",
      "Current prediction:  61.20022964477539 \n",
      "\n",
      "Iteration 3406, Loss: 36.46394348144531, L1: 10.547235488891602, L3: 25.916706085205078\n",
      "Current prediction:  61.20376205444336 \n",
      "\n",
      "Iteration 3407, Loss: 35.92210388183594, L1: 10.54423713684082, L3: 25.37786865234375\n",
      "Current prediction:  61.20806121826172 \n",
      "\n",
      "Iteration 3408, Loss: 36.230491638183594, L1: 10.540594100952148, L3: 25.689897537231445\n",
      "Current prediction:  61.21263885498047 \n",
      "\n",
      "Iteration 3409, Loss: 36.49330139160156, L1: 10.536712646484375, L3: 25.956588745117188\n",
      "Current prediction:  61.217018127441406 \n",
      "\n",
      "Iteration 3410, Loss: 37.054725646972656, L1: 10.532999038696289, L3: 26.521724700927734\n",
      "Current prediction:  61.22026062011719 \n",
      "\n",
      "Iteration 3411, Loss: 36.44607162475586, L1: 10.621450424194336, L3: 25.824621200561523\n",
      "Current prediction:  61.22257995605469 \n",
      "\n",
      "Iteration 3412, Loss: 36.735633850097656, L1: 10.52828311920166, L3: 26.20734977722168\n",
      "Current prediction:  61.223262786865234 \n",
      "\n",
      "Iteration 3413, Loss: 37.10405349731445, L1: 10.52769660949707, L3: 26.576356887817383\n",
      "Current prediction:  61.22380447387695 \n",
      "\n",
      "Iteration 3414, Loss: 36.516029357910156, L1: 10.527243614196777, L3: 25.988786697387695\n",
      "Current prediction:  61.22454071044922 \n",
      "\n",
      "Iteration 3415, Loss: 36.44496154785156, L1: 10.526618957519531, L3: 25.918344497680664\n",
      "Current prediction:  61.22519302368164 \n",
      "\n",
      "Iteration 3416, Loss: 36.75148391723633, L1: 10.526065826416016, L3: 26.225418090820312\n",
      "Current prediction:  61.22547912597656 \n",
      "\n",
      "Iteration 3417, Loss: 36.418521881103516, L1: 10.525818824768066, L3: 25.892704010009766\n",
      "Current prediction:  61.22562026977539 \n",
      "\n",
      "Iteration 3418, Loss: 36.216827392578125, L1: 10.525703430175781, L3: 25.691125869750977\n",
      "Current prediction:  61.225215911865234 \n",
      "\n",
      "Iteration 3419, Loss: 35.126190185546875, L1: 10.526044845581055, L3: 24.600143432617188\n",
      "Current prediction:  61.22513198852539 \n",
      "\n",
      "Iteration 3420, Loss: 36.822635650634766, L1: 10.526118278503418, L3: 26.296518325805664\n",
      "Current prediction:  61.2254524230957 \n",
      "\n",
      "Iteration 3421, Loss: 37.270301818847656, L1: 10.52584457397461, L3: 26.74445915222168\n",
      "Current prediction:  61.224769592285156 \n",
      "\n",
      "Iteration 3422, Loss: 36.7003173828125, L1: 10.526426315307617, L3: 26.173892974853516\n",
      "Current prediction:  61.22184753417969 \n",
      "\n",
      "Iteration 3423, Loss: 36.76994705200195, L1: 10.528900146484375, L3: 26.241046905517578\n",
      "Current prediction:  61.22111511230469 \n",
      "\n",
      "Iteration 3424, Loss: 35.31988525390625, L1: 10.529521942138672, L3: 24.79036521911621\n",
      "Current prediction:  61.22159957885742 \n",
      "\n",
      "Iteration 3425, Loss: 37.03993225097656, L1: 10.52910041809082, L3: 26.510833740234375\n",
      "Current prediction:  61.22240447998047 \n",
      "\n",
      "Iteration 3426, Loss: 36.30122375488281, L1: 10.528429985046387, L3: 25.772794723510742\n",
      "Current prediction:  61.22365951538086 \n",
      "\n",
      "Iteration 3427, Loss: 36.805274963378906, L1: 10.527363777160645, L3: 26.277912139892578\n",
      "Current prediction:  61.22542953491211 \n",
      "\n",
      "Iteration 3428, Loss: 35.43588638305664, L1: 10.525856971740723, L3: 24.910030364990234\n",
      "Current prediction:  61.2281379699707 \n",
      "\n",
      "Iteration 3429, Loss: 35.962581634521484, L1: 10.523564338684082, L3: 25.439016342163086\n",
      "Current prediction:  61.230831146240234 \n",
      "\n",
      "Iteration 3430, Loss: 36.965999603271484, L1: 10.521286964416504, L3: 26.444711685180664\n",
      "Current prediction:  61.23221969604492 \n",
      "\n",
      "Iteration 3431, Loss: 36.037811279296875, L1: 10.52010440826416, L3: 25.5177059173584\n",
      "Current prediction:  61.23385238647461 \n",
      "\n",
      "Iteration 3432, Loss: 37.66196060180664, L1: 10.518717765808105, L3: 27.14324378967285\n",
      "Current prediction:  61.234046936035156 \n",
      "\n",
      "Iteration 3433, Loss: 36.148460388183594, L1: 10.518556594848633, L3: 25.629905700683594\n",
      "Current prediction:  61.230892181396484 \n",
      "\n",
      "Iteration 3434, Loss: 37.31013488769531, L1: 10.52122974395752, L3: 26.78890609741211\n",
      "Current prediction:  61.22699737548828 \n",
      "\n",
      "Iteration 3435, Loss: 36.39707946777344, L1: 10.52453327178955, L3: 25.872547149658203\n",
      "Current prediction:  61.22232437133789 \n",
      "\n",
      "Iteration 3436, Loss: 35.985557556152344, L1: 10.528499603271484, L3: 25.457059860229492\n",
      "Current prediction:  61.21782684326172 \n",
      "\n",
      "Iteration 3437, Loss: 35.93253707885742, L1: 10.532312393188477, L3: 25.400224685668945\n",
      "Current prediction:  61.21456527709961 \n",
      "\n",
      "Iteration 3438, Loss: 36.462581634521484, L1: 10.535076141357422, L3: 25.927505493164062\n",
      "Current prediction:  61.21467590332031 \n",
      "\n",
      "Iteration 3439, Loss: 37.73488998413086, L1: 10.534981727600098, L3: 27.199909210205078\n",
      "Current prediction:  61.213958740234375 \n",
      "\n",
      "Iteration 3440, Loss: 36.68351745605469, L1: 10.53559398651123, L3: 26.14792251586914\n",
      "Current prediction:  61.21464920043945 \n",
      "\n",
      "Iteration 3441, Loss: 36.63069152832031, L1: 10.535001754760742, L3: 26.095687866210938\n",
      "Current prediction:  61.21521759033203 \n",
      "\n",
      "Iteration 3442, Loss: 36.040733337402344, L1: 10.534524917602539, L3: 25.506206512451172\n",
      "Current prediction:  61.21575927734375 \n",
      "\n",
      "Iteration 3443, Loss: 37.05171585083008, L1: 10.534064292907715, L3: 26.51765251159668\n",
      "Current prediction:  61.2132682800293 \n",
      "\n",
      "Iteration 3444, Loss: 36.503273010253906, L1: 10.536176681518555, L3: 25.967098236083984\n",
      "Current prediction:  61.20980453491211 \n",
      "\n",
      "Iteration 3445, Loss: 36.73981475830078, L1: 10.539111137390137, L3: 26.200702667236328\n",
      "Current prediction:  61.207454681396484 \n",
      "\n",
      "Iteration 3446, Loss: 36.90138244628906, L1: 10.541107177734375, L3: 26.360275268554688\n",
      "Current prediction:  61.20610427856445 \n",
      "\n",
      "Iteration 3447, Loss: 36.289085388183594, L1: 10.542252540588379, L3: 25.7468318939209\n",
      "Current prediction:  61.205570220947266 \n",
      "\n",
      "Iteration 3448, Loss: 36.4207878112793, L1: 10.542701721191406, L3: 25.87808609008789\n",
      "Current prediction:  61.206321716308594 \n",
      "\n",
      "Iteration 3449, Loss: 37.47861099243164, L1: 10.542061805725098, L3: 26.93655014038086\n",
      "Current prediction:  61.206302642822266 \n",
      "\n",
      "Iteration 3450, Loss: 36.68596649169922, L1: 10.542081832885742, L3: 26.14388656616211\n",
      "Current prediction:  61.20599365234375 \n",
      "\n",
      "Iteration 3451, Loss: 35.75361251831055, L1: 10.542349815368652, L3: 25.21126365661621\n",
      "Current prediction:  61.20529556274414 \n",
      "\n",
      "Iteration 3452, Loss: 36.66597366333008, L1: 10.542936325073242, L3: 26.123037338256836\n",
      "Current prediction:  61.20334243774414 \n",
      "\n",
      "Iteration 3453, Loss: 36.13089370727539, L1: 10.544587135314941, L3: 25.586305618286133\n",
      "Current prediction:  61.20146942138672 \n",
      "\n",
      "Iteration 3454, Loss: 36.44280242919922, L1: 10.546181678771973, L3: 25.896621704101562\n",
      "Current prediction:  61.201499938964844 \n",
      "\n",
      "Iteration 3455, Loss: 37.35570526123047, L1: 10.546156883239746, L3: 26.80954933166504\n",
      "Current prediction:  61.20164489746094 \n",
      "\n",
      "Iteration 3456, Loss: 35.88165283203125, L1: 10.546037673950195, L3: 25.335615158081055\n",
      "Current prediction:  61.201080322265625 \n",
      "\n",
      "Iteration 3457, Loss: 36.33564376831055, L1: 10.546506881713867, L3: 25.78913688659668\n",
      "Current prediction:  61.20111846923828 \n",
      "\n",
      "Iteration 3458, Loss: 36.725284576416016, L1: 10.546480178833008, L3: 26.178804397583008\n",
      "Current prediction:  61.19964599609375 \n",
      "\n",
      "Iteration 3459, Loss: 37.34028625488281, L1: 10.547727584838867, L3: 26.792556762695312\n",
      "Current prediction:  61.19806671142578 \n",
      "\n",
      "Iteration 3460, Loss: 36.26278305053711, L1: 10.549071311950684, L3: 25.713712692260742\n",
      "Current prediction:  61.19633102416992 \n",
      "\n",
      "Iteration 3461, Loss: 37.254493713378906, L1: 10.550537109375, L3: 26.703956604003906\n",
      "Current prediction:  61.195735931396484 \n",
      "\n",
      "Iteration 3462, Loss: 36.146018981933594, L1: 10.551047325134277, L3: 25.594970703125\n",
      "Current prediction:  61.19652557373047 \n",
      "\n",
      "Iteration 3463, Loss: 37.01238250732422, L1: 10.550374984741211, L3: 26.46200942993164\n",
      "Current prediction:  61.196502685546875 \n",
      "\n",
      "Iteration 3464, Loss: 36.39899444580078, L1: 10.550393104553223, L3: 25.848602294921875\n",
      "Current prediction:  61.19591522216797 \n",
      "\n",
      "Iteration 3465, Loss: 36.63173294067383, L1: 10.550888061523438, L3: 26.08084487915039\n",
      "Current prediction:  61.1966552734375 \n",
      "\n",
      "Iteration 3466, Loss: 35.8221549987793, L1: 10.550265312194824, L3: 25.27189064025879\n",
      "Current prediction:  61.199920654296875 \n",
      "\n",
      "Iteration 3467, Loss: 36.079185485839844, L1: 10.54749870300293, L3: 25.531686782836914\n",
      "Current prediction:  61.20337677001953 \n",
      "\n",
      "Iteration 3468, Loss: 36.78862762451172, L1: 10.544563293457031, L3: 26.24406623840332\n",
      "Current prediction:  61.20781707763672 \n",
      "\n",
      "Iteration 3469, Loss: 36.837615966796875, L1: 10.540802001953125, L3: 26.296812057495117\n",
      "Current prediction:  61.212032318115234 \n",
      "\n",
      "Iteration 3470, Loss: 36.341461181640625, L1: 10.537227630615234, L3: 25.804235458374023\n",
      "Current prediction:  61.21620559692383 \n",
      "\n",
      "Iteration 3471, Loss: 36.5616340637207, L1: 10.533685684204102, L3: 26.0279483795166\n",
      "Current prediction:  61.219242095947266 \n",
      "\n",
      "Iteration 3472, Loss: 37.06101608276367, L1: 10.531112670898438, L3: 26.529903411865234\n",
      "Current prediction:  61.22236251831055 \n",
      "\n",
      "Iteration 3473, Loss: 37.27085876464844, L1: 10.528460502624512, L3: 26.742399215698242\n",
      "Current prediction:  61.22547149658203 \n",
      "\n",
      "Iteration 3474, Loss: 36.74136734008789, L1: 10.52582836151123, L3: 26.215538024902344\n",
      "Current prediction:  61.22690200805664 \n",
      "\n",
      "Iteration 3475, Loss: 36.559104919433594, L1: 10.524614334106445, L3: 26.03449058532715\n",
      "Current prediction:  61.22805404663086 \n",
      "\n",
      "Iteration 3476, Loss: 36.5198860168457, L1: 10.523640632629395, L3: 25.996244430541992\n",
      "Current prediction:  61.22794723510742 \n",
      "\n",
      "Iteration 3477, Loss: 36.86772918701172, L1: 10.523725509643555, L3: 26.344003677368164\n",
      "Current prediction:  61.22749710083008 \n",
      "\n",
      "Iteration 3478, Loss: 36.263771057128906, L1: 10.524110794067383, L3: 25.739660263061523\n",
      "Current prediction:  61.22857666015625 \n",
      "\n",
      "Iteration 3479, Loss: 36.27963638305664, L1: 10.523194313049316, L3: 25.75644302368164\n",
      "Current prediction:  61.228919982910156 \n",
      "\n",
      "Iteration 3480, Loss: 36.69841003417969, L1: 10.522905349731445, L3: 26.17550277709961\n",
      "Current prediction:  61.22792053222656 \n",
      "\n",
      "Iteration 3481, Loss: 36.445655822753906, L1: 10.523757934570312, L3: 25.92189598083496\n",
      "Current prediction:  61.22642135620117 \n",
      "\n",
      "Iteration 3482, Loss: 37.333282470703125, L1: 10.5250244140625, L3: 26.808256149291992\n",
      "Current prediction:  61.22502136230469 \n",
      "\n",
      "Iteration 3483, Loss: 38.13920974731445, L1: 10.526209831237793, L3: 27.613000869750977\n",
      "Current prediction:  61.2210693359375 \n",
      "\n",
      "Iteration 3484, Loss: 36.406959533691406, L1: 10.529557228088379, L3: 25.87740135192871\n",
      "Current prediction:  61.21567916870117 \n",
      "\n",
      "Iteration 3485, Loss: 35.53752517700195, L1: 10.534133911132812, L3: 25.00339126586914\n",
      "Current prediction:  61.21246337890625 \n",
      "\n",
      "Iteration 3486, Loss: 36.98344802856445, L1: 10.536858558654785, L3: 26.446590423583984\n",
      "Current prediction:  61.210304260253906 \n",
      "\n",
      "Iteration 3487, Loss: 38.42510223388672, L1: 10.538687705993652, L3: 27.886415481567383\n",
      "Current prediction:  61.20630645751953 \n",
      "\n",
      "Iteration 3488, Loss: 37.44248962402344, L1: 10.542073249816895, L3: 26.900415420532227\n",
      "Current prediction:  61.201011657714844 \n",
      "\n",
      "Iteration 3489, Loss: 37.46490478515625, L1: 10.546568870544434, L3: 26.918336868286133\n",
      "Current prediction:  61.198150634765625 \n",
      "\n",
      "Iteration 3490, Loss: 36.573673248291016, L1: 10.548993110656738, L3: 26.024681091308594\n",
      "Current prediction:  61.1964225769043 \n",
      "\n",
      "Iteration 3491, Loss: 36.27572250366211, L1: 10.550466537475586, L3: 25.725255966186523\n",
      "Current prediction:  61.19695281982422 \n",
      "\n",
      "Iteration 3492, Loss: 36.875003814697266, L1: 10.550005912780762, L3: 26.324996948242188\n",
      "Current prediction:  61.197208404541016 \n",
      "\n",
      "Iteration 3493, Loss: 35.87861633300781, L1: 10.549797058105469, L3: 25.328821182250977\n",
      "Current prediction:  61.19789123535156 \n",
      "\n",
      "Iteration 3494, Loss: 35.669952392578125, L1: 10.549216270446777, L3: 25.120737075805664\n",
      "Current prediction:  61.19975280761719 \n",
      "\n",
      "Iteration 3495, Loss: 36.931846618652344, L1: 10.547637939453125, L3: 26.384206771850586\n",
      "Current prediction:  61.20127868652344 \n",
      "\n",
      "Iteration 3496, Loss: 36.181583404541016, L1: 10.546341896057129, L3: 25.635242462158203\n",
      "Current prediction:  61.20402908325195 \n",
      "\n",
      "Iteration 3497, Loss: 35.520484924316406, L1: 10.544007301330566, L3: 24.976476669311523\n",
      "Current prediction:  61.20718002319336 \n",
      "\n",
      "Iteration 3498, Loss: 36.24924850463867, L1: 10.541340827941895, L3: 25.70790672302246\n",
      "Current prediction:  61.211063385009766 \n",
      "\n",
      "Iteration 3499, Loss: 38.00065231323242, L1: 10.538044929504395, L3: 27.46260643005371\n",
      "Current prediction:  61.213314056396484 \n",
      "\n",
      "Iteration 3500, Loss: 37.54050064086914, L1: 10.536138534545898, L3: 27.004362106323242\n",
      "Current prediction:  61.21415328979492 \n",
      "\n",
      "Iteration 3501, Loss: 36.50642395019531, L1: 10.535425186157227, L3: 25.970996856689453\n",
      "Current prediction:  61.21426010131836 \n",
      "\n",
      "Iteration 3502, Loss: 37.62940979003906, L1: 10.535335540771484, L3: 27.094074249267578\n",
      "Current prediction:  61.21178436279297 \n",
      "\n",
      "Iteration 3503, Loss: 36.85805130004883, L1: 10.537436485290527, L3: 26.320615768432617\n",
      "Current prediction:  61.209415435791016 \n",
      "\n",
      "Iteration 3504, Loss: 36.49855041503906, L1: 10.53944206237793, L3: 25.959108352661133\n",
      "Current prediction:  61.20561981201172 \n",
      "\n",
      "Iteration 3505, Loss: 36.953147888183594, L1: 10.542662620544434, L3: 26.410484313964844\n",
      "Current prediction:  61.200809478759766 \n",
      "\n",
      "Iteration 3506, Loss: 36.343257904052734, L1: 10.546737670898438, L3: 25.796520233154297\n",
      "Current prediction:  61.19625473022461 \n",
      "\n",
      "Iteration 3507, Loss: 36.55780029296875, L1: 10.550599098205566, L3: 26.0072021484375\n",
      "Current prediction:  61.19233322143555 \n",
      "\n",
      "Iteration 3508, Loss: 36.67668533325195, L1: 10.553924560546875, L3: 26.122760772705078\n",
      "Current prediction:  61.18956756591797 \n",
      "\n",
      "Iteration 3509, Loss: 36.9354248046875, L1: 10.5562744140625, L3: 26.379152297973633\n",
      "Current prediction:  61.18749237060547 \n",
      "\n",
      "Iteration 3510, Loss: 36.80250549316406, L1: 10.558032035827637, L3: 26.244474411010742\n",
      "Current prediction:  61.185791015625 \n",
      "\n",
      "Iteration 3511, Loss: 37.35274124145508, L1: 10.55947494506836, L3: 26.79326629638672\n",
      "Current prediction:  61.18581771850586 \n",
      "\n",
      "Iteration 3512, Loss: 35.845027923583984, L1: 10.568537712097168, L3: 25.2764892578125\n",
      "Current prediction:  61.18648910522461 \n",
      "\n",
      "Iteration 3513, Loss: 36.08272933959961, L1: 10.558880805969238, L3: 25.523847579956055\n",
      "Current prediction:  61.1884651184082 \n",
      "\n",
      "Iteration 3514, Loss: 36.126895904541016, L1: 10.557211875915527, L3: 25.569683074951172\n",
      "Current prediction:  61.190589904785156 \n",
      "\n",
      "Iteration 3515, Loss: 36.97588348388672, L1: 10.55540657043457, L3: 26.42047882080078\n",
      "Current prediction:  61.19110107421875 \n",
      "\n",
      "Iteration 3516, Loss: 37.01226806640625, L1: 10.554970741271973, L3: 26.45729637145996\n",
      "Current prediction:  61.18952178955078 \n",
      "\n",
      "Iteration 3517, Loss: 37.58469009399414, L1: 10.55790901184082, L3: 27.02678108215332\n",
      "Current prediction:  61.1879768371582 \n",
      "\n",
      "Iteration 3518, Loss: 36.77486801147461, L1: 10.557623863220215, L3: 26.217243194580078\n",
      "Current prediction:  61.1871452331543 \n",
      "\n",
      "Iteration 3519, Loss: 36.451904296875, L1: 10.558364868164062, L3: 25.893539428710938\n",
      "Current prediction:  61.188575744628906 \n",
      "\n",
      "Iteration 3520, Loss: 36.61034393310547, L1: 10.557113647460938, L3: 26.0532283782959\n",
      "Current prediction:  61.190059661865234 \n",
      "\n",
      "Iteration 3521, Loss: 36.91722106933594, L1: 10.556129455566406, L3: 26.3610897064209\n",
      "Current prediction:  61.192012786865234 \n",
      "\n",
      "Iteration 3522, Loss: 36.02703094482422, L1: 10.554195404052734, L3: 25.47283363342285\n",
      "Current prediction:  61.19378662109375 \n",
      "\n",
      "Iteration 3523, Loss: 36.6340217590332, L1: 10.564223289489746, L3: 26.069799423217773\n",
      "Current prediction:  61.196876525878906 \n",
      "\n",
      "Iteration 3524, Loss: 36.12543869018555, L1: 10.558229446411133, L3: 25.567209243774414\n",
      "Current prediction:  61.19960403442383 \n",
      "\n",
      "Iteration 3525, Loss: 36.827388763427734, L1: 10.55087661743164, L3: 26.276512145996094\n",
      "Current prediction:  61.20173645019531 \n",
      "\n",
      "Iteration 3526, Loss: 36.686790466308594, L1: 10.54284381866455, L3: 26.14394760131836\n",
      "Current prediction:  61.20416259765625 \n",
      "\n",
      "Iteration 3527, Loss: 35.603790283203125, L1: 10.547078132629395, L3: 25.056711196899414\n",
      "Current prediction:  61.20663070678711 \n",
      "\n",
      "Iteration 3528, Loss: 36.66453170776367, L1: 10.54013442993164, L3: 26.12439727783203\n",
      "Current prediction:  61.207637786865234 \n",
      "\n",
      "Iteration 3529, Loss: 37.14982223510742, L1: 10.540600776672363, L3: 26.609222412109375\n",
      "Current prediction:  61.20692443847656 \n",
      "\n",
      "Iteration 3530, Loss: 36.22748565673828, L1: 10.541553497314453, L3: 25.68593406677246\n",
      "Current prediction:  61.20582962036133 \n",
      "\n",
      "Iteration 3531, Loss: 36.18014144897461, L1: 10.542481422424316, L3: 25.63766098022461\n",
      "Current prediction:  61.2049446105957 \n",
      "\n",
      "Iteration 3532, Loss: 36.15538024902344, L1: 10.543237686157227, L3: 25.612140655517578\n",
      "Current prediction:  61.202728271484375 \n",
      "\n",
      "Iteration 3533, Loss: 35.887123107910156, L1: 10.545114517211914, L3: 25.342008590698242\n",
      "Current prediction:  61.2005500793457 \n",
      "\n",
      "Iteration 3534, Loss: 36.47926330566406, L1: 10.546955108642578, L3: 25.932310104370117\n",
      "Current prediction:  61.199951171875 \n",
      "\n",
      "Iteration 3535, Loss: 36.8018798828125, L1: 10.547469139099121, L3: 26.254411697387695\n",
      "Current prediction:  61.199249267578125 \n",
      "\n",
      "Iteration 3536, Loss: 36.461246490478516, L1: 10.54806137084961, L3: 25.913185119628906\n",
      "Current prediction:  61.197566986083984 \n",
      "\n",
      "Iteration 3537, Loss: 36.79137420654297, L1: 10.549487113952637, L3: 26.24188804626465\n",
      "Current prediction:  61.1966438293457 \n",
      "\n",
      "Iteration 3538, Loss: 36.33100891113281, L1: 10.55026912689209, L3: 25.78074073791504\n",
      "Current prediction:  61.19693374633789 \n",
      "\n",
      "Iteration 3539, Loss: 37.05931091308594, L1: 10.550027847290039, L3: 26.509281158447266\n",
      "Current prediction:  61.19838333129883 \n",
      "\n",
      "Iteration 3540, Loss: 37.65411376953125, L1: 10.548798561096191, L3: 27.105316162109375\n",
      "Current prediction:  61.198585510253906 \n",
      "\n",
      "Iteration 3541, Loss: 36.586856842041016, L1: 10.548624992370605, L3: 26.038232803344727\n",
      "Current prediction:  61.20015335083008 \n",
      "\n",
      "Iteration 3542, Loss: 36.171775817871094, L1: 10.547293663024902, L3: 25.624481201171875\n",
      "Current prediction:  61.2017707824707 \n",
      "\n",
      "Iteration 3543, Loss: 35.54249954223633, L1: 10.545926094055176, L3: 24.99657440185547\n",
      "Current prediction:  61.20180130004883 \n",
      "\n",
      "Iteration 3544, Loss: 36.347900390625, L1: 10.546905517578125, L3: 25.800992965698242\n",
      "Current prediction:  61.20061492919922 \n",
      "\n",
      "Iteration 3545, Loss: 36.79924011230469, L1: 10.546905517578125, L3: 26.252334594726562\n",
      "Current prediction:  61.199851989746094 \n",
      "\n",
      "Iteration 3546, Loss: 36.233238220214844, L1: 10.54755687713623, L3: 25.685680389404297\n",
      "Current prediction:  61.20000457763672 \n",
      "\n",
      "Iteration 3547, Loss: 36.117122650146484, L1: 10.547423362731934, L3: 25.569700241088867\n",
      "Current prediction:  61.19847869873047 \n",
      "\n",
      "Iteration 3548, Loss: 36.68550109863281, L1: 10.54871940612793, L3: 26.136781692504883\n",
      "Current prediction:  61.19706726074219 \n",
      "\n",
      "Iteration 3549, Loss: 36.88494110107422, L1: 10.549917221069336, L3: 26.33502197265625\n",
      "Current prediction:  61.19668960571289 \n",
      "\n",
      "Iteration 3550, Loss: 36.90575408935547, L1: 10.5502347946167, L3: 26.355518341064453\n",
      "Current prediction:  61.197021484375 \n",
      "\n",
      "Iteration 3551, Loss: 36.71589660644531, L1: 10.54995059967041, L3: 26.16594696044922\n",
      "Current prediction:  61.19609832763672 \n",
      "\n",
      "Iteration 3552, Loss: 36.02046585083008, L1: 10.550735473632812, L3: 25.469730377197266\n",
      "Current prediction:  61.19588088989258 \n",
      "\n",
      "Iteration 3553, Loss: 37.03053665161133, L1: 10.550917625427246, L3: 26.479618072509766\n",
      "Current prediction:  61.19395446777344 \n",
      "\n",
      "Iteration 3554, Loss: 36.785858154296875, L1: 10.55255126953125, L3: 26.233308792114258\n",
      "Current prediction:  61.19215774536133 \n",
      "\n",
      "Iteration 3555, Loss: 36.45697784423828, L1: 10.554075241088867, L3: 25.90290069580078\n",
      "Current prediction:  61.19091796875 \n",
      "\n",
      "Iteration 3556, Loss: 36.85099792480469, L1: 10.555130004882812, L3: 26.295866012573242\n",
      "Current prediction:  61.19032287597656 \n",
      "\n",
      "Iteration 3557, Loss: 37.448360443115234, L1: 10.555631637573242, L3: 26.892728805541992\n",
      "Current prediction:  61.1900520324707 \n",
      "\n",
      "Iteration 3558, Loss: 37.52519989013672, L1: 10.555863380432129, L3: 26.969337463378906\n",
      "Current prediction:  61.18936538696289 \n",
      "\n",
      "Iteration 3559, Loss: 35.96322250366211, L1: 10.556443214416504, L3: 25.40677833557129\n",
      "Current prediction:  61.19056701660156 \n",
      "\n",
      "Iteration 3560, Loss: 36.34341049194336, L1: 10.555424690246582, L3: 25.787986755371094\n",
      "Current prediction:  61.19053649902344 \n",
      "\n",
      "Iteration 3561, Loss: 35.674949645996094, L1: 10.555450439453125, L3: 25.1195011138916\n",
      "Current prediction:  61.1910514831543 \n",
      "\n",
      "Iteration 3562, Loss: 36.83868408203125, L1: 10.555007934570312, L3: 26.28367805480957\n",
      "Current prediction:  61.191890716552734 \n",
      "\n",
      "Iteration 3563, Loss: 36.019073486328125, L1: 10.554305076599121, L3: 25.46476936340332\n",
      "Current prediction:  61.19228744506836 \n",
      "\n",
      "Iteration 3564, Loss: 35.52459716796875, L1: 10.553963661193848, L3: 24.97063446044922\n",
      "Current prediction:  61.193748474121094 \n",
      "\n",
      "Iteration 3565, Loss: 36.407100677490234, L1: 10.552725791931152, L3: 25.8543758392334\n",
      "Current prediction:  61.19424057006836 \n",
      "\n",
      "Iteration 3566, Loss: 37.03281021118164, L1: 10.552313804626465, L3: 26.480497360229492\n",
      "Current prediction:  61.194244384765625 \n",
      "\n",
      "Iteration 3567, Loss: 36.992210388183594, L1: 10.55230712890625, L3: 26.439903259277344\n",
      "Current prediction:  61.19652557373047 \n",
      "\n",
      "Iteration 3568, Loss: 36.626502990722656, L1: 10.550370216369629, L3: 26.076133728027344\n",
      "Current prediction:  61.19932556152344 \n",
      "\n",
      "Iteration 3569, Loss: 36.47611999511719, L1: 10.54800033569336, L3: 25.92812156677246\n",
      "Current prediction:  61.20154571533203 \n",
      "\n",
      "Iteration 3570, Loss: 37.217708587646484, L1: 10.546113014221191, L3: 26.671594619750977\n",
      "Current prediction:  61.204132080078125 \n",
      "\n",
      "Iteration 3571, Loss: 36.21185302734375, L1: 10.543925285339355, L3: 25.66792869567871\n",
      "Current prediction:  61.20757293701172 \n",
      "\n",
      "Iteration 3572, Loss: 35.29558181762695, L1: 10.541007041931152, L3: 24.754573822021484\n",
      "Current prediction:  61.210933685302734 \n",
      "\n",
      "Iteration 3573, Loss: 36.37810134887695, L1: 10.5381498336792, L3: 25.83995246887207\n",
      "Current prediction:  61.21330642700195 \n",
      "\n",
      "Iteration 3574, Loss: 36.897029876708984, L1: 10.53614330291748, L3: 26.36088752746582\n",
      "Current prediction:  61.214054107666016 \n",
      "\n",
      "Iteration 3575, Loss: 37.051734924316406, L1: 10.535506248474121, L3: 26.51622772216797\n",
      "Current prediction:  61.21394348144531 \n",
      "\n",
      "Iteration 3576, Loss: 36.508750915527344, L1: 10.535601615905762, L3: 25.9731502532959\n",
      "Current prediction:  61.2140007019043 \n",
      "\n",
      "Iteration 3577, Loss: 36.212440490722656, L1: 10.535553932189941, L3: 25.67688751220703\n",
      "Current prediction:  61.2125244140625 \n",
      "\n",
      "Iteration 3578, Loss: 36.60226821899414, L1: 10.536808967590332, L3: 26.065460205078125\n",
      "Current prediction:  61.210330963134766 \n",
      "\n",
      "Iteration 3579, Loss: 36.52313995361328, L1: 10.538665771484375, L3: 25.984472274780273\n",
      "Current prediction:  61.20747375488281 \n",
      "\n",
      "Iteration 3580, Loss: 37.02172088623047, L1: 10.541090965270996, L3: 26.480628967285156\n",
      "Current prediction:  61.20576858520508 \n",
      "\n",
      "Iteration 3581, Loss: 37.582088470458984, L1: 10.542537689208984, L3: 27.03955078125\n",
      "Current prediction:  61.20257568359375 \n",
      "\n",
      "Iteration 3582, Loss: 36.32748031616211, L1: 10.545243263244629, L3: 25.782238006591797\n",
      "Current prediction:  61.200172424316406 \n",
      "\n",
      "Iteration 3583, Loss: 37.21196746826172, L1: 10.547283172607422, L3: 26.664684295654297\n",
      "Current prediction:  61.196800231933594 \n",
      "\n",
      "Iteration 3584, Loss: 37.024505615234375, L1: 10.550138473510742, L3: 26.474369049072266\n",
      "Current prediction:  61.191795349121094 \n",
      "\n",
      "Iteration 3585, Loss: 37.53160095214844, L1: 10.554386138916016, L3: 26.977214813232422\n",
      "Current prediction:  61.188568115234375 \n",
      "\n",
      "Iteration 3586, Loss: 36.244544982910156, L1: 10.55711841583252, L3: 25.68742561340332\n",
      "Current prediction:  61.18574523925781 \n",
      "\n",
      "Iteration 3587, Loss: 35.98763656616211, L1: 10.559510231018066, L3: 25.428125381469727\n",
      "Current prediction:  61.183387756347656 \n",
      "\n",
      "Iteration 3588, Loss: 36.69789123535156, L1: 10.561508178710938, L3: 26.136381149291992\n",
      "Current prediction:  61.18016815185547 \n",
      "\n",
      "Iteration 3589, Loss: 37.30345916748047, L1: 10.564239501953125, L3: 26.739219665527344\n",
      "Current prediction:  61.17621994018555 \n",
      "\n",
      "Iteration 3590, Loss: 36.51732635498047, L1: 10.567584037780762, L3: 25.949743270874023\n",
      "Current prediction:  61.17415237426758 \n",
      "\n",
      "Iteration 3591, Loss: 36.731101989746094, L1: 10.569336891174316, L3: 26.16176414489746\n",
      "Current prediction:  61.172447204589844 \n",
      "\n",
      "Iteration 3592, Loss: 37.155147552490234, L1: 10.57078742980957, L3: 26.584360122680664\n",
      "Current prediction:  61.17280960083008 \n",
      "\n",
      "Iteration 3593, Loss: 36.28125, L1: 10.570479393005371, L3: 25.710769653320312\n",
      "Current prediction:  61.17206954956055 \n",
      "\n",
      "Iteration 3594, Loss: 36.066585540771484, L1: 10.571106910705566, L3: 25.4954776763916\n",
      "Current prediction:  61.17143630981445 \n",
      "\n",
      "Iteration 3595, Loss: 36.94839859008789, L1: 10.57164192199707, L3: 26.37675666809082\n",
      "Current prediction:  61.17262649536133 \n",
      "\n",
      "Iteration 3596, Loss: 36.35689926147461, L1: 10.570633888244629, L3: 25.786266326904297\n",
      "Current prediction:  61.17433166503906 \n",
      "\n",
      "Iteration 3597, Loss: 36.453697204589844, L1: 10.569190979003906, L3: 25.884506225585938\n",
      "Current prediction:  61.1749267578125 \n",
      "\n",
      "Iteration 3598, Loss: 36.13679122924805, L1: 10.568686485290527, L3: 25.568103790283203\n",
      "Current prediction:  61.17600631713867 \n",
      "\n",
      "Iteration 3599, Loss: 36.31516647338867, L1: 10.567765235900879, L3: 25.74740219116211\n",
      "Current prediction:  61.177101135253906 \n",
      "\n",
      "Iteration 3600, Loss: 36.61813735961914, L1: 10.566840171813965, L3: 26.05129623413086\n",
      "Current prediction:  61.176517486572266 \n",
      "\n",
      "Iteration 3601, Loss: 36.365318298339844, L1: 10.567333221435547, L3: 25.797985076904297\n",
      "Current prediction:  61.17695236206055 \n",
      "\n",
      "Iteration 3602, Loss: 36.19511795043945, L1: 10.566967964172363, L3: 25.628149032592773\n",
      "Current prediction:  61.18050003051758 \n",
      "\n",
      "Iteration 3603, Loss: 37.44561767578125, L1: 10.563961029052734, L3: 26.88165855407715\n",
      "Current prediction:  61.18440246582031 \n",
      "\n",
      "Iteration 3604, Loss: 36.3694953918457, L1: 10.560651779174805, L3: 25.8088436126709\n",
      "Current prediction:  61.188926696777344 \n",
      "\n",
      "Iteration 3605, Loss: 37.49087905883789, L1: 10.556816101074219, L3: 26.934062957763672\n",
      "Current prediction:  61.19154357910156 \n",
      "\n",
      "Iteration 3606, Loss: 36.24815368652344, L1: 10.554594993591309, L3: 25.693559646606445\n",
      "Current prediction:  61.19523239135742 \n",
      "\n",
      "Iteration 3607, Loss: 36.862388610839844, L1: 10.551468849182129, L3: 26.3109188079834\n",
      "Current prediction:  61.19976043701172 \n",
      "\n",
      "Iteration 3608, Loss: 36.725006103515625, L1: 10.547637939453125, L3: 26.1773681640625\n",
      "Current prediction:  61.206077575683594 \n",
      "\n",
      "Iteration 3609, Loss: 37.19549560546875, L1: 10.542274475097656, L3: 26.653221130371094\n",
      "Current prediction:  61.21068572998047 \n",
      "\n",
      "Iteration 3610, Loss: 37.0837516784668, L1: 10.538371086120605, L3: 26.545381546020508\n",
      "Current prediction:  61.21583557128906 \n",
      "\n",
      "Iteration 3611, Loss: 36.31879806518555, L1: 10.534002304077148, L3: 25.7847957611084\n",
      "Current prediction:  61.22248077392578 \n",
      "\n",
      "Iteration 3612, Loss: 36.28447723388672, L1: 10.528364181518555, L3: 25.756114959716797\n",
      "Current prediction:  61.227970123291016 \n",
      "\n",
      "Iteration 3613, Loss: 36.948020935058594, L1: 10.523711204528809, L3: 26.42430877685547\n",
      "Current prediction:  61.23139953613281 \n",
      "\n",
      "Iteration 3614, Loss: 36.671241760253906, L1: 10.52079963684082, L3: 26.150440216064453\n",
      "Current prediction:  61.23374557495117 \n",
      "\n",
      "Iteration 3615, Loss: 36.55818176269531, L1: 10.518815994262695, L3: 26.039363861083984\n",
      "Current prediction:  61.235870361328125 \n",
      "\n",
      "Iteration 3616, Loss: 36.95098876953125, L1: 10.517011642456055, L3: 26.433979034423828\n",
      "Current prediction:  61.23833084106445 \n",
      "\n",
      "Iteration 3617, Loss: 36.991207122802734, L1: 10.514924049377441, L3: 26.476282119750977\n",
      "Current prediction:  61.2397346496582 \n",
      "\n",
      "Iteration 3618, Loss: 36.502532958984375, L1: 10.51373291015625, L3: 25.988798141479492\n",
      "Current prediction:  61.24159622192383 \n",
      "\n",
      "Iteration 3619, Loss: 37.27665328979492, L1: 10.512154579162598, L3: 26.764497756958008\n",
      "Current prediction:  61.24186325073242 \n",
      "\n",
      "Iteration 3620, Loss: 36.25072479248047, L1: 10.511932373046875, L3: 25.738794326782227\n",
      "Current prediction:  61.24186706542969 \n",
      "\n",
      "Iteration 3621, Loss: 36.178123474121094, L1: 10.511924743652344, L3: 25.666196823120117\n",
      "Current prediction:  61.24052810668945 \n",
      "\n",
      "Iteration 3622, Loss: 36.39925003051758, L1: 10.513062477111816, L3: 25.886188507080078\n",
      "Current prediction:  61.23873519897461 \n",
      "\n",
      "Iteration 3623, Loss: 36.12187194824219, L1: 10.514577865600586, L3: 25.60729217529297\n",
      "Current prediction:  61.239540100097656 \n",
      "\n",
      "Iteration 3624, Loss: 35.93891906738281, L1: 10.513894081115723, L3: 25.425024032592773\n",
      "Current prediction:  61.238975524902344 \n",
      "\n",
      "Iteration 3625, Loss: 36.254173278808594, L1: 10.51437759399414, L3: 25.739795684814453\n",
      "Current prediction:  61.23644256591797 \n",
      "\n",
      "Iteration 3626, Loss: 36.175262451171875, L1: 10.516525268554688, L3: 25.658735275268555\n",
      "Current prediction:  61.234920501708984 \n",
      "\n",
      "Iteration 3627, Loss: 36.00177764892578, L1: 10.517814636230469, L3: 25.483963012695312\n",
      "Current prediction:  61.23588943481445 \n",
      "\n",
      "Iteration 3628, Loss: 36.37179946899414, L1: 10.516993522644043, L3: 25.854806900024414\n",
      "Current prediction:  61.23528289794922 \n",
      "\n",
      "Iteration 3629, Loss: 35.91580581665039, L1: 10.51750659942627, L3: 25.398300170898438\n",
      "Current prediction:  61.23371887207031 \n",
      "\n",
      "Iteration 3630, Loss: 37.184104919433594, L1: 10.518832206726074, L3: 26.665271759033203\n",
      "Current prediction:  61.23050308227539 \n",
      "\n",
      "Iteration 3631, Loss: 36.1173210144043, L1: 10.521563529968262, L3: 25.59575653076172\n",
      "Current prediction:  61.22739028930664 \n",
      "\n",
      "Iteration 3632, Loss: 36.683563232421875, L1: 10.524200439453125, L3: 26.159360885620117\n",
      "Current prediction:  61.22296142578125 \n",
      "\n",
      "Iteration 3633, Loss: 36.794708251953125, L1: 10.52795696258545, L3: 26.26675033569336\n",
      "Current prediction:  61.217350006103516 \n",
      "\n",
      "Iteration 3634, Loss: 37.006431579589844, L1: 10.532712936401367, L3: 26.473718643188477\n",
      "Current prediction:  61.215057373046875 \n",
      "\n",
      "Iteration 3635, Loss: 37.620567321777344, L1: 10.53465747833252, L3: 27.085908889770508\n",
      "Current prediction:  61.21125793457031 \n",
      "\n",
      "Iteration 3636, Loss: 36.33698654174805, L1: 10.537879943847656, L3: 25.79910659790039\n",
      "Current prediction:  61.210269927978516 \n",
      "\n",
      "Iteration 3637, Loss: 36.466827392578125, L1: 10.53872013092041, L3: 25.9281063079834\n",
      "Current prediction:  61.20990753173828 \n",
      "\n",
      "Iteration 3638, Loss: 35.20085144042969, L1: 10.53902530670166, L3: 24.661827087402344\n",
      "Current prediction:  61.21147155761719 \n",
      "\n",
      "Iteration 3639, Loss: 35.55728530883789, L1: 10.537705421447754, L3: 25.01957893371582\n",
      "Current prediction:  61.214805603027344 \n",
      "\n",
      "Iteration 3640, Loss: 36.38004684448242, L1: 10.53487777709961, L3: 25.845169067382812\n",
      "Current prediction:  61.216796875 \n",
      "\n",
      "Iteration 3641, Loss: 36.63957595825195, L1: 10.533181190490723, L3: 26.106393814086914\n",
      "Current prediction:  61.2191162109375 \n",
      "\n",
      "Iteration 3642, Loss: 36.654930114746094, L1: 10.531213760375977, L3: 26.123716354370117\n",
      "Current prediction:  61.22137451171875 \n",
      "\n",
      "Iteration 3643, Loss: 36.4937744140625, L1: 10.529304504394531, L3: 25.9644718170166\n",
      "Current prediction:  61.222434997558594 \n",
      "\n",
      "Iteration 3644, Loss: 37.60069274902344, L1: 10.528398513793945, L3: 27.072294235229492\n",
      "Current prediction:  61.22206115722656 \n",
      "\n",
      "Iteration 3645, Loss: 37.34623718261719, L1: 10.528719902038574, L3: 26.81751823425293\n",
      "Current prediction:  61.220054626464844 \n",
      "\n",
      "Iteration 3646, Loss: 35.73368835449219, L1: 10.530418395996094, L3: 25.20326805114746\n",
      "Current prediction:  61.21879196166992 \n",
      "\n",
      "Iteration 3647, Loss: 36.66012954711914, L1: 10.531495094299316, L3: 26.128633499145508\n",
      "Current prediction:  61.21681594848633 \n",
      "\n",
      "Iteration 3648, Loss: 35.8545036315918, L1: 10.533173561096191, L3: 25.321331024169922\n",
      "Current prediction:  61.213993072509766 \n",
      "\n",
      "Iteration 3649, Loss: 37.032936096191406, L1: 10.535558700561523, L3: 26.497377395629883\n",
      "Current prediction:  61.21285629272461 \n",
      "\n",
      "Iteration 3650, Loss: 35.94088363647461, L1: 10.536526679992676, L3: 25.40435791015625\n",
      "Current prediction:  61.21101760864258 \n",
      "\n",
      "Iteration 3651, Loss: 37.02972412109375, L1: 10.5380859375, L3: 26.491640090942383\n",
      "Current prediction:  61.2074089050293 \n",
      "\n",
      "Iteration 3652, Loss: 36.17353820800781, L1: 10.541146278381348, L3: 25.63239288330078\n",
      "Current prediction:  61.203941345214844 \n",
      "\n",
      "Iteration 3653, Loss: 36.25931167602539, L1: 10.544081687927246, L3: 25.715229034423828\n",
      "Current prediction:  61.20064163208008 \n",
      "\n",
      "Iteration 3654, Loss: 36.77948760986328, L1: 10.546881675720215, L3: 26.232606887817383\n",
      "Current prediction:  61.19755172729492 \n",
      "\n",
      "Iteration 3655, Loss: 35.89037322998047, L1: 10.549504280090332, L3: 25.34086799621582\n",
      "Current prediction:  61.19524383544922 \n",
      "\n",
      "Iteration 3656, Loss: 36.36016082763672, L1: 10.551462173461914, L3: 25.808700561523438\n",
      "Current prediction:  61.19485855102539 \n",
      "\n",
      "Iteration 3657, Loss: 37.11304473876953, L1: 10.551787376403809, L3: 26.561256408691406\n",
      "Current prediction:  61.19429016113281 \n",
      "\n",
      "Iteration 3658, Loss: 36.013877868652344, L1: 10.552267074584961, L3: 25.46160888671875\n",
      "Current prediction:  61.19382858276367 \n",
      "\n",
      "Iteration 3659, Loss: 35.99968338012695, L1: 10.552657127380371, L3: 25.447025299072266\n",
      "Current prediction:  61.19537353515625 \n",
      "\n",
      "Iteration 3660, Loss: 36.78695297241211, L1: 10.551348686218262, L3: 26.23560333251953\n",
      "Current prediction:  61.196693420410156 \n",
      "\n",
      "Iteration 3661, Loss: 36.99918746948242, L1: 10.55023193359375, L3: 26.448955535888672\n",
      "Current prediction:  61.19646453857422 \n",
      "\n",
      "Iteration 3662, Loss: 36.36399459838867, L1: 10.550424575805664, L3: 25.813570022583008\n",
      "Current prediction:  61.195396423339844 \n",
      "\n",
      "Iteration 3663, Loss: 36.6036491394043, L1: 10.551331520080566, L3: 26.052318572998047\n",
      "Current prediction:  61.19569396972656 \n",
      "\n",
      "Iteration 3664, Loss: 36.35151672363281, L1: 10.55107307434082, L3: 25.800443649291992\n",
      "Current prediction:  61.19773864746094 \n",
      "\n",
      "Iteration 3665, Loss: 36.177677154541016, L1: 10.549345016479492, L3: 25.628332138061523\n",
      "Current prediction:  61.2015266418457 \n",
      "\n",
      "Iteration 3666, Loss: 36.738616943359375, L1: 10.546133995056152, L3: 26.192481994628906\n",
      "Current prediction:  61.20473861694336 \n",
      "\n",
      "Iteration 3667, Loss: 36.747493743896484, L1: 10.543412208557129, L3: 26.204082489013672\n",
      "Current prediction:  61.207481384277344 \n",
      "\n",
      "Iteration 3668, Loss: 37.82734298706055, L1: 10.541086196899414, L3: 27.286256790161133\n",
      "Current prediction:  61.2091064453125 \n",
      "\n",
      "Iteration 3669, Loss: 36.14799499511719, L1: 10.539704322814941, L3: 25.608291625976562\n",
      "Current prediction:  61.211002349853516 \n",
      "\n",
      "Iteration 3670, Loss: 36.87607192993164, L1: 10.538100242614746, L3: 26.33797264099121\n",
      "Current prediction:  61.21162033081055 \n",
      "\n",
      "Iteration 3671, Loss: 35.94110107421875, L1: 10.53757381439209, L3: 25.403526306152344\n",
      "Current prediction:  61.21294021606445 \n",
      "\n",
      "Iteration 3672, Loss: 36.23268127441406, L1: 10.53645133972168, L3: 25.696231842041016\n",
      "Current prediction:  61.21430587768555 \n",
      "\n",
      "Iteration 3673, Loss: 37.19403839111328, L1: 10.535293579101562, L3: 26.658742904663086\n",
      "Current prediction:  61.214962005615234 \n",
      "\n",
      "Iteration 3674, Loss: 36.219146728515625, L1: 10.534743309020996, L3: 25.684402465820312\n",
      "Current prediction:  61.217796325683594 \n",
      "\n",
      "Iteration 3675, Loss: 37.17826843261719, L1: 10.532341003417969, L3: 26.645925521850586\n",
      "Current prediction:  61.218971252441406 \n",
      "\n",
      "Iteration 3676, Loss: 36.09849548339844, L1: 10.531342506408691, L3: 25.567153930664062\n",
      "Current prediction:  61.22130584716797 \n",
      "\n",
      "Iteration 3677, Loss: 36.26155090332031, L1: 10.529365539550781, L3: 25.732187271118164\n",
      "Current prediction:  61.223506927490234 \n",
      "\n",
      "Iteration 3678, Loss: 37.231300354003906, L1: 10.527494430541992, L3: 26.703805923461914\n",
      "Current prediction:  61.22477340698242 \n",
      "\n",
      "Iteration 3679, Loss: 36.358612060546875, L1: 10.526418685913086, L3: 25.832195281982422\n",
      "Current prediction:  61.22641372680664 \n",
      "\n",
      "Iteration 3680, Loss: 37.082889556884766, L1: 10.525031089782715, L3: 26.557859420776367\n",
      "Current prediction:  61.228267669677734 \n",
      "\n",
      "Iteration 3681, Loss: 36.43543243408203, L1: 10.523454666137695, L3: 25.911975860595703\n",
      "Current prediction:  61.23072052001953 \n",
      "\n",
      "Iteration 3682, Loss: 35.25320053100586, L1: 10.521376609802246, L3: 24.731822967529297\n",
      "Current prediction:  61.235801696777344 \n",
      "\n",
      "Iteration 3683, Loss: 36.72178268432617, L1: 10.517067909240723, L3: 26.204715728759766\n",
      "Current prediction:  61.239463806152344 \n",
      "\n",
      "Iteration 3684, Loss: 36.0564079284668, L1: 10.513962745666504, L3: 25.542444229125977\n",
      "Current prediction:  61.242401123046875 \n",
      "\n",
      "Iteration 3685, Loss: 37.0303955078125, L1: 10.511474609375, L3: 26.518918991088867\n",
      "Current prediction:  61.245235443115234 \n",
      "\n",
      "Iteration 3686, Loss: 35.76045227050781, L1: 10.509066581726074, L3: 25.251386642456055\n",
      "Current prediction:  61.24533462524414 \n",
      "\n",
      "Iteration 3687, Loss: 35.84775161743164, L1: 10.508980751037598, L3: 25.33877182006836\n",
      "Current prediction:  61.24495315551758 \n",
      "\n",
      "Iteration 3688, Loss: 36.386165618896484, L1: 10.509312629699707, L3: 25.87685203552246\n",
      "Current prediction:  61.24529266357422 \n",
      "\n",
      "Iteration 3689, Loss: 37.06926345825195, L1: 10.50901985168457, L3: 26.560243606567383\n",
      "Current prediction:  61.244590759277344 \n",
      "\n",
      "Iteration 3690, Loss: 35.90727996826172, L1: 10.509613990783691, L3: 25.397666931152344\n",
      "Current prediction:  61.24361038208008 \n",
      "\n",
      "Iteration 3691, Loss: 36.5450553894043, L1: 10.510443687438965, L3: 26.03461265563965\n",
      "Current prediction:  61.24284362792969 \n",
      "\n",
      "Iteration 3692, Loss: 36.37846374511719, L1: 10.511094093322754, L3: 25.867368698120117\n",
      "Current prediction:  61.24336242675781 \n",
      "\n",
      "Iteration 3693, Loss: 36.21181106567383, L1: 10.510651588439941, L3: 25.701160430908203\n",
      "Current prediction:  61.243812561035156 \n",
      "\n",
      "Iteration 3694, Loss: 36.58882522583008, L1: 10.510273933410645, L3: 26.07855224609375\n",
      "Current prediction:  61.24369812011719 \n",
      "\n",
      "Iteration 3695, Loss: 37.04679870605469, L1: 10.510375022888184, L3: 26.53642463684082\n",
      "Current prediction:  61.242652893066406 \n",
      "\n",
      "Iteration 3696, Loss: 36.524925231933594, L1: 10.51125717163086, L3: 26.013668060302734\n",
      "Current prediction:  61.2418098449707 \n",
      "\n",
      "Iteration 3697, Loss: 37.40528869628906, L1: 10.51197338104248, L3: 26.8933162689209\n",
      "Current prediction:  61.241127014160156 \n",
      "\n",
      "Iteration 3698, Loss: 36.14516830444336, L1: 10.512553215026855, L3: 25.63261604309082\n",
      "Current prediction:  61.23735809326172 \n",
      "\n",
      "Iteration 3699, Loss: 36.57316970825195, L1: 10.515748023986816, L3: 26.057422637939453\n",
      "Current prediction:  61.2342643737793 \n",
      "\n",
      "Iteration 3700, Loss: 36.43244934082031, L1: 10.51837158203125, L3: 25.914077758789062\n",
      "Current prediction:  61.23122024536133 \n",
      "\n",
      "Iteration 3701, Loss: 36.28580093383789, L1: 10.52095890045166, L3: 25.764841079711914\n",
      "Current prediction:  61.22914123535156 \n",
      "\n",
      "Iteration 3702, Loss: 35.89942932128906, L1: 10.522713661193848, L3: 25.3767147064209\n",
      "Current prediction:  61.22837829589844 \n",
      "\n",
      "Iteration 3703, Loss: 36.69585037231445, L1: 10.523364067077637, L3: 26.172487258911133\n",
      "Current prediction:  61.228477478027344 \n",
      "\n",
      "Iteration 3704, Loss: 37.1644287109375, L1: 10.523283958435059, L3: 26.641143798828125\n",
      "Current prediction:  61.2274284362793 \n",
      "\n",
      "Iteration 3705, Loss: 36.37209701538086, L1: 10.52417278289795, L3: 25.847923278808594\n",
      "Current prediction:  61.22800064086914 \n",
      "\n",
      "Iteration 3706, Loss: 36.95972442626953, L1: 10.523681640625, L3: 26.436044692993164\n",
      "Current prediction:  61.22692108154297 \n",
      "\n",
      "Iteration 3707, Loss: 36.02904510498047, L1: 10.524598121643066, L3: 25.504446029663086\n",
      "Current prediction:  61.22296142578125 \n",
      "\n",
      "Iteration 3708, Loss: 36.9507942199707, L1: 10.527955055236816, L3: 26.42283821105957\n",
      "Current prediction:  61.21980667114258 \n",
      "\n",
      "Iteration 3709, Loss: 36.07154846191406, L1: 10.530630111694336, L3: 25.540916442871094\n",
      "Current prediction:  61.21584701538086 \n",
      "\n",
      "Iteration 3710, Loss: 37.340843200683594, L1: 10.533990859985352, L3: 26.806852340698242\n",
      "Current prediction:  61.210235595703125 \n",
      "\n",
      "Iteration 3711, Loss: 37.72841262817383, L1: 10.53874683380127, L3: 27.189664840698242\n",
      "Current prediction:  61.20589065551758 \n",
      "\n",
      "Iteration 3712, Loss: 36.482421875, L1: 10.542434692382812, L3: 25.939985275268555\n",
      "Current prediction:  61.20374298095703 \n",
      "\n",
      "Iteration 3713, Loss: 35.96771240234375, L1: 10.54425048828125, L3: 25.423460006713867\n",
      "Current prediction:  61.200462341308594 \n",
      "\n",
      "Iteration 3714, Loss: 37.24799728393555, L1: 10.547028541564941, L3: 26.700969696044922\n",
      "Current prediction:  61.196006774902344 \n",
      "\n",
      "Iteration 3715, Loss: 35.16069412231445, L1: 10.550811767578125, L3: 24.609882354736328\n",
      "Current prediction:  61.19224548339844 \n",
      "\n",
      "Iteration 3716, Loss: 36.96279525756836, L1: 10.553999900817871, L3: 26.408796310424805\n",
      "Current prediction:  61.18934631347656 \n",
      "\n",
      "Iteration 3717, Loss: 37.24110794067383, L1: 10.556456565856934, L3: 26.684650421142578\n",
      "Current prediction:  61.185516357421875 \n",
      "\n",
      "Iteration 3718, Loss: 37.05318832397461, L1: 10.559706687927246, L3: 26.493480682373047\n",
      "Current prediction:  61.184356689453125 \n",
      "\n",
      "Iteration 3719, Loss: 37.42280197143555, L1: 10.560687065124512, L3: 26.86211585998535\n",
      "Current prediction:  61.18282699584961 \n",
      "\n",
      "Iteration 3720, Loss: 36.13895797729492, L1: 10.56198787689209, L3: 25.57697105407715\n",
      "Current prediction:  61.180789947509766 \n",
      "\n",
      "Iteration 3721, Loss: 36.37626266479492, L1: 10.563712120056152, L3: 25.812551498413086\n",
      "Current prediction:  61.180538177490234 \n",
      "\n",
      "Iteration 3722, Loss: 36.88345718383789, L1: 10.56392765045166, L3: 26.319530487060547\n",
      "Current prediction:  61.18077087402344 \n",
      "\n",
      "Iteration 3723, Loss: 35.520362854003906, L1: 10.563730239868164, L3: 24.95663070678711\n",
      "Current prediction:  61.18220901489258 \n",
      "\n",
      "Iteration 3724, Loss: 36.00608825683594, L1: 10.562508583068848, L3: 25.443578720092773\n",
      "Current prediction:  61.1846923828125 \n",
      "\n",
      "Iteration 3725, Loss: 36.96942901611328, L1: 10.560404777526855, L3: 26.409025192260742\n",
      "Current prediction:  61.18588638305664 \n",
      "\n",
      "Iteration 3726, Loss: 36.94321823120117, L1: 10.559388160705566, L3: 26.383831024169922\n",
      "Current prediction:  61.18617630004883 \n",
      "\n",
      "Iteration 3727, Loss: 36.213626861572266, L1: 10.559149742126465, L3: 25.654478073120117\n",
      "Current prediction:  61.18824768066406 \n",
      "\n",
      "Iteration 3728, Loss: 36.60313415527344, L1: 10.557394981384277, L3: 26.045740127563477\n",
      "Current prediction:  61.19086456298828 \n",
      "\n",
      "Iteration 3729, Loss: 36.0882682800293, L1: 10.555169105529785, L3: 25.533100128173828\n",
      "Current prediction:  61.19398880004883 \n",
      "\n",
      "Iteration 3730, Loss: 37.201080322265625, L1: 10.55252456665039, L3: 26.6485538482666\n",
      "Current prediction:  61.19488525390625 \n",
      "\n",
      "Iteration 3731, Loss: 37.536006927490234, L1: 10.551763534545898, L3: 26.984243392944336\n",
      "Current prediction:  61.1944694519043 \n",
      "\n",
      "Iteration 3732, Loss: 36.671119689941406, L1: 10.552114486694336, L3: 26.119007110595703\n",
      "Current prediction:  61.19366455078125 \n",
      "\n",
      "Iteration 3733, Loss: 35.949790954589844, L1: 10.552797317504883, L3: 25.396991729736328\n",
      "Current prediction:  61.19394302368164 \n",
      "\n",
      "Iteration 3734, Loss: 36.476158142089844, L1: 10.552558898925781, L3: 25.92359733581543\n",
      "Current prediction:  61.194847106933594 \n",
      "\n",
      "Iteration 3735, Loss: 36.45028305053711, L1: 10.551793098449707, L3: 25.89849090576172\n",
      "Current prediction:  61.19639587402344 \n",
      "\n",
      "Iteration 3736, Loss: 36.70384979248047, L1: 10.550483703613281, L3: 26.153366088867188\n",
      "Current prediction:  61.19756317138672 \n",
      "\n",
      "Iteration 3737, Loss: 36.46470642089844, L1: 10.54949951171875, L3: 25.915206909179688\n",
      "Current prediction:  61.20041275024414 \n",
      "\n",
      "Iteration 3738, Loss: 36.423492431640625, L1: 10.547075271606445, L3: 25.87641716003418\n",
      "Current prediction:  61.202335357666016 \n",
      "\n",
      "Iteration 3739, Loss: 36.63664245605469, L1: 10.545450210571289, L3: 26.0911922454834\n",
      "Current prediction:  61.204044342041016 \n",
      "\n",
      "Iteration 3740, Loss: 37.46555709838867, L1: 10.543999671936035, L3: 26.92155647277832\n",
      "Current prediction:  61.20445251464844 \n",
      "\n",
      "Iteration 3741, Loss: 35.867637634277344, L1: 10.543654441833496, L3: 25.32398223876953\n",
      "Current prediction:  61.20634078979492 \n",
      "\n",
      "Iteration 3742, Loss: 35.41863250732422, L1: 10.5420503616333, L3: 24.8765811920166\n",
      "Current prediction:  61.20855712890625 \n",
      "\n",
      "Iteration 3743, Loss: 37.09830856323242, L1: 10.540175437927246, L3: 26.55813217163086\n",
      "Current prediction:  61.21061706542969 \n",
      "\n",
      "Iteration 3744, Loss: 36.03266906738281, L1: 10.538419723510742, L3: 25.49424934387207\n",
      "Current prediction:  61.21382141113281 \n",
      "\n",
      "Iteration 3745, Loss: 36.287353515625, L1: 10.535709381103516, L3: 25.751644134521484\n",
      "Current prediction:  61.21714401245117 \n",
      "\n",
      "Iteration 3746, Loss: 37.203678131103516, L1: 10.532889366149902, L3: 26.670787811279297\n",
      "Current prediction:  61.218509674072266 \n",
      "\n",
      "Iteration 3747, Loss: 36.545799255371094, L1: 10.531736373901367, L3: 26.01406478881836\n",
      "Current prediction:  61.220767974853516 \n",
      "\n",
      "Iteration 3748, Loss: 37.38518524169922, L1: 10.529817581176758, L3: 26.855369567871094\n",
      "Current prediction:  61.22114944458008 \n",
      "\n",
      "Iteration 3749, Loss: 36.20259094238281, L1: 10.529494285583496, L3: 25.673097610473633\n",
      "Current prediction:  61.221683502197266 \n",
      "\n",
      "Iteration 3750, Loss: 36.097328186035156, L1: 10.529045104980469, L3: 25.56828498840332\n",
      "Current prediction:  61.223426818847656 \n",
      "\n",
      "Iteration 3751, Loss: 36.58491134643555, L1: 10.527558326721191, L3: 26.05735206604004\n",
      "Current prediction:  61.22531509399414 \n",
      "\n",
      "Iteration 3752, Loss: 36.569278717041016, L1: 10.525962829589844, L3: 26.043315887451172\n",
      "Current prediction:  61.22560119628906 \n",
      "\n",
      "Iteration 3753, Loss: 36.929931640625, L1: 10.525721549987793, L3: 26.40420913696289\n",
      "Current prediction:  61.22475051879883 \n",
      "\n",
      "Iteration 3754, Loss: 37.222633361816406, L1: 10.526444435119629, L3: 26.69618797302246\n",
      "Current prediction:  61.22415542602539 \n",
      "\n",
      "Iteration 3755, Loss: 35.7818717956543, L1: 10.526944160461426, L3: 25.254926681518555\n",
      "Current prediction:  61.22356033325195 \n",
      "\n",
      "Iteration 3756, Loss: 36.284019470214844, L1: 10.527450561523438, L3: 25.756567001342773\n",
      "Current prediction:  61.222938537597656 \n",
      "\n",
      "Iteration 3757, Loss: 36.76686477661133, L1: 10.527975082397461, L3: 26.238889694213867\n",
      "Current prediction:  61.22227478027344 \n",
      "\n",
      "Iteration 3758, Loss: 37.38768005371094, L1: 10.52853775024414, L3: 26.859140396118164\n",
      "Current prediction:  61.22321701049805 \n",
      "\n",
      "Iteration 3759, Loss: 36.780025482177734, L1: 10.527739524841309, L3: 26.25228500366211\n",
      "Current prediction:  61.223052978515625 \n",
      "\n",
      "Iteration 3760, Loss: 36.927425384521484, L1: 10.527881622314453, L3: 26.39954376220703\n",
      "Current prediction:  61.22166061401367 \n",
      "\n",
      "Iteration 3761, Loss: 37.23958969116211, L1: 10.529057502746582, L3: 26.71053123474121\n",
      "Current prediction:  61.21921157836914 \n",
      "\n",
      "Iteration 3762, Loss: 35.86133575439453, L1: 10.531136512756348, L3: 25.330198287963867\n",
      "Current prediction:  61.218772888183594 \n",
      "\n",
      "Iteration 3763, Loss: 36.339237213134766, L1: 10.531511306762695, L3: 25.80772590637207\n",
      "Current prediction:  61.219329833984375 \n",
      "\n",
      "Iteration 3764, Loss: 36.03610610961914, L1: 10.53103256225586, L3: 25.50507354736328\n",
      "Current prediction:  61.2197265625 \n",
      "\n",
      "Iteration 3765, Loss: 36.07597732543945, L1: 10.530701637268066, L3: 25.545276641845703\n",
      "Current prediction:  61.21927261352539 \n",
      "\n",
      "Iteration 3766, Loss: 36.8669548034668, L1: 10.531086921691895, L3: 26.33586883544922\n",
      "Current prediction:  61.2177734375 \n",
      "\n",
      "Iteration 3767, Loss: 35.3326416015625, L1: 10.532356262207031, L3: 24.800283432006836\n",
      "Current prediction:  61.21831512451172 \n",
      "\n",
      "Iteration 3768, Loss: 36.11256408691406, L1: 10.531895637512207, L3: 25.580669403076172\n",
      "Current prediction:  61.21942138671875 \n",
      "\n",
      "Iteration 3769, Loss: 37.02370834350586, L1: 10.530959129333496, L3: 26.49275016784668\n",
      "Current prediction:  61.21969223022461 \n",
      "\n",
      "Iteration 3770, Loss: 37.17779541015625, L1: 10.530729293823242, L3: 26.64706802368164\n",
      "Current prediction:  61.22055435180664 \n",
      "\n",
      "Iteration 3771, Loss: 35.37109375, L1: 10.529998779296875, L3: 24.841093063354492\n",
      "Current prediction:  61.222496032714844 \n",
      "\n",
      "Iteration 3772, Loss: 35.82516860961914, L1: 10.528351783752441, L3: 25.296815872192383\n",
      "Current prediction:  61.22557830810547 \n",
      "\n",
      "Iteration 3773, Loss: 36.711143493652344, L1: 10.525738716125488, L3: 26.18540382385254\n",
      "Current prediction:  61.226802825927734 \n",
      "\n",
      "Iteration 3774, Loss: 37.71192169189453, L1: 10.524697303771973, L3: 27.187225341796875\n",
      "Current prediction:  61.2254524230957 \n",
      "\n",
      "Iteration 3775, Loss: 37.102840423583984, L1: 10.525839805603027, L3: 26.57699966430664\n",
      "Current prediction:  61.224647521972656 \n",
      "\n",
      "Iteration 3776, Loss: 36.46540451049805, L1: 10.52652645111084, L3: 25.938879013061523\n",
      "Current prediction:  61.222862243652344 \n",
      "\n",
      "Iteration 3777, Loss: 37.17516326904297, L1: 10.528044700622559, L3: 26.647119522094727\n",
      "Current prediction:  61.21939468383789 \n",
      "\n",
      "Iteration 3778, Loss: 36.13340759277344, L1: 10.530981063842773, L3: 25.602426528930664\n",
      "Current prediction:  61.215293884277344 \n",
      "\n",
      "Iteration 3779, Loss: 36.295623779296875, L1: 10.534455299377441, L3: 25.76116943359375\n",
      "Current prediction:  61.21177291870117 \n",
      "\n",
      "Iteration 3780, Loss: 36.04942321777344, L1: 10.537446022033691, L3: 25.511978149414062\n",
      "Current prediction:  61.20856475830078 \n",
      "\n",
      "Iteration 3781, Loss: 36.567054748535156, L1: 10.540173530578613, L3: 26.026880264282227\n",
      "Current prediction:  61.20539093017578 \n",
      "\n",
      "Iteration 3782, Loss: 36.437889099121094, L1: 10.542858123779297, L3: 25.895030975341797\n",
      "Current prediction:  61.20232391357422 \n",
      "\n",
      "Iteration 3783, Loss: 35.624786376953125, L1: 10.545455932617188, L3: 25.079330444335938\n",
      "Current prediction:  61.2015266418457 \n",
      "\n",
      "Iteration 3784, Loss: 36.36030197143555, L1: 10.546130180358887, L3: 25.814172744750977\n",
      "Current prediction:  61.20075988769531 \n",
      "\n",
      "Iteration 3785, Loss: 36.636451721191406, L1: 10.546782493591309, L3: 26.089670181274414\n",
      "Current prediction:  61.19940185546875 \n",
      "\n",
      "Iteration 3786, Loss: 36.903099060058594, L1: 10.547935485839844, L3: 26.355161666870117\n",
      "Current prediction:  61.196990966796875 \n",
      "\n",
      "Iteration 3787, Loss: 37.02727127075195, L1: 10.549980163574219, L3: 26.477291107177734\n",
      "Current prediction:  61.193458557128906 \n",
      "\n",
      "Iteration 3788, Loss: 37.80481719970703, L1: 10.552973747253418, L3: 27.25184440612793\n",
      "Current prediction:  61.1920166015625 \n",
      "\n",
      "Iteration 3789, Loss: 36.1185302734375, L1: 10.554194450378418, L3: 25.5643367767334\n",
      "Current prediction:  61.19377899169922 \n",
      "\n",
      "Iteration 3790, Loss: 36.44365692138672, L1: 10.552702903747559, L3: 25.890953063964844\n",
      "Current prediction:  61.196800231933594 \n",
      "\n",
      "Iteration 3791, Loss: 36.71757125854492, L1: 10.550139427185059, L3: 26.167430877685547\n",
      "Current prediction:  61.199058532714844 \n",
      "\n",
      "Iteration 3792, Loss: 37.51997375488281, L1: 10.548226356506348, L3: 26.97174644470215\n",
      "Current prediction:  61.20100021362305 \n",
      "\n",
      "Iteration 3793, Loss: 36.559200286865234, L1: 10.546574592590332, L3: 26.012624740600586\n",
      "Current prediction:  61.20281982421875 \n",
      "\n",
      "Iteration 3794, Loss: 36.75444030761719, L1: 10.545032501220703, L3: 26.20940589904785\n",
      "Current prediction:  61.20367431640625 \n",
      "\n",
      "Iteration 3795, Loss: 36.98472595214844, L1: 10.544312477111816, L3: 26.440412521362305\n",
      "Current prediction:  61.20359420776367 \n",
      "\n",
      "Iteration 3796, Loss: 37.134403228759766, L1: 10.5443754196167, L3: 26.590028762817383\n",
      "Current prediction:  61.201236724853516 \n",
      "\n",
      "Iteration 3797, Loss: 36.24128341674805, L1: 10.546382904052734, L3: 25.694900512695312\n",
      "Current prediction:  61.19813919067383 \n",
      "\n",
      "Iteration 3798, Loss: 37.403297424316406, L1: 10.548998832702637, L3: 26.854299545288086\n",
      "Current prediction:  61.19536209106445 \n",
      "\n",
      "Iteration 3799, Loss: 37.315059661865234, L1: 10.551360130310059, L3: 26.763700485229492\n",
      "Current prediction:  61.191802978515625 \n",
      "\n",
      "Iteration 3800, Loss: 36.98985290527344, L1: 10.554376602172852, L3: 26.435474395751953\n",
      "Current prediction:  61.18754196166992 \n",
      "\n",
      "Iteration 3801, Loss: 36.41625213623047, L1: 10.557988166809082, L3: 25.85826301574707\n",
      "Current prediction:  61.183197021484375 \n",
      "\n",
      "Iteration 3802, Loss: 36.71562194824219, L1: 10.561676979064941, L3: 26.153945922851562\n",
      "Current prediction:  61.17897415161133 \n",
      "\n",
      "Iteration 3803, Loss: 36.44810485839844, L1: 10.565248489379883, L3: 25.882858276367188\n",
      "Current prediction:  61.17597961425781 \n",
      "\n",
      "Iteration 3804, Loss: 37.14552688598633, L1: 10.567793846130371, L3: 26.57773208618164\n",
      "Current prediction:  61.173465728759766 \n",
      "\n",
      "Iteration 3805, Loss: 36.466651916503906, L1: 10.569921493530273, L3: 25.896728515625\n",
      "Current prediction:  61.173370361328125 \n",
      "\n",
      "Iteration 3806, Loss: 36.79402542114258, L1: 10.5700044631958, L3: 26.224021911621094\n",
      "Current prediction:  61.17662811279297 \n",
      "\n",
      "Iteration 3807, Loss: 36.13826370239258, L1: 10.56723690032959, L3: 25.571025848388672\n",
      "Current prediction:  61.18185806274414 \n",
      "\n",
      "Iteration 3808, Loss: 36.29201889038086, L1: 10.562807083129883, L3: 25.729211807250977\n",
      "Current prediction:  61.18519973754883 \n",
      "\n",
      "Iteration 3809, Loss: 36.57823181152344, L1: 10.559974670410156, L3: 26.018259048461914\n",
      "Current prediction:  61.1900520324707 \n",
      "\n",
      "Iteration 3810, Loss: 36.576107025146484, L1: 10.555862426757812, L3: 26.020244598388672\n",
      "Current prediction:  61.19551086425781 \n",
      "\n",
      "Iteration 3811, Loss: 34.953147888183594, L1: 10.55123233795166, L3: 24.401914596557617\n",
      "Current prediction:  61.202301025390625 \n",
      "\n",
      "Iteration 3812, Loss: 37.04127883911133, L1: 10.545473098754883, L3: 26.495805740356445\n",
      "Current prediction:  61.20723342895508 \n",
      "\n",
      "Iteration 3813, Loss: 36.36612319946289, L1: 10.541289329528809, L3: 25.8248348236084\n",
      "Current prediction:  61.21248245239258 \n",
      "\n",
      "Iteration 3814, Loss: 36.3702507019043, L1: 10.536839485168457, L3: 25.833410263061523\n",
      "Current prediction:  61.21916580200195 \n",
      "\n",
      "Iteration 3815, Loss: 35.86747360229492, L1: 10.531172752380371, L3: 25.336301803588867\n",
      "Current prediction:  61.225990295410156 \n",
      "\n",
      "Iteration 3816, Loss: 36.55274963378906, L1: 10.525388717651367, L3: 26.027359008789062\n",
      "Current prediction:  61.2325325012207 \n",
      "\n",
      "Iteration 3817, Loss: 36.82946014404297, L1: 10.519843101501465, L3: 26.30961799621582\n",
      "Current prediction:  61.23831558227539 \n",
      "\n",
      "Iteration 3818, Loss: 36.162479400634766, L1: 10.514935493469238, L3: 25.64754295349121\n",
      "Current prediction:  61.243446350097656 \n",
      "\n",
      "Iteration 3819, Loss: 37.02580261230469, L1: 10.510581970214844, L3: 26.515222549438477\n",
      "Current prediction:  61.24765396118164 \n",
      "\n",
      "Iteration 3820, Loss: 36.834251403808594, L1: 10.50701904296875, L3: 26.327234268188477\n",
      "Current prediction:  61.251888275146484 \n",
      "\n",
      "Iteration 3821, Loss: 35.61882400512695, L1: 10.50342845916748, L3: 25.115394592285156\n",
      "Current prediction:  61.25560760498047 \n",
      "\n",
      "Iteration 3822, Loss: 36.556514739990234, L1: 10.50026798248291, L3: 26.05624771118164\n",
      "Current prediction:  61.25910568237305 \n",
      "\n",
      "Iteration 3823, Loss: 36.4963264465332, L1: 10.49730110168457, L3: 25.999025344848633\n",
      "Current prediction:  61.26206970214844 \n",
      "\n",
      "Iteration 3824, Loss: 35.910179138183594, L1: 10.49478816986084, L3: 25.41539192199707\n",
      "Current prediction:  61.26213455200195 \n",
      "\n",
      "Iteration 3825, Loss: 35.76081848144531, L1: 10.523239135742188, L3: 25.237581253051758\n",
      "Current prediction:  61.2603874206543 \n",
      "\n",
      "Iteration 3826, Loss: 37.32795715332031, L1: 10.49621868133545, L3: 26.831737518310547\n",
      "Current prediction:  61.25820541381836 \n",
      "\n",
      "Iteration 3827, Loss: 36.940818786621094, L1: 10.498065948486328, L3: 26.442752838134766\n",
      "Current prediction:  61.2545051574707 \n",
      "\n",
      "Iteration 3828, Loss: 36.46847152709961, L1: 10.50120735168457, L3: 25.96726417541504\n",
      "Current prediction:  61.24953079223633 \n",
      "\n",
      "Iteration 3829, Loss: 37.618309020996094, L1: 10.50541877746582, L3: 27.11288833618164\n",
      "Current prediction:  61.24186706542969 \n",
      "\n",
      "Iteration 3830, Loss: 36.781768798828125, L1: 10.511926651000977, L3: 26.26984405517578\n",
      "Current prediction:  61.23346710205078 \n",
      "\n",
      "Iteration 3831, Loss: 36.780548095703125, L1: 10.519046783447266, L3: 26.261503219604492\n",
      "Current prediction:  61.22522735595703 \n",
      "\n",
      "Iteration 3832, Loss: 36.18335723876953, L1: 10.526031494140625, L3: 25.65732765197754\n",
      "Current prediction:  61.21784973144531 \n",
      "\n",
      "Iteration 3833, Loss: 37.158111572265625, L1: 10.532291412353516, L3: 26.625822067260742\n",
      "Current prediction:  61.209075927734375 \n",
      "\n",
      "Iteration 3834, Loss: 36.452369689941406, L1: 10.539726257324219, L3: 25.912643432617188\n",
      "Current prediction:  61.20302200317383 \n",
      "\n",
      "Iteration 3835, Loss: 37.157711029052734, L1: 10.544861793518066, L3: 26.612850189208984\n",
      "Current prediction:  61.1956672668457 \n",
      "\n",
      "Iteration 3836, Loss: 36.72297286987305, L1: 10.551100730895996, L3: 26.171871185302734\n",
      "Current prediction:  61.18947982788086 \n",
      "\n",
      "Iteration 3837, Loss: 35.599159240722656, L1: 10.556341171264648, L3: 25.042818069458008\n",
      "Current prediction:  61.185943603515625 \n",
      "\n",
      "Iteration 3838, Loss: 37.259586334228516, L1: 10.559342384338379, L3: 26.70024299621582\n",
      "Current prediction:  61.18285369873047 \n",
      "\n",
      "Iteration 3839, Loss: 35.94083786010742, L1: 10.561962127685547, L3: 25.378875732421875\n",
      "Current prediction:  61.178897857666016 \n",
      "\n",
      "Iteration 3840, Loss: 37.07255554199219, L1: 10.565314292907715, L3: 26.507240295410156\n",
      "Current prediction:  61.17619323730469 \n",
      "\n",
      "Iteration 3841, Loss: 36.44111633300781, L1: 10.567605018615723, L3: 25.873512268066406\n",
      "Current prediction:  61.17435073852539 \n",
      "\n",
      "Iteration 3842, Loss: 36.35369110107422, L1: 10.569174766540527, L3: 25.784517288208008\n",
      "Current prediction:  61.174869537353516 \n",
      "\n",
      "Iteration 3843, Loss: 37.281280517578125, L1: 10.568730354309082, L3: 26.712549209594727\n",
      "Current prediction:  61.17305374145508 \n",
      "\n",
      "Iteration 3844, Loss: 36.586978912353516, L1: 10.570268630981445, L3: 26.01671028137207\n",
      "Current prediction:  61.172306060791016 \n",
      "\n",
      "Iteration 3845, Loss: 37.020790100097656, L1: 10.570901870727539, L3: 26.449888229370117\n",
      "Current prediction:  61.17317199707031 \n",
      "\n",
      "Iteration 3846, Loss: 36.66506576538086, L1: 10.570170402526855, L3: 26.094894409179688\n",
      "Current prediction:  61.173248291015625 \n",
      "\n",
      "Iteration 3847, Loss: 36.46855163574219, L1: 10.570108413696289, L3: 25.898441314697266\n",
      "Current prediction:  61.17464065551758 \n",
      "\n",
      "Iteration 3848, Loss: 36.221588134765625, L1: 10.568925857543945, L3: 25.65266227722168\n",
      "Current prediction:  61.1786003112793 \n",
      "\n",
      "Iteration 3849, Loss: 36.712642669677734, L1: 10.565567970275879, L3: 26.147075653076172\n",
      "Current prediction:  61.180938720703125 \n",
      "\n",
      "Iteration 3850, Loss: 36.172847747802734, L1: 10.563586235046387, L3: 25.609262466430664\n",
      "Current prediction:  61.184879302978516 \n",
      "\n",
      "Iteration 3851, Loss: 36.510154724121094, L1: 10.560242652893066, L3: 25.949913024902344\n",
      "Current prediction:  61.18888854980469 \n",
      "\n",
      "Iteration 3852, Loss: 37.538787841796875, L1: 10.556844711303711, L3: 26.981943130493164\n",
      "Current prediction:  61.194026947021484 \n",
      "\n",
      "Iteration 3853, Loss: 36.648155212402344, L1: 10.55249309539795, L3: 26.09566307067871\n",
      "Current prediction:  61.199134826660156 \n",
      "\n",
      "Iteration 3854, Loss: 36.432411193847656, L1: 10.548164367675781, L3: 25.884244918823242\n",
      "Current prediction:  61.2034797668457 \n",
      "\n",
      "Iteration 3855, Loss: 37.057579040527344, L1: 10.544474601745605, L3: 26.513105392456055\n",
      "Current prediction:  61.2082405090332 \n",
      "\n",
      "Iteration 3856, Loss: 36.563499450683594, L1: 10.540437698364258, L3: 26.023059844970703\n",
      "Current prediction:  61.214107513427734 \n",
      "\n",
      "Iteration 3857, Loss: 36.711387634277344, L1: 10.535469055175781, L3: 26.17591667175293\n",
      "Current prediction:  61.217041015625 \n",
      "\n",
      "Iteration 3858, Loss: 36.16117858886719, L1: 10.532976150512695, L3: 25.628202438354492\n",
      "Current prediction:  61.22013473510742 \n",
      "\n",
      "Iteration 3859, Loss: 37.067909240722656, L1: 10.530353546142578, L3: 26.537555694580078\n",
      "Current prediction:  61.223236083984375 \n",
      "\n",
      "Iteration 3860, Loss: 37.114784240722656, L1: 10.527725219726562, L3: 26.58705711364746\n",
      "Current prediction:  61.224586486816406 \n",
      "\n",
      "Iteration 3861, Loss: 36.35639953613281, L1: 10.526580810546875, L3: 25.829816818237305\n",
      "Current prediction:  61.22505569458008 \n",
      "\n",
      "Iteration 3862, Loss: 37.00953674316406, L1: 10.526182174682617, L3: 26.483352661132812\n",
      "Current prediction:  61.22222900390625 \n",
      "\n",
      "Iteration 3863, Loss: 36.17906188964844, L1: 10.528576850891113, L3: 25.65048599243164\n",
      "Current prediction:  61.21931076049805 \n",
      "\n",
      "Iteration 3864, Loss: 35.764652252197266, L1: 10.531054496765137, L3: 25.233598709106445\n",
      "Current prediction:  61.21757888793945 \n",
      "\n",
      "Iteration 3865, Loss: 35.83954620361328, L1: 10.532519340515137, L3: 25.307025909423828\n",
      "Current prediction:  61.21430587768555 \n",
      "\n",
      "Iteration 3866, Loss: 35.87246322631836, L1: 10.535294532775879, L3: 25.337167739868164\n",
      "Current prediction:  61.211307525634766 \n",
      "\n",
      "Iteration 3867, Loss: 36.41207504272461, L1: 10.537837028503418, L3: 25.874238967895508\n",
      "Current prediction:  61.209651947021484 \n",
      "\n",
      "Iteration 3868, Loss: 36.607208251953125, L1: 10.539241790771484, L3: 26.067968368530273\n",
      "Current prediction:  61.208274841308594 \n",
      "\n",
      "Iteration 3869, Loss: 38.60353469848633, L1: 12.936100959777832, L3: 25.667434692382812\n",
      "Current prediction:  61.208251953125 \n",
      "\n",
      "Iteration 3870, Loss: 36.28845977783203, L1: 10.540430068969727, L3: 25.748031616210938\n",
      "Current prediction:  61.207515716552734 \n",
      "\n",
      "Iteration 3871, Loss: 37.584381103515625, L1: 10.764699935913086, L3: 26.81968116760254\n",
      "Current prediction:  61.205955505371094 \n",
      "\n",
      "Iteration 3872, Loss: 36.24576950073242, L1: 10.88969898223877, L3: 25.356069564819336\n",
      "Current prediction:  61.20460891723633 \n",
      "\n",
      "Iteration 3873, Loss: 36.47936248779297, L1: 10.72415828704834, L3: 25.755205154418945\n",
      "Current prediction:  61.206504821777344 \n",
      "\n",
      "Iteration 3874, Loss: 36.1519889831543, L1: 10.803196907043457, L3: 25.348793029785156\n",
      "Current prediction:  60.36428451538086 \n",
      "\n",
      "Iteration 3875, Loss: 36.80787658691406, L1: 10.931167602539062, L3: 25.876707077026367\n",
      "Current prediction:  60.37690734863281 \n",
      "\n",
      "Iteration 3876, Loss: 36.7629280090332, L1: 11.053784370422363, L3: 25.709142684936523\n",
      "Current prediction:  60.399879455566406 \n",
      "\n",
      "Iteration 3877, Loss: 36.81451416015625, L1: 11.141002655029297, L3: 25.673513412475586\n",
      "Current prediction:  60.4322395324707 \n",
      "\n",
      "Iteration 3878, Loss: 37.51538848876953, L1: 11.268392562866211, L3: 26.246994018554688\n",
      "Current prediction:  60.47341537475586 \n",
      "\n",
      "Iteration 3879, Loss: 37.56514358520508, L1: 11.460675239562988, L3: 26.104469299316406\n",
      "Current prediction:  60.519744873046875 \n",
      "\n",
      "Iteration 3880, Loss: 36.99907684326172, L1: 11.399092674255371, L3: 25.599985122680664\n",
      "Current prediction:  60.573856353759766 \n",
      "\n",
      "Iteration 3881, Loss: 37.26239776611328, L1: 11.308602333068848, L3: 25.953794479370117\n",
      "Current prediction:  60.63043975830078 \n",
      "\n",
      "Iteration 3882, Loss: 36.94830322265625, L1: 11.143288612365723, L3: 25.805015563964844\n",
      "Current prediction:  60.68882751464844 \n",
      "\n",
      "Iteration 3883, Loss: 37.9696159362793, L1: 11.029664039611816, L3: 26.939950942993164\n",
      "Current prediction:  60.74736022949219 \n",
      "\n",
      "Iteration 3884, Loss: 36.23846435546875, L1: 10.844405174255371, L3: 25.394058227539062\n",
      "Current prediction:  60.802852630615234 \n",
      "\n",
      "Iteration 3885, Loss: 35.90297317504883, L1: 10.690293312072754, L3: 25.212678909301758\n",
      "Current prediction:  60.854393005371094 \n",
      "\n",
      "Iteration 3886, Loss: 36.78666687011719, L1: 10.540552139282227, L3: 26.24611473083496\n",
      "Current prediction:  61.36033630371094 \n",
      "\n",
      "Iteration 3887, Loss: 36.31139373779297, L1: 10.438983917236328, L3: 25.872411727905273\n",
      "Current prediction:  61.798255920410156 \n",
      "\n",
      "Iteration 3888, Loss: 37.01935958862305, L1: 10.348597526550293, L3: 26.67076301574707\n",
      "Current prediction:  61.83272933959961 \n",
      "\n",
      "Iteration 3889, Loss: 37.694793701171875, L1: 10.259693145751953, L3: 27.435102462768555\n",
      "Current prediction:  61.85624313354492 \n",
      "\n",
      "Iteration 3890, Loss: 36.577354431152344, L1: 10.207159042358398, L3: 26.370193481445312\n",
      "Current prediction:  61.87029266357422 \n",
      "\n",
      "Iteration 3891, Loss: 36.76448059082031, L1: 10.192241668701172, L3: 26.572237014770508\n",
      "Current prediction:  61.87356185913086 \n",
      "\n",
      "Iteration 3892, Loss: 36.470497131347656, L1: 10.143575668334961, L3: 26.326919555664062\n",
      "Current prediction:  61.87119674682617 \n",
      "\n",
      "Iteration 3893, Loss: 36.184837341308594, L1: 10.147305488586426, L3: 26.03753089904785\n",
      "Current prediction:  61.86210250854492 \n",
      "\n",
      "Iteration 3894, Loss: 36.21897888183594, L1: 10.153968811035156, L3: 26.06501007080078\n",
      "Current prediction:  61.84678268432617 \n",
      "\n",
      "Iteration 3895, Loss: 37.58182907104492, L1: 10.198222160339355, L3: 27.38360595703125\n",
      "Current prediction:  61.82255935668945 \n",
      "\n",
      "Iteration 3896, Loss: 36.6026611328125, L1: 10.184765815734863, L3: 26.41789436340332\n",
      "Current prediction:  61.7921028137207 \n",
      "\n",
      "Iteration 3897, Loss: 36.337135314941406, L1: 10.250162124633789, L3: 26.08697509765625\n",
      "Current prediction:  61.756248474121094 \n",
      "\n",
      "Iteration 3898, Loss: 36.94036102294922, L1: 10.29166030883789, L3: 26.648700714111328\n",
      "Current prediction:  61.718048095703125 \n",
      "\n",
      "Iteration 3899, Loss: 37.26813507080078, L1: 10.345758438110352, L3: 26.922374725341797\n",
      "Current prediction:  61.680118560791016 \n",
      "\n",
      "Iteration 3900, Loss: 36.98931884765625, L1: 10.364912986755371, L3: 26.624406814575195\n",
      "Current prediction:  61.63971710205078 \n",
      "\n",
      "Iteration 3901, Loss: 36.92323303222656, L1: 10.471029281616211, L3: 26.452205657958984\n",
      "Current prediction:  61.26840591430664 \n",
      "\n",
      "Iteration 3902, Loss: 37.687747955322266, L1: 10.560147285461426, L3: 27.127599716186523\n",
      "Current prediction:  60.715782165527344 \n",
      "\n",
      "Iteration 3903, Loss: 36.829566955566406, L1: 10.642656326293945, L3: 26.18691062927246\n",
      "Current prediction:  60.68669891357422 \n",
      "\n",
      "Iteration 3904, Loss: 35.82986831665039, L1: 10.721271514892578, L3: 25.108596801757812\n",
      "Current prediction:  60.66510772705078 \n",
      "\n",
      "Iteration 3905, Loss: 37.12945556640625, L1: 10.775344848632812, L3: 26.35411262512207\n",
      "Current prediction:  60.650123596191406 \n",
      "\n",
      "Iteration 3906, Loss: 37.058837890625, L1: 10.849640846252441, L3: 26.209196090698242\n",
      "Current prediction:  60.64241409301758 \n",
      "\n",
      "Iteration 3907, Loss: 36.834991455078125, L1: 10.890216827392578, L3: 25.944772720336914\n",
      "Current prediction:  60.64277267456055 \n",
      "\n",
      "Iteration 3908, Loss: 36.39802932739258, L1: 10.940281867980957, L3: 25.457748413085938\n",
      "Current prediction:  60.64988327026367 \n",
      "\n",
      "Iteration 3909, Loss: 37.686214447021484, L1: 10.974563598632812, L3: 26.711650848388672\n",
      "Current prediction:  60.66021728515625 \n",
      "\n",
      "Iteration 3910, Loss: 37.06645202636719, L1: 10.990062713623047, L3: 26.07638931274414\n",
      "Current prediction:  60.67756652832031 \n",
      "\n",
      "Iteration 3911, Loss: 37.078372955322266, L1: 11.066815376281738, L3: 26.011558532714844\n",
      "Current prediction:  60.70035171508789 \n",
      "\n",
      "Iteration 3912, Loss: 36.98533248901367, L1: 11.040421485900879, L3: 25.944910049438477\n",
      "Current prediction:  60.72779846191406 \n",
      "\n",
      "Iteration 3913, Loss: 36.422794342041016, L1: 11.038493156433105, L3: 25.384302139282227\n",
      "Current prediction:  60.75999069213867 \n",
      "\n",
      "Iteration 3914, Loss: 36.80547332763672, L1: 11.03947925567627, L3: 25.765995025634766\n",
      "Current prediction:  60.79364776611328 \n",
      "\n",
      "Iteration 3915, Loss: 37.465904235839844, L1: 11.081029891967773, L3: 26.384872436523438\n",
      "Current prediction:  60.828041076660156 \n",
      "\n",
      "Iteration 3916, Loss: 37.74186706542969, L1: 11.185591697692871, L3: 26.5562744140625\n",
      "Current prediction:  60.86455154418945 \n",
      "\n",
      "Iteration 3917, Loss: 37.428226470947266, L1: 10.952202796936035, L3: 26.476024627685547\n",
      "Current prediction:  60.902435302734375 \n",
      "\n",
      "Iteration 3918, Loss: 37.289066314697266, L1: 10.846089363098145, L3: 26.442977905273438\n",
      "Current prediction:  60.94014358520508 \n",
      "\n",
      "Iteration 3919, Loss: 36.805335998535156, L1: 10.597551345825195, L3: 26.207786560058594\n",
      "Current prediction:  60.975135803222656 \n",
      "\n",
      "Iteration 3920, Loss: 36.164268493652344, L1: 10.455533981323242, L3: 25.708736419677734\n",
      "Current prediction:  61.72218704223633 \n",
      "\n",
      "Iteration 3921, Loss: 37.200904846191406, L1: 10.327444076538086, L3: 26.873458862304688\n",
      "Current prediction:  61.87381362915039 \n",
      "\n",
      "Iteration 3922, Loss: 36.582515716552734, L1: 10.223932266235352, L3: 26.358583450317383\n",
      "Current prediction:  61.886898040771484 \n",
      "\n",
      "Iteration 3923, Loss: 38.105953216552734, L1: 10.193102836608887, L3: 27.91284942626953\n",
      "Current prediction:  61.88761901855469 \n",
      "\n",
      "Iteration 3924, Loss: 36.929603576660156, L1: 10.1343994140625, L3: 26.795202255249023\n",
      "Current prediction:  61.880924224853516 \n",
      "\n",
      "Iteration 3925, Loss: 37.892765045166016, L1: 10.174249649047852, L3: 27.718515396118164\n",
      "Current prediction:  61.8634033203125 \n",
      "\n",
      "Iteration 3926, Loss: 37.171653747558594, L1: 10.206890106201172, L3: 26.96476173400879\n",
      "Current prediction:  61.836727142333984 \n",
      "\n",
      "Iteration 3927, Loss: 36.40660858154297, L1: 10.343141555786133, L3: 26.063467025756836\n",
      "Current prediction:  61.80174255371094 \n",
      "\n",
      "Iteration 3928, Loss: 38.34101867675781, L1: 10.31579875946045, L3: 28.025218963623047\n",
      "Current prediction:  61.75799560546875 \n",
      "\n",
      "Iteration 3929, Loss: 36.37512969970703, L1: 10.428620338439941, L3: 25.946510314941406\n",
      "Current prediction:  61.70923614501953 \n",
      "\n",
      "Iteration 3930, Loss: 36.05371856689453, L1: 10.467620849609375, L3: 25.586097717285156\n",
      "Current prediction:  61.65781021118164 \n",
      "\n",
      "Iteration 3931, Loss: 37.69996643066406, L1: 10.38277816772461, L3: 27.317190170288086\n",
      "Current prediction:  61.6042366027832 \n",
      "\n",
      "Iteration 3932, Loss: 35.8073616027832, L1: 10.411918640136719, L3: 25.395442962646484\n",
      "Current prediction:  61.552066802978516 \n",
      "\n",
      "Iteration 3933, Loss: 36.66307830810547, L1: 10.53323745727539, L3: 26.129840850830078\n",
      "Current prediction:  61.49916076660156 \n",
      "\n",
      "Iteration 3934, Loss: 37.29937744140625, L1: 10.586069107055664, L3: 26.713306427001953\n",
      "Current prediction:  61.446720123291016 \n",
      "\n",
      "Iteration 3935, Loss: 37.480186462402344, L1: 10.669598579406738, L3: 26.81058692932129\n",
      "Current prediction:  61.39318084716797 \n",
      "\n",
      "Iteration 3936, Loss: 38.363189697265625, L1: 10.693072319030762, L3: 27.670116424560547\n",
      "Current prediction:  61.34117889404297 \n",
      "\n",
      "Iteration 3937, Loss: 37.421661376953125, L1: 10.6524658203125, L3: 26.769193649291992\n",
      "Current prediction:  61.29135513305664 \n",
      "\n",
      "Iteration 3938, Loss: 36.66986083984375, L1: 10.691373825073242, L3: 25.978485107421875\n",
      "Current prediction:  61.24934768676758 \n",
      "\n",
      "Iteration 3939, Loss: 35.76385498046875, L1: 10.696765899658203, L3: 25.067087173461914\n",
      "Current prediction:  61.215538024902344 \n",
      "\n",
      "Iteration 3940, Loss: 36.729637145996094, L1: 10.750984191894531, L3: 25.978654861450195\n",
      "Current prediction:  61.188541412353516 \n",
      "\n",
      "Iteration 3941, Loss: 36.04180145263672, L1: 10.84080982208252, L3: 25.200992584228516\n",
      "Current prediction:  60.468467712402344 \n",
      "\n",
      "Iteration 3942, Loss: 36.31848907470703, L1: 10.915669441223145, L3: 25.402820587158203\n",
      "Current prediction:  60.329307556152344 \n",
      "\n",
      "Iteration 3943, Loss: 36.86552810668945, L1: 10.980454444885254, L3: 25.885072708129883\n",
      "Current prediction:  60.33216857910156 \n",
      "\n",
      "Iteration 3944, Loss: 37.27164840698242, L1: 11.027938842773438, L3: 26.243709564208984\n",
      "Current prediction:  60.34454345703125 \n",
      "\n",
      "Iteration 3945, Loss: 36.401092529296875, L1: 11.038749694824219, L3: 25.362342834472656\n",
      "Current prediction:  60.36347961425781 \n",
      "\n",
      "Iteration 3946, Loss: 36.88066482543945, L1: 11.050320625305176, L3: 25.830345153808594\n",
      "Current prediction:  60.38903045654297 \n",
      "\n",
      "Iteration 3947, Loss: 38.31702423095703, L1: 11.460201263427734, L3: 26.85682487487793\n",
      "Current prediction:  60.42066955566406 \n",
      "\n",
      "Iteration 3948, Loss: 37.038116455078125, L1: 11.04173469543457, L3: 25.996383666992188\n",
      "Current prediction:  60.4570198059082 \n",
      "\n",
      "Iteration 3949, Loss: 36.954891204833984, L1: 11.213993072509766, L3: 25.74089813232422\n",
      "Current prediction:  60.49980163574219 \n",
      "\n",
      "Iteration 3950, Loss: 37.25975036621094, L1: 11.518653869628906, L3: 25.741098403930664\n",
      "Current prediction:  60.54836654663086 \n",
      "\n",
      "Iteration 3951, Loss: 36.8441047668457, L1: 11.137215614318848, L3: 25.70688819885254\n",
      "Current prediction:  60.59771728515625 \n",
      "\n",
      "Iteration 3952, Loss: 37.2274169921875, L1: 11.1895751953125, L3: 26.037843704223633\n",
      "Current prediction:  59.77945327758789 \n",
      "\n",
      "Iteration 3953, Loss: 38.58335494995117, L1: 11.32781982421875, L3: 27.255535125732422\n",
      "Current prediction:  59.847591400146484 \n",
      "\n",
      "Iteration 3954, Loss: 38.83079528808594, L1: 11.607749938964844, L3: 27.22304344177246\n",
      "Current prediction:  59.923946380615234 \n",
      "\n",
      "Iteration 3955, Loss: 37.76667785644531, L1: 11.844956398010254, L3: 25.921722412109375\n",
      "Current prediction:  60.0101203918457 \n",
      "\n",
      "Iteration 3956, Loss: 38.204071044921875, L1: 11.538984298706055, L3: 26.66508674621582\n",
      "Current prediction:  60.42601776123047 \n",
      "\n",
      "Iteration 3957, Loss: 37.73701477050781, L1: 11.343765258789062, L3: 26.39324951171875\n",
      "Current prediction:  61.026893615722656 \n",
      "\n",
      "Iteration 3958, Loss: 37.54182815551758, L1: 11.210991859436035, L3: 26.33083724975586\n",
      "Current prediction:  61.118656158447266 \n",
      "\n",
      "Iteration 3959, Loss: 37.553253173828125, L1: 10.972747802734375, L3: 26.58050537109375\n",
      "Current prediction:  61.204010009765625 \n",
      "\n",
      "Iteration 3960, Loss: 35.99555969238281, L1: 10.825475692749023, L3: 25.170082092285156\n",
      "Current prediction:  61.282928466796875 \n",
      "\n",
      "Iteration 3961, Loss: 37.385498046875, L1: 10.637428283691406, L3: 26.74806785583496\n",
      "Current prediction:  61.3523063659668 \n",
      "\n",
      "Iteration 3962, Loss: 36.44414520263672, L1: 10.455168724060059, L3: 25.988977432250977\n",
      "Current prediction:  61.41148376464844 \n",
      "\n",
      "Iteration 3963, Loss: 37.7889518737793, L1: 10.334092140197754, L3: 27.454858779907227\n",
      "Current prediction:  61.45821762084961 \n",
      "\n",
      "Iteration 3964, Loss: 36.71255111694336, L1: 10.240926742553711, L3: 26.47162437438965\n",
      "Current prediction:  61.49222946166992 \n",
      "\n",
      "Iteration 3965, Loss: 37.36872100830078, L1: 10.180360794067383, L3: 27.1883602142334\n",
      "Current prediction:  61.55356216430664 \n",
      "\n",
      "Iteration 3966, Loss: 37.21023941040039, L1: 10.137351989746094, L3: 27.072887420654297\n",
      "Current prediction:  62.0921516418457 \n",
      "\n",
      "Iteration 3967, Loss: 37.36766815185547, L1: 10.029523849487305, L3: 27.338146209716797\n",
      "Current prediction:  62.383453369140625 \n",
      "\n",
      "Iteration 3968, Loss: 37.0361328125, L1: 10.10356330871582, L3: 26.93256950378418\n",
      "Current prediction:  62.38322448730469 \n",
      "\n",
      "Iteration 3969, Loss: 36.91178894042969, L1: 10.204394340515137, L3: 26.707393646240234\n",
      "Current prediction:  62.3586311340332 \n",
      "\n",
      "Iteration 3970, Loss: 37.88251495361328, L1: 10.08925724029541, L3: 27.793256759643555\n",
      "Current prediction:  62.3217887878418 \n",
      "\n",
      "Iteration 3971, Loss: 37.5110969543457, L1: 10.27991008758545, L3: 27.23118782043457\n",
      "Current prediction:  62.27857971191406 \n",
      "\n",
      "Iteration 3972, Loss: 37.49897003173828, L1: 10.160470962524414, L3: 27.338497161865234\n",
      "Current prediction:  62.22856140136719 \n",
      "\n",
      "Iteration 3973, Loss: 37.471927642822266, L1: 10.198534965515137, L3: 27.273393630981445\n",
      "Current prediction:  62.173797607421875 \n",
      "\n",
      "Iteration 3974, Loss: 37.032203674316406, L1: 10.195775032043457, L3: 26.836427688598633\n",
      "Current prediction:  62.115966796875 \n",
      "\n",
      "Iteration 3975, Loss: 37.09147644042969, L1: 10.254234313964844, L3: 26.83724021911621\n",
      "Current prediction:  62.0605583190918 \n",
      "\n",
      "Iteration 3976, Loss: 37.01789093017578, L1: 10.317946434020996, L3: 26.69994354248047\n",
      "Current prediction:  62.00456237792969 \n",
      "\n",
      "Iteration 3977, Loss: 36.79050827026367, L1: 10.379136085510254, L3: 26.4113712310791\n",
      "Current prediction:  61.95304489135742 \n",
      "\n",
      "Iteration 3978, Loss: 36.310386657714844, L1: 10.40950870513916, L3: 25.900876998901367\n",
      "Current prediction:  61.906795501708984 \n",
      "\n",
      "Iteration 3979, Loss: 37.45538330078125, L1: 10.478860855102539, L3: 26.976524353027344\n",
      "Current prediction:  61.86400604248047 \n",
      "\n",
      "Iteration 3980, Loss: 36.486534118652344, L1: 10.55226993560791, L3: 25.93426513671875\n",
      "Current prediction:  61.81580352783203 \n",
      "\n",
      "Iteration 3981, Loss: 36.19524002075195, L1: 10.617277145385742, L3: 25.57796287536621\n",
      "Current prediction:  61.64463806152344 \n",
      "\n",
      "Iteration 3982, Loss: 37.35740661621094, L1: 10.635594367980957, L3: 26.721813201904297\n",
      "Current prediction:  61.22160339355469 \n",
      "\n",
      "Iteration 3983, Loss: 38.06214904785156, L1: 10.668960571289062, L3: 27.393186569213867\n",
      "Current prediction:  60.94276809692383 \n",
      "\n",
      "Iteration 3984, Loss: 35.76513671875, L1: 10.691082954406738, L3: 25.074054718017578\n",
      "Current prediction:  60.91279602050781 \n",
      "\n",
      "Iteration 3985, Loss: 37.39381408691406, L1: 10.697017669677734, L3: 26.696796417236328\n",
      "Current prediction:  60.90156555175781 \n",
      "\n",
      "Iteration 3986, Loss: 36.39934539794922, L1: 10.686182022094727, L3: 25.713163375854492\n",
      "Current prediction:  60.90060043334961 \n",
      "\n",
      "Iteration 3987, Loss: 35.966209411621094, L1: 10.682559967041016, L3: 25.283647537231445\n",
      "Current prediction:  60.903438568115234 \n",
      "\n",
      "Iteration 3988, Loss: 36.485137939453125, L1: 10.662846565246582, L3: 25.822290420532227\n",
      "Current prediction:  60.92476272583008 \n",
      "\n",
      "Iteration 3989, Loss: 37.65182113647461, L1: 10.652235984802246, L3: 26.999584197998047\n",
      "Current prediction:  60.995784759521484 \n",
      "\n",
      "Iteration 3990, Loss: 36.78013610839844, L1: 10.645206451416016, L3: 26.134929656982422\n",
      "Current prediction:  61.323524475097656 \n",
      "\n",
      "Iteration 3991, Loss: 36.831268310546875, L1: 10.603086471557617, L3: 26.22818374633789\n",
      "Current prediction:  61.59613800048828 \n",
      "\n",
      "Iteration 3992, Loss: 37.00025177001953, L1: 10.58963680267334, L3: 26.410615921020508\n",
      "Current prediction:  61.67787170410156 \n",
      "\n",
      "Iteration 3993, Loss: 37.061424255371094, L1: 10.63066291809082, L3: 26.430763244628906\n",
      "Current prediction:  61.752044677734375 \n",
      "\n",
      "Iteration 3994, Loss: 37.282379150390625, L1: 10.568452835083008, L3: 26.71392822265625\n",
      "Current prediction:  61.775672912597656 \n",
      "\n",
      "Iteration 3995, Loss: 36.0084342956543, L1: 10.539032936096191, L3: 25.469402313232422\n",
      "Current prediction:  61.785438537597656 \n",
      "\n",
      "Iteration 3996, Loss: 36.802268981933594, L1: 10.533636093139648, L3: 26.268634796142578\n",
      "Current prediction:  61.795005798339844 \n",
      "\n",
      "Iteration 3997, Loss: 37.582210540771484, L1: 10.488694190979004, L3: 27.093515396118164\n",
      "Current prediction:  61.80427169799805 \n",
      "\n",
      "Iteration 3998, Loss: 37.48064041137695, L1: 10.491887092590332, L3: 26.988752365112305\n",
      "Current prediction:  61.81310272216797 \n",
      "\n",
      "â†³ LR reduced to 1.0e-03 at iteration 4000 \n",
      "\n",
      "Iteration 3999, Loss: 37.02853775024414, L1: 10.456530570983887, L3: 26.572006225585938\n",
      "Current prediction:  61.81874084472656 \n",
      "\n",
      "Iteration 4000, Loss: 37.29490661621094, L1: 10.409557342529297, L3: 26.88534927368164\n",
      "Current prediction:  61.823486328125 \n",
      "\n",
      "Iteration 4001, Loss: 36.88908767700195, L1: 10.43752384185791, L3: 26.45156478881836\n",
      "Current prediction:  61.82864761352539 \n",
      "\n",
      "Iteration 4002, Loss: 36.658966064453125, L1: 10.450329780578613, L3: 26.208635330200195\n",
      "Current prediction:  61.831298828125 \n",
      "\n",
      "Iteration 4003, Loss: 37.01780319213867, L1: 10.42679500579834, L3: 26.59100914001465\n",
      "Current prediction:  61.8321418762207 \n",
      "\n",
      "Iteration 4004, Loss: 36.253395080566406, L1: 10.429309844970703, L3: 25.82408332824707\n",
      "Current prediction:  61.83186721801758 \n",
      "\n",
      "Iteration 4005, Loss: 37.23519515991211, L1: 10.407126426696777, L3: 26.82806968688965\n",
      "Current prediction:  61.83121871948242 \n",
      "\n",
      "Iteration 4006, Loss: 37.24827194213867, L1: 10.380805015563965, L3: 26.86746597290039\n",
      "Current prediction:  61.827571868896484 \n",
      "\n",
      "Iteration 4007, Loss: 36.85771560668945, L1: 10.446330070495605, L3: 26.41138458251953\n",
      "Current prediction:  61.82408905029297 \n",
      "\n",
      "Iteration 4008, Loss: 36.688758850097656, L1: 10.426483154296875, L3: 26.26227569580078\n",
      "Current prediction:  61.8198356628418 \n",
      "\n",
      "Iteration 4009, Loss: 37.138343811035156, L1: 10.489749908447266, L3: 26.64859390258789\n",
      "Current prediction:  61.81766128540039 \n",
      "\n",
      "Iteration 4010, Loss: 35.79432678222656, L1: 10.397320747375488, L3: 25.39700698852539\n",
      "Current prediction:  61.813377380371094 \n",
      "\n",
      "Iteration 4011, Loss: 35.857154846191406, L1: 10.473915100097656, L3: 25.38323974609375\n",
      "Current prediction:  61.80732727050781 \n",
      "\n",
      "Iteration 4012, Loss: 36.972259521484375, L1: 10.431931495666504, L3: 26.540328979492188\n",
      "Current prediction:  61.801239013671875 \n",
      "\n",
      "Iteration 4013, Loss: 36.24527359008789, L1: 10.481295585632324, L3: 25.76397705078125\n",
      "Current prediction:  61.79491424560547 \n",
      "\n",
      "Iteration 4014, Loss: 37.45659637451172, L1: 10.50589656829834, L3: 26.950700759887695\n",
      "Current prediction:  61.78755187988281 \n",
      "\n",
      "Iteration 4015, Loss: 37.06401062011719, L1: 10.5260591506958, L3: 26.53795051574707\n",
      "Current prediction:  61.777400970458984 \n",
      "\n",
      "Iteration 4016, Loss: 36.814109802246094, L1: 10.544008255004883, L3: 26.270103454589844\n",
      "Current prediction:  61.76333236694336 \n",
      "\n",
      "Iteration 4017, Loss: 37.56464767456055, L1: 10.529892921447754, L3: 27.034753799438477\n",
      "Current prediction:  61.745365142822266 \n",
      "\n",
      "Iteration 4018, Loss: 35.838829040527344, L1: 10.541147232055664, L3: 25.297683715820312\n",
      "Current prediction:  61.72428894042969 \n",
      "\n",
      "Iteration 4019, Loss: 37.820159912109375, L1: 10.642728805541992, L3: 27.177431106567383\n",
      "Current prediction:  61.70391082763672 \n",
      "\n",
      "Iteration 4020, Loss: 36.150638580322266, L1: 10.631319999694824, L3: 25.519317626953125\n",
      "Current prediction:  61.68522644042969 \n",
      "\n",
      "Iteration 4021, Loss: 35.73070526123047, L1: 10.651867866516113, L3: 25.07883644104004\n",
      "Current prediction:  61.665435791015625 \n",
      "\n",
      "Iteration 4022, Loss: 37.04121398925781, L1: 10.539963722229004, L3: 26.501249313354492\n",
      "Current prediction:  61.6461181640625 \n",
      "\n",
      "Iteration 4023, Loss: 36.54146194458008, L1: 10.59003734588623, L3: 25.951425552368164\n",
      "Current prediction:  61.62765121459961 \n",
      "\n",
      "Iteration 4024, Loss: 36.57182693481445, L1: 10.595783233642578, L3: 25.976043701171875\n",
      "Current prediction:  61.60792922973633 \n",
      "\n",
      "Iteration 4025, Loss: 37.22322463989258, L1: 10.629014015197754, L3: 26.59421157836914\n",
      "Current prediction:  61.58808517456055 \n",
      "\n",
      "Iteration 4026, Loss: 37.33879852294922, L1: 10.7609224319458, L3: 26.5778751373291\n",
      "Current prediction:  61.56754684448242 \n",
      "\n",
      "Iteration 4027, Loss: 37.516082763671875, L1: 10.749917984008789, L3: 26.766164779663086\n",
      "Current prediction:  61.54575729370117 \n",
      "\n",
      "Iteration 4028, Loss: 37.52717971801758, L1: 10.616921424865723, L3: 26.910259246826172\n",
      "Current prediction:  61.52564239501953 \n",
      "\n",
      "Iteration 4029, Loss: 35.77047348022461, L1: 10.67634105682373, L3: 25.094133377075195\n",
      "Current prediction:  61.50869369506836 \n",
      "\n",
      "Iteration 4030, Loss: 36.40830612182617, L1: 10.653579711914062, L3: 25.75472640991211\n",
      "Current prediction:  61.49747848510742 \n",
      "\n",
      "Iteration 4031, Loss: 35.629722595214844, L1: 10.656366348266602, L3: 24.973356246948242\n",
      "Current prediction:  61.493350982666016 \n",
      "\n",
      "Iteration 4032, Loss: 36.95463562011719, L1: 10.726371765136719, L3: 26.228261947631836\n",
      "Current prediction:  60.78248977661133 \n",
      "\n",
      "Iteration 4033, Loss: 36.35486602783203, L1: 10.806178092956543, L3: 25.548688888549805\n",
      "Current prediction:  60.686920166015625 \n",
      "\n",
      "Iteration 4034, Loss: 36.96198272705078, L1: 10.885981559753418, L3: 26.07600212097168\n",
      "Current prediction:  60.706993103027344 \n",
      "\n",
      "Iteration 4035, Loss: 37.306114196777344, L1: 10.93941879272461, L3: 26.3666934967041\n",
      "Current prediction:  60.73624801635742 \n",
      "\n",
      "Iteration 4036, Loss: 37.5040283203125, L1: 10.973797798156738, L3: 26.530231475830078\n",
      "Current prediction:  60.77051544189453 \n",
      "\n",
      "Iteration 4037, Loss: 36.076507568359375, L1: 10.971366882324219, L3: 25.10514259338379\n",
      "Current prediction:  60.81385040283203 \n",
      "\n",
      "Iteration 4038, Loss: 37.24826431274414, L1: 10.949808120727539, L3: 26.2984561920166\n",
      "Current prediction:  60.861541748046875 \n",
      "\n",
      "Iteration 4039, Loss: 36.48970413208008, L1: 10.921422004699707, L3: 25.568281173706055\n",
      "Current prediction:  60.91074752807617 \n",
      "\n",
      "Iteration 4040, Loss: 37.238487243652344, L1: 10.900236129760742, L3: 26.33824920654297\n",
      "Current prediction:  60.96034240722656 \n",
      "\n",
      "Iteration 4041, Loss: 37.09564971923828, L1: 10.795867919921875, L3: 26.299781799316406\n",
      "Current prediction:  61.008323669433594 \n",
      "\n",
      "Iteration 4042, Loss: 36.76817321777344, L1: 10.784120559692383, L3: 25.984050750732422\n",
      "Current prediction:  61.05691146850586 \n",
      "\n",
      "Iteration 4043, Loss: 37.45743942260742, L1: 10.72325611114502, L3: 26.73418426513672\n",
      "Current prediction:  61.10382843017578 \n",
      "\n",
      "Iteration 4044, Loss: 37.529293060302734, L1: 10.637215614318848, L3: 26.892078399658203\n",
      "Current prediction:  61.147579193115234 \n",
      "\n",
      "Iteration 4045, Loss: 37.435218811035156, L1: 10.616130828857422, L3: 26.819089889526367\n",
      "Current prediction:  61.18844985961914 \n",
      "\n",
      "Iteration 4046, Loss: 38.59320831298828, L1: 10.551349639892578, L3: 28.04185676574707\n",
      "Current prediction:  61.22468185424805 \n",
      "\n",
      "Iteration 4047, Loss: 36.82644271850586, L1: 10.526948928833008, L3: 26.29949378967285\n",
      "Current prediction:  61.25863265991211 \n",
      "\n",
      "Iteration 4048, Loss: 36.434852600097656, L1: 10.490439414978027, L3: 25.944412231445312\n",
      "Current prediction:  61.2868766784668 \n",
      "\n",
      "Iteration 4049, Loss: 37.64391326904297, L1: 10.433199882507324, L3: 27.210712432861328\n",
      "Current prediction:  61.3125 \n",
      "\n",
      "Iteration 4050, Loss: 36.764991760253906, L1: 10.414772987365723, L3: 26.350217819213867\n",
      "Current prediction:  61.33623123168945 \n",
      "\n",
      "Iteration 4051, Loss: 36.95793914794922, L1: 10.359208106994629, L3: 26.598731994628906\n",
      "Current prediction:  61.36652755737305 \n",
      "\n",
      "Iteration 4052, Loss: 37.20568084716797, L1: 10.33301830291748, L3: 26.872661590576172\n",
      "Current prediction:  61.63288116455078 \n",
      "\n",
      "Iteration 4053, Loss: 37.215911865234375, L1: 10.26905632019043, L3: 26.946855545043945\n",
      "Current prediction:  62.169742584228516 \n",
      "\n",
      "Iteration 4054, Loss: 37.44223403930664, L1: 10.267304420471191, L3: 27.174930572509766\n",
      "Current prediction:  62.14516067504883 \n",
      "\n",
      "Iteration 4055, Loss: 37.19249725341797, L1: 10.21950912475586, L3: 26.972990036010742\n",
      "Current prediction:  62.034461975097656 \n",
      "\n",
      "Iteration 4056, Loss: 36.30802917480469, L1: 10.203129768371582, L3: 26.104900360107422\n",
      "Current prediction:  61.852230072021484 \n",
      "\n",
      "Iteration 4057, Loss: 35.941959381103516, L1: 10.21781063079834, L3: 25.72414779663086\n",
      "Current prediction:  61.68199920654297 \n",
      "\n",
      "Iteration 4058, Loss: 36.12609100341797, L1: 10.21345043182373, L3: 25.912641525268555\n",
      "Current prediction:  61.58163070678711 \n",
      "\n",
      "Iteration 4059, Loss: 36.0787467956543, L1: 10.213433265686035, L3: 25.865314483642578\n",
      "Current prediction:  61.46998977661133 \n",
      "\n",
      "Iteration 4060, Loss: 37.589805603027344, L1: 10.236859321594238, L3: 27.352947235107422\n",
      "Current prediction:  61.415496826171875 \n",
      "\n",
      "Iteration 4061, Loss: 37.10086441040039, L1: 10.248567581176758, L3: 26.852296829223633\n",
      "Current prediction:  61.42857360839844 \n",
      "\n",
      "Iteration 4062, Loss: 36.53066635131836, L1: 10.253823280334473, L3: 26.276844024658203\n",
      "Current prediction:  61.32627868652344 \n",
      "\n",
      "Iteration 4063, Loss: 35.683990478515625, L1: 10.289037704467773, L3: 25.39495086669922\n",
      "Current prediction:  61.3203010559082 \n",
      "\n",
      "Iteration 4064, Loss: 37.0418815612793, L1: 10.302075386047363, L3: 26.73980712890625\n",
      "Current prediction:  61.38406753540039 \n",
      "\n",
      "Iteration 4065, Loss: 37.944557189941406, L1: 10.321979522705078, L3: 27.622575759887695\n",
      "Current prediction:  61.56560134887695 \n",
      "\n",
      "Iteration 4066, Loss: 36.84667205810547, L1: 10.33986759185791, L3: 26.506805419921875\n",
      "Current prediction:  61.730648040771484 \n",
      "\n",
      "Iteration 4067, Loss: 36.956764221191406, L1: 10.3412446975708, L3: 26.61551856994629\n",
      "Current prediction:  61.902122497558594 \n",
      "\n",
      "Iteration 4068, Loss: 37.25225830078125, L1: 10.349349975585938, L3: 26.90290641784668\n",
      "Current prediction:  61.92052459716797 \n",
      "\n",
      "Iteration 4069, Loss: 36.341331481933594, L1: 10.370366096496582, L3: 25.970966339111328\n",
      "Current prediction:  61.9170036315918 \n",
      "\n",
      "Iteration 4070, Loss: 37.081199645996094, L1: 10.380292892456055, L3: 26.700908660888672\n",
      "Current prediction:  61.90624237060547 \n",
      "\n",
      "Iteration 4071, Loss: 37.36414337158203, L1: 10.378823280334473, L3: 26.985319137573242\n",
      "Current prediction:  61.88074493408203 \n",
      "\n",
      "Iteration 4072, Loss: 36.592506408691406, L1: 10.420391082763672, L3: 26.172117233276367\n",
      "Current prediction:  61.8748779296875 \n",
      "\n",
      "Iteration 4073, Loss: 37.61891555786133, L1: 10.414434432983398, L3: 27.20448112487793\n",
      "Current prediction:  61.868247985839844 \n",
      "\n",
      "Iteration 4074, Loss: 37.15194320678711, L1: 10.394403457641602, L3: 26.757539749145508\n",
      "Current prediction:  61.85750198364258 \n",
      "\n",
      "Iteration 4075, Loss: 36.38626480102539, L1: 10.388616561889648, L3: 25.997648239135742\n",
      "Current prediction:  61.84398651123047 \n",
      "\n",
      "Iteration 4076, Loss: 35.604759216308594, L1: 10.393332481384277, L3: 25.21142578125\n",
      "Current prediction:  61.82924270629883 \n",
      "\n",
      "Iteration 4077, Loss: 36.89231872558594, L1: 10.397880554199219, L3: 26.49443817138672\n",
      "Current prediction:  61.817604064941406 \n",
      "\n",
      "Iteration 4078, Loss: 36.730796813964844, L1: 10.409051895141602, L3: 26.321746826171875\n",
      "Current prediction:  61.80769729614258 \n",
      "\n",
      "Iteration 4079, Loss: 36.522159576416016, L1: 10.431678771972656, L3: 26.09048080444336\n",
      "Current prediction:  61.799468994140625 \n",
      "\n",
      "Iteration 4080, Loss: 36.11021423339844, L1: 10.406578063964844, L3: 25.703636169433594\n",
      "Current prediction:  61.793582916259766 \n",
      "\n",
      "Iteration 4081, Loss: 37.04771423339844, L1: 10.424652099609375, L3: 26.62306022644043\n",
      "Current prediction:  61.788108825683594 \n",
      "\n",
      "Iteration 4082, Loss: 36.99658203125, L1: 10.424736022949219, L3: 26.57184410095215\n",
      "Current prediction:  61.782527923583984 \n",
      "\n",
      "Iteration 4083, Loss: 36.124183654785156, L1: 10.387917518615723, L3: 25.73626708984375\n",
      "Current prediction:  61.775657653808594 \n",
      "\n",
      "Iteration 4084, Loss: 36.79771041870117, L1: 10.413639068603516, L3: 26.384071350097656\n",
      "Current prediction:  61.767486572265625 \n",
      "\n",
      "Iteration 4085, Loss: 36.79934310913086, L1: 10.413130760192871, L3: 26.386213302612305\n",
      "Current prediction:  61.75784683227539 \n",
      "\n",
      "Iteration 4086, Loss: 35.49671173095703, L1: 10.385660171508789, L3: 25.11104965209961\n",
      "Current prediction:  61.74894332885742 \n",
      "\n",
      "Iteration 4087, Loss: 36.886348724365234, L1: 10.436097145080566, L3: 26.45025062561035\n",
      "Current prediction:  61.742515563964844 \n",
      "\n",
      "Iteration 4088, Loss: 36.525054931640625, L1: 10.495816230773926, L3: 26.029239654541016\n",
      "Current prediction:  61.73532485961914 \n",
      "\n",
      "Iteration 4089, Loss: 35.44355773925781, L1: 10.488603591918945, L3: 24.954954147338867\n",
      "Current prediction:  61.72993087768555 \n",
      "\n",
      "Iteration 4090, Loss: 37.6026725769043, L1: 10.513559341430664, L3: 27.089113235473633\n",
      "Current prediction:  61.727752685546875 \n",
      "\n",
      "Iteration 4091, Loss: 36.75103759765625, L1: 10.464017868041992, L3: 26.28702163696289\n",
      "Current prediction:  61.72535705566406 \n",
      "\n",
      "Iteration 4092, Loss: 37.4063835144043, L1: 10.49535083770752, L3: 26.911033630371094\n",
      "Current prediction:  61.72569274902344 \n",
      "\n",
      "Iteration 4093, Loss: 36.03911209106445, L1: 10.526566505432129, L3: 25.51254653930664\n",
      "Current prediction:  61.72700500488281 \n",
      "\n",
      "Iteration 4094, Loss: 36.40430450439453, L1: 10.525293350219727, L3: 25.879013061523438\n",
      "Current prediction:  61.731563568115234 \n",
      "\n",
      "Iteration 4095, Loss: 37.2641716003418, L1: 10.516629219055176, L3: 26.747543334960938\n",
      "Current prediction:  61.737388610839844 \n",
      "\n",
      "Iteration 4096, Loss: 36.95138168334961, L1: 10.541182518005371, L3: 26.410200119018555\n",
      "Current prediction:  61.74570083618164 \n",
      "\n",
      "Iteration 4097, Loss: 36.56390380859375, L1: 10.527795791625977, L3: 26.03610610961914\n",
      "Current prediction:  61.75293731689453 \n",
      "\n",
      "Iteration 4098, Loss: 35.40736770629883, L1: 10.506168365478516, L3: 24.901199340820312\n",
      "Current prediction:  61.76340866088867 \n",
      "\n",
      "Iteration 4099, Loss: 36.872440338134766, L1: 10.501347541809082, L3: 26.37109375\n",
      "Current prediction:  61.7773551940918 \n",
      "\n",
      "Iteration 4100, Loss: 37.049652099609375, L1: 10.493925094604492, L3: 26.55572509765625\n",
      "Current prediction:  61.79180908203125 \n",
      "\n",
      "Iteration 4101, Loss: 35.91412353515625, L1: 10.461689949035645, L3: 25.452434539794922\n",
      "Current prediction:  61.80577087402344 \n",
      "\n",
      "Iteration 4102, Loss: 36.462955474853516, L1: 10.445984840393066, L3: 26.016971588134766\n",
      "Current prediction:  61.819129943847656 \n",
      "\n",
      "Iteration 4103, Loss: 36.84151840209961, L1: 10.41494369506836, L3: 26.42657470703125\n",
      "Current prediction:  61.830753326416016 \n",
      "\n",
      "Iteration 4104, Loss: 36.92345428466797, L1: 10.379997253417969, L3: 26.54345703125\n",
      "Current prediction:  61.840972900390625 \n",
      "\n",
      "Iteration 4105, Loss: 37.249385833740234, L1: 10.36989688873291, L3: 26.87948989868164\n",
      "Current prediction:  61.8470344543457 \n",
      "\n",
      "Iteration 4106, Loss: 36.595890045166016, L1: 10.372889518737793, L3: 26.222999572753906\n",
      "Current prediction:  61.8505744934082 \n",
      "\n",
      "Iteration 4107, Loss: 36.53496170043945, L1: 10.365569114685059, L3: 26.16939353942871\n",
      "Current prediction:  61.85277557373047 \n",
      "\n",
      "Iteration 4108, Loss: 36.457576751708984, L1: 10.34761905670166, L3: 26.10995864868164\n",
      "Current prediction:  61.85221481323242 \n",
      "\n",
      "Iteration 4109, Loss: 36.745208740234375, L1: 10.302032470703125, L3: 26.443178176879883\n",
      "Current prediction:  61.84840393066406 \n",
      "\n",
      "Iteration 4110, Loss: 36.52057647705078, L1: 10.367610931396484, L3: 26.152965545654297\n",
      "Current prediction:  61.84495544433594 \n",
      "\n",
      "Iteration 4111, Loss: 35.966331481933594, L1: 10.351579666137695, L3: 25.61475372314453\n",
      "Current prediction:  61.84212112426758 \n",
      "\n",
      "Iteration 4112, Loss: 35.6702766418457, L1: 10.357600212097168, L3: 25.31267738342285\n",
      "Current prediction:  61.83810043334961 \n",
      "\n",
      "Iteration 4113, Loss: 37.15032196044922, L1: 10.386980056762695, L3: 26.763343811035156\n",
      "Current prediction:  61.83229064941406 \n",
      "\n",
      "Iteration 4114, Loss: 37.0821418762207, L1: 10.381590843200684, L3: 26.700550079345703\n",
      "Current prediction:  61.82652282714844 \n",
      "\n",
      "Iteration 4115, Loss: 36.421295166015625, L1: 10.368335723876953, L3: 26.052961349487305\n",
      "Current prediction:  61.82081985473633 \n",
      "\n",
      "Iteration 4116, Loss: 36.62431335449219, L1: 10.399994850158691, L3: 26.224319458007812\n",
      "Current prediction:  61.81513214111328 \n",
      "\n",
      "Iteration 4117, Loss: 35.51475524902344, L1: 10.40509033203125, L3: 25.109663009643555\n",
      "Current prediction:  61.812347412109375 \n",
      "\n",
      "Iteration 4118, Loss: 36.45734405517578, L1: 10.42688274383545, L3: 26.030460357666016\n",
      "Current prediction:  61.81032943725586 \n",
      "\n",
      "Iteration 4119, Loss: 36.896297454833984, L1: 10.403922080993652, L3: 26.492374420166016\n",
      "Current prediction:  61.80901336669922 \n",
      "\n",
      "Iteration 4120, Loss: 36.47367858886719, L1: 10.397056579589844, L3: 26.07662010192871\n",
      "Current prediction:  61.80893325805664 \n",
      "\n",
      "Iteration 4121, Loss: 36.049560546875, L1: 10.403448104858398, L3: 25.646114349365234\n",
      "Current prediction:  61.810821533203125 \n",
      "\n",
      "Iteration 4122, Loss: 36.73857879638672, L1: 10.367558479309082, L3: 26.37101936340332\n",
      "Current prediction:  61.81380844116211 \n",
      "\n",
      "Iteration 4123, Loss: 37.08538818359375, L1: 10.393930435180664, L3: 26.691455841064453\n",
      "Current prediction:  61.81416702270508 \n",
      "\n",
      "Iteration 4124, Loss: 36.4464225769043, L1: 10.360107421875, L3: 26.086315155029297\n",
      "Current prediction:  61.81336212158203 \n",
      "\n",
      "Iteration 4125, Loss: 36.19331359863281, L1: 10.415781021118164, L3: 25.77753257751465\n",
      "Current prediction:  61.809165954589844 \n",
      "\n",
      "Iteration 4126, Loss: 37.94927978515625, L1: 10.379791259765625, L3: 27.569490432739258\n",
      "Current prediction:  61.80328369140625 \n",
      "\n",
      "Iteration 4127, Loss: 36.79446029663086, L1: 10.355908393859863, L3: 26.43855094909668\n",
      "Current prediction:  61.79601287841797 \n",
      "\n",
      "Iteration 4128, Loss: 37.893798828125, L1: 10.410041809082031, L3: 27.48375701904297\n",
      "Current prediction:  61.7854118347168 \n",
      "\n",
      "Iteration 4129, Loss: 36.04179382324219, L1: 10.420443534851074, L3: 25.621349334716797\n",
      "Current prediction:  61.77357864379883 \n",
      "\n",
      "Iteration 4130, Loss: 37.04245376586914, L1: 10.405709266662598, L3: 26.636743545532227\n",
      "Current prediction:  61.76334762573242 \n",
      "\n",
      "Iteration 4131, Loss: 36.806732177734375, L1: 10.382715225219727, L3: 26.424015045166016\n",
      "Current prediction:  61.751216888427734 \n",
      "\n",
      "Iteration 4132, Loss: 36.62958908081055, L1: 10.455120086669922, L3: 26.174468994140625\n",
      "Current prediction:  61.740718841552734 \n",
      "\n",
      "Iteration 4133, Loss: 37.03491973876953, L1: 10.443002700805664, L3: 26.5919189453125\n",
      "Current prediction:  61.73369598388672 \n",
      "\n",
      "Iteration 4134, Loss: 36.1157112121582, L1: 10.43455982208252, L3: 25.681150436401367\n",
      "Current prediction:  61.72593307495117 \n",
      "\n",
      "Iteration 4135, Loss: 36.99513626098633, L1: 10.483704566955566, L3: 26.511430740356445\n",
      "Current prediction:  61.690696716308594 \n",
      "\n",
      "Iteration 4136, Loss: 37.1031379699707, L1: 10.456969261169434, L3: 26.646169662475586\n",
      "Current prediction:  61.38536834716797 \n",
      "\n",
      "Iteration 4137, Loss: 36.07726287841797, L1: 10.498237609863281, L3: 25.579025268554688\n",
      "Current prediction:  61.05524444580078 \n",
      "\n",
      "Iteration 4138, Loss: 36.413429260253906, L1: 10.511014938354492, L3: 25.902416229248047\n",
      "Current prediction:  60.92661666870117 \n",
      "\n",
      "Iteration 4139, Loss: 37.0928840637207, L1: 10.517220497131348, L3: 26.57566261291504\n",
      "Current prediction:  60.9494514465332 \n",
      "\n",
      "Iteration 4140, Loss: 37.15016174316406, L1: 10.548542022705078, L3: 26.601621627807617\n",
      "Current prediction:  60.96109390258789 \n",
      "\n",
      "Iteration 4141, Loss: 36.55488586425781, L1: 10.540878295898438, L3: 26.014009475708008\n",
      "Current prediction:  61.053794860839844 \n",
      "\n",
      "Iteration 4142, Loss: 37.15845489501953, L1: 10.548245429992676, L3: 26.61020851135254\n",
      "Current prediction:  61.42245101928711 \n",
      "\n",
      "Iteration 4143, Loss: 36.459354400634766, L1: 10.524052619934082, L3: 25.935300827026367\n",
      "Current prediction:  61.75117111206055 \n",
      "\n",
      "Iteration 4144, Loss: 37.50210952758789, L1: 10.514059066772461, L3: 26.98805046081543\n",
      "Current prediction:  61.766143798828125 \n",
      "\n",
      "Iteration 4145, Loss: 36.558502197265625, L1: 10.512140274047852, L3: 26.046363830566406\n",
      "Current prediction:  61.52340316772461 \n",
      "\n",
      "Iteration 4146, Loss: 35.3693733215332, L1: 10.470758438110352, L3: 24.89861488342285\n",
      "Current prediction:  61.22317886352539 \n",
      "\n",
      "Iteration 4147, Loss: 36.75494384765625, L1: 10.465179443359375, L3: 26.289762496948242\n",
      "Current prediction:  61.09882736206055 \n",
      "\n",
      "Iteration 4148, Loss: 37.37798309326172, L1: 10.432382583618164, L3: 26.945602416992188\n",
      "Current prediction:  61.075504302978516 \n",
      "\n",
      "Iteration 4149, Loss: 36.10596466064453, L1: 10.415231704711914, L3: 25.690731048583984\n",
      "Current prediction:  61.113433837890625 \n",
      "\n",
      "Iteration 4150, Loss: 36.80409240722656, L1: 10.380475997924805, L3: 26.42361831665039\n",
      "Current prediction:  61.17026138305664 \n",
      "\n",
      "Iteration 4151, Loss: 36.0692138671875, L1: 10.447429656982422, L3: 25.621784210205078\n",
      "Current prediction:  61.246337890625 \n",
      "\n",
      "Iteration 4152, Loss: 36.58929443359375, L1: 10.439739227294922, L3: 26.149553298950195\n",
      "Current prediction:  61.37083053588867 \n",
      "\n",
      "Iteration 4153, Loss: 36.23896026611328, L1: 10.497529983520508, L3: 25.741432189941406\n",
      "Current prediction:  61.39052200317383 \n",
      "\n",
      "Iteration 4154, Loss: 36.7884635925293, L1: 10.370963096618652, L3: 26.417499542236328\n",
      "Current prediction:  61.57720947265625 \n",
      "\n",
      "Iteration 4155, Loss: 36.1808967590332, L1: 10.38341999053955, L3: 25.79747772216797\n",
      "Current prediction:  61.85980987548828 \n",
      "\n",
      "Iteration 4156, Loss: 36.76078796386719, L1: 10.387645721435547, L3: 26.373144149780273\n",
      "Current prediction:  61.896888732910156 \n",
      "\n",
      "Iteration 4157, Loss: 37.848358154296875, L1: 10.354608535766602, L3: 27.49374771118164\n",
      "Current prediction:  61.89491271972656 \n",
      "\n",
      "Iteration 4158, Loss: 36.69541931152344, L1: 10.295724868774414, L3: 26.399694442749023\n",
      "Current prediction:  61.886863708496094 \n",
      "\n",
      "Iteration 4159, Loss: 36.47607421875, L1: 10.347329139709473, L3: 26.12874412536621\n",
      "Current prediction:  61.876251220703125 \n",
      "\n",
      "Iteration 4160, Loss: 36.764408111572266, L1: 10.383116722106934, L3: 26.381290435791016\n",
      "Current prediction:  61.86433410644531 \n",
      "\n",
      "Iteration 4161, Loss: 36.35284423828125, L1: 10.40415096282959, L3: 25.948694229125977\n",
      "Current prediction:  61.85271453857422 \n",
      "\n",
      "Iteration 4162, Loss: 36.78919219970703, L1: 10.415460586547852, L3: 26.373733520507812\n",
      "Current prediction:  61.805484771728516 \n",
      "\n",
      "Iteration 4163, Loss: 36.82472229003906, L1: 10.434979438781738, L3: 26.38974380493164\n",
      "Current prediction:  61.64393997192383 \n",
      "\n",
      "Iteration 4164, Loss: 36.55018615722656, L1: 10.448379516601562, L3: 26.101808547973633\n",
      "Current prediction:  61.54087448120117 \n",
      "\n",
      "Iteration 4165, Loss: 36.71430969238281, L1: 10.455665588378906, L3: 26.25864601135254\n",
      "Current prediction:  61.281776428222656 \n",
      "\n",
      "Iteration 4166, Loss: 36.1436882019043, L1: 10.446510314941406, L3: 25.69717788696289\n",
      "Current prediction:  61.25716018676758 \n",
      "\n",
      "Iteration 4167, Loss: 36.84882354736328, L1: 10.444596290588379, L3: 26.40422821044922\n",
      "Current prediction:  61.49909973144531 \n",
      "\n",
      "Iteration 4168, Loss: 35.974037170410156, L1: 10.4126558303833, L3: 25.56138038635254\n",
      "Current prediction:  61.79462432861328 \n",
      "\n",
      "Iteration 4169, Loss: 36.1258430480957, L1: 10.388312339782715, L3: 25.737529754638672\n",
      "Current prediction:  61.85169219970703 \n",
      "\n",
      "Iteration 4170, Loss: 36.784034729003906, L1: 10.380831718444824, L3: 26.4032039642334\n",
      "Current prediction:  61.867305755615234 \n",
      "\n",
      "Iteration 4171, Loss: 36.366615295410156, L1: 10.337448120117188, L3: 26.0291690826416\n",
      "Current prediction:  61.87464141845703 \n",
      "\n",
      "Iteration 4172, Loss: 37.09770202636719, L1: 10.336650848388672, L3: 26.761049270629883\n",
      "Current prediction:  61.88010787963867 \n",
      "\n",
      "Iteration 4173, Loss: 36.24375915527344, L1: 10.294339179992676, L3: 25.949420928955078\n",
      "Current prediction:  61.88232421875 \n",
      "\n",
      "Iteration 4174, Loss: 36.332603454589844, L1: 10.298299789428711, L3: 26.034305572509766\n",
      "Current prediction:  61.883583068847656 \n",
      "\n",
      "Iteration 4175, Loss: 36.674644470214844, L1: 10.286795616149902, L3: 26.387849807739258\n",
      "Current prediction:  61.8845329284668 \n",
      "\n",
      "Iteration 4176, Loss: 35.566314697265625, L1: 10.299736976623535, L3: 25.266576766967773\n",
      "Current prediction:  61.883323669433594 \n",
      "\n",
      "Iteration 4177, Loss: 36.885597229003906, L1: 10.280104637145996, L3: 26.605493545532227\n",
      "Current prediction:  61.88025665283203 \n",
      "\n",
      "Iteration 4178, Loss: 37.29131317138672, L1: 10.251639366149902, L3: 27.039674758911133\n",
      "Current prediction:  61.8739013671875 \n",
      "\n",
      "Iteration 4179, Loss: 36.097469329833984, L1: 10.294447898864746, L3: 25.803020477294922\n",
      "Current prediction:  61.8661994934082 \n",
      "\n",
      "Iteration 4180, Loss: 36.50841522216797, L1: 10.296558380126953, L3: 26.21185874938965\n",
      "Current prediction:  61.85615539550781 \n",
      "\n",
      "Iteration 4181, Loss: 36.7963981628418, L1: 10.322717666625977, L3: 26.47368049621582\n",
      "Current prediction:  61.84466552734375 \n",
      "\n",
      "Iteration 4182, Loss: 36.217254638671875, L1: 10.321261405944824, L3: 25.895994186401367\n",
      "Current prediction:  61.833587646484375 \n",
      "\n",
      "Iteration 4183, Loss: 35.97333526611328, L1: 10.327821731567383, L3: 25.645511627197266\n",
      "Current prediction:  61.82221221923828 \n",
      "\n",
      "Iteration 4184, Loss: 37.57546615600586, L1: 10.347463607788086, L3: 27.228002548217773\n",
      "Current prediction:  61.8106803894043 \n",
      "\n",
      "Iteration 4185, Loss: 36.828643798828125, L1: 10.347450256347656, L3: 26.48119354248047\n",
      "Current prediction:  61.80191421508789 \n",
      "\n",
      "Iteration 4186, Loss: 37.10791778564453, L1: 10.347919464111328, L3: 26.75999641418457\n",
      "Current prediction:  61.79199981689453 \n",
      "\n",
      "Iteration 4187, Loss: 36.84696960449219, L1: 10.341326713562012, L3: 26.505643844604492\n",
      "Current prediction:  61.78094482421875 \n",
      "\n",
      "Iteration 4188, Loss: 37.23069763183594, L1: 10.372254371643066, L3: 26.858444213867188\n",
      "Current prediction:  61.76923370361328 \n",
      "\n",
      "Iteration 4189, Loss: 36.29814910888672, L1: 10.385671615600586, L3: 25.9124755859375\n",
      "Current prediction:  61.76071548461914 \n",
      "\n",
      "Iteration 4190, Loss: 36.44078063964844, L1: 10.419052124023438, L3: 26.021730422973633\n",
      "Current prediction:  61.75621795654297 \n",
      "\n",
      "Iteration 4191, Loss: 36.90929412841797, L1: 10.433382034301758, L3: 26.475910186767578\n",
      "Current prediction:  61.75262451171875 \n",
      "\n",
      "Iteration 4192, Loss: 37.536705017089844, L1: 10.413986206054688, L3: 27.122718811035156\n",
      "Current prediction:  61.750404357910156 \n",
      "\n",
      "Iteration 4193, Loss: 36.789127349853516, L1: 10.397311210632324, L3: 26.391815185546875\n",
      "Current prediction:  61.750423431396484 \n",
      "\n",
      "Iteration 4194, Loss: 36.88362503051758, L1: 10.388856887817383, L3: 26.494768142700195\n",
      "Current prediction:  61.75167465209961 \n",
      "\n",
      "Iteration 4195, Loss: 37.0012321472168, L1: 10.4091215133667, L3: 26.592111587524414\n",
      "Current prediction:  61.75069046020508 \n",
      "\n",
      "Iteration 4196, Loss: 36.64076232910156, L1: 10.406031608581543, L3: 26.234731674194336\n",
      "Current prediction:  61.751399993896484 \n",
      "\n",
      "Iteration 4197, Loss: 37.114505767822266, L1: 10.438780784606934, L3: 26.67572593688965\n",
      "Current prediction:  61.75477981567383 \n",
      "\n",
      "Iteration 4198, Loss: 36.936607360839844, L1: 10.403374671936035, L3: 26.533233642578125\n",
      "Current prediction:  61.75887680053711 \n",
      "\n",
      "Iteration 4199, Loss: 36.26029968261719, L1: 10.421852111816406, L3: 25.83844757080078\n",
      "Current prediction:  61.7665901184082 \n",
      "\n",
      "Iteration 4200, Loss: 37.18890380859375, L1: 10.379911422729492, L3: 26.808990478515625\n",
      "Current prediction:  61.77401351928711 \n",
      "\n",
      "Iteration 4201, Loss: 37.539676666259766, L1: 10.39627742767334, L3: 27.143400192260742\n",
      "Current prediction:  61.777130126953125 \n",
      "\n",
      "Iteration 4202, Loss: 36.51234436035156, L1: 10.385581970214844, L3: 26.126760482788086\n",
      "Current prediction:  61.78131866455078 \n",
      "\n",
      "Iteration 4203, Loss: 36.61510467529297, L1: 10.419147491455078, L3: 26.19595718383789\n",
      "Current prediction:  61.78532028198242 \n",
      "\n",
      "Iteration 4204, Loss: 35.61275100708008, L1: 10.407221794128418, L3: 25.205528259277344\n",
      "Current prediction:  61.792945861816406 \n",
      "\n",
      "Iteration 4205, Loss: 36.790138244628906, L1: 10.379829406738281, L3: 26.410308837890625\n",
      "Current prediction:  61.80080032348633 \n",
      "\n",
      "Iteration 4206, Loss: 36.34528350830078, L1: 10.361394882202148, L3: 25.983890533447266\n",
      "Current prediction:  61.80946350097656 \n",
      "\n",
      "Iteration 4207, Loss: 36.010704040527344, L1: 10.373781204223633, L3: 25.636924743652344\n",
      "Current prediction:  61.81821060180664 \n",
      "\n",
      "Iteration 4208, Loss: 37.475502014160156, L1: 10.356935501098633, L3: 27.118568420410156\n",
      "Current prediction:  61.8277473449707 \n",
      "\n",
      "Iteration 4209, Loss: 36.62961196899414, L1: 10.372574806213379, L3: 26.257038116455078\n",
      "Current prediction:  61.83509063720703 \n",
      "\n",
      "Iteration 4210, Loss: 36.31678009033203, L1: 10.343949317932129, L3: 25.972829818725586\n",
      "Current prediction:  61.842803955078125 \n",
      "\n",
      "Iteration 4211, Loss: 35.283504486083984, L1: 10.364720344543457, L3: 24.918785095214844\n",
      "Current prediction:  61.852134704589844 \n",
      "\n",
      "Iteration 4212, Loss: 36.39014434814453, L1: 10.364480018615723, L3: 26.025663375854492\n",
      "Current prediction:  61.85725021362305 \n",
      "\n",
      "Iteration 4213, Loss: 36.54048156738281, L1: 10.344114303588867, L3: 26.196369171142578\n",
      "Current prediction:  61.86109924316406 \n",
      "\n",
      "Iteration 4214, Loss: 37.32154083251953, L1: 10.345440864562988, L3: 26.97610092163086\n",
      "Current prediction:  61.86262893676758 \n",
      "\n",
      "Iteration 4215, Loss: 37.77723693847656, L1: 10.349320411682129, L3: 27.427915573120117\n",
      "Current prediction:  61.86226272583008 \n",
      "\n",
      "Iteration 4216, Loss: 37.09215545654297, L1: 10.339841842651367, L3: 26.75231170654297\n",
      "Current prediction:  61.86077117919922 \n",
      "\n",
      "Iteration 4217, Loss: 36.417396545410156, L1: 10.352395057678223, L3: 26.06500244140625\n",
      "Current prediction:  61.86001205444336 \n",
      "\n",
      "Iteration 4218, Loss: 36.31690979003906, L1: 10.330031394958496, L3: 25.98687744140625\n",
      "Current prediction:  61.85783004760742 \n",
      "\n",
      "Iteration 4219, Loss: 36.822017669677734, L1: 10.327778816223145, L3: 26.494239807128906\n",
      "Current prediction:  61.85707473754883 \n",
      "\n",
      "Iteration 4220, Loss: 38.01937484741211, L1: 10.3305082321167, L3: 27.688865661621094\n",
      "Current prediction:  61.85369873046875 \n",
      "\n",
      "Iteration 4221, Loss: 37.004146575927734, L1: 10.320021629333496, L3: 26.684125900268555\n",
      "Current prediction:  61.85175323486328 \n",
      "\n",
      "Iteration 4222, Loss: 36.445194244384766, L1: 10.31638240814209, L3: 26.12881088256836\n",
      "Current prediction:  61.84819412231445 \n",
      "\n",
      "Iteration 4223, Loss: 37.57682800292969, L1: 10.326762199401855, L3: 27.250064849853516\n",
      "Current prediction:  61.84405517578125 \n",
      "\n",
      "Iteration 4224, Loss: 36.28553009033203, L1: 10.359353065490723, L3: 25.926176071166992\n",
      "Current prediction:  61.84043502807617 \n",
      "\n",
      "Iteration 4225, Loss: 36.94179916381836, L1: 10.356547355651855, L3: 26.58525276184082\n",
      "Current prediction:  61.836647033691406 \n",
      "\n",
      "Iteration 4226, Loss: 36.76670455932617, L1: 10.369373321533203, L3: 26.39733123779297\n",
      "Current prediction:  61.75940704345703 \n",
      "\n",
      "Iteration 4227, Loss: 36.671932220458984, L1: 10.367984771728516, L3: 26.30394744873047\n",
      "Current prediction:  61.221492767333984 \n",
      "\n",
      "Iteration 4228, Loss: 37.430030822753906, L1: 10.411969184875488, L3: 27.018062591552734\n",
      "Current prediction:  61.042236328125 \n",
      "\n",
      "Iteration 4229, Loss: 36.75450134277344, L1: 10.42052936553955, L3: 26.33397102355957\n",
      "Current prediction:  61.032432556152344 \n",
      "\n",
      "Iteration 4230, Loss: 36.48918914794922, L1: 10.448975563049316, L3: 26.040212631225586\n",
      "Current prediction:  61.031890869140625 \n",
      "\n",
      "Iteration 4231, Loss: 37.304954528808594, L1: 10.441096305847168, L3: 26.863859176635742\n",
      "Current prediction:  61.03357696533203 \n",
      "\n",
      "Iteration 4232, Loss: 36.70159149169922, L1: 10.530890464782715, L3: 26.17070198059082\n",
      "Current prediction:  61.03645706176758 \n",
      "\n",
      "Iteration 4233, Loss: 37.20663070678711, L1: 10.50664234161377, L3: 26.699987411499023\n",
      "Current prediction:  61.039974212646484 \n",
      "\n",
      "Iteration 4234, Loss: 36.829345703125, L1: 10.519440650939941, L3: 26.309906005859375\n",
      "Current prediction:  61.04408645629883 \n",
      "\n",
      "Iteration 4235, Loss: 36.4134521484375, L1: 10.444069862365723, L3: 25.96938133239746\n",
      "Current prediction:  61.04951858520508 \n",
      "\n",
      "Iteration 4236, Loss: 36.36172866821289, L1: 10.524645805358887, L3: 25.83708381652832\n",
      "Current prediction:  61.05400085449219 \n",
      "\n",
      "Iteration 4237, Loss: 36.17350769042969, L1: 10.592848777770996, L3: 25.580657958984375\n",
      "Current prediction:  61.05772399902344 \n",
      "\n",
      "Iteration 4238, Loss: 36.479644775390625, L1: 10.538905143737793, L3: 25.94074058532715\n",
      "Current prediction:  61.06059646606445 \n",
      "\n",
      "Iteration 4239, Loss: 38.11542510986328, L1: 10.548556327819824, L3: 27.56686782836914\n",
      "Current prediction:  61.06209945678711 \n",
      "\n",
      "Iteration 4240, Loss: 36.96806335449219, L1: 10.451520919799805, L3: 26.516544342041016\n",
      "Current prediction:  61.060401916503906 \n",
      "\n",
      "Iteration 4241, Loss: 36.6239013671875, L1: 10.408885955810547, L3: 26.215015411376953\n",
      "Current prediction:  61.06126022338867 \n",
      "\n",
      "Iteration 4242, Loss: 36.78522872924805, L1: 10.43361759185791, L3: 26.351612091064453\n",
      "Current prediction:  61.07044982910156 \n",
      "\n",
      "Iteration 4243, Loss: 37.20680236816406, L1: 10.426383972167969, L3: 26.780418395996094\n",
      "Current prediction:  61.17475891113281 \n",
      "\n",
      "Iteration 4244, Loss: 36.06449890136719, L1: 10.341262817382812, L3: 25.723236083984375\n",
      "Current prediction:  61.64621353149414 \n",
      "\n",
      "Iteration 4245, Loss: 36.723228454589844, L1: 10.365665435791016, L3: 26.35756492614746\n",
      "Current prediction:  61.799278259277344 \n",
      "\n",
      "Iteration 4246, Loss: 36.11338806152344, L1: 10.354703903198242, L3: 25.758686065673828\n",
      "Current prediction:  61.8388671875 \n",
      "\n",
      "Iteration 4247, Loss: 36.05474090576172, L1: 10.361824989318848, L3: 25.692914962768555\n",
      "Current prediction:  61.83989715576172 \n",
      "\n",
      "Iteration 4248, Loss: 36.43123245239258, L1: 10.370131492614746, L3: 26.06110191345215\n",
      "Current prediction:  61.6800422668457 \n",
      "\n",
      "Iteration 4249, Loss: 37.4352912902832, L1: 10.37590503692627, L3: 27.05938720703125\n",
      "Current prediction:  61.05447769165039 \n",
      "\n",
      "Iteration 4250, Loss: 35.93576431274414, L1: 10.413784980773926, L3: 25.5219783782959\n",
      "Current prediction:  60.999942779541016 \n",
      "\n",
      "Iteration 4251, Loss: 37.56147003173828, L1: 10.431625366210938, L3: 27.129846572875977\n",
      "Current prediction:  60.996864318847656 \n",
      "\n",
      "Iteration 4252, Loss: 36.72120666503906, L1: 10.451029777526855, L3: 26.270177841186523\n",
      "Current prediction:  61.000091552734375 \n",
      "\n",
      "Iteration 4253, Loss: 37.67754364013672, L1: 10.445971488952637, L3: 27.231571197509766\n",
      "Current prediction:  61.00460433959961 \n",
      "\n",
      "Iteration 4254, Loss: 37.527503967285156, L1: 10.461928367614746, L3: 27.065574645996094\n",
      "Current prediction:  61.01017761230469 \n",
      "\n",
      "Iteration 4255, Loss: 36.54977035522461, L1: 10.452556610107422, L3: 26.097213745117188\n",
      "Current prediction:  61.02185821533203 \n",
      "\n",
      "Iteration 4256, Loss: 37.65199279785156, L1: 10.384642601013184, L3: 27.267349243164062\n",
      "Current prediction:  61.06356430053711 \n",
      "\n",
      "Iteration 4257, Loss: 37.45566940307617, L1: 10.373539924621582, L3: 27.082128524780273\n",
      "Current prediction:  61.48415756225586 \n",
      "\n",
      "Iteration 4258, Loss: 37.342926025390625, L1: 10.28628158569336, L3: 27.056644439697266\n",
      "Current prediction:  61.858612060546875 \n",
      "\n",
      "Iteration 4259, Loss: 35.60057067871094, L1: 10.316007614135742, L3: 25.284563064575195\n",
      "Current prediction:  61.88560104370117 \n",
      "\n",
      "Iteration 4260, Loss: 36.01677703857422, L1: 10.290437698364258, L3: 25.726341247558594\n",
      "Current prediction:  61.885616302490234 \n",
      "\n",
      "Iteration 4261, Loss: 36.75935363769531, L1: 10.227149963378906, L3: 26.532203674316406\n",
      "Current prediction:  61.884552001953125 \n",
      "\n",
      "Iteration 4262, Loss: 36.83642578125, L1: 10.275115966796875, L3: 26.561309814453125\n",
      "Current prediction:  61.88164138793945 \n",
      "\n",
      "Iteration 4263, Loss: 36.0487060546875, L1: 10.255773544311523, L3: 25.792930603027344\n",
      "Current prediction:  61.880043029785156 \n",
      "\n",
      "Iteration 4264, Loss: 36.29704666137695, L1: 10.278151512145996, L3: 26.01889419555664\n",
      "Current prediction:  61.875640869140625 \n",
      "\n",
      "Iteration 4265, Loss: 35.510189056396484, L1: 10.227355003356934, L3: 25.282833099365234\n",
      "Current prediction:  61.86853790283203 \n",
      "\n",
      "Iteration 4266, Loss: 36.973758697509766, L1: 10.2650785446167, L3: 26.708681106567383\n",
      "Current prediction:  61.8593864440918 \n",
      "\n",
      "Iteration 4267, Loss: 36.29095458984375, L1: 10.287856101989746, L3: 26.00309944152832\n",
      "Current prediction:  61.85152816772461 \n",
      "\n",
      "Iteration 4268, Loss: 36.916969299316406, L1: 10.273072242736816, L3: 26.643896102905273\n",
      "Current prediction:  61.84298324584961 \n",
      "\n",
      "Iteration 4269, Loss: 36.36425018310547, L1: 10.282209396362305, L3: 26.082042694091797\n",
      "Current prediction:  61.83407211303711 \n",
      "\n",
      "Iteration 4270, Loss: 36.11216735839844, L1: 10.265670776367188, L3: 25.846494674682617\n",
      "Current prediction:  61.82354736328125 \n",
      "\n",
      "Iteration 4271, Loss: 37.77446365356445, L1: 10.33459186553955, L3: 27.43987274169922\n",
      "Current prediction:  61.81179428100586 \n",
      "\n",
      "Iteration 4272, Loss: 36.58737564086914, L1: 10.290144920349121, L3: 26.297231674194336\n",
      "Current prediction:  61.8021354675293 \n",
      "\n",
      "Iteration 4273, Loss: 36.14633560180664, L1: 10.297216415405273, L3: 25.849119186401367\n",
      "Current prediction:  61.79033660888672 \n",
      "\n",
      "Iteration 4274, Loss: 36.7729606628418, L1: 10.274685859680176, L3: 26.498275756835938\n",
      "Current prediction:  61.77906799316406 \n",
      "\n",
      "Iteration 4275, Loss: 36.51616668701172, L1: 10.34142017364502, L3: 26.174747467041016\n",
      "Current prediction:  61.76477813720703 \n",
      "\n",
      "Iteration 4276, Loss: 37.53268051147461, L1: 10.330387115478516, L3: 27.202293395996094\n",
      "Current prediction:  61.74821472167969 \n",
      "\n",
      "Iteration 4277, Loss: 36.34166717529297, L1: 10.336106300354004, L3: 26.00556182861328\n",
      "Current prediction:  61.73179626464844 \n",
      "\n",
      "Iteration 4278, Loss: 36.58454132080078, L1: 10.403120040893555, L3: 26.181419372558594\n",
      "Current prediction:  61.71466827392578 \n",
      "\n",
      "Iteration 4279, Loss: 35.950965881347656, L1: 10.40434455871582, L3: 25.546621322631836\n",
      "Current prediction:  61.70077133178711 \n",
      "\n",
      "Iteration 4280, Loss: 37.27783203125, L1: 10.42566204071045, L3: 26.852169036865234\n",
      "Current prediction:  61.68893051147461 \n",
      "\n",
      "Iteration 4281, Loss: 36.741363525390625, L1: 10.45340347290039, L3: 26.287961959838867\n",
      "Current prediction:  61.67759704589844 \n",
      "\n",
      "Iteration 4282, Loss: 35.745361328125, L1: 10.421499252319336, L3: 25.323862075805664\n",
      "Current prediction:  61.669891357421875 \n",
      "\n",
      "Iteration 4283, Loss: 37.45347595214844, L1: 10.41569709777832, L3: 27.03778076171875\n",
      "Current prediction:  61.664398193359375 \n",
      "\n",
      "Iteration 4284, Loss: 36.61178207397461, L1: 10.466891288757324, L3: 26.14488983154297\n",
      "Current prediction:  61.65959930419922 \n",
      "\n",
      "Iteration 4285, Loss: 36.38726806640625, L1: 10.43738079071045, L3: 25.949888229370117\n",
      "Current prediction:  61.66013717651367 \n",
      "\n",
      "Iteration 4286, Loss: 35.70677185058594, L1: 10.462810516357422, L3: 25.243961334228516\n",
      "Current prediction:  61.65998840332031 \n",
      "\n",
      "Iteration 4287, Loss: 36.796913146972656, L1: 10.475543975830078, L3: 26.321369171142578\n",
      "Current prediction:  61.660396575927734 \n",
      "\n",
      "Iteration 4288, Loss: 37.33057403564453, L1: 10.503419876098633, L3: 26.827152252197266\n",
      "Current prediction:  61.42689514160156 \n",
      "\n",
      "Iteration 4289, Loss: 36.780906677246094, L1: 10.517797470092773, L3: 26.26310920715332\n",
      "Current prediction:  60.94515609741211 \n",
      "\n",
      "Iteration 4290, Loss: 36.597129821777344, L1: 10.512497901916504, L3: 26.084630966186523\n",
      "Current prediction:  60.92386245727539 \n",
      "\n",
      "Iteration 4291, Loss: 36.88717269897461, L1: 10.5118989944458, L3: 26.375274658203125\n",
      "Current prediction:  60.947052001953125 \n",
      "\n",
      "Iteration 4292, Loss: 36.778751373291016, L1: 10.522496223449707, L3: 26.256256103515625\n",
      "Current prediction:  60.979068756103516 \n",
      "\n",
      "Iteration 4293, Loss: 37.10630798339844, L1: 10.489415168762207, L3: 26.616893768310547\n",
      "Current prediction:  61.02313995361328 \n",
      "\n",
      "Iteration 4294, Loss: 37.89316177368164, L1: 10.44357967376709, L3: 27.449581146240234\n",
      "Current prediction:  61.312252044677734 \n",
      "\n",
      "Iteration 4295, Loss: 37.75800323486328, L1: 10.37255859375, L3: 27.385446548461914\n",
      "Current prediction:  61.83187484741211 \n",
      "\n",
      "Iteration 4296, Loss: 35.496490478515625, L1: 10.308626174926758, L3: 25.1878662109375\n",
      "Current prediction:  61.87343215942383 \n",
      "\n",
      "Iteration 4297, Loss: 36.81425857543945, L1: 10.293889045715332, L3: 26.520368576049805\n",
      "Current prediction:  61.89295196533203 \n",
      "\n",
      "Iteration 4298, Loss: 36.22520446777344, L1: 10.250743865966797, L3: 25.974462509155273\n",
      "Current prediction:  61.9084358215332 \n",
      "\n",
      "Iteration 4299, Loss: 36.0245475769043, L1: 10.221997261047363, L3: 25.80255126953125\n",
      "Current prediction:  61.91846466064453 \n",
      "\n",
      "Iteration 4300, Loss: 36.41199493408203, L1: 10.197083473205566, L3: 26.21491241455078\n",
      "Current prediction:  61.92347717285156 \n",
      "\n",
      "Iteration 4301, Loss: 37.93734359741211, L1: 10.2069730758667, L3: 27.730371475219727\n",
      "Current prediction:  61.92277526855469 \n",
      "\n",
      "Iteration 4302, Loss: 37.83224105834961, L1: 10.170846939086914, L3: 27.661394119262695\n",
      "Current prediction:  61.9157600402832 \n",
      "\n",
      "Iteration 4303, Loss: 36.65099334716797, L1: 10.215418815612793, L3: 26.435575485229492\n",
      "Current prediction:  61.90216827392578 \n",
      "\n",
      "Iteration 4304, Loss: 36.27366638183594, L1: 10.195796012878418, L3: 26.077869415283203\n",
      "Current prediction:  61.88467025756836 \n",
      "\n",
      "Iteration 4305, Loss: 35.81710433959961, L1: 10.185138702392578, L3: 25.63196563720703\n",
      "Current prediction:  61.8663444519043 \n",
      "\n",
      "Iteration 4306, Loss: 36.020606994628906, L1: 10.23677921295166, L3: 25.78382682800293\n",
      "Current prediction:  61.84309387207031 \n",
      "\n",
      "Iteration 4307, Loss: 37.028656005859375, L1: 10.284035682678223, L3: 26.744619369506836\n",
      "Current prediction:  61.81898498535156 \n",
      "\n",
      "Iteration 4308, Loss: 36.39964294433594, L1: 10.33519172668457, L3: 26.064453125\n",
      "Current prediction:  61.748714447021484 \n",
      "\n",
      "Iteration 4309, Loss: 36.83772277832031, L1: 10.340295791625977, L3: 26.497425079345703\n",
      "Current prediction:  61.1197395324707 \n",
      "\n",
      "Iteration 4310, Loss: 36.23764419555664, L1: 10.398972511291504, L3: 25.838672637939453\n",
      "Current prediction:  60.943748474121094 \n",
      "\n",
      "Iteration 4311, Loss: 36.306724548339844, L1: 10.455719947814941, L3: 25.85100555419922\n",
      "Current prediction:  60.92982482910156 \n",
      "\n",
      "Iteration 4312, Loss: 36.48933410644531, L1: 10.484006881713867, L3: 26.005329132080078\n",
      "Current prediction:  60.92894744873047 \n",
      "\n",
      "Iteration 4313, Loss: 36.39391326904297, L1: 10.463973045349121, L3: 25.929941177368164\n",
      "Current prediction:  60.96183395385742 \n",
      "\n",
      "Iteration 4314, Loss: 37.343345642089844, L1: 10.46574592590332, L3: 26.87759780883789\n",
      "Current prediction:  61.50577163696289 \n",
      "\n",
      "Iteration 4315, Loss: 36.089969635009766, L1: 10.443685531616211, L3: 25.646284103393555\n",
      "Current prediction:  61.75533676147461 \n",
      "\n",
      "Iteration 4316, Loss: 36.17865753173828, L1: 10.42124080657959, L3: 25.757415771484375\n",
      "Current prediction:  61.778099060058594 \n",
      "\n",
      "Iteration 4317, Loss: 36.678680419921875, L1: 10.363876342773438, L3: 26.31480598449707\n",
      "Current prediction:  61.79600524902344 \n",
      "\n",
      "Iteration 4318, Loss: 38.0077018737793, L1: 10.338570594787598, L3: 27.669130325317383\n",
      "Current prediction:  61.81345748901367 \n",
      "\n",
      "Iteration 4319, Loss: 37.569942474365234, L1: 10.324471473693848, L3: 27.245471954345703\n",
      "Current prediction:  61.82379150390625 \n",
      "\n",
      "Iteration 4320, Loss: 37.15557861328125, L1: 10.285820007324219, L3: 26.8697566986084\n",
      "Current prediction:  61.83052444458008 \n",
      "\n",
      "Iteration 4321, Loss: 37.14006805419922, L1: 10.334863662719727, L3: 26.80520248413086\n",
      "Current prediction:  61.833805084228516 \n",
      "\n",
      "Iteration 4322, Loss: 35.28621292114258, L1: 10.284109115600586, L3: 25.002103805541992\n",
      "Current prediction:  61.835533142089844 \n",
      "\n",
      "Iteration 4323, Loss: 37.752838134765625, L1: 10.309661865234375, L3: 27.443178176879883\n",
      "Current prediction:  61.836830139160156 \n",
      "\n",
      "Iteration 4324, Loss: 36.68174362182617, L1: 10.278861045837402, L3: 26.402881622314453\n",
      "Current prediction:  61.83711624145508 \n",
      "\n",
      "Iteration 4325, Loss: 35.90324401855469, L1: 10.281702995300293, L3: 25.621540069580078\n",
      "Current prediction:  61.837242126464844 \n",
      "\n",
      "Iteration 4326, Loss: 37.835853576660156, L1: 10.304340362548828, L3: 27.53151512145996\n",
      "Current prediction:  61.835323333740234 \n",
      "\n",
      "Iteration 4327, Loss: 37.545413970947266, L1: 10.285914421081543, L3: 27.25950050354004\n",
      "Current prediction:  61.83454513549805 \n",
      "\n",
      "Iteration 4328, Loss: 36.375892639160156, L1: 10.310531616210938, L3: 26.06536102294922\n",
      "Current prediction:  61.818878173828125 \n",
      "\n",
      "Iteration 4329, Loss: 36.54912567138672, L1: 10.298500061035156, L3: 26.250625610351562\n",
      "Current prediction:  61.54234313964844 \n",
      "\n",
      "Iteration 4330, Loss: 37.3919563293457, L1: 10.31579875946045, L3: 27.076156616210938\n",
      "Current prediction:  61.17414093017578 \n",
      "\n",
      "Iteration 4331, Loss: 36.81444549560547, L1: 10.331343650817871, L3: 26.483102798461914\n",
      "Current prediction:  61.05000686645508 \n",
      "\n",
      "Iteration 4332, Loss: 36.26701354980469, L1: 10.374953269958496, L3: 25.892061233520508\n",
      "Current prediction:  61.02378463745117 \n",
      "\n",
      "Iteration 4333, Loss: 36.1530647277832, L1: 10.347262382507324, L3: 25.805801391601562\n",
      "Current prediction:  61.03373336791992 \n",
      "\n",
      "Iteration 4334, Loss: 36.491058349609375, L1: 10.355268478393555, L3: 26.135791778564453\n",
      "Current prediction:  61.09482955932617 \n",
      "\n",
      "Iteration 4335, Loss: 36.80837631225586, L1: 10.340019226074219, L3: 26.46835708618164\n",
      "Current prediction:  61.25541305541992 \n",
      "\n",
      "Iteration 4336, Loss: 36.42183303833008, L1: 10.305228233337402, L3: 26.11660385131836\n",
      "Current prediction:  61.563385009765625 \n",
      "\n",
      "Iteration 4337, Loss: 36.08437728881836, L1: 10.286526679992676, L3: 25.797849655151367\n",
      "Current prediction:  61.805782318115234 \n",
      "\n",
      "Iteration 4338, Loss: 36.096710205078125, L1: 10.27755355834961, L3: 25.819156646728516\n",
      "Current prediction:  61.862972259521484 \n",
      "\n",
      "Iteration 4339, Loss: 36.80265808105469, L1: 10.29699420928955, L3: 26.50566291809082\n",
      "Current prediction:  61.87672805786133 \n",
      "\n",
      "Iteration 4340, Loss: 37.19194793701172, L1: 10.280508995056152, L3: 26.911439895629883\n",
      "Current prediction:  61.87614822387695 \n",
      "\n",
      "Iteration 4341, Loss: 35.863372802734375, L1: 10.232507705688477, L3: 25.63086700439453\n",
      "Current prediction:  61.87425231933594 \n",
      "\n",
      "Iteration 4342, Loss: 35.943519592285156, L1: 10.28213119506836, L3: 25.661388397216797\n",
      "Current prediction:  61.869293212890625 \n",
      "\n",
      "Iteration 4343, Loss: 37.24272537231445, L1: 10.278119087219238, L3: 26.9646053314209\n",
      "Current prediction:  61.859642028808594 \n",
      "\n",
      "Iteration 4344, Loss: 35.79128646850586, L1: 10.27802562713623, L3: 25.513261795043945\n",
      "Current prediction:  61.847984313964844 \n",
      "\n",
      "Iteration 4345, Loss: 36.713294982910156, L1: 10.230451583862305, L3: 26.48284339904785\n",
      "Current prediction:  61.83186340332031 \n",
      "\n",
      "Iteration 4346, Loss: 37.71222686767578, L1: 10.249737739562988, L3: 27.462488174438477\n",
      "Current prediction:  61.81244659423828 \n",
      "\n",
      "Iteration 4347, Loss: 36.257057189941406, L1: 10.295554161071777, L3: 25.961502075195312\n",
      "Current prediction:  61.79507064819336 \n",
      "\n",
      "Iteration 4348, Loss: 35.798805236816406, L1: 10.292312622070312, L3: 25.506494522094727\n",
      "Current prediction:  61.77607727050781 \n",
      "\n",
      "Iteration 4349, Loss: 36.00358963012695, L1: 10.352243423461914, L3: 25.65134620666504\n",
      "Current prediction:  61.75641632080078 \n",
      "\n",
      "Iteration 4350, Loss: 37.33671188354492, L1: 10.373272895812988, L3: 26.963438034057617\n",
      "Current prediction:  61.73458480834961 \n",
      "\n",
      "Iteration 4351, Loss: 36.648704528808594, L1: 10.401870727539062, L3: 26.2468318939209\n",
      "Current prediction:  61.65455627441406 \n",
      "\n",
      "Iteration 4352, Loss: 36.099571228027344, L1: 10.422564506530762, L3: 25.677005767822266\n",
      "Current prediction:  61.26570510864258 \n",
      "\n",
      "Iteration 4353, Loss: 37.142173767089844, L1: 10.455768585205078, L3: 26.686403274536133\n",
      "Current prediction:  60.93101119995117 \n",
      "\n",
      "Iteration 4354, Loss: 36.97798156738281, L1: 10.457236289978027, L3: 26.5207462310791\n",
      "Current prediction:  60.844791412353516 \n",
      "\n",
      "Iteration 4355, Loss: 36.14706802368164, L1: 10.463337898254395, L3: 25.68372917175293\n",
      "Current prediction:  60.90026092529297 \n",
      "\n",
      "Iteration 4356, Loss: 37.09394073486328, L1: 10.467792510986328, L3: 26.626150131225586\n",
      "Current prediction:  61.125877380371094 \n",
      "\n",
      "Iteration 4357, Loss: 37.26288986206055, L1: 10.42944622039795, L3: 26.833444595336914\n",
      "Current prediction:  61.448909759521484 \n",
      "\n",
      "Iteration 4358, Loss: 37.491302490234375, L1: 10.394598007202148, L3: 27.09670639038086\n",
      "Current prediction:  61.64527893066406 \n",
      "\n",
      "Iteration 4359, Loss: 36.20105743408203, L1: 10.377063751220703, L3: 25.823991775512695\n",
      "Current prediction:  61.73823547363281 \n",
      "\n",
      "Iteration 4360, Loss: 37.18532943725586, L1: 10.353318214416504, L3: 26.832012176513672\n",
      "Current prediction:  61.78129959106445 \n",
      "\n",
      "Iteration 4361, Loss: 35.621856689453125, L1: 10.350313186645508, L3: 25.271541595458984\n",
      "Current prediction:  61.80173873901367 \n",
      "\n",
      "Iteration 4362, Loss: 36.29176330566406, L1: 10.317575454711914, L3: 25.97418785095215\n",
      "Current prediction:  61.8039665222168 \n",
      "\n",
      "Iteration 4363, Loss: 36.96680450439453, L1: 10.298797607421875, L3: 26.66800880432129\n",
      "Current prediction:  61.826324462890625 \n",
      "\n",
      "Iteration 4364, Loss: 37.439308166503906, L1: 10.30085277557373, L3: 27.138456344604492\n",
      "Current prediction:  61.853816986083984 \n",
      "\n",
      "Iteration 4365, Loss: 37.11100387573242, L1: 10.266216278076172, L3: 26.84478759765625\n",
      "Current prediction:  61.87405776977539 \n",
      "\n",
      "Iteration 4366, Loss: 36.8137321472168, L1: 10.258869171142578, L3: 26.55486297607422\n",
      "Current prediction:  61.85188674926758 \n",
      "\n",
      "Iteration 4367, Loss: 36.94782257080078, L1: 10.232868194580078, L3: 26.714956283569336\n",
      "Current prediction:  61.841854095458984 \n",
      "\n",
      "Iteration 4368, Loss: 35.67138671875, L1: 10.235090255737305, L3: 25.436296463012695\n",
      "Current prediction:  61.89121627807617 \n",
      "\n",
      "Iteration 4369, Loss: 35.99700164794922, L1: 10.231647491455078, L3: 25.765352249145508\n",
      "Current prediction:  61.9504280090332 \n",
      "\n",
      "Iteration 4370, Loss: 36.67008972167969, L1: 10.224843978881836, L3: 26.44524383544922\n",
      "Current prediction:  61.96139144897461 \n",
      "\n",
      "Iteration 4371, Loss: 36.21562576293945, L1: 10.205440521240234, L3: 26.01018524169922\n",
      "Current prediction:  61.96295928955078 \n",
      "\n",
      "Iteration 4372, Loss: 35.41588592529297, L1: 10.211347579956055, L3: 25.204538345336914\n",
      "Current prediction:  61.96184158325195 \n",
      "\n",
      "Iteration 4373, Loss: 36.384307861328125, L1: 10.182644844055176, L3: 26.201663970947266\n",
      "Current prediction:  61.93562316894531 \n",
      "\n",
      "Iteration 4374, Loss: 36.23676300048828, L1: 10.136061668395996, L3: 26.1007022857666\n",
      "Current prediction:  61.66657257080078 \n",
      "\n",
      "Iteration 4375, Loss: 36.861515045166016, L1: 10.167891502380371, L3: 26.69362449645996\n",
      "Current prediction:  61.209285736083984 \n",
      "\n",
      "Iteration 4376, Loss: 36.761898040771484, L1: 10.217719078063965, L3: 26.544178009033203\n",
      "Current prediction:  61.11454391479492 \n",
      "\n",
      "Iteration 4377, Loss: 36.67517852783203, L1: 10.297511100769043, L3: 26.377666473388672\n",
      "Current prediction:  61.09864044189453 \n",
      "\n",
      "Iteration 4378, Loss: 36.43489456176758, L1: 10.286018371582031, L3: 26.148876190185547\n",
      "Current prediction:  61.08637619018555 \n",
      "\n",
      "Iteration 4379, Loss: 37.09721374511719, L1: 10.30770206451416, L3: 26.789512634277344\n",
      "Current prediction:  61.07572555541992 \n",
      "\n",
      "Iteration 4380, Loss: 37.289188385009766, L1: 10.359519004821777, L3: 26.929668426513672\n",
      "Current prediction:  61.067291259765625 \n",
      "\n",
      "Iteration 4381, Loss: 36.748050689697266, L1: 10.317780494689941, L3: 26.430269241333008\n",
      "Current prediction:  61.06057357788086 \n",
      "\n",
      "Iteration 4382, Loss: 37.04547119140625, L1: 10.295934677124023, L3: 26.74953842163086\n",
      "Current prediction:  61.064109802246094 \n",
      "\n",
      "Iteration 4383, Loss: 37.26179504394531, L1: 10.312986373901367, L3: 26.948808670043945\n",
      "Current prediction:  61.21592712402344 \n",
      "\n",
      "Iteration 4384, Loss: 36.11798858642578, L1: 10.288768768310547, L3: 25.829221725463867\n",
      "Current prediction:  61.80021286010742 \n",
      "\n",
      "Iteration 4385, Loss: 36.6082649230957, L1: 10.27021312713623, L3: 26.33805274963379\n",
      "Current prediction:  61.8624153137207 \n",
      "\n",
      "Iteration 4386, Loss: 38.8958854675293, L1: 10.2532377243042, L3: 28.64264678955078\n",
      "Current prediction:  61.86128616333008 \n",
      "\n",
      "Iteration 4387, Loss: 37.478355407714844, L1: 10.247416496276855, L3: 27.230937957763672\n",
      "Current prediction:  61.85856246948242 \n",
      "\n",
      "Iteration 4388, Loss: 36.63695526123047, L1: 10.221235275268555, L3: 26.41571807861328\n",
      "Current prediction:  61.852848052978516 \n",
      "\n",
      "Iteration 4389, Loss: 36.37248992919922, L1: 10.233940124511719, L3: 26.1385498046875\n",
      "Current prediction:  61.846832275390625 \n",
      "\n",
      "Iteration 4390, Loss: 37.35139465332031, L1: 10.241321563720703, L3: 27.110071182250977\n",
      "Current prediction:  61.839447021484375 \n",
      "\n",
      "Iteration 4391, Loss: 36.88337326049805, L1: 10.274791717529297, L3: 26.60858154296875\n",
      "Current prediction:  61.83253479003906 \n",
      "\n",
      "Iteration 4392, Loss: 36.232322692871094, L1: 10.272833824157715, L3: 25.959489822387695\n",
      "Current prediction:  61.82440185546875 \n",
      "\n",
      "Iteration 4393, Loss: 36.72798538208008, L1: 10.283342361450195, L3: 26.444643020629883\n",
      "Current prediction:  61.81532669067383 \n",
      "\n",
      "Iteration 4394, Loss: 36.13249206542969, L1: 10.270569801330566, L3: 25.861921310424805\n",
      "Current prediction:  61.80481719970703 \n",
      "\n",
      "Iteration 4395, Loss: 35.81856155395508, L1: 10.29886245727539, L3: 25.519699096679688\n",
      "Current prediction:  61.79082489013672 \n",
      "\n",
      "Iteration 4396, Loss: 37.02840042114258, L1: 10.263984680175781, L3: 26.764415740966797\n",
      "Current prediction:  61.762760162353516 \n",
      "\n",
      "Iteration 4397, Loss: 36.784393310546875, L1: 10.304119110107422, L3: 26.48027229309082\n",
      "Current prediction:  61.70937728881836 \n",
      "\n",
      "Iteration 4398, Loss: 36.00984191894531, L1: 10.334980010986328, L3: 25.674861907958984\n",
      "Current prediction:  61.71148681640625 \n",
      "\n",
      "Iteration 4399, Loss: 37.1721076965332, L1: 10.297749519348145, L3: 26.874359130859375\n",
      "Current prediction:  61.73756790161133 \n",
      "\n",
      "Iteration 4400, Loss: 37.23452377319336, L1: 10.591458320617676, L3: 26.64306640625\n",
      "Current prediction:  61.77672576904297 \n",
      "\n",
      "Iteration 4401, Loss: 37.04288864135742, L1: 10.347973823547363, L3: 26.694915771484375\n",
      "Current prediction:  61.783199310302734 \n",
      "\n",
      "Iteration 4402, Loss: 36.26372528076172, L1: 10.561460494995117, L3: 25.70226287841797\n",
      "Current prediction:  61.79500961303711 \n",
      "\n",
      "Iteration 4403, Loss: 36.91132354736328, L1: 10.495595932006836, L3: 26.415727615356445\n",
      "Current prediction:  61.80592727661133 \n",
      "\n",
      "Iteration 4404, Loss: 36.318199157714844, L1: 10.44454574584961, L3: 25.8736515045166\n",
      "Current prediction:  61.474456787109375 \n",
      "\n",
      "Iteration 4405, Loss: 37.577293395996094, L1: 10.572832107543945, L3: 27.00446319580078\n",
      "Current prediction:  61.102718353271484 \n",
      "\n",
      "Iteration 4406, Loss: 37.11618423461914, L1: 10.663031578063965, L3: 26.453153610229492\n",
      "Current prediction:  61.13225555419922 \n",
      "\n",
      "Iteration 4407, Loss: 36.53002166748047, L1: 10.680590629577637, L3: 25.849430084228516\n",
      "Current prediction:  61.16849136352539 \n",
      "\n",
      "Iteration 4408, Loss: 36.494808197021484, L1: 10.63898754119873, L3: 25.85582160949707\n",
      "Current prediction:  61.21084213256836 \n",
      "\n",
      "Iteration 4409, Loss: 36.45953369140625, L1: 10.558799743652344, L3: 25.900732040405273\n",
      "Current prediction:  61.97610855102539 \n",
      "\n",
      "Iteration 4410, Loss: 36.276763916015625, L1: 10.332479476928711, L3: 25.944284439086914\n",
      "Current prediction:  62.03705596923828 \n",
      "\n",
      "Iteration 4411, Loss: 36.14366149902344, L1: 10.224569320678711, L3: 25.91909408569336\n",
      "Current prediction:  62.07809829711914 \n",
      "\n",
      "Iteration 4412, Loss: 37.00199890136719, L1: 10.241619110107422, L3: 26.760377883911133\n",
      "Current prediction:  62.10942077636719 \n",
      "\n",
      "Iteration 4413, Loss: 37.23292541503906, L1: 10.138147354125977, L3: 27.094778060913086\n",
      "Current prediction:  62.13025665283203 \n",
      "\n",
      "Iteration 4414, Loss: 36.84754180908203, L1: 10.164708137512207, L3: 26.68283462524414\n",
      "Current prediction:  62.14459991455078 \n",
      "\n",
      "Iteration 4415, Loss: 36.13467025756836, L1: 10.19947338104248, L3: 25.935197830200195\n",
      "Current prediction:  62.15468215942383 \n",
      "\n",
      "Iteration 4416, Loss: 37.16562271118164, L1: 10.279651641845703, L3: 26.885971069335938\n",
      "Current prediction:  62.15949249267578 \n",
      "\n",
      "Iteration 4417, Loss: 36.42371368408203, L1: 10.240169525146484, L3: 26.18354606628418\n",
      "Current prediction:  62.16633605957031 \n",
      "\n",
      "Iteration 4418, Loss: 37.33409118652344, L1: 10.265589714050293, L3: 27.068500518798828\n",
      "Current prediction:  62.17321014404297 \n",
      "\n",
      "Iteration 4419, Loss: 37.40342712402344, L1: 10.182212829589844, L3: 27.221216201782227\n",
      "Current prediction:  62.17169952392578 \n",
      "\n",
      "Iteration 4420, Loss: 36.96298599243164, L1: 10.228190422058105, L3: 26.73479652404785\n",
      "Current prediction:  62.1634407043457 \n",
      "\n",
      "Iteration 4421, Loss: 37.26114273071289, L1: 10.233593940734863, L3: 27.027549743652344\n",
      "Current prediction:  62.14703369140625 \n",
      "\n",
      "Iteration 4422, Loss: 37.23795700073242, L1: 10.21119499206543, L3: 27.026762008666992\n",
      "Current prediction:  62.11652755737305 \n",
      "\n",
      "Iteration 4423, Loss: 37.41761016845703, L1: 10.193782806396484, L3: 27.223827362060547\n",
      "Current prediction:  62.027557373046875 \n",
      "\n",
      "Iteration 4424, Loss: 37.338172912597656, L1: 10.302481651306152, L3: 27.035690307617188\n",
      "Current prediction:  61.795101165771484 \n",
      "\n",
      "Iteration 4425, Loss: 36.83795928955078, L1: 10.308432579040527, L3: 26.529525756835938\n",
      "Current prediction:  61.534156799316406 \n",
      "\n",
      "Iteration 4426, Loss: 36.359344482421875, L1: 10.344907760620117, L3: 26.014434814453125\n",
      "Current prediction:  61.290157318115234 \n",
      "\n",
      "Iteration 4427, Loss: 36.767826080322266, L1: 10.380309104919434, L3: 26.387516021728516\n",
      "Current prediction:  61.2392463684082 \n",
      "\n",
      "Iteration 4428, Loss: 37.04971694946289, L1: 10.333623886108398, L3: 26.716093063354492\n",
      "Current prediction:  61.22797393798828 \n",
      "\n",
      "Iteration 4429, Loss: 36.6558723449707, L1: 10.307559967041016, L3: 26.348312377929688\n",
      "Current prediction:  61.27470016479492 \n",
      "\n",
      "Iteration 4430, Loss: 37.86552047729492, L1: 10.33813762664795, L3: 27.527381896972656\n",
      "Current prediction:  61.45826721191406 \n",
      "\n",
      "Iteration 4431, Loss: 36.21962356567383, L1: 10.31110668182373, L3: 25.908517837524414\n",
      "Current prediction:  61.76958465576172 \n",
      "\n",
      "Iteration 4432, Loss: 36.2641716003418, L1: 10.336819648742676, L3: 25.927352905273438\n",
      "Current prediction:  61.86363983154297 \n",
      "\n",
      "Iteration 4433, Loss: 36.58885955810547, L1: 10.29310417175293, L3: 26.29575538635254\n",
      "Current prediction:  61.6958122253418 \n",
      "\n",
      "Iteration 4434, Loss: 36.69380187988281, L1: 10.379478454589844, L3: 26.3143253326416\n",
      "Current prediction:  61.26232147216797 \n",
      "\n",
      "Iteration 4435, Loss: 37.4134521484375, L1: 10.611140251159668, L3: 26.80231285095215\n",
      "Current prediction:  61.27801513671875 \n",
      "\n",
      "Iteration 4436, Loss: 36.67599868774414, L1: 10.863624572753906, L3: 25.812374114990234\n",
      "Current prediction:  61.29594802856445 \n",
      "\n",
      "Iteration 4437, Loss: 37.13570022583008, L1: 10.80509090423584, L3: 26.330610275268555\n",
      "Current prediction:  61.31882858276367 \n",
      "\n",
      "Iteration 4438, Loss: 38.34865951538086, L1: 10.96132755279541, L3: 27.387331008911133\n",
      "Current prediction:  61.34626007080078 \n",
      "\n",
      "Iteration 4439, Loss: 36.76586151123047, L1: 10.821314811706543, L3: 25.944547653198242\n",
      "Current prediction:  61.37936782836914 \n",
      "\n",
      "Iteration 4440, Loss: 35.30392074584961, L1: 10.5947265625, L3: 24.70919418334961\n",
      "Current prediction:  61.4143180847168 \n",
      "\n",
      "Iteration 4441, Loss: 37.126766204833984, L1: 10.656571388244629, L3: 26.47019386291504\n",
      "Current prediction:  61.44866180419922 \n",
      "\n",
      "Iteration 4442, Loss: 36.90617752075195, L1: 10.565752029418945, L3: 26.340425491333008\n",
      "Current prediction:  61.483238220214844 \n",
      "\n",
      "Iteration 4443, Loss: 36.71590805053711, L1: 10.626361846923828, L3: 26.08954620361328\n",
      "Current prediction:  61.52037811279297 \n",
      "\n",
      "Iteration 4444, Loss: 36.04343795776367, L1: 10.500875473022461, L3: 25.54256248474121\n",
      "Current prediction:  61.55451965332031 \n",
      "\n",
      "Iteration 4445, Loss: 36.668190002441406, L1: 10.626331329345703, L3: 26.04185676574707\n",
      "Current prediction:  61.58561706542969 \n",
      "\n",
      "Iteration 4446, Loss: 37.523582458496094, L1: 10.513689041137695, L3: 27.0098934173584\n",
      "Current prediction:  61.608951568603516 \n",
      "\n",
      "Iteration 4447, Loss: 36.202674865722656, L1: 10.460662841796875, L3: 25.74201011657715\n",
      "Current prediction:  61.62872314453125 \n",
      "\n",
      "Iteration 4448, Loss: 36.41999435424805, L1: 10.46131706237793, L3: 25.958677291870117\n",
      "Current prediction:  61.6453971862793 \n",
      "\n",
      "Iteration 4449, Loss: 36.679630279541016, L1: 10.39011287689209, L3: 26.289518356323242\n",
      "Current prediction:  61.660221099853516 \n",
      "\n",
      "Iteration 4450, Loss: 36.209442138671875, L1: 10.375905990600586, L3: 25.833538055419922\n",
      "Current prediction:  61.67502212524414 \n",
      "\n",
      "Iteration 4451, Loss: 37.36672592163086, L1: 10.405993461608887, L3: 26.960731506347656\n",
      "Current prediction:  61.461204528808594 \n",
      "\n",
      "Iteration 4452, Loss: 37.234832763671875, L1: 10.451878547668457, L3: 26.7829532623291\n",
      "Current prediction:  60.87137222290039 \n",
      "\n",
      "Iteration 4453, Loss: 37.214847564697266, L1: 10.514175415039062, L3: 26.700672149658203\n",
      "Current prediction:  60.88090133666992 \n",
      "\n",
      "Iteration 4454, Loss: 36.52791213989258, L1: 10.593294143676758, L3: 25.93461799621582\n",
      "Current prediction:  60.891395568847656 \n",
      "\n",
      "Iteration 4455, Loss: 37.086605072021484, L1: 10.6798677444458, L3: 26.40673828125\n",
      "Current prediction:  60.9013786315918 \n",
      "\n",
      "Iteration 4456, Loss: 36.54822540283203, L1: 10.627245903015137, L3: 25.92098045349121\n",
      "Current prediction:  60.91316604614258 \n",
      "\n",
      "Iteration 4457, Loss: 37.051788330078125, L1: 10.62205696105957, L3: 26.429731369018555\n",
      "Current prediction:  60.92765808105469 \n",
      "\n",
      "Iteration 4458, Loss: 37.559391021728516, L1: 10.609272956848145, L3: 26.950119018554688\n",
      "Current prediction:  60.94537353515625 \n",
      "\n",
      "Iteration 4459, Loss: 36.57145690917969, L1: 10.61752986907959, L3: 25.953927993774414\n",
      "Current prediction:  60.95454406738281 \n",
      "\n",
      "Iteration 4460, Loss: 36.766029357910156, L1: 10.57620620727539, L3: 26.189821243286133\n",
      "Current prediction:  60.27349090576172 \n",
      "\n",
      "Iteration 4461, Loss: 37.10980987548828, L1: 10.598958015441895, L3: 26.51085090637207\n",
      "Current prediction:  60.23490905761719 \n",
      "\n",
      "Iteration 4462, Loss: 36.54445266723633, L1: 10.527914047241211, L3: 26.016538619995117\n",
      "Current prediction:  60.26319885253906 \n",
      "\n",
      "Iteration 4463, Loss: 37.23257064819336, L1: 10.465815544128418, L3: 26.766756057739258\n",
      "Current prediction:  60.30146789550781 \n",
      "\n",
      "Iteration 4464, Loss: 37.01869201660156, L1: 10.396273612976074, L3: 26.622419357299805\n",
      "Current prediction:  61.06332015991211 \n",
      "\n",
      "Iteration 4465, Loss: 36.96794128417969, L1: 10.374424934387207, L3: 26.593515396118164\n",
      "Current prediction:  61.197227478027344 \n",
      "\n",
      "Iteration 4466, Loss: 35.955078125, L1: 10.225942611694336, L3: 25.729137420654297\n",
      "Current prediction:  61.302371978759766 \n",
      "\n",
      "Iteration 4467, Loss: 36.83576202392578, L1: 10.123882293701172, L3: 26.711881637573242\n",
      "Current prediction:  61.73119354248047 \n",
      "\n",
      "Iteration 4468, Loss: 35.88775634765625, L1: 10.118600845336914, L3: 25.769155502319336\n",
      "Current prediction:  62.024574279785156 \n",
      "\n",
      "Iteration 4469, Loss: 36.28752136230469, L1: 10.072673797607422, L3: 26.214847564697266\n",
      "Current prediction:  62.05124282836914 \n",
      "\n",
      "Iteration 4470, Loss: 37.09309005737305, L1: 10.054705619812012, L3: 27.03838348388672\n",
      "Current prediction:  62.05535125732422 \n",
      "\n",
      "Iteration 4471, Loss: 36.35710144042969, L1: 10.051767349243164, L3: 26.305334091186523\n",
      "Current prediction:  62.04903793334961 \n",
      "\n",
      "Iteration 4472, Loss: 36.77080154418945, L1: 10.046470642089844, L3: 26.72433090209961\n",
      "Current prediction:  62.037818908691406 \n",
      "\n",
      "Iteration 4473, Loss: 37.05569839477539, L1: 10.034737586975098, L3: 27.020959854125977\n",
      "Current prediction:  62.02268981933594 \n",
      "\n",
      "Iteration 4474, Loss: 36.76925277709961, L1: 10.075661659240723, L3: 26.69359016418457\n",
      "Current prediction:  62.001678466796875 \n",
      "\n",
      "Iteration 4475, Loss: 36.51674270629883, L1: 10.11327075958252, L3: 26.403470993041992\n",
      "Current prediction:  61.9808349609375 \n",
      "\n",
      "Iteration 4476, Loss: 37.20269012451172, L1: 10.122661590576172, L3: 27.080028533935547\n",
      "Current prediction:  61.95842742919922 \n",
      "\n",
      "Iteration 4477, Loss: 36.33596420288086, L1: 10.126609802246094, L3: 26.209354400634766\n",
      "Current prediction:  61.92850112915039 \n",
      "\n",
      "Iteration 4478, Loss: 35.62663269042969, L1: 10.160954475402832, L3: 25.46567726135254\n",
      "Current prediction:  61.844329833984375 \n",
      "\n",
      "Iteration 4479, Loss: 37.62605285644531, L1: 10.191142082214355, L3: 27.43490982055664\n",
      "Current prediction:  61.60685729980469 \n",
      "\n",
      "Iteration 4480, Loss: 37.31353759765625, L1: 10.23361587524414, L3: 27.07992172241211\n",
      "Current prediction:  61.0341682434082 \n",
      "\n",
      "Iteration 4481, Loss: 37.44828796386719, L1: 10.28951358795166, L3: 27.158775329589844\n",
      "Current prediction:  60.51568603515625 \n",
      "\n",
      "Iteration 4482, Loss: 36.31291580200195, L1: 10.326303482055664, L3: 25.98661231994629\n",
      "Current prediction:  60.343955993652344 \n",
      "\n",
      "Iteration 4483, Loss: 36.058982849121094, L1: 10.377392768859863, L3: 25.681591033935547\n",
      "Current prediction:  60.37381362915039 \n",
      "\n",
      "Iteration 4484, Loss: 36.96125411987305, L1: 10.37116527557373, L3: 26.590087890625\n",
      "Current prediction:  60.4840087890625 \n",
      "\n",
      "Iteration 4485, Loss: 36.46965408325195, L1: 10.324177742004395, L3: 26.145475387573242\n",
      "Current prediction:  60.99995803833008 \n",
      "\n",
      "Iteration 4486, Loss: 36.661277770996094, L1: 10.26864242553711, L3: 26.392637252807617\n",
      "Current prediction:  61.70559310913086 \n",
      "\n",
      "Iteration 4487, Loss: 35.893218994140625, L1: 10.250564575195312, L3: 25.642656326293945\n",
      "Current prediction:  61.83793258666992 \n",
      "\n",
      "Iteration 4488, Loss: 36.75263214111328, L1: 10.267786026000977, L3: 26.484846115112305\n",
      "Current prediction:  61.853515625 \n",
      "\n",
      "Iteration 4489, Loss: 36.36293029785156, L1: 10.250885009765625, L3: 26.112043380737305\n",
      "Current prediction:  61.85923767089844 \n",
      "\n",
      "Iteration 4490, Loss: 36.960750579833984, L1: 10.200599670410156, L3: 26.760150909423828\n",
      "Current prediction:  61.86256790161133 \n",
      "\n",
      "Iteration 4491, Loss: 37.63370895385742, L1: 10.179301261901855, L3: 27.454408645629883\n",
      "Current prediction:  61.861881256103516 \n",
      "\n",
      "Iteration 4492, Loss: 36.14558792114258, L1: 10.204367637634277, L3: 25.941221237182617\n",
      "Current prediction:  61.858612060546875 \n",
      "\n",
      "Iteration 4493, Loss: 35.8673095703125, L1: 10.193358421325684, L3: 25.673952102661133\n",
      "Current prediction:  61.85496520996094 \n",
      "\n",
      "Iteration 4494, Loss: 36.096153259277344, L1: 10.18893051147461, L3: 25.907222747802734\n",
      "Current prediction:  61.85103988647461 \n",
      "\n",
      "Iteration 4495, Loss: 36.189605712890625, L1: 10.217531204223633, L3: 25.97207260131836\n",
      "Current prediction:  61.84555435180664 \n",
      "\n",
      "Iteration 4496, Loss: 36.39882278442383, L1: 10.214755058288574, L3: 26.18406867980957\n",
      "Current prediction:  61.834129333496094 \n",
      "\n",
      "Iteration 4497, Loss: 37.104469299316406, L1: 10.230052947998047, L3: 26.874418258666992\n",
      "Current prediction:  61.81964111328125 \n",
      "\n",
      "Iteration 4498, Loss: 37.055152893066406, L1: 10.204326629638672, L3: 26.850828170776367\n",
      "Current prediction:  61.80424118041992 \n",
      "\n",
      "Iteration 4499, Loss: 36.459049224853516, L1: 10.220532417297363, L3: 26.238515853881836\n",
      "Current prediction:  61.787593841552734 \n",
      "\n",
      "Iteration 4500, Loss: 36.481632232666016, L1: 10.289382934570312, L3: 26.192249298095703\n",
      "Current prediction:  61.77154541015625 \n",
      "\n",
      "Iteration 4501, Loss: 36.371944427490234, L1: 10.245809555053711, L3: 26.126134872436523\n",
      "Current prediction:  61.7558708190918 \n",
      "\n",
      "Iteration 4502, Loss: 36.35479736328125, L1: 10.288010597229004, L3: 26.066787719726562\n",
      "Current prediction:  61.72932815551758 \n",
      "\n",
      "Iteration 4503, Loss: 37.59217834472656, L1: 10.306808471679688, L3: 27.285367965698242\n",
      "Current prediction:  61.251224517822266 \n",
      "\n",
      "Iteration 4504, Loss: 36.45266342163086, L1: 10.321900367736816, L3: 26.13076400756836\n",
      "Current prediction:  60.92451477050781 \n",
      "\n",
      "Iteration 4505, Loss: 37.22757339477539, L1: 10.396448135375977, L3: 26.831125259399414\n",
      "Current prediction:  60.90726089477539 \n",
      "\n",
      "Iteration 4506, Loss: 36.275596618652344, L1: 10.407724380493164, L3: 25.86787223815918\n",
      "Current prediction:  60.91130447387695 \n",
      "\n",
      "Iteration 4507, Loss: 36.35773468017578, L1: 10.407493591308594, L3: 25.95024299621582\n",
      "Current prediction:  60.926910400390625 \n",
      "\n",
      "Iteration 4508, Loss: 36.227291107177734, L1: 10.373520851135254, L3: 25.853771209716797\n",
      "Current prediction:  60.98420715332031 \n",
      "\n",
      "Iteration 4509, Loss: 36.41910934448242, L1: 10.349030494689941, L3: 26.070077896118164\n",
      "Current prediction:  61.34767532348633 \n",
      "\n",
      "Iteration 4510, Loss: 36.86572265625, L1: 10.289633750915527, L3: 26.57608985900879\n",
      "Current prediction:  61.695716857910156 \n",
      "\n",
      "Iteration 4511, Loss: 35.30968475341797, L1: 10.296470642089844, L3: 25.013214111328125\n",
      "Current prediction:  61.79102325439453 \n",
      "\n",
      "Iteration 4512, Loss: 36.433589935302734, L1: 10.262574195861816, L3: 26.1710147857666\n",
      "Current prediction:  61.810401916503906 \n",
      "\n",
      "Iteration 4513, Loss: 36.42538070678711, L1: 10.232791900634766, L3: 26.192588806152344\n",
      "Current prediction:  61.826271057128906 \n",
      "\n",
      "Iteration 4514, Loss: 37.162837982177734, L1: 10.220931053161621, L3: 26.941905975341797\n",
      "Current prediction:  61.83867645263672 \n",
      "\n",
      "Iteration 4515, Loss: 36.48716735839844, L1: 10.169180870056152, L3: 26.3179874420166\n",
      "Current prediction:  61.847259521484375 \n",
      "\n",
      "Iteration 4516, Loss: 36.29911804199219, L1: 10.18339729309082, L3: 26.115718841552734\n",
      "Current prediction:  61.85115432739258 \n",
      "\n",
      "Iteration 4517, Loss: 37.218475341796875, L1: 10.156956672668457, L3: 27.061519622802734\n",
      "Current prediction:  61.84866714477539 \n",
      "\n",
      "Iteration 4518, Loss: 36.46826171875, L1: 10.148173332214355, L3: 26.320087432861328\n",
      "Current prediction:  61.84443283081055 \n",
      "\n",
      "Iteration 4519, Loss: 36.10637283325195, L1: 10.179888725280762, L3: 25.926485061645508\n",
      "Current prediction:  61.8397102355957 \n",
      "\n",
      "Iteration 4520, Loss: 37.25829315185547, L1: 10.196319580078125, L3: 27.061975479125977\n",
      "Current prediction:  61.82987976074219 \n",
      "\n",
      "Iteration 4521, Loss: 37.34459686279297, L1: 10.217616081237793, L3: 27.126981735229492\n",
      "Current prediction:  61.816585540771484 \n",
      "\n",
      "Iteration 4522, Loss: 36.22087097167969, L1: 10.19163703918457, L3: 26.029232025146484\n",
      "Current prediction:  61.79933166503906 \n",
      "\n",
      "Iteration 4523, Loss: 35.65924835205078, L1: 10.248888969421387, L3: 25.410358428955078\n",
      "Current prediction:  61.781959533691406 \n",
      "\n",
      "Iteration 4524, Loss: 36.845062255859375, L1: 10.273231506347656, L3: 26.57183074951172\n",
      "Current prediction:  61.7453498840332 \n",
      "\n",
      "Iteration 4525, Loss: 36.86988830566406, L1: 10.284074783325195, L3: 26.585811614990234\n",
      "Current prediction:  61.06006622314453 \n",
      "\n",
      "Iteration 4526, Loss: 36.60148620605469, L1: 10.356632232666016, L3: 26.244853973388672\n",
      "Current prediction:  60.91817855834961 \n",
      "\n",
      "Iteration 4527, Loss: 37.02562713623047, L1: 10.439720153808594, L3: 26.585906982421875\n",
      "Current prediction:  60.90745162963867 \n",
      "\n",
      "Iteration 4528, Loss: 37.37104034423828, L1: 10.504840850830078, L3: 26.86619758605957\n",
      "Current prediction:  60.90104293823242 \n",
      "\n",
      "Iteration 4529, Loss: 36.59644317626953, L1: 10.489432334899902, L3: 26.107011795043945\n",
      "Current prediction:  60.89596176147461 \n",
      "\n",
      "Iteration 4530, Loss: 36.23785400390625, L1: 10.435916900634766, L3: 25.80193519592285\n",
      "Current prediction:  60.89618682861328 \n",
      "\n",
      "Iteration 4531, Loss: 36.17561721801758, L1: 10.534296989440918, L3: 25.641319274902344\n",
      "Current prediction:  60.990604400634766 \n",
      "\n",
      "Iteration 4532, Loss: 37.16763687133789, L1: 10.485304832458496, L3: 26.682331085205078\n",
      "Current prediction:  61.4781379699707 \n",
      "\n",
      "Iteration 4533, Loss: 36.142173767089844, L1: 10.357834815979004, L3: 25.784337997436523\n",
      "Current prediction:  61.68611526489258 \n",
      "\n",
      "Iteration 4534, Loss: 36.65193557739258, L1: 10.409968376159668, L3: 26.241968154907227\n",
      "Current prediction:  61.6935920715332 \n",
      "\n",
      "Iteration 4535, Loss: 36.98035430908203, L1: 10.335674285888672, L3: 26.644681930541992\n",
      "Current prediction:  61.6927490234375 \n",
      "\n",
      "Iteration 4536, Loss: 36.774879455566406, L1: 10.324524879455566, L3: 26.450353622436523\n",
      "Current prediction:  61.6928825378418 \n",
      "\n",
      "Iteration 4537, Loss: 37.03490447998047, L1: 10.31778335571289, L3: 26.717119216918945\n",
      "Current prediction:  61.69273376464844 \n",
      "\n",
      "Iteration 4538, Loss: 36.919036865234375, L1: 10.372365951538086, L3: 26.546672821044922\n",
      "Current prediction:  61.68946838378906 \n",
      "\n",
      "Iteration 4539, Loss: 35.671932220458984, L1: 10.328558921813965, L3: 25.343374252319336\n",
      "Current prediction:  61.68650436401367 \n",
      "\n",
      "Iteration 4540, Loss: 35.747955322265625, L1: 10.27980899810791, L3: 25.46814727783203\n",
      "Current prediction:  61.68416976928711 \n",
      "\n",
      "Iteration 4541, Loss: 35.606563568115234, L1: 10.316964149475098, L3: 25.28959846496582\n",
      "Current prediction:  61.67967224121094 \n",
      "\n",
      "Iteration 4542, Loss: 37.05509948730469, L1: 10.34858226776123, L3: 26.706518173217773\n",
      "Current prediction:  61.6712646484375 \n",
      "\n",
      "Iteration 4543, Loss: 36.60865783691406, L1: 10.375213623046875, L3: 26.23344612121582\n",
      "Current prediction:  61.66378402709961 \n",
      "\n",
      "Iteration 4544, Loss: 37.557899475097656, L1: 10.378656387329102, L3: 27.179243087768555\n",
      "Current prediction:  61.65396499633789 \n",
      "\n",
      "Iteration 4545, Loss: 37.08165740966797, L1: 10.38754653930664, L3: 26.69411277770996\n",
      "Current prediction:  61.64411544799805 \n",
      "\n",
      "Iteration 4546, Loss: 36.42683792114258, L1: 10.363201141357422, L3: 26.063636779785156\n",
      "Current prediction:  61.635772705078125 \n",
      "\n",
      "Iteration 4547, Loss: 36.955780029296875, L1: 10.384169578552246, L3: 26.571609497070312\n",
      "Current prediction:  61.627986907958984 \n",
      "\n",
      "Iteration 4548, Loss: 36.41128921508789, L1: 10.368728637695312, L3: 26.042560577392578\n",
      "Current prediction:  61.58478927612305 \n",
      "\n",
      "Iteration 4549, Loss: 36.43533706665039, L1: 10.387066841125488, L3: 26.048269271850586\n",
      "Current prediction:  61.358009338378906 \n",
      "\n",
      "Iteration 4550, Loss: 36.972713470458984, L1: 10.409881591796875, L3: 26.56283187866211\n",
      "Current prediction:  61.048736572265625 \n",
      "\n",
      "Iteration 4551, Loss: 36.54343795776367, L1: 10.446819305419922, L3: 26.09661865234375\n",
      "Current prediction:  60.94767761230469 \n",
      "\n",
      "Iteration 4552, Loss: 36.12957763671875, L1: 10.44064998626709, L3: 25.688928604125977\n",
      "Current prediction:  60.84813690185547 \n",
      "\n",
      "Iteration 4553, Loss: 36.35612487792969, L1: 10.48737907409668, L3: 25.868743896484375\n",
      "Current prediction:  60.78504180908203 \n",
      "\n",
      "Iteration 4554, Loss: 36.244911193847656, L1: 10.429468154907227, L3: 25.815444946289062\n",
      "Current prediction:  60.85602951049805 \n",
      "\n",
      "Iteration 4555, Loss: 37.34572982788086, L1: 10.396645545959473, L3: 26.949085235595703\n",
      "Current prediction:  61.03631591796875 \n",
      "\n",
      "Iteration 4556, Loss: 36.80201721191406, L1: 10.403186798095703, L3: 26.39883041381836\n",
      "Current prediction:  61.103668212890625 \n",
      "\n",
      "Iteration 4557, Loss: 37.127540588378906, L1: 10.403082847595215, L3: 26.724458694458008\n",
      "Current prediction:  61.49211883544922 \n",
      "\n",
      "Iteration 4558, Loss: 36.83684539794922, L1: 10.415422439575195, L3: 26.421424865722656\n",
      "Current prediction:  61.3713493347168 \n",
      "\n",
      "Iteration 4559, Loss: 37.01482391357422, L1: 10.411234855651855, L3: 26.603588104248047\n",
      "Current prediction:  61.40348815917969 \n",
      "\n",
      "Iteration 4560, Loss: 36.904380798339844, L1: 10.42274284362793, L3: 26.481637954711914\n",
      "Current prediction:  61.43977355957031 \n",
      "\n",
      "Iteration 4561, Loss: 36.96083068847656, L1: 10.430778503417969, L3: 26.530052185058594\n",
      "Current prediction:  61.47593307495117 \n",
      "\n",
      "Iteration 4562, Loss: 36.297569274902344, L1: 10.433262825012207, L3: 25.86430549621582\n",
      "Current prediction:  61.51430130004883 \n",
      "\n",
      "Iteration 4563, Loss: 37.27888107299805, L1: 10.418512344360352, L3: 26.860368728637695\n",
      "Current prediction:  61.55162811279297 \n",
      "\n",
      "Iteration 4564, Loss: 37.565216064453125, L1: 10.42194938659668, L3: 27.143268585205078\n",
      "Current prediction:  61.58259963989258 \n",
      "\n",
      "Iteration 4565, Loss: 36.960819244384766, L1: 10.394194602966309, L3: 26.566625595092773\n",
      "Current prediction:  61.60967254638672 \n",
      "\n",
      "Iteration 4566, Loss: 36.537437438964844, L1: 10.453661918640137, L3: 26.083776473999023\n",
      "Current prediction:  61.63487243652344 \n",
      "\n",
      "Iteration 4567, Loss: 37.111183166503906, L1: 10.431915283203125, L3: 26.67926788330078\n",
      "Current prediction:  61.65812301635742 \n",
      "\n",
      "Iteration 4568, Loss: 37.52182388305664, L1: 10.337516784667969, L3: 27.184307098388672\n",
      "Current prediction:  61.67978286743164 \n",
      "\n",
      "Iteration 4569, Loss: 37.65263366699219, L1: 10.358634948730469, L3: 27.29400062561035\n",
      "Current prediction:  61.69700622558594 \n",
      "\n",
      "Iteration 4570, Loss: 36.32529830932617, L1: 10.402839660644531, L3: 25.92245864868164\n",
      "Current prediction:  61.71065902709961 \n",
      "\n",
      "Iteration 4571, Loss: 35.960906982421875, L1: 10.266063690185547, L3: 25.69484519958496\n",
      "Current prediction:  61.721858978271484 \n",
      "\n",
      "Iteration 4572, Loss: 37.487709045410156, L1: 10.284527778625488, L3: 27.20318031311035\n",
      "Current prediction:  61.72853088378906 \n",
      "\n",
      "Iteration 4573, Loss: 36.094478607177734, L1: 10.249868392944336, L3: 25.8446102142334\n",
      "Current prediction:  61.73313522338867 \n",
      "\n",
      "Iteration 4574, Loss: 36.71418380737305, L1: 10.26899242401123, L3: 26.4451904296875\n",
      "Current prediction:  61.73744201660156 \n",
      "\n",
      "Iteration 4575, Loss: 36.7149658203125, L1: 10.303101539611816, L3: 26.411863327026367\n",
      "Current prediction:  61.73958969116211 \n",
      "\n",
      "Iteration 4576, Loss: 36.884490966796875, L1: 10.278809547424316, L3: 26.605680465698242\n",
      "Current prediction:  61.74047088623047 \n",
      "\n",
      "Iteration 4577, Loss: 37.46834945678711, L1: 10.29006290435791, L3: 27.178287506103516\n",
      "Current prediction:  61.737979888916016 \n",
      "\n",
      "Iteration 4578, Loss: 36.867984771728516, L1: 10.307044982910156, L3: 26.56093978881836\n",
      "Current prediction:  61.732696533203125 \n",
      "\n",
      "Iteration 4579, Loss: 36.82496643066406, L1: 10.334197044372559, L3: 26.490768432617188\n",
      "Current prediction:  61.72513198852539 \n",
      "\n",
      "Iteration 4580, Loss: 37.382423400878906, L1: 10.408124923706055, L3: 26.97429656982422\n",
      "Current prediction:  61.71659469604492 \n",
      "\n",
      "Iteration 4581, Loss: 36.61024856567383, L1: 10.489628791809082, L3: 26.120620727539062\n",
      "Current prediction:  61.7050666809082 \n",
      "\n",
      "Iteration 4582, Loss: 36.9427375793457, L1: 10.478011131286621, L3: 26.464725494384766\n",
      "Current prediction:  61.68704605102539 \n",
      "\n",
      "Iteration 4583, Loss: 36.59044647216797, L1: 10.477607727050781, L3: 26.11284065246582\n",
      "Current prediction:  61.66978073120117 \n",
      "\n",
      "Iteration 4584, Loss: 38.18074417114258, L1: 10.685118675231934, L3: 27.495624542236328\n",
      "Current prediction:  61.646888732910156 \n",
      "\n",
      "Iteration 4585, Loss: 35.573143005371094, L1: 10.495471954345703, L3: 25.07767105102539\n",
      "Current prediction:  61.60334777832031 \n",
      "\n",
      "Iteration 4586, Loss: 35.95927810668945, L1: 10.276090621948242, L3: 25.68318748474121\n",
      "Current prediction:  61.357975006103516 \n",
      "\n",
      "Iteration 4587, Loss: 36.764434814453125, L1: 10.294493675231934, L3: 26.469940185546875\n",
      "Current prediction:  60.94478225708008 \n",
      "\n",
      "Iteration 4588, Loss: 36.097328186035156, L1: 10.342994689941406, L3: 25.754335403442383\n",
      "Current prediction:  60.850730895996094 \n",
      "\n",
      "Iteration 4589, Loss: 37.145751953125, L1: 10.384713172912598, L3: 26.761037826538086\n",
      "Current prediction:  60.87109375 \n",
      "\n",
      "Iteration 4590, Loss: 35.92840576171875, L1: 10.377994537353516, L3: 25.550413131713867\n",
      "Current prediction:  61.003326416015625 \n",
      "\n",
      "Iteration 4591, Loss: 37.196144104003906, L1: 10.378246307373047, L3: 26.81789779663086\n",
      "Current prediction:  61.29410934448242 \n",
      "\n",
      "Iteration 4592, Loss: 35.81011199951172, L1: 10.356721878051758, L3: 25.453388214111328\n",
      "Current prediction:  61.4755744934082 \n",
      "\n",
      "Iteration 4593, Loss: 36.83421325683594, L1: 10.368696212768555, L3: 26.46551513671875\n",
      "Current prediction:  61.50334167480469 \n",
      "\n",
      "Iteration 4594, Loss: 36.45574951171875, L1: 10.377366065979004, L3: 26.078384399414062\n",
      "Current prediction:  61.50949478149414 \n",
      "\n",
      "Iteration 4595, Loss: 37.109832763671875, L1: 10.389297485351562, L3: 26.720537185668945\n",
      "Current prediction:  61.50607681274414 \n",
      "\n",
      "Iteration 4596, Loss: 36.4911003112793, L1: 10.37436580657959, L3: 26.11673355102539\n",
      "Current prediction:  61.50145721435547 \n",
      "\n",
      "Iteration 4597, Loss: 36.95064926147461, L1: 10.391335487365723, L3: 26.55931282043457\n",
      "Current prediction:  61.497169494628906 \n",
      "\n",
      "Iteration 4598, Loss: 36.05070114135742, L1: 10.383550643920898, L3: 25.667150497436523\n",
      "Current prediction:  61.49562454223633 \n",
      "\n",
      "Iteration 4599, Loss: 36.884307861328125, L1: 10.389397621154785, L3: 26.494909286499023\n",
      "Current prediction:  61.492950439453125 \n",
      "\n",
      "Iteration 4600, Loss: 36.05338668823242, L1: 10.409477233886719, L3: 25.643909454345703\n",
      "Current prediction:  61.49034118652344 \n",
      "\n",
      "Iteration 4601, Loss: 35.89604568481445, L1: 10.388032913208008, L3: 25.508012771606445\n",
      "Current prediction:  61.486751556396484 \n",
      "\n",
      "Iteration 4602, Loss: 36.5759391784668, L1: 10.402519226074219, L3: 26.173419952392578\n",
      "Current prediction:  61.45675277709961 \n",
      "\n",
      "Iteration 4603, Loss: 36.896915435791016, L1: 10.392807960510254, L3: 26.504108428955078\n",
      "Current prediction:  61.34877395629883 \n",
      "\n",
      "Iteration 4604, Loss: 36.345436096191406, L1: 10.409534454345703, L3: 25.93589973449707\n",
      "Current prediction:  61.29990005493164 \n",
      "\n",
      "Iteration 4605, Loss: 36.21708679199219, L1: 10.39476490020752, L3: 25.82232093811035\n",
      "Current prediction:  61.23920822143555 \n",
      "\n",
      "Iteration 4606, Loss: 36.56928253173828, L1: 10.411648750305176, L3: 26.157634735107422\n",
      "Current prediction:  61.35348892211914 \n",
      "\n",
      "Iteration 4607, Loss: 36.78768539428711, L1: 10.417340278625488, L3: 26.370344161987305\n",
      "Current prediction:  61.42035675048828 \n",
      "\n",
      "Iteration 4608, Loss: 36.590702056884766, L1: 10.381600379943848, L3: 26.209102630615234\n",
      "Current prediction:  61.45964813232422 \n",
      "\n",
      "Iteration 4609, Loss: 36.420650482177734, L1: 10.386773109436035, L3: 26.033878326416016\n",
      "Current prediction:  61.51884460449219 \n",
      "\n",
      "Iteration 4610, Loss: 37.27617263793945, L1: 10.365479469299316, L3: 26.910694122314453\n",
      "Current prediction:  61.53965759277344 \n",
      "\n",
      "Iteration 4611, Loss: 36.39319610595703, L1: 10.36444091796875, L3: 26.02875518798828\n",
      "Current prediction:  61.54820251464844 \n",
      "\n",
      "Iteration 4612, Loss: 36.336280822753906, L1: 10.346259117126465, L3: 25.990022659301758\n",
      "Current prediction:  61.55515670776367 \n",
      "\n",
      "Iteration 4613, Loss: 35.55577087402344, L1: 10.330806732177734, L3: 25.22496223449707\n",
      "Current prediction:  61.56040573120117 \n",
      "\n",
      "Iteration 4614, Loss: 36.12032699584961, L1: 10.330620765686035, L3: 25.78970718383789\n",
      "Current prediction:  61.56372833251953 \n",
      "\n",
      "Iteration 4615, Loss: 36.45628356933594, L1: 10.320053100585938, L3: 26.13623046875\n",
      "Current prediction:  61.563209533691406 \n",
      "\n",
      "Iteration 4616, Loss: 37.308563232421875, L1: 10.538843154907227, L3: 26.769718170166016\n",
      "Current prediction:  61.55979919433594 \n",
      "\n",
      "Iteration 4617, Loss: 37.46483612060547, L1: 10.682756423950195, L3: 26.782081604003906\n",
      "Current prediction:  61.55060958862305 \n",
      "\n",
      "Iteration 4618, Loss: 36.5175895690918, L1: 10.515152931213379, L3: 26.0024356842041\n",
      "Current prediction:  61.50379943847656 \n",
      "\n",
      "Iteration 4619, Loss: 36.690826416015625, L1: 10.55014419555664, L3: 26.14068031311035\n",
      "Current prediction:  61.28667449951172 \n",
      "\n",
      "Iteration 4620, Loss: 36.215049743652344, L1: 10.609439849853516, L3: 25.60561180114746\n",
      "Current prediction:  60.8283576965332 \n",
      "\n",
      "Iteration 4621, Loss: 36.579795837402344, L1: 10.68002700805664, L3: 25.899770736694336\n",
      "Current prediction:  60.11002731323242 \n",
      "\n",
      "Iteration 4622, Loss: 37.35688781738281, L1: 10.746418952941895, L3: 26.6104679107666\n",
      "Current prediction:  60.48862838745117 \n",
      "\n",
      "Iteration 4623, Loss: 36.43620300292969, L1: 10.728655815124512, L3: 25.70754623413086\n",
      "Current prediction:  60.76380157470703 \n",
      "\n",
      "Iteration 4624, Loss: 37.434024810791016, L1: 10.704453468322754, L3: 26.729572296142578\n",
      "Current prediction:  60.84383773803711 \n",
      "\n",
      "Iteration 4625, Loss: 36.764793395996094, L1: 10.639060020446777, L3: 26.125734329223633\n",
      "Current prediction:  60.89952850341797 \n",
      "\n",
      "Iteration 4626, Loss: 36.43213653564453, L1: 10.556964874267578, L3: 25.875173568725586\n",
      "Current prediction:  61.42194747924805 \n",
      "\n",
      "Iteration 4627, Loss: 37.57811737060547, L1: 10.407333374023438, L3: 27.1707820892334\n",
      "Current prediction:  61.88643264770508 \n",
      "\n",
      "Iteration 4628, Loss: 36.38098907470703, L1: 10.32868480682373, L3: 26.052305221557617\n",
      "Current prediction:  61.92885971069336 \n",
      "\n",
      "Iteration 4629, Loss: 36.61113739013672, L1: 10.249282836914062, L3: 26.361852645874023\n",
      "Current prediction:  61.96380615234375 \n",
      "\n",
      "Iteration 4630, Loss: 36.25902557373047, L1: 10.150980949401855, L3: 26.108043670654297\n",
      "Current prediction:  61.987953186035156 \n",
      "\n",
      "Iteration 4631, Loss: 37.075653076171875, L1: 10.19920825958252, L3: 26.87644386291504\n",
      "Current prediction:  62.00025177001953 \n",
      "\n",
      "Iteration 4632, Loss: 37.10820007324219, L1: 10.347138404846191, L3: 26.761062622070312\n",
      "Current prediction:  61.999176025390625 \n",
      "\n",
      "Iteration 4633, Loss: 36.400455474853516, L1: 10.3063325881958, L3: 26.0941219329834\n",
      "Current prediction:  61.98746871948242 \n",
      "\n",
      "Iteration 4634, Loss: 37.585506439208984, L1: 10.275956153869629, L3: 27.30954933166504\n",
      "Current prediction:  61.963294982910156 \n",
      "\n",
      "Iteration 4635, Loss: 37.5914306640625, L1: 10.362648010253906, L3: 27.22878074645996\n",
      "Current prediction:  61.92823028564453 \n",
      "\n",
      "Iteration 4636, Loss: 36.56072998046875, L1: 10.28947639465332, L3: 26.27125358581543\n",
      "Current prediction:  61.88452911376953 \n",
      "\n",
      "Iteration 4637, Loss: 37.6564826965332, L1: 10.207425117492676, L3: 27.44905662536621\n",
      "Current prediction:  61.83321762084961 \n",
      "\n",
      "Iteration 4638, Loss: 36.079376220703125, L1: 10.102761268615723, L3: 25.97661590576172\n",
      "Current prediction:  61.19911575317383 \n",
      "\n",
      "Iteration 4639, Loss: 36.415283203125, L1: 10.180337905883789, L3: 26.234947204589844\n",
      "Current prediction:  60.96086883544922 \n",
      "\n",
      "Iteration 4640, Loss: 36.3759651184082, L1: 10.289578437805176, L3: 26.086387634277344\n",
      "Current prediction:  60.912437438964844 \n",
      "\n",
      "Iteration 4641, Loss: 36.151519775390625, L1: 10.378599166870117, L3: 25.772918701171875\n",
      "Current prediction:  60.86832046508789 \n",
      "\n",
      "Iteration 4642, Loss: 36.273712158203125, L1: 10.440129280090332, L3: 25.83358383178711\n",
      "Current prediction:  60.830726623535156 \n",
      "\n",
      "Iteration 4643, Loss: 36.297794342041016, L1: 10.488009452819824, L3: 25.809783935546875\n",
      "Current prediction:  60.79867935180664 \n",
      "\n",
      "Iteration 4644, Loss: 36.45891571044922, L1: 10.526863098144531, L3: 25.932050704956055\n",
      "Current prediction:  60.77296447753906 \n",
      "\n",
      "Iteration 4645, Loss: 37.46409606933594, L1: 10.552511215209961, L3: 26.911584854125977\n",
      "Current prediction:  60.75186538696289 \n",
      "\n",
      "Iteration 4646, Loss: 36.31850814819336, L1: 10.556872367858887, L3: 25.761634826660156\n",
      "Current prediction:  60.73681640625 \n",
      "\n",
      "Iteration 4647, Loss: 35.47048568725586, L1: 10.511137962341309, L3: 24.959348678588867\n",
      "Current prediction:  60.727752685546875 \n",
      "\n",
      "Iteration 4648, Loss: 36.76444625854492, L1: 10.474058151245117, L3: 26.290388107299805\n",
      "Current prediction:  60.7502326965332 \n",
      "\n",
      "Iteration 4649, Loss: 36.264404296875, L1: 10.468147277832031, L3: 25.796255111694336\n",
      "Current prediction:  61.403564453125 \n",
      "\n",
      "Iteration 4650, Loss: 35.80345916748047, L1: 10.398794174194336, L3: 25.404664993286133\n",
      "Current prediction:  61.479305267333984 \n",
      "\n",
      "Iteration 4651, Loss: 36.33579635620117, L1: 10.380914688110352, L3: 25.95488166809082\n",
      "Current prediction:  61.48019027709961 \n",
      "\n",
      "Iteration 4652, Loss: 36.17161560058594, L1: 10.386129379272461, L3: 25.785484313964844\n",
      "Current prediction:  61.483131408691406 \n",
      "\n",
      "Iteration 4653, Loss: 36.001441955566406, L1: 10.379464149475098, L3: 25.621978759765625\n",
      "Current prediction:  61.48426818847656 \n",
      "\n",
      "Iteration 4654, Loss: 36.62375259399414, L1: 10.358345985412598, L3: 26.265405654907227\n",
      "Current prediction:  61.48255157470703 \n",
      "\n",
      "Iteration 4655, Loss: 36.10491943359375, L1: 10.408990859985352, L3: 25.695926666259766\n",
      "Current prediction:  61.47926330566406 \n",
      "\n",
      "Iteration 4656, Loss: 37.112640380859375, L1: 10.335126876831055, L3: 26.777511596679688\n",
      "Current prediction:  61.474082946777344 \n",
      "\n",
      "Iteration 4657, Loss: 36.42388916015625, L1: 10.443521499633789, L3: 25.980369567871094\n",
      "Current prediction:  61.466590881347656 \n",
      "\n",
      "Iteration 4658, Loss: 37.00811004638672, L1: 10.395537376403809, L3: 26.612571716308594\n",
      "Current prediction:  61.457664489746094 \n",
      "\n",
      "Iteration 4659, Loss: 38.01319122314453, L1: 10.412126541137695, L3: 27.601062774658203\n",
      "Current prediction:  61.44856262207031 \n",
      "\n",
      "Iteration 4660, Loss: 36.44499206542969, L1: 10.402704238891602, L3: 26.04228973388672\n",
      "Current prediction:  61.441986083984375 \n",
      "\n",
      "Iteration 4661, Loss: 37.46636962890625, L1: 10.420939445495605, L3: 27.04543113708496\n",
      "Current prediction:  61.43344497680664 \n",
      "\n",
      "Iteration 4662, Loss: 36.93248748779297, L1: 10.407968521118164, L3: 26.524518966674805\n",
      "Current prediction:  61.42244338989258 \n",
      "\n",
      "Iteration 4663, Loss: 37.26319122314453, L1: 10.392721176147461, L3: 26.870471954345703\n",
      "Current prediction:  61.41192626953125 \n",
      "\n",
      "Iteration 4664, Loss: 36.68321228027344, L1: 10.424081802368164, L3: 26.259130477905273\n",
      "Current prediction:  61.402061462402344 \n",
      "\n",
      "Iteration 4665, Loss: 36.9805908203125, L1: 10.431358337402344, L3: 26.549230575561523\n",
      "Current prediction:  61.3937873840332 \n",
      "\n",
      "Iteration 4666, Loss: 37.16818618774414, L1: 10.44114875793457, L3: 26.72703742980957\n",
      "Current prediction:  61.387474060058594 \n",
      "\n",
      "Iteration 4667, Loss: 36.240169525146484, L1: 10.466979026794434, L3: 25.773189544677734\n",
      "Current prediction:  61.38481521606445 \n",
      "\n",
      "Iteration 4668, Loss: 36.785438537597656, L1: 10.467702865600586, L3: 26.317737579345703\n",
      "Current prediction:  61.374847412109375 \n",
      "\n",
      "Iteration 4669, Loss: 36.79390335083008, L1: 10.474017143249512, L3: 26.31988525390625\n",
      "Current prediction:  61.25173568725586 \n",
      "\n",
      "Iteration 4670, Loss: 37.1273307800293, L1: 10.492457389831543, L3: 26.63487434387207\n",
      "Current prediction:  61.064002990722656 \n",
      "\n",
      "Iteration 4671, Loss: 36.534549713134766, L1: 10.461459159851074, L3: 26.073091506958008\n",
      "Current prediction:  60.890953063964844 \n",
      "\n",
      "Iteration 4672, Loss: 36.417503356933594, L1: 10.455810546875, L3: 25.961692810058594\n",
      "Current prediction:  60.839454650878906 \n",
      "\n",
      "Iteration 4673, Loss: 36.06943893432617, L1: 10.469661712646484, L3: 25.599777221679688\n",
      "Current prediction:  61.267005920410156 \n",
      "\n",
      "Iteration 4674, Loss: 36.15303039550781, L1: 10.427277565002441, L3: 25.725753784179688\n",
      "Current prediction:  61.44062805175781 \n",
      "\n",
      "Iteration 4675, Loss: 36.54069900512695, L1: 10.4002103805542, L3: 26.140487670898438\n",
      "Current prediction:  61.49720764160156 \n",
      "\n",
      "Iteration 4676, Loss: 36.74665832519531, L1: 10.373065948486328, L3: 26.373594284057617\n",
      "Current prediction:  61.520362854003906 \n",
      "\n",
      "Iteration 4677, Loss: 36.784061431884766, L1: 10.36038589477539, L3: 26.423675537109375\n",
      "Current prediction:  61.53831481933594 \n",
      "\n",
      "Iteration 4678, Loss: 36.12138366699219, L1: 10.344260215759277, L3: 25.777124404907227\n",
      "Current prediction:  61.55371856689453 \n",
      "\n",
      "Iteration 4679, Loss: 36.47721481323242, L1: 10.320880889892578, L3: 26.156333923339844\n",
      "Current prediction:  61.566612243652344 \n",
      "\n",
      "Iteration 4680, Loss: 36.739253997802734, L1: 10.293912887573242, L3: 26.445341110229492\n",
      "Current prediction:  61.57746124267578 \n",
      "\n",
      "Iteration 4681, Loss: 36.06134033203125, L1: 10.284082412719727, L3: 25.777257919311523\n",
      "Current prediction:  61.58244323730469 \n",
      "\n",
      "Iteration 4682, Loss: 36.527992248535156, L1: 10.289070129394531, L3: 26.238922119140625\n",
      "Current prediction:  61.58491134643555 \n",
      "\n",
      "Iteration 4683, Loss: 36.13774490356445, L1: 10.285573959350586, L3: 25.852170944213867\n",
      "Current prediction:  61.583831787109375 \n",
      "\n",
      "Iteration 4684, Loss: 37.18708801269531, L1: 10.306983947753906, L3: 26.880104064941406\n",
      "Current prediction:  61.58020782470703 \n",
      "\n",
      "Iteration 4685, Loss: 37.73066711425781, L1: 10.277669906616211, L3: 27.4529972076416\n",
      "Current prediction:  61.574462890625 \n",
      "\n",
      "Iteration 4686, Loss: 35.70119857788086, L1: 10.310626983642578, L3: 25.39057159423828\n",
      "Current prediction:  61.56444549560547 \n",
      "\n",
      "Iteration 4687, Loss: 37.47456359863281, L1: 10.303677558898926, L3: 27.17088508605957\n",
      "Current prediction:  61.54555130004883 \n",
      "\n",
      "Iteration 4688, Loss: 35.929344177246094, L1: 10.30526351928711, L3: 25.624080657958984\n",
      "Current prediction:  61.50434112548828 \n",
      "\n",
      "Iteration 4689, Loss: 37.338279724121094, L1: 10.321903228759766, L3: 27.016374588012695\n",
      "Current prediction:  61.229251861572266 \n",
      "\n",
      "Iteration 4690, Loss: 35.847389221191406, L1: 10.34810733795166, L3: 25.499282836914062\n",
      "Current prediction:  60.80711364746094 \n",
      "\n",
      "Iteration 4691, Loss: 36.74609375, L1: 10.39981746673584, L3: 26.346275329589844\n",
      "Current prediction:  60.76917266845703 \n",
      "\n",
      "Iteration 4692, Loss: 37.527626037597656, L1: 10.451336860656738, L3: 27.076290130615234\n",
      "Current prediction:  60.7598876953125 \n",
      "\n",
      "Iteration 4693, Loss: 36.41709518432617, L1: 10.468596458435059, L3: 25.948497772216797\n",
      "Current prediction:  60.754581451416016 \n",
      "\n",
      "Iteration 4694, Loss: 37.40146255493164, L1: 10.465581893920898, L3: 26.935880661010742\n",
      "Current prediction:  60.75277328491211 \n",
      "\n",
      "Iteration 4695, Loss: 37.5096435546875, L1: 10.473356246948242, L3: 27.036287307739258\n",
      "Current prediction:  60.75265884399414 \n",
      "\n",
      "Iteration 4696, Loss: 35.90241241455078, L1: 10.446101188659668, L3: 25.456310272216797\n",
      "Current prediction:  60.84001159667969 \n",
      "\n",
      "Iteration 4697, Loss: 36.44396209716797, L1: 10.395853996276855, L3: 26.048107147216797\n",
      "Current prediction:  61.43718719482422 \n",
      "\n",
      "Iteration 4698, Loss: 37.86463165283203, L1: 10.357349395751953, L3: 27.50728416442871\n",
      "Current prediction:  61.52164077758789 \n",
      "\n",
      "Iteration 4699, Loss: 37.132118225097656, L1: 10.353404998779297, L3: 26.77871322631836\n",
      "Current prediction:  61.52711486816406 \n",
      "\n",
      "Iteration 4700, Loss: 35.99925231933594, L1: 10.348779678344727, L3: 25.650470733642578\n",
      "Current prediction:  61.52954864501953 \n",
      "\n",
      "Iteration 4701, Loss: 36.034996032714844, L1: 10.331815719604492, L3: 25.703182220458984\n",
      "Current prediction:  61.53322219848633 \n",
      "\n",
      "Iteration 4702, Loss: 37.329994201660156, L1: 10.323759078979492, L3: 27.006237030029297\n",
      "Current prediction:  61.53230667114258 \n",
      "\n",
      "Iteration 4703, Loss: 36.221534729003906, L1: 10.29614543914795, L3: 25.92538833618164\n",
      "Current prediction:  61.52928924560547 \n",
      "\n",
      "Iteration 4704, Loss: 36.93008804321289, L1: 10.310662269592285, L3: 26.61942481994629\n",
      "Current prediction:  61.52423095703125 \n",
      "\n",
      "Iteration 4705, Loss: 36.864097595214844, L1: 10.338554382324219, L3: 26.525541305541992\n",
      "Current prediction:  61.51425552368164 \n",
      "\n",
      "Iteration 4706, Loss: 36.54816436767578, L1: 10.35695743560791, L3: 26.191205978393555\n",
      "Current prediction:  61.50074768066406 \n",
      "\n",
      "Iteration 4707, Loss: 37.29856872558594, L1: 10.344012260437012, L3: 26.95455551147461\n",
      "Current prediction:  61.483238220214844 \n",
      "\n",
      "Iteration 4708, Loss: 37.295406341552734, L1: 10.445718765258789, L3: 26.849687576293945\n",
      "Current prediction:  61.4642219543457 \n",
      "\n",
      "Iteration 4709, Loss: 37.06666564941406, L1: 10.377521514892578, L3: 26.689146041870117\n",
      "Current prediction:  61.44532775878906 \n",
      "\n",
      "Iteration 4710, Loss: 36.246734619140625, L1: 10.476821899414062, L3: 25.769912719726562\n",
      "Current prediction:  61.42715835571289 \n",
      "\n",
      "Iteration 4711, Loss: 36.66309356689453, L1: 10.456422805786133, L3: 26.2066707611084\n",
      "Current prediction:  61.410545349121094 \n",
      "\n",
      "Iteration 4712, Loss: 35.86017608642578, L1: 10.426301956176758, L3: 25.433876037597656\n",
      "Current prediction:  61.39657974243164 \n",
      "\n",
      "Iteration 4713, Loss: 36.707237243652344, L1: 10.45842170715332, L3: 26.248815536499023\n",
      "Current prediction:  61.38161849975586 \n",
      "\n",
      "Iteration 4714, Loss: 37.59864044189453, L1: 10.443063735961914, L3: 27.15557861328125\n",
      "Current prediction:  61.372528076171875 \n",
      "\n",
      "Iteration 4715, Loss: 36.762264251708984, L1: 10.469144821166992, L3: 26.293119430541992\n",
      "Current prediction:  61.363433837890625 \n",
      "\n",
      "Iteration 4716, Loss: 36.46437072753906, L1: 10.48157024383545, L3: 25.982799530029297\n",
      "Current prediction:  61.34886169433594 \n",
      "\n",
      "Iteration 4717, Loss: 36.82795333862305, L1: 10.486577033996582, L3: 26.34137535095215\n",
      "Current prediction:  61.271331787109375 \n",
      "\n",
      "Iteration 4718, Loss: 36.067806243896484, L1: 10.462723731994629, L3: 25.605083465576172\n",
      "Current prediction:  60.83028793334961 \n",
      "\n",
      "Iteration 4719, Loss: 36.72725296020508, L1: 10.486156463623047, L3: 26.24109649658203\n",
      "Current prediction:  60.655235290527344 \n",
      "\n",
      "Iteration 4720, Loss: 35.282447814941406, L1: 10.516468048095703, L3: 24.765981674194336\n",
      "Current prediction:  60.65709686279297 \n",
      "\n",
      "Iteration 4721, Loss: 36.03818893432617, L1: 10.508896827697754, L3: 25.5292911529541\n",
      "Current prediction:  60.67900848388672 \n",
      "\n",
      "Iteration 4722, Loss: 36.66969680786133, L1: 10.488094329833984, L3: 26.181602478027344\n",
      "Current prediction:  60.7145881652832 \n",
      "\n",
      "Iteration 4723, Loss: 36.5306282043457, L1: 10.44817066192627, L3: 26.08245849609375\n",
      "Current prediction:  60.87888717651367 \n",
      "\n",
      "Iteration 4724, Loss: 36.07734680175781, L1: 10.423600196838379, L3: 25.65374755859375\n",
      "Current prediction:  61.33868408203125 \n",
      "\n",
      "Iteration 4725, Loss: 36.15742874145508, L1: 10.368457794189453, L3: 25.788970947265625\n",
      "Current prediction:  61.49836349487305 \n",
      "\n",
      "Iteration 4726, Loss: 37.164794921875, L1: 10.36098861694336, L3: 26.80380630493164\n",
      "Current prediction:  61.52933120727539 \n",
      "\n",
      "Iteration 4727, Loss: 36.54325866699219, L1: 10.335983276367188, L3: 26.207275390625\n",
      "Current prediction:  61.543678283691406 \n",
      "\n",
      "Iteration 4728, Loss: 36.8216552734375, L1: 10.298699378967285, L3: 26.52295684814453\n",
      "Current prediction:  61.55281066894531 \n",
      "\n",
      "Iteration 4729, Loss: 36.37424850463867, L1: 10.326272964477539, L3: 26.047975540161133\n",
      "Current prediction:  61.55876922607422 \n",
      "\n",
      "Iteration 4730, Loss: 35.682655334472656, L1: 10.300130844116211, L3: 25.382522583007812\n",
      "Current prediction:  61.56199645996094 \n",
      "\n",
      "Iteration 4731, Loss: 36.69795227050781, L1: 10.315051078796387, L3: 26.38290023803711\n",
      "Current prediction:  61.56453323364258 \n",
      "\n",
      "Iteration 4732, Loss: 36.94050979614258, L1: 10.295463562011719, L3: 26.64504623413086\n",
      "Current prediction:  61.5648307800293 \n",
      "\n",
      "Iteration 4733, Loss: 37.046451568603516, L1: 10.286907196044922, L3: 26.759544372558594\n",
      "Current prediction:  61.558380126953125 \n",
      "\n",
      "Iteration 4734, Loss: 36.386024475097656, L1: 10.303650856018066, L3: 26.082372665405273\n",
      "Current prediction:  61.55152893066406 \n",
      "\n",
      "Iteration 4735, Loss: 36.23933792114258, L1: 10.300259590148926, L3: 25.939077377319336\n",
      "Current prediction:  61.545166015625 \n",
      "\n",
      "Iteration 4736, Loss: 36.16493225097656, L1: 10.31956958770752, L3: 25.845361709594727\n",
      "Current prediction:  61.537803649902344 \n",
      "\n",
      "Iteration 4737, Loss: 37.8590202331543, L1: 10.732292175292969, L3: 27.126728057861328\n",
      "Current prediction:  61.52762985229492 \n",
      "\n",
      "Iteration 4738, Loss: 36.64632797241211, L1: 10.32515811920166, L3: 26.321168899536133\n",
      "Current prediction:  61.512229919433594 \n",
      "\n",
      "Iteration 4739, Loss: 37.36893081665039, L1: 10.323866844177246, L3: 27.045063018798828\n",
      "Current prediction:  61.47317123413086 \n",
      "\n",
      "Iteration 4740, Loss: 35.7308235168457, L1: 10.353232383728027, L3: 25.37759017944336\n",
      "Current prediction:  61.349449157714844 \n",
      "\n",
      "Iteration 4741, Loss: 35.981807708740234, L1: 10.338451385498047, L3: 25.643356323242188\n",
      "Current prediction:  61.127872467041016 \n",
      "\n",
      "Iteration 4742, Loss: 36.71836853027344, L1: 10.358176231384277, L3: 26.360191345214844\n",
      "Current prediction:  61.12790298461914 \n",
      "\n",
      "Iteration 4743, Loss: 36.71918487548828, L1: 10.38042163848877, L3: 26.338762283325195\n",
      "Current prediction:  61.31690216064453 \n",
      "\n",
      "Iteration 4744, Loss: 37.13762283325195, L1: 10.383137702941895, L3: 26.754484176635742\n",
      "Current prediction:  61.443729400634766 \n",
      "\n",
      "Iteration 4745, Loss: 37.242637634277344, L1: 10.343528747558594, L3: 26.899106979370117\n",
      "Current prediction:  61.47184753417969 \n",
      "\n",
      "Iteration 4746, Loss: 35.65581512451172, L1: 10.354488372802734, L3: 25.30132484436035\n",
      "Current prediction:  61.47136306762695 \n",
      "\n",
      "Iteration 4747, Loss: 37.015018463134766, L1: 10.338702201843262, L3: 26.676315307617188\n",
      "Current prediction:  61.48262023925781 \n",
      "\n",
      "Iteration 4748, Loss: 35.797794342041016, L1: 10.348790168762207, L3: 25.449003219604492\n",
      "Current prediction:  61.483665466308594 \n",
      "\n",
      "Iteration 4749, Loss: 36.42927932739258, L1: 10.351075172424316, L3: 26.078205108642578\n",
      "Current prediction:  61.50261306762695 \n",
      "\n",
      "Iteration 4750, Loss: 36.75019836425781, L1: 10.356181144714355, L3: 26.39401626586914\n",
      "Current prediction:  61.509437561035156 \n",
      "\n",
      "Iteration 4751, Loss: 37.52585220336914, L1: 10.34679889678955, L3: 27.179052352905273\n",
      "Current prediction:  61.50766372680664 \n",
      "\n",
      "Iteration 4752, Loss: 36.58546829223633, L1: 10.34697151184082, L3: 26.238496780395508\n",
      "Current prediction:  61.50237274169922 \n",
      "\n",
      "Iteration 4753, Loss: 35.87639236450195, L1: 10.320226669311523, L3: 25.55616569519043\n",
      "Current prediction:  61.504051208496094 \n",
      "\n",
      "Iteration 4754, Loss: 36.326602935791016, L1: 10.338852882385254, L3: 25.987749099731445\n",
      "Current prediction:  61.50282669067383 \n",
      "\n",
      "Iteration 4755, Loss: 36.6512451171875, L1: 10.294676780700684, L3: 26.356569290161133\n",
      "Current prediction:  61.47212219238281 \n",
      "\n",
      "Iteration 4756, Loss: 36.78120040893555, L1: 10.339227676391602, L3: 26.441972732543945\n",
      "Current prediction:  61.510108947753906 \n",
      "\n",
      "Iteration 4757, Loss: 36.88390350341797, L1: 10.32288646697998, L3: 26.561017990112305\n",
      "Current prediction:  61.52634811401367 \n",
      "\n",
      "Iteration 4758, Loss: 36.89251708984375, L1: 10.292303085327148, L3: 26.600215911865234\n",
      "Current prediction:  61.496482849121094 \n",
      "\n",
      "Iteration 4759, Loss: 37.07741928100586, L1: 10.303668975830078, L3: 26.77375030517578\n",
      "Current prediction:  61.36322784423828 \n",
      "\n",
      "Iteration 4760, Loss: 36.347198486328125, L1: 10.310934066772461, L3: 26.03626251220703\n",
      "Current prediction:  61.23127746582031 \n",
      "\n",
      "Iteration 4761, Loss: 35.58692169189453, L1: 10.314552307128906, L3: 25.272369384765625\n",
      "Current prediction:  61.032997131347656 \n",
      "\n",
      "Iteration 4762, Loss: 36.07968521118164, L1: 10.333571434020996, L3: 25.74611473083496\n",
      "Current prediction:  61.061988830566406 \n",
      "\n",
      "Iteration 4763, Loss: 36.98890686035156, L1: 10.328676223754883, L3: 26.66023063659668\n",
      "Current prediction:  61.33134078979492 \n",
      "\n",
      "Iteration 4764, Loss: 37.006649017333984, L1: 10.298296928405762, L3: 26.70835304260254\n",
      "Current prediction:  61.44260025024414 \n",
      "\n",
      "Iteration 4765, Loss: 36.752586364746094, L1: 10.309883117675781, L3: 26.442703247070312\n",
      "Current prediction:  61.548789978027344 \n",
      "\n",
      "Iteration 4766, Loss: 36.91062927246094, L1: 10.295709609985352, L3: 26.614919662475586\n",
      "Current prediction:  61.55842590332031 \n",
      "\n",
      "Iteration 4767, Loss: 36.04524612426758, L1: 10.303119659423828, L3: 25.74212646484375\n",
      "Current prediction:  61.564659118652344 \n",
      "\n",
      "Iteration 4768, Loss: 35.78950500488281, L1: 10.307188987731934, L3: 25.482316970825195\n",
      "Current prediction:  61.572845458984375 \n",
      "\n",
      "Iteration 4769, Loss: 37.144954681396484, L1: 10.306490898132324, L3: 26.838462829589844\n",
      "Current prediction:  61.57621765136719 \n",
      "\n",
      "Iteration 4770, Loss: 36.41194534301758, L1: 10.2937650680542, L3: 26.118181228637695\n",
      "Current prediction:  61.57796096801758 \n",
      "\n",
      "Iteration 4771, Loss: 37.094627380371094, L1: 10.283860206604004, L3: 26.810768127441406\n",
      "Current prediction:  61.57266616821289 \n",
      "\n",
      "Iteration 4772, Loss: 36.98253631591797, L1: 10.282254219055176, L3: 26.700281143188477\n",
      "Current prediction:  61.5645751953125 \n",
      "\n",
      "Iteration 4773, Loss: 35.77303695678711, L1: 10.286752700805664, L3: 25.486284255981445\n",
      "Current prediction:  61.5567512512207 \n",
      "\n",
      "Iteration 4774, Loss: 36.6312141418457, L1: 10.310328483581543, L3: 26.320886611938477\n",
      "Current prediction:  61.54690933227539 \n",
      "\n",
      "Iteration 4775, Loss: 36.84513854980469, L1: 10.305334091186523, L3: 26.539806365966797\n",
      "Current prediction:  61.53578186035156 \n",
      "\n",
      "Iteration 4776, Loss: 37.089332580566406, L1: 10.317855834960938, L3: 26.771474838256836\n",
      "Current prediction:  61.52378845214844 \n",
      "\n",
      "Iteration 4777, Loss: 36.41046905517578, L1: 10.323662757873535, L3: 26.086807250976562\n",
      "Current prediction:  61.511810302734375 \n",
      "\n",
      "Iteration 4778, Loss: 36.97257614135742, L1: 10.331812858581543, L3: 26.640762329101562\n",
      "Current prediction:  61.501895904541016 \n",
      "\n",
      "Iteration 4779, Loss: 36.45832061767578, L1: 10.353357315063477, L3: 26.104965209960938\n",
      "Current prediction:  61.49513244628906 \n",
      "\n",
      "Iteration 4780, Loss: 35.59879684448242, L1: 10.3558931350708, L3: 25.242904663085938\n",
      "Current prediction:  61.4935302734375 \n",
      "\n",
      "Iteration 4781, Loss: 36.43507385253906, L1: 10.35032844543457, L3: 26.084747314453125\n",
      "Current prediction:  61.48957061767578 \n",
      "\n",
      "Iteration 4782, Loss: 35.6224479675293, L1: 10.380854606628418, L3: 25.241594314575195\n",
      "Current prediction:  61.49062728881836 \n",
      "\n",
      "Iteration 4783, Loss: 37.06010818481445, L1: 10.347835540771484, L3: 26.71227264404297\n",
      "Current prediction:  61.49143600463867 \n",
      "\n",
      "Iteration 4784, Loss: 37.09257507324219, L1: 10.349185943603516, L3: 26.74338722229004\n",
      "Current prediction:  61.491188049316406 \n",
      "\n",
      "Iteration 4785, Loss: 35.80828094482422, L1: 10.34256362915039, L3: 25.465715408325195\n",
      "Current prediction:  61.49227523803711 \n",
      "\n",
      "Iteration 4786, Loss: 36.35272979736328, L1: 10.338740348815918, L3: 26.01399040222168\n",
      "Current prediction:  61.491390228271484 \n",
      "\n",
      "Iteration 4787, Loss: 36.08982849121094, L1: 10.350359916687012, L3: 25.739469528198242\n",
      "Current prediction:  61.49219512939453 \n",
      "\n",
      "Iteration 4788, Loss: 36.54747772216797, L1: 10.340034484863281, L3: 26.20744514465332\n",
      "Current prediction:  61.49631118774414 \n",
      "\n",
      "Iteration 4789, Loss: 36.54541015625, L1: 10.353316307067871, L3: 26.192092895507812\n",
      "Current prediction:  61.50003433227539 \n",
      "\n",
      "Iteration 4790, Loss: 35.677860260009766, L1: 10.325925827026367, L3: 25.3519344329834\n",
      "Current prediction:  61.50542068481445 \n",
      "\n",
      "Iteration 4791, Loss: 37.15787124633789, L1: 10.33509349822998, L3: 26.822778701782227\n",
      "Current prediction:  61.503604888916016 \n",
      "\n",
      "Iteration 4792, Loss: 36.36785888671875, L1: 10.335765838623047, L3: 26.032093048095703\n",
      "Current prediction:  61.473628997802734 \n",
      "\n",
      "Iteration 4793, Loss: 36.244285583496094, L1: 10.323988914489746, L3: 25.92029571533203\n",
      "Current prediction:  61.34769058227539 \n",
      "\n",
      "Iteration 4794, Loss: 36.218894958496094, L1: 10.33401107788086, L3: 25.8848819732666\n",
      "Current prediction:  61.00691223144531 \n",
      "\n",
      "Iteration 4795, Loss: 36.6082763671875, L1: 10.315390586853027, L3: 26.29288673400879\n",
      "Current prediction:  60.937896728515625 \n",
      "\n",
      "Iteration 4796, Loss: 37.021034240722656, L1: 10.321852684020996, L3: 26.699180603027344\n",
      "Current prediction:  60.88837814331055 \n",
      "\n",
      "Iteration 4797, Loss: 36.41339111328125, L1: 10.301898956298828, L3: 26.111494064331055\n",
      "Current prediction:  60.904502868652344 \n",
      "\n",
      "Iteration 4798, Loss: 36.6910400390625, L1: 10.304738998413086, L3: 26.38629913330078\n",
      "Current prediction:  60.914451599121094 \n",
      "\n",
      "Iteration 4799, Loss: 36.52399444580078, L1: 10.311925888061523, L3: 26.212066650390625\n",
      "Current prediction:  60.998783111572266 \n",
      "\n",
      "Iteration 4800, Loss: 36.82381057739258, L1: 10.293143272399902, L3: 26.530668258666992\n",
      "Current prediction:  61.28107452392578 \n",
      "\n",
      "Iteration 4801, Loss: 36.705257415771484, L1: 10.280057907104492, L3: 26.425199508666992\n",
      "Current prediction:  61.40922927856445 \n",
      "\n",
      "Iteration 4802, Loss: 36.75413513183594, L1: 10.237587928771973, L3: 26.51654624938965\n",
      "Current prediction:  61.498619079589844 \n",
      "\n",
      "Iteration 4803, Loss: 36.45261764526367, L1: 10.24707317352295, L3: 26.205543518066406\n",
      "Current prediction:  61.537559509277344 \n",
      "\n",
      "Iteration 4804, Loss: 34.933563232421875, L1: 10.243549346923828, L3: 24.690011978149414\n",
      "Current prediction:  61.576690673828125 \n",
      "\n",
      "Iteration 4805, Loss: 35.634117126464844, L1: 10.24560546875, L3: 25.388511657714844\n",
      "Current prediction:  61.57166290283203 \n",
      "\n",
      "Iteration 4806, Loss: 36.25556564331055, L1: 10.238944053649902, L3: 26.01662254333496\n",
      "Current prediction:  61.41693878173828 \n",
      "\n",
      "Iteration 4807, Loss: 36.8969612121582, L1: 10.239517211914062, L3: 26.65744400024414\n",
      "Current prediction:  61.03797149658203 \n",
      "\n",
      "Iteration 4808, Loss: 36.66435623168945, L1: 10.253588676452637, L3: 26.410768508911133\n",
      "Current prediction:  60.993873596191406 \n",
      "\n",
      "Iteration 4809, Loss: 35.94207000732422, L1: 10.299384117126465, L3: 25.64268684387207\n",
      "Current prediction:  61.06208419799805 \n",
      "\n",
      "Iteration 4810, Loss: 36.23600387573242, L1: 10.273428916931152, L3: 25.962575912475586\n",
      "Current prediction:  61.378822326660156 \n",
      "\n",
      "Iteration 4811, Loss: 36.945838928222656, L1: 10.285806655883789, L3: 26.660030364990234\n",
      "Current prediction:  61.52512741088867 \n",
      "\n",
      "Iteration 4812, Loss: 36.882442474365234, L1: 10.2672119140625, L3: 26.615230560302734\n",
      "Current prediction:  61.54243850708008 \n",
      "\n",
      "Iteration 4813, Loss: 37.6961784362793, L1: 10.27440357208252, L3: 27.421775817871094\n",
      "Current prediction:  61.49109649658203 \n",
      "\n",
      "Iteration 4814, Loss: 37.39872741699219, L1: 10.270269393920898, L3: 27.128459930419922\n",
      "Current prediction:  61.344993591308594 \n",
      "\n",
      "Iteration 4815, Loss: 36.895450592041016, L1: 10.307373046875, L3: 26.588077545166016\n",
      "Current prediction:  60.95774459838867 \n",
      "\n",
      "Iteration 4816, Loss: 37.393497467041016, L1: 10.333710670471191, L3: 27.05978775024414\n",
      "Current prediction:  60.80143356323242 \n",
      "\n",
      "Iteration 4817, Loss: 37.893558502197266, L1: 10.350207328796387, L3: 27.543350219726562\n",
      "Current prediction:  60.80095672607422 \n",
      "\n",
      "Iteration 4818, Loss: 37.526344299316406, L1: 10.354248046875, L3: 27.172094345092773\n",
      "Current prediction:  60.906742095947266 \n",
      "\n",
      "Iteration 4819, Loss: 37.48529815673828, L1: 10.356783866882324, L3: 27.12851333618164\n",
      "Current prediction:  61.05910873413086 \n",
      "\n",
      "Iteration 4820, Loss: 36.79895782470703, L1: 10.356588363647461, L3: 26.442371368408203\n",
      "Current prediction:  61.40143585205078 \n",
      "\n",
      "Iteration 4821, Loss: 36.9179801940918, L1: 10.323810577392578, L3: 26.59416961669922\n",
      "Current prediction:  61.43130874633789 \n",
      "\n",
      "Iteration 4822, Loss: 36.315826416015625, L1: 10.363658905029297, L3: 25.95216941833496\n",
      "Current prediction:  61.443721771240234 \n",
      "\n",
      "Iteration 4823, Loss: 36.30504608154297, L1: 10.374547958374023, L3: 25.930500030517578\n",
      "Current prediction:  61.46776580810547 \n",
      "\n",
      "Iteration 4824, Loss: 35.716514587402344, L1: 10.345662117004395, L3: 25.370851516723633\n",
      "Current prediction:  61.45194625854492 \n",
      "\n",
      "Iteration 4825, Loss: 35.494781494140625, L1: 10.354698181152344, L3: 25.14008331298828\n",
      "Current prediction:  61.4196891784668 \n",
      "\n",
      "Iteration 4826, Loss: 37.44366455078125, L1: 10.363215446472168, L3: 27.0804500579834\n",
      "Current prediction:  61.153350830078125 \n",
      "\n",
      "Iteration 4827, Loss: 36.615760803222656, L1: 10.374812126159668, L3: 26.240947723388672\n",
      "Current prediction:  60.9767951965332 \n",
      "\n",
      "Iteration 4828, Loss: 35.803367614746094, L1: 10.37778091430664, L3: 25.425586700439453\n",
      "Current prediction:  61.05300521850586 \n",
      "\n",
      "Iteration 4829, Loss: 36.77496337890625, L1: 10.363059997558594, L3: 26.411903381347656\n",
      "Current prediction:  61.205360412597656 \n",
      "\n",
      "Iteration 4830, Loss: 36.67561340332031, L1: 10.353508949279785, L3: 26.322105407714844\n",
      "Current prediction:  61.40727233886719 \n",
      "\n",
      "Iteration 4831, Loss: 37.289154052734375, L1: 10.32761287689209, L3: 26.96154022216797\n",
      "Current prediction:  61.491519927978516 \n",
      "\n",
      "Iteration 4832, Loss: 38.935787200927734, L1: 13.176253318786621, L3: 25.75953483581543\n",
      "Current prediction:  61.49872589111328 \n",
      "\n",
      "Iteration 4833, Loss: 37.102073669433594, L1: 10.329172134399414, L3: 26.772903442382812\n",
      "Current prediction:  61.48231887817383 \n",
      "\n",
      "Iteration 4834, Loss: 36.392852783203125, L1: 10.33287525177002, L3: 26.05997657775879\n",
      "Current prediction:  61.418785095214844 \n",
      "\n",
      "Iteration 4835, Loss: 36.79763412475586, L1: 10.297550201416016, L3: 26.500083923339844\n",
      "Current prediction:  61.09272384643555 \n",
      "\n",
      "Iteration 4836, Loss: 36.54204559326172, L1: 10.338205337524414, L3: 26.203840255737305\n",
      "Current prediction:  60.87995529174805 \n",
      "\n",
      "Iteration 4837, Loss: 35.62251281738281, L1: 10.3340425491333, L3: 25.288471221923828\n",
      "Current prediction:  60.89996337890625 \n",
      "\n",
      "Iteration 4838, Loss: 37.1205940246582, L1: 10.341560363769531, L3: 26.779033660888672\n",
      "Current prediction:  60.96498107910156 \n",
      "\n",
      "Iteration 4839, Loss: 36.29185104370117, L1: 10.36474895477295, L3: 25.927101135253906\n",
      "Current prediction:  61.11641311645508 \n",
      "\n",
      "Iteration 4840, Loss: 35.97026062011719, L1: 10.341628074645996, L3: 25.628631591796875\n",
      "Current prediction:  61.3873405456543 \n",
      "\n",
      "Iteration 4841, Loss: 36.26390075683594, L1: 10.316984176635742, L3: 25.946914672851562\n",
      "Current prediction:  61.47233200073242 \n",
      "\n",
      "Iteration 4842, Loss: 36.65559387207031, L1: 10.325471878051758, L3: 26.330121994018555\n",
      "Current prediction:  61.4841423034668 \n",
      "\n",
      "Iteration 4843, Loss: 36.26340103149414, L1: 10.31363582611084, L3: 25.949766159057617\n",
      "Current prediction:  61.49346923828125 \n",
      "\n",
      "Iteration 4844, Loss: 37.652591705322266, L1: 10.327677726745605, L3: 27.324914932250977\n",
      "Current prediction:  61.457176208496094 \n",
      "\n",
      "Iteration 4845, Loss: 36.43513870239258, L1: 10.291699409484863, L3: 26.14344024658203\n",
      "Current prediction:  61.405582427978516 \n",
      "\n",
      "Iteration 4846, Loss: 35.91065216064453, L1: 10.303454399108887, L3: 25.60719871520996\n",
      "Current prediction:  61.25840759277344 \n",
      "\n",
      "Iteration 4847, Loss: 36.75762176513672, L1: 10.34292984008789, L3: 26.414690017700195\n",
      "Current prediction:  61.07469940185547 \n",
      "\n",
      "Iteration 4848, Loss: 37.090431213378906, L1: 10.319740295410156, L3: 26.77069091796875\n",
      "Current prediction:  60.898681640625 \n",
      "\n",
      "Iteration 4849, Loss: 35.84678649902344, L1: 10.325414657592773, L3: 25.521371841430664\n",
      "Current prediction:  60.87052536010742 \n",
      "\n",
      "Iteration 4850, Loss: 38.238731384277344, L1: 10.358449935913086, L3: 27.880281448364258\n",
      "Current prediction:  60.878028869628906 \n",
      "\n",
      "Iteration 4851, Loss: 35.91840744018555, L1: 10.362256050109863, L3: 25.55615234375\n",
      "Current prediction:  60.92464065551758 \n",
      "\n",
      "Iteration 4852, Loss: 37.21837615966797, L1: 10.358184814453125, L3: 26.860191345214844\n",
      "Current prediction:  61.226036071777344 \n",
      "\n",
      "Iteration 4853, Loss: 36.62055206298828, L1: 10.309832572937012, L3: 26.310718536376953\n",
      "Current prediction:  61.44366455078125 \n",
      "\n",
      "Iteration 4854, Loss: 36.269439697265625, L1: 10.315877914428711, L3: 25.953561782836914\n",
      "Current prediction:  61.500484466552734 \n",
      "\n",
      "Iteration 4855, Loss: 36.50611114501953, L1: 10.325775146484375, L3: 26.18033790588379\n",
      "Current prediction:  61.50632095336914 \n",
      "\n",
      "Iteration 4856, Loss: 36.485198974609375, L1: 10.314526557922363, L3: 26.170673370361328\n",
      "Current prediction:  61.50682067871094 \n",
      "\n",
      "Iteration 4857, Loss: 35.709049224853516, L1: 10.320931434631348, L3: 25.38811683654785\n",
      "Current prediction:  61.5003662109375 \n",
      "\n",
      "Iteration 4858, Loss: 36.768253326416016, L1: 10.326156616210938, L3: 26.442096710205078\n",
      "Current prediction:  61.502254486083984 \n",
      "\n",
      "Iteration 4859, Loss: 35.70561218261719, L1: 10.296055793762207, L3: 25.409557342529297\n",
      "Current prediction:  61.4991340637207 \n",
      "\n",
      "Iteration 4860, Loss: 37.61217498779297, L1: 10.296704292297363, L3: 27.315471649169922\n",
      "Current prediction:  61.40835189819336 \n",
      "\n",
      "Iteration 4861, Loss: 36.796287536621094, L1: 10.320802688598633, L3: 26.47548484802246\n",
      "Current prediction:  61.214393615722656 \n",
      "\n",
      "Iteration 4862, Loss: 35.62401580810547, L1: 10.297163963317871, L3: 25.326852798461914\n",
      "Current prediction:  61.12533187866211 \n",
      "\n",
      "Iteration 4863, Loss: 35.902435302734375, L1: 10.306882858276367, L3: 25.595552444458008\n",
      "Current prediction:  61.21772003173828 \n",
      "\n",
      "Iteration 4864, Loss: 36.65876007080078, L1: 10.315589904785156, L3: 26.343172073364258\n",
      "Current prediction:  61.38057327270508 \n",
      "\n",
      "Iteration 4865, Loss: 36.712860107421875, L1: 10.301118850708008, L3: 26.411741256713867\n",
      "Current prediction:  61.428340911865234 \n",
      "\n",
      "Iteration 4866, Loss: 36.65522766113281, L1: 10.303417205810547, L3: 26.351808547973633\n",
      "Current prediction:  61.460323333740234 \n",
      "\n",
      "Iteration 4867, Loss: 35.29645538330078, L1: 10.26534652709961, L3: 25.031108856201172\n",
      "Current prediction:  61.499874114990234 \n",
      "\n",
      "Iteration 4868, Loss: 37.18925857543945, L1: 10.285210609436035, L3: 26.9040470123291\n",
      "Current prediction:  61.51434326171875 \n",
      "\n",
      "Iteration 4869, Loss: 36.720367431640625, L1: 10.253365516662598, L3: 26.467002868652344\n",
      "Current prediction:  61.527530670166016 \n",
      "\n",
      "Iteration 4870, Loss: 36.70943069458008, L1: 10.276189804077148, L3: 26.43324089050293\n",
      "Current prediction:  61.53547286987305 \n",
      "\n",
      "Iteration 4871, Loss: 37.26156234741211, L1: 10.294283866882324, L3: 26.96727752685547\n",
      "Current prediction:  61.509761810302734 \n",
      "\n",
      "Iteration 4872, Loss: 36.16224670410156, L1: 10.267044067382812, L3: 25.895200729370117\n",
      "Current prediction:  61.50083923339844 \n",
      "\n",
      "Iteration 4873, Loss: 36.75688934326172, L1: 10.274986267089844, L3: 26.481903076171875\n",
      "Current prediction:  61.48814010620117 \n",
      "\n",
      "Iteration 4874, Loss: 36.75062942504883, L1: 10.283935546875, L3: 26.466693878173828\n",
      "Current prediction:  61.3814697265625 \n",
      "\n",
      "Iteration 4875, Loss: 38.169395446777344, L1: 10.29794979095459, L3: 27.871444702148438\n",
      "Current prediction:  61.08546829223633 \n",
      "\n",
      "Iteration 4876, Loss: 36.43092727661133, L1: 10.301358222961426, L3: 26.129568099975586\n",
      "Current prediction:  60.94619369506836 \n",
      "\n",
      "Iteration 4877, Loss: 37.22795867919922, L1: 10.319475173950195, L3: 26.908483505249023\n",
      "Current prediction:  60.95583724975586 \n",
      "\n",
      "Iteration 4878, Loss: 37.2939453125, L1: 10.33517837524414, L3: 26.958765029907227\n",
      "Current prediction:  60.90458297729492 \n",
      "\n",
      "Iteration 4879, Loss: 36.726383209228516, L1: 10.316040992736816, L3: 26.410343170166016\n",
      "Current prediction:  60.95011901855469 \n",
      "\n",
      "Iteration 4880, Loss: 37.22645950317383, L1: 10.325088500976562, L3: 26.901371002197266\n",
      "Current prediction:  61.11845016479492 \n",
      "\n",
      "Iteration 4881, Loss: 36.508460998535156, L1: 10.318839073181152, L3: 26.189620971679688\n",
      "Current prediction:  61.369468688964844 \n",
      "\n",
      "Iteration 4882, Loss: 36.68401336669922, L1: 10.301360130310059, L3: 26.382652282714844\n",
      "Current prediction:  61.48297882080078 \n",
      "\n",
      "Iteration 4883, Loss: 37.45928192138672, L1: 10.302306175231934, L3: 27.15697479248047\n",
      "Current prediction:  61.51535415649414 \n",
      "\n",
      "Iteration 4884, Loss: 37.474388122558594, L1: 10.310487747192383, L3: 27.16390037536621\n",
      "Current prediction:  61.51536178588867 \n",
      "\n",
      "Iteration 4885, Loss: 36.52444839477539, L1: 10.290532112121582, L3: 26.233915328979492\n",
      "Current prediction:  61.511741638183594 \n",
      "\n",
      "Iteration 4886, Loss: 36.99414825439453, L1: 10.32082462310791, L3: 26.673324584960938\n",
      "Current prediction:  61.509063720703125 \n",
      "\n",
      "Iteration 4887, Loss: 35.85525894165039, L1: 10.318646430969238, L3: 25.536611557006836\n",
      "Current prediction:  61.50925064086914 \n",
      "\n",
      "Iteration 4888, Loss: 35.5712890625, L1: 10.303793907165527, L3: 25.26749610900879\n",
      "Current prediction:  61.49868392944336 \n",
      "\n",
      "Iteration 4889, Loss: 36.478355407714844, L1: 10.313299179077148, L3: 26.165058135986328\n",
      "Current prediction:  61.276283264160156 \n",
      "\n",
      "Iteration 4890, Loss: 36.56504821777344, L1: 10.32505989074707, L3: 26.239990234375\n",
      "Current prediction:  60.94076919555664 \n",
      "\n",
      "Iteration 4891, Loss: 37.15568542480469, L1: 10.312532424926758, L3: 26.84315299987793\n",
      "Current prediction:  60.88960266113281 \n",
      "\n",
      "Iteration 4892, Loss: 37.21753692626953, L1: 10.338482856750488, L3: 26.879053115844727\n",
      "Current prediction:  60.98850631713867 \n",
      "\n",
      "Iteration 4893, Loss: 36.89738464355469, L1: 10.301484107971191, L3: 26.59589958190918\n",
      "Current prediction:  61.28791046142578 \n",
      "\n",
      "Iteration 4894, Loss: 36.2595100402832, L1: 10.301339149475098, L3: 25.95816993713379\n",
      "Current prediction:  61.4507942199707 \n",
      "\n",
      "Iteration 4895, Loss: 37.070457458496094, L1: 10.317194938659668, L3: 26.75326156616211\n",
      "Current prediction:  61.48551940917969 \n",
      "\n",
      "Iteration 4896, Loss: 35.63545227050781, L1: 10.297624588012695, L3: 25.337827682495117\n",
      "Current prediction:  61.52729797363281 \n",
      "\n",
      "Iteration 4897, Loss: 36.35451889038086, L1: 10.282187461853027, L3: 26.072330474853516\n",
      "Current prediction:  61.52309036254883 \n",
      "\n",
      "Iteration 4898, Loss: 37.44038391113281, L1: 10.27509880065918, L3: 27.165287017822266\n",
      "Current prediction:  61.5280647277832 \n",
      "\n",
      "Iteration 4899, Loss: 35.74376678466797, L1: 10.27722454071045, L3: 25.466541290283203\n",
      "Current prediction:  61.53087615966797 \n",
      "\n",
      "Iteration 4900, Loss: 36.56404113769531, L1: 10.278876304626465, L3: 26.285165786743164\n",
      "Current prediction:  61.413307189941406 \n",
      "\n",
      "Iteration 4901, Loss: 36.07063674926758, L1: 10.250371932983398, L3: 25.82026481628418\n",
      "Current prediction:  60.93584060668945 \n",
      "\n",
      "Iteration 4902, Loss: 37.70240020751953, L1: 10.294798851013184, L3: 27.40760040283203\n",
      "Current prediction:  60.832401275634766 \n",
      "\n",
      "Iteration 4903, Loss: 36.80850601196289, L1: 10.33697509765625, L3: 26.47153091430664\n",
      "Current prediction:  60.83345031738281 \n",
      "\n",
      "Iteration 4904, Loss: 35.99476623535156, L1: 10.326773643493652, L3: 25.667991638183594\n",
      "Current prediction:  60.8321533203125 \n",
      "\n",
      "Iteration 4905, Loss: 35.43836975097656, L1: 10.318132400512695, L3: 25.120235443115234\n",
      "Current prediction:  60.82763671875 \n",
      "\n",
      "Iteration 4906, Loss: 36.49470138549805, L1: 10.328658103942871, L3: 26.16604232788086\n",
      "Current prediction:  60.83220291137695 \n",
      "\n",
      "Iteration 4907, Loss: 36.48057556152344, L1: 10.303606033325195, L3: 26.176971435546875\n",
      "Current prediction:  60.91489791870117 \n",
      "\n",
      "Iteration 4908, Loss: 37.88428497314453, L1: 10.297138214111328, L3: 27.587146759033203\n",
      "Current prediction:  61.33746337890625 \n",
      "\n",
      "Iteration 4909, Loss: 36.664215087890625, L1: 10.270570755004883, L3: 26.393644332885742\n",
      "Current prediction:  61.54678726196289 \n",
      "\n",
      "Iteration 4910, Loss: 36.920650482177734, L1: 10.287176132202148, L3: 26.633474349975586\n",
      "Current prediction:  61.54561233520508 \n",
      "\n",
      "Iteration 4911, Loss: 37.17597961425781, L1: 10.278643608093262, L3: 26.897336959838867\n",
      "Current prediction:  61.53792953491211 \n",
      "\n",
      "Iteration 4912, Loss: 36.981971740722656, L1: 10.282756805419922, L3: 26.699214935302734\n",
      "Current prediction:  61.53030014038086 \n",
      "\n",
      "Iteration 4913, Loss: 36.099884033203125, L1: 10.2979097366333, L3: 25.801973342895508\n",
      "Current prediction:  61.52473831176758 \n",
      "\n",
      "Iteration 4914, Loss: 36.652198791503906, L1: 10.295679092407227, L3: 26.356521606445312\n",
      "Current prediction:  61.5166015625 \n",
      "\n",
      "Iteration 4915, Loss: 36.29011535644531, L1: 10.297834396362305, L3: 25.99228286743164\n",
      "Current prediction:  61.5067024230957 \n",
      "\n",
      "Iteration 4916, Loss: 35.797080993652344, L1: 10.29450511932373, L3: 25.50257682800293\n",
      "Current prediction:  61.499759674072266 \n",
      "\n",
      "Iteration 4917, Loss: 35.74162292480469, L1: 10.310603141784668, L3: 25.431018829345703\n",
      "Current prediction:  61.49496078491211 \n",
      "\n",
      "Iteration 4918, Loss: 36.630653381347656, L1: 10.326008796691895, L3: 26.304643630981445\n",
      "Current prediction:  61.49159240722656 \n",
      "\n",
      "Iteration 4919, Loss: 36.679405212402344, L1: 10.334871292114258, L3: 26.344533920288086\n",
      "Current prediction:  61.48921585083008 \n",
      "\n",
      "Iteration 4920, Loss: 37.061424255371094, L1: 10.32170581817627, L3: 26.73971939086914\n",
      "Current prediction:  61.483917236328125 \n",
      "\n",
      "Iteration 4921, Loss: 35.34349822998047, L1: 10.31508731842041, L3: 25.028411865234375\n",
      "Current prediction:  61.47944259643555 \n",
      "\n",
      "Iteration 4922, Loss: 36.695777893066406, L1: 10.341472625732422, L3: 26.35430335998535\n",
      "Current prediction:  61.47379684448242 \n",
      "\n",
      "Iteration 4923, Loss: 36.343101501464844, L1: 10.35446548461914, L3: 25.988636016845703\n",
      "Current prediction:  61.4671745300293 \n",
      "\n",
      "Iteration 4924, Loss: 36.884620666503906, L1: 10.35423469543457, L3: 26.53038787841797\n",
      "Current prediction:  61.456993103027344 \n",
      "\n",
      "Iteration 4925, Loss: 36.65382385253906, L1: 10.393378257751465, L3: 26.260446548461914\n",
      "Current prediction:  61.44706344604492 \n",
      "\n",
      "Iteration 4926, Loss: 36.44279098510742, L1: 10.366305351257324, L3: 26.07648468017578\n",
      "Current prediction:  61.434059143066406 \n",
      "\n",
      "Iteration 4927, Loss: 36.672393798828125, L1: 10.36936092376709, L3: 26.30303192138672\n",
      "Current prediction:  61.41938400268555 \n",
      "\n",
      "Iteration 4928, Loss: 35.5429801940918, L1: 10.387981414794922, L3: 25.154998779296875\n",
      "Current prediction:  61.408203125 \n",
      "\n",
      "Iteration 4929, Loss: 37.09089279174805, L1: 10.461688041687012, L3: 26.62920570373535\n",
      "Current prediction:  61.40007781982422 \n",
      "\n",
      "Iteration 4930, Loss: 36.876731872558594, L1: 10.428948402404785, L3: 26.447784423828125\n",
      "Current prediction:  61.39223861694336 \n",
      "\n",
      "Iteration 4931, Loss: 37.11355972290039, L1: 10.414240837097168, L3: 26.699317932128906\n",
      "Current prediction:  61.38383102416992 \n",
      "\n",
      "Iteration 4932, Loss: 35.510536193847656, L1: 10.45803165435791, L3: 25.052505493164062\n",
      "Current prediction:  61.37544631958008 \n",
      "\n",
      "Iteration 4933, Loss: 37.764732360839844, L1: 10.424919128417969, L3: 27.339815139770508\n",
      "Current prediction:  61.37029266357422 \n",
      "\n",
      "Iteration 4934, Loss: 36.53528594970703, L1: 10.475627899169922, L3: 26.059659957885742\n",
      "Current prediction:  61.36747741699219 \n",
      "\n",
      "Iteration 4935, Loss: 37.27520751953125, L1: 10.422653198242188, L3: 26.852556228637695\n",
      "Current prediction:  61.36119079589844 \n",
      "\n",
      "Iteration 4936, Loss: 36.48204040527344, L1: 10.480311393737793, L3: 26.001728057861328\n",
      "Current prediction:  61.35762405395508 \n",
      "\n",
      "Iteration 4937, Loss: 37.0225830078125, L1: 10.434754371643066, L3: 26.587827682495117\n",
      "Current prediction:  61.35721969604492 \n",
      "\n",
      "Iteration 4938, Loss: 36.99558639526367, L1: 10.428838729858398, L3: 26.566747665405273\n",
      "Current prediction:  61.355308532714844 \n",
      "\n",
      "Iteration 4939, Loss: 36.337120056152344, L1: 10.439102172851562, L3: 25.898019790649414\n",
      "Current prediction:  61.35086441040039 \n",
      "\n",
      "Iteration 4940, Loss: 36.385765075683594, L1: 10.430093765258789, L3: 25.955671310424805\n",
      "Current prediction:  61.30080032348633 \n",
      "\n",
      "Iteration 4941, Loss: 36.854164123535156, L1: 10.42245101928711, L3: 26.431713104248047\n",
      "Current prediction:  60.81766128540039 \n",
      "\n",
      "Iteration 4942, Loss: 35.337646484375, L1: 10.435503005981445, L3: 24.902145385742188\n",
      "Current prediction:  60.630985260009766 \n",
      "\n",
      "Iteration 4943, Loss: 35.851531982421875, L1: 10.505430221557617, L3: 25.34610366821289\n",
      "Current prediction:  60.636940002441406 \n",
      "\n",
      "Iteration 4944, Loss: 37.6068229675293, L1: 10.526429176330566, L3: 27.080392837524414\n",
      "Current prediction:  60.65654373168945 \n",
      "\n",
      "Iteration 4945, Loss: 36.107826232910156, L1: 10.511392593383789, L3: 25.596435546875\n",
      "Current prediction:  60.67904281616211 \n",
      "\n",
      "Iteration 4946, Loss: 37.033660888671875, L1: 10.489091873168945, L3: 26.54456901550293\n",
      "Current prediction:  60.70684051513672 \n",
      "\n",
      "Iteration 4947, Loss: 37.23479461669922, L1: 10.483175277709961, L3: 26.751619338989258\n",
      "Current prediction:  60.736480712890625 \n",
      "\n",
      "Iteration 4948, Loss: 35.946624755859375, L1: 10.436529159545898, L3: 25.510095596313477\n",
      "Current prediction:  60.771949768066406 \n",
      "\n",
      "Iteration 4949, Loss: 37.093963623046875, L1: 10.364999771118164, L3: 26.728961944580078\n",
      "Current prediction:  61.06291961669922 \n",
      "\n",
      "Iteration 4950, Loss: 36.137176513671875, L1: 10.294500350952148, L3: 25.842674255371094\n",
      "Current prediction:  61.565216064453125 \n",
      "\n",
      "Iteration 4951, Loss: 36.27714920043945, L1: 10.2418794631958, L3: 26.035268783569336\n",
      "Current prediction:  61.59404373168945 \n",
      "\n",
      "Iteration 4952, Loss: 36.939945220947266, L1: 10.228028297424316, L3: 26.711915969848633\n",
      "Current prediction:  61.60985565185547 \n",
      "\n",
      "Iteration 4953, Loss: 35.46219253540039, L1: 10.201062202453613, L3: 25.261131286621094\n",
      "Current prediction:  61.623565673828125 \n",
      "\n",
      "Iteration 4954, Loss: 36.47013473510742, L1: 10.200417518615723, L3: 26.269716262817383\n",
      "Current prediction:  61.632205963134766 \n",
      "\n",
      "Iteration 4955, Loss: 37.28963851928711, L1: 10.199965476989746, L3: 27.089672088623047\n",
      "Current prediction:  61.633018493652344 \n",
      "\n",
      "Iteration 4956, Loss: 36.08929443359375, L1: 10.228211402893066, L3: 25.861082077026367\n",
      "Current prediction:  61.6236572265625 \n",
      "\n",
      "Iteration 4957, Loss: 36.859920501708984, L1: 10.21455192565918, L3: 26.645368576049805\n",
      "Current prediction:  61.60512924194336 \n",
      "\n",
      "Iteration 4958, Loss: 37.080020904541016, L1: 10.250733375549316, L3: 26.829286575317383\n",
      "Current prediction:  61.5816535949707 \n",
      "\n",
      "Iteration 4959, Loss: 36.64462661743164, L1: 10.210381507873535, L3: 26.434246063232422\n",
      "Current prediction:  61.55892562866211 \n",
      "\n",
      "Iteration 4960, Loss: 36.90884780883789, L1: 10.304352760314941, L3: 26.604494094848633\n",
      "Current prediction:  61.53458786010742 \n",
      "\n",
      "Iteration 4961, Loss: 36.92341232299805, L1: 10.2929105758667, L3: 26.630502700805664\n",
      "Current prediction:  61.509342193603516 \n",
      "\n",
      "Iteration 4962, Loss: 37.306541442871094, L1: 10.326541900634766, L3: 26.979997634887695\n",
      "Current prediction:  61.483489990234375 \n",
      "\n",
      "Iteration 4963, Loss: 36.09668731689453, L1: 10.366165161132812, L3: 25.73052406311035\n",
      "Current prediction:  61.457000732421875 \n",
      "\n",
      "Iteration 4964, Loss: 36.26630401611328, L1: 10.346392631530762, L3: 25.919912338256836\n",
      "Current prediction:  61.431793212890625 \n",
      "\n",
      "Iteration 4965, Loss: 35.903953552246094, L1: 10.365339279174805, L3: 25.538616180419922\n",
      "Current prediction:  61.409812927246094 \n",
      "\n",
      "Iteration 4966, Loss: 36.737510681152344, L1: 10.383007049560547, L3: 26.35450553894043\n",
      "Current prediction:  61.3858642578125 \n",
      "\n",
      "Iteration 4967, Loss: 35.58940505981445, L1: 10.400405883789062, L3: 25.18899917602539\n",
      "Current prediction:  60.88413619995117 \n",
      "\n",
      "Iteration 4968, Loss: 36.27330017089844, L1: 10.420888900756836, L3: 25.852413177490234\n",
      "Current prediction:  60.63105773925781 \n",
      "\n",
      "Iteration 4969, Loss: 37.20784378051758, L1: 10.490721702575684, L3: 26.717121124267578\n",
      "Current prediction:  60.625953674316406 \n",
      "\n",
      "Iteration 4970, Loss: 36.62590408325195, L1: 10.541411399841309, L3: 26.08449363708496\n",
      "Current prediction:  60.63118362426758 \n",
      "\n",
      "Iteration 4971, Loss: 37.11820983886719, L1: 10.561636924743652, L3: 26.55657196044922\n",
      "Current prediction:  60.642555236816406 \n",
      "\n",
      "Iteration 4972, Loss: 36.4862174987793, L1: 10.520551681518555, L3: 25.965665817260742\n",
      "Current prediction:  60.66123580932617 \n",
      "\n",
      "Iteration 4973, Loss: 36.90380096435547, L1: 10.505254745483398, L3: 26.398544311523438\n",
      "Current prediction:  60.701480865478516 \n",
      "\n",
      "Iteration 4974, Loss: 36.30344772338867, L1: 10.397326469421387, L3: 25.9061222076416\n",
      "Current prediction:  61.241355895996094 \n",
      "\n",
      "Iteration 4975, Loss: 36.89677047729492, L1: 10.3432035446167, L3: 26.55356788635254\n",
      "Current prediction:  61.48055648803711 \n",
      "\n",
      "Iteration 4976, Loss: 35.24287414550781, L1: 10.316184043884277, L3: 24.92669105529785\n",
      "Current prediction:  61.50629806518555 \n",
      "\n",
      "Iteration 4977, Loss: 36.27338790893555, L1: 10.304594993591309, L3: 25.968793869018555\n",
      "Current prediction:  61.528228759765625 \n",
      "\n",
      "Iteration 4978, Loss: 36.03614044189453, L1: 10.276411056518555, L3: 25.759727478027344\n",
      "Current prediction:  61.54750442504883 \n",
      "\n",
      "Iteration 4979, Loss: 37.06235122680664, L1: 10.26834774017334, L3: 26.794002532958984\n",
      "Current prediction:  61.55991744995117 \n",
      "\n",
      "Iteration 4980, Loss: 37.47544479370117, L1: 10.281922340393066, L3: 27.19352149963379\n",
      "Current prediction:  61.564422607421875 \n",
      "\n",
      "Iteration 4981, Loss: 36.0455207824707, L1: 10.259939193725586, L3: 25.785581588745117\n",
      "Current prediction:  61.5668830871582 \n",
      "\n",
      "Iteration 4982, Loss: 37.03598403930664, L1: 10.250788688659668, L3: 26.785194396972656\n",
      "Current prediction:  61.55815505981445 \n",
      "\n",
      "Iteration 4983, Loss: 35.29452133178711, L1: 10.255385398864746, L3: 25.03913688659668\n",
      "Current prediction:  61.27374267578125 \n",
      "\n",
      "Iteration 4984, Loss: 35.301536560058594, L1: 10.25617504119873, L3: 25.045360565185547\n",
      "Current prediction:  60.868934631347656 \n",
      "\n",
      "Iteration 4985, Loss: 36.448753356933594, L1: 10.295165061950684, L3: 26.153587341308594\n",
      "Current prediction:  60.82513427734375 \n",
      "\n",
      "Iteration 4986, Loss: 36.18220520019531, L1: 10.313128471374512, L3: 25.869077682495117\n",
      "Current prediction:  60.82463073730469 \n",
      "\n",
      "Iteration 4987, Loss: 36.15313720703125, L1: 10.328332901000977, L3: 25.82480239868164\n",
      "Current prediction:  60.835697174072266 \n",
      "\n",
      "Iteration 4988, Loss: 36.54639434814453, L1: 10.31660270690918, L3: 26.22978973388672\n",
      "Current prediction:  61.01350021362305 \n",
      "\n",
      "Iteration 4989, Loss: 36.51963424682617, L1: 10.27431869506836, L3: 26.245315551757812\n",
      "Current prediction:  61.55046844482422 \n",
      "\n",
      "Iteration 4990, Loss: 36.762115478515625, L1: 10.25296401977539, L3: 26.5091495513916\n",
      "Current prediction:  61.5693473815918 \n",
      "\n",
      "Iteration 4991, Loss: 36.094757080078125, L1: 10.23190975189209, L3: 25.86284637451172\n",
      "Current prediction:  61.56570816040039 \n",
      "\n",
      "Iteration 4992, Loss: 37.070465087890625, L1: 10.24323558807373, L3: 26.827228546142578\n",
      "Current prediction:  61.55785369873047 \n",
      "\n",
      "Iteration 4993, Loss: 36.93846893310547, L1: 10.249099731445312, L3: 26.68937110900879\n",
      "Current prediction:  61.54795455932617 \n",
      "\n",
      "Iteration 4994, Loss: 37.594932556152344, L1: 10.273051261901855, L3: 27.321880340576172\n",
      "Current prediction:  61.52870559692383 \n",
      "\n",
      "Iteration 4995, Loss: 36.705631256103516, L1: 10.282012939453125, L3: 26.42361831665039\n",
      "Current prediction:  61.50738525390625 \n",
      "\n",
      "Iteration 4996, Loss: 36.42731857299805, L1: 10.296134948730469, L3: 26.131183624267578\n",
      "Current prediction:  61.48809051513672 \n",
      "\n",
      "Iteration 4997, Loss: 36.75115966796875, L1: 10.323945999145508, L3: 26.42721176147461\n",
      "Current prediction:  61.46833419799805 \n",
      "\n",
      "Iteration 4998, Loss: 37.00376510620117, L1: 10.329217910766602, L3: 26.67454719543457\n",
      "Current prediction:  61.45000457763672 \n",
      "\n",
      "â†³ LR reduced to 5.0e-04 at iteration 5000 \n",
      "\n",
      "Iteration 4999, Loss: 36.14662551879883, L1: 10.353850364685059, L3: 25.792774200439453\n",
      "Current prediction:  61.43208694458008 \n",
      "\n",
      "Iteration 5000, Loss: 36.89082336425781, L1: 10.36789321899414, L3: 26.52292823791504\n",
      "Current prediction:  61.42461013793945 \n",
      "\n",
      "Iteration 5001, Loss: 36.555686950683594, L1: 10.350730895996094, L3: 26.2049560546875\n",
      "Current prediction:  61.419010162353516 \n",
      "\n",
      "Iteration 5002, Loss: 36.39665985107422, L1: 10.364431381225586, L3: 26.032228469848633\n",
      "Current prediction:  61.413883209228516 \n",
      "\n",
      "Iteration 5003, Loss: 35.689414978027344, L1: 10.36451530456543, L3: 25.324899673461914\n",
      "Current prediction:  61.41008377075195 \n",
      "\n",
      "Iteration 5004, Loss: 36.59892272949219, L1: 10.377180099487305, L3: 26.221744537353516\n",
      "Current prediction:  61.40138244628906 \n",
      "\n",
      "Iteration 5005, Loss: 36.312625885009766, L1: 10.365439414978027, L3: 25.947185516357422\n",
      "Current prediction:  61.36709976196289 \n",
      "\n",
      "Iteration 5006, Loss: 36.582176208496094, L1: 10.365374565124512, L3: 26.216800689697266\n",
      "Current prediction:  61.160621643066406 \n",
      "\n",
      "Iteration 5007, Loss: 35.66419982910156, L1: 10.367465019226074, L3: 25.296735763549805\n",
      "Current prediction:  60.84028244018555 \n",
      "\n",
      "Iteration 5008, Loss: 36.486541748046875, L1: 10.372411727905273, L3: 26.114131927490234\n",
      "Current prediction:  60.7067756652832 \n",
      "\n",
      "Iteration 5009, Loss: 35.83359146118164, L1: 10.40121078491211, L3: 25.43238067626953\n",
      "Current prediction:  60.6905403137207 \n",
      "\n",
      "Iteration 5010, Loss: 36.09048843383789, L1: 10.435506820678711, L3: 25.65498161315918\n",
      "Current prediction:  60.69584655761719 \n",
      "\n",
      "Iteration 5011, Loss: 35.97368621826172, L1: 10.415164947509766, L3: 25.55851936340332\n",
      "Current prediction:  60.71433639526367 \n",
      "\n",
      "Iteration 5012, Loss: 36.81431579589844, L1: 10.409870147705078, L3: 26.404443740844727\n",
      "Current prediction:  60.75784683227539 \n",
      "\n",
      "Iteration 5013, Loss: 37.02627182006836, L1: 10.385640144348145, L3: 26.6406307220459\n",
      "Current prediction:  60.926395416259766 \n",
      "\n",
      "Iteration 5014, Loss: 36.22051239013672, L1: 10.328706741333008, L3: 25.89180564880371\n",
      "Current prediction:  61.27457046508789 \n",
      "\n",
      "Iteration 5015, Loss: 36.15847396850586, L1: 10.288339614868164, L3: 25.870134353637695\n",
      "Current prediction:  61.48198318481445 \n",
      "\n",
      "Iteration 5016, Loss: 36.076541900634766, L1: 10.287507057189941, L3: 25.789033889770508\n",
      "Current prediction:  61.52041244506836 \n",
      "\n",
      "Iteration 5017, Loss: 36.854000091552734, L1: 10.28622817993164, L3: 26.567771911621094\n",
      "Current prediction:  61.53495407104492 \n",
      "\n",
      "Iteration 5018, Loss: 36.9771728515625, L1: 10.265961647033691, L3: 26.711212158203125\n",
      "Current prediction:  61.545799255371094 \n",
      "\n",
      "Iteration 5019, Loss: 36.46877670288086, L1: 10.2438325881958, L3: 26.224943161010742\n",
      "Current prediction:  61.55443572998047 \n",
      "\n",
      "Iteration 5020, Loss: 36.70204162597656, L1: 10.25199031829834, L3: 26.45005226135254\n",
      "Current prediction:  61.55786895751953 \n",
      "\n",
      "Iteration 5021, Loss: 36.7421989440918, L1: 10.25368881225586, L3: 26.488510131835938\n",
      "Current prediction:  61.55266571044922 \n",
      "\n",
      "Iteration 5022, Loss: 36.816551208496094, L1: 10.225831031799316, L3: 26.590721130371094\n",
      "Current prediction:  61.53937530517578 \n",
      "\n",
      "Iteration 5023, Loss: 36.304595947265625, L1: 10.218775749206543, L3: 26.0858211517334\n",
      "Current prediction:  61.48335266113281 \n",
      "\n",
      "Iteration 5024, Loss: 36.994964599609375, L1: 10.232948303222656, L3: 26.76201629638672\n",
      "Current prediction:  61.43811798095703 \n",
      "\n",
      "Iteration 5025, Loss: 36.74481201171875, L1: 10.237617492675781, L3: 26.50719451904297\n",
      "Current prediction:  61.35260772705078 \n",
      "\n",
      "Iteration 5026, Loss: 37.39582443237305, L1: 10.24906063079834, L3: 27.146764755249023\n",
      "Current prediction:  61.21284103393555 \n",
      "\n",
      "Iteration 5027, Loss: 36.14122772216797, L1: 10.236676216125488, L3: 25.904552459716797\n",
      "Current prediction:  61.134639739990234 \n",
      "\n",
      "Iteration 5028, Loss: 35.898197174072266, L1: 10.21056842803955, L3: 25.6876277923584\n",
      "Current prediction:  61.0821647644043 \n",
      "\n",
      "Iteration 5029, Loss: 36.03097915649414, L1: 10.233999252319336, L3: 25.796979904174805\n",
      "Current prediction:  61.22036361694336 \n",
      "\n",
      "Iteration 5030, Loss: 36.64655303955078, L1: 10.238790512084961, L3: 26.40776252746582\n",
      "Current prediction:  61.2891960144043 \n",
      "\n",
      "Iteration 5031, Loss: 36.28166961669922, L1: 10.207513809204102, L3: 26.074153900146484\n",
      "Current prediction:  61.46433639526367 \n",
      "\n",
      "Iteration 5032, Loss: 37.55867385864258, L1: 10.227757453918457, L3: 27.330915451049805\n",
      "Current prediction:  61.533485412597656 \n",
      "\n",
      "Iteration 5033, Loss: 35.72368240356445, L1: 10.245607376098633, L3: 25.47807502746582\n",
      "Current prediction:  61.55286407470703 \n",
      "\n",
      "Iteration 5034, Loss: 37.08495330810547, L1: 10.22285270690918, L3: 26.862102508544922\n",
      "Current prediction:  61.55156707763672 \n",
      "\n",
      "Iteration 5035, Loss: 35.70774459838867, L1: 10.25081729888916, L3: 25.456928253173828\n",
      "Current prediction:  61.549072265625 \n",
      "\n",
      "Iteration 5036, Loss: 37.21136474609375, L1: 10.26457405090332, L3: 26.94679069519043\n",
      "Current prediction:  61.545867919921875 \n",
      "\n",
      "Iteration 5037, Loss: 36.44310760498047, L1: 10.257774353027344, L3: 26.185333251953125\n",
      "Current prediction:  61.54033279418945 \n",
      "\n",
      "Iteration 5038, Loss: 36.934791564941406, L1: 10.288593292236328, L3: 26.646196365356445\n",
      "Current prediction:  61.53411102294922 \n",
      "\n",
      "Iteration 5039, Loss: 36.80430603027344, L1: 10.261442184448242, L3: 26.542865753173828\n",
      "Current prediction:  61.524723052978516 \n",
      "\n",
      "Iteration 5040, Loss: 36.237335205078125, L1: 10.27071475982666, L3: 25.96661949157715\n",
      "Current prediction:  61.510498046875 \n",
      "\n",
      "Iteration 5041, Loss: 36.25695037841797, L1: 10.255912780761719, L3: 26.001035690307617\n",
      "Current prediction:  61.48740768432617 \n",
      "\n",
      "Iteration 5042, Loss: 35.65370178222656, L1: 10.27935791015625, L3: 25.374343872070312\n",
      "Current prediction:  61.43264389038086 \n",
      "\n",
      "Iteration 5043, Loss: 36.28246307373047, L1: 10.278656959533691, L3: 26.003807067871094\n",
      "Current prediction:  61.33620834350586 \n",
      "\n",
      "Iteration 5044, Loss: 36.99890899658203, L1: 10.286943435668945, L3: 26.711965560913086\n",
      "Current prediction:  61.23759841918945 \n",
      "\n",
      "Iteration 5045, Loss: 36.03447723388672, L1: 10.296724319458008, L3: 25.737754821777344\n",
      "Current prediction:  61.183998107910156 \n",
      "\n",
      "Iteration 5046, Loss: 36.42167663574219, L1: 10.30017375946045, L3: 26.121503829956055\n",
      "Current prediction:  61.22037124633789 \n",
      "\n",
      "Iteration 5047, Loss: 36.79830551147461, L1: 10.314940452575684, L3: 26.483366012573242\n",
      "Current prediction:  61.34372329711914 \n",
      "\n",
      "Iteration 5048, Loss: 36.79280471801758, L1: 10.289911270141602, L3: 26.502893447875977\n",
      "Current prediction:  61.38201904296875 \n",
      "\n",
      "Iteration 5049, Loss: 36.790916442871094, L1: 10.28102970123291, L3: 26.5098876953125\n",
      "Current prediction:  61.40226364135742 \n",
      "\n",
      "Iteration 5050, Loss: 36.807918548583984, L1: 10.28931713104248, L3: 26.518600463867188\n",
      "Current prediction:  61.39319610595703 \n",
      "\n",
      "Iteration 5051, Loss: 36.712711334228516, L1: 10.265847206115723, L3: 26.446863174438477\n",
      "Current prediction:  61.23952102661133 \n",
      "\n",
      "Iteration 5052, Loss: 35.760623931884766, L1: 10.2963285446167, L3: 25.46429443359375\n",
      "Current prediction:  61.11185073852539 \n",
      "\n",
      "Iteration 5053, Loss: 36.857295989990234, L1: 10.2709321975708, L3: 26.586362838745117\n",
      "Current prediction:  61.188968658447266 \n",
      "\n",
      "Iteration 5054, Loss: 36.49793243408203, L1: 10.24917984008789, L3: 26.248750686645508\n",
      "Current prediction:  61.30270004272461 \n",
      "\n",
      "Iteration 5055, Loss: 37.66347122192383, L1: 10.263751983642578, L3: 27.39971923828125\n",
      "Current prediction:  61.43397903442383 \n",
      "\n",
      "Iteration 5056, Loss: 35.99217224121094, L1: 10.26024055480957, L3: 25.731929779052734\n",
      "Current prediction:  61.494911193847656 \n",
      "\n",
      "Iteration 5057, Loss: 36.06999588012695, L1: 10.245047569274902, L3: 25.824947357177734\n",
      "Current prediction:  61.50874710083008 \n",
      "\n",
      "Iteration 5058, Loss: 36.45249938964844, L1: 10.250974655151367, L3: 26.20152473449707\n",
      "Current prediction:  61.51530075073242 \n",
      "\n",
      "Iteration 5059, Loss: 35.71104049682617, L1: 10.247910499572754, L3: 25.463130950927734\n",
      "Current prediction:  61.43753433227539 \n",
      "\n",
      "Iteration 5060, Loss: 36.49226760864258, L1: 10.259939193725586, L3: 26.232328414916992\n",
      "Current prediction:  61.31216812133789 \n",
      "\n",
      "Iteration 5061, Loss: 37.098854064941406, L1: 10.24023723602295, L3: 26.85861587524414\n",
      "Current prediction:  61.101768493652344 \n",
      "\n",
      "Iteration 5062, Loss: 37.30027389526367, L1: 10.267596244812012, L3: 27.032678604125977\n",
      "Current prediction:  61.0202522277832 \n",
      "\n",
      "Iteration 5063, Loss: 35.899017333984375, L1: 10.218531608581543, L3: 25.680484771728516\n",
      "Current prediction:  61.066280364990234 \n",
      "\n",
      "Iteration 5064, Loss: 36.477294921875, L1: 10.256050109863281, L3: 26.22124671936035\n",
      "Current prediction:  61.232398986816406 \n",
      "\n",
      "Iteration 5065, Loss: 36.848106384277344, L1: 10.255147933959961, L3: 26.592958450317383\n",
      "Current prediction:  61.363407135009766 \n",
      "\n",
      "Iteration 5066, Loss: 37.28369140625, L1: 10.252068519592285, L3: 27.0316219329834\n",
      "Current prediction:  61.4313850402832 \n",
      "\n",
      "Iteration 5067, Loss: 36.890926361083984, L1: 10.271529197692871, L3: 26.619396209716797\n",
      "Current prediction:  61.486541748046875 \n",
      "\n",
      "Iteration 5068, Loss: 36.85125732421875, L1: 10.25467586517334, L3: 26.596580505371094\n",
      "Current prediction:  61.51333999633789 \n",
      "\n",
      "Iteration 5069, Loss: 37.18832015991211, L1: 10.25313663482666, L3: 26.935184478759766\n",
      "Current prediction:  61.504459381103516 \n",
      "\n",
      "Iteration 5070, Loss: 35.86322784423828, L1: 10.241354942321777, L3: 25.621871948242188\n",
      "Current prediction:  61.494873046875 \n",
      "\n",
      "Iteration 5071, Loss: 36.824527740478516, L1: 10.264443397521973, L3: 26.56008529663086\n",
      "Current prediction:  61.47017288208008 \n",
      "\n",
      "Iteration 5072, Loss: 35.24538040161133, L1: 10.24536418914795, L3: 25.000015258789062\n",
      "Current prediction:  61.3995475769043 \n",
      "\n",
      "Iteration 5073, Loss: 36.715492248535156, L1: 10.288509368896484, L3: 26.426984786987305\n",
      "Current prediction:  61.32670211791992 \n",
      "\n",
      "Iteration 5074, Loss: 36.94081115722656, L1: 10.282669067382812, L3: 26.658143997192383\n",
      "Current prediction:  61.259090423583984 \n",
      "\n",
      "Iteration 5075, Loss: 37.11064529418945, L1: 10.281689643859863, L3: 26.828956604003906\n",
      "Current prediction:  61.21120071411133 \n",
      "\n",
      "Iteration 5076, Loss: 36.81615447998047, L1: 10.301801681518555, L3: 26.514352798461914\n",
      "Current prediction:  61.15409469604492 \n",
      "\n",
      "Iteration 5077, Loss: 37.12324142456055, L1: 10.286928176879883, L3: 26.836313247680664\n",
      "Current prediction:  61.18107604980469 \n",
      "\n",
      "Iteration 5078, Loss: 36.90974426269531, L1: 10.292420387268066, L3: 26.617324829101562\n",
      "Current prediction:  61.10559844970703 \n",
      "\n",
      "Iteration 5079, Loss: 36.09852600097656, L1: 10.322479248046875, L3: 25.77604866027832\n",
      "Current prediction:  61.14007568359375 \n",
      "\n",
      "Iteration 5080, Loss: 36.568267822265625, L1: 10.293025970458984, L3: 26.275239944458008\n",
      "Current prediction:  61.2294807434082 \n",
      "\n",
      "Iteration 5081, Loss: 36.070281982421875, L1: 10.300291061401367, L3: 25.769990921020508\n",
      "Current prediction:  61.30675506591797 \n",
      "\n",
      "Iteration 5082, Loss: 37.477909088134766, L1: 10.292205810546875, L3: 27.18570327758789\n",
      "Current prediction:  61.33713150024414 \n",
      "\n",
      "Iteration 5083, Loss: 36.825218200683594, L1: 10.297757148742676, L3: 26.527462005615234\n",
      "Current prediction:  61.29090881347656 \n",
      "\n",
      "Iteration 5084, Loss: 36.230567932128906, L1: 10.289345741271973, L3: 25.941221237182617\n",
      "Current prediction:  61.255889892578125 \n",
      "\n",
      "Iteration 5085, Loss: 36.777427673339844, L1: 10.306297302246094, L3: 26.47113037109375\n",
      "Current prediction:  61.26504898071289 \n",
      "\n",
      "Iteration 5086, Loss: 37.0279655456543, L1: 10.324516296386719, L3: 26.703449249267578\n",
      "Current prediction:  61.26868438720703 \n",
      "\n",
      "Iteration 5087, Loss: 35.84416198730469, L1: 10.28836441040039, L3: 25.555797576904297\n",
      "Current prediction:  61.17652893066406 \n",
      "\n",
      "Iteration 5088, Loss: 36.6585578918457, L1: 10.307948112487793, L3: 26.350608825683594\n",
      "Current prediction:  61.14376449584961 \n",
      "\n",
      "Iteration 5089, Loss: 36.03032684326172, L1: 10.285845756530762, L3: 25.744482040405273\n",
      "Current prediction:  61.19467544555664 \n",
      "\n",
      "Iteration 5090, Loss: 35.64543151855469, L1: 10.27668285369873, L3: 25.368749618530273\n",
      "Current prediction:  61.220069885253906 \n",
      "\n",
      "Iteration 5091, Loss: 36.26522445678711, L1: 10.283991813659668, L3: 25.981233596801758\n",
      "Current prediction:  61.283966064453125 \n",
      "\n",
      "Iteration 5092, Loss: 35.77626419067383, L1: 10.302416801452637, L3: 25.473846435546875\n",
      "Current prediction:  61.35299301147461 \n",
      "\n",
      "Iteration 5093, Loss: 35.81155776977539, L1: 10.269536018371582, L3: 25.542022705078125\n",
      "Current prediction:  61.45869064331055 \n",
      "\n",
      "Iteration 5094, Loss: 36.319847106933594, L1: 10.261822700500488, L3: 26.058025360107422\n",
      "Current prediction:  61.496543884277344 \n",
      "\n",
      "Iteration 5095, Loss: 36.38026809692383, L1: 10.292174339294434, L3: 26.088092803955078\n",
      "Current prediction:  61.50441360473633 \n",
      "\n",
      "Iteration 5096, Loss: 36.23679733276367, L1: 10.25534725189209, L3: 25.9814510345459\n",
      "Current prediction:  61.51424026489258 \n",
      "\n",
      "Iteration 5097, Loss: 36.20043182373047, L1: 10.238308906555176, L3: 25.96212387084961\n",
      "Current prediction:  61.4989128112793 \n",
      "\n",
      "Iteration 5098, Loss: 36.51075744628906, L1: 10.261401176452637, L3: 26.24935531616211\n",
      "Current prediction:  61.469547271728516 \n",
      "\n",
      "Iteration 5099, Loss: 36.21502685546875, L1: 10.26514720916748, L3: 25.949880599975586\n",
      "Current prediction:  61.395721435546875 \n",
      "\n",
      "Iteration 5100, Loss: 35.63328552246094, L1: 10.24470043182373, L3: 25.38858413696289\n",
      "Current prediction:  61.327232360839844 \n",
      "\n",
      "Iteration 5101, Loss: 36.234291076660156, L1: 10.216455459594727, L3: 26.017833709716797\n",
      "Current prediction:  61.14786148071289 \n",
      "\n",
      "Iteration 5102, Loss: 36.41090774536133, L1: 10.24450969696045, L3: 26.166399002075195\n",
      "Current prediction:  60.99928283691406 \n",
      "\n",
      "Iteration 5103, Loss: 36.36390686035156, L1: 10.262197494506836, L3: 26.101709365844727\n",
      "Current prediction:  60.95356369018555 \n",
      "\n",
      "Iteration 5104, Loss: 36.594032287597656, L1: 10.277356147766113, L3: 26.316675186157227\n",
      "Current prediction:  61.064361572265625 \n",
      "\n",
      "Iteration 5105, Loss: 36.893035888671875, L1: 10.237945556640625, L3: 26.65509033203125\n",
      "Current prediction:  61.19888687133789 \n",
      "\n",
      "Iteration 5106, Loss: 36.721534729003906, L1: 10.23705005645752, L3: 26.48448371887207\n",
      "Current prediction:  61.45656967163086 \n",
      "\n",
      "Iteration 5107, Loss: 38.072059631347656, L1: 10.222516059875488, L3: 27.849544525146484\n",
      "Current prediction:  61.52765655517578 \n",
      "\n",
      "Iteration 5108, Loss: 35.62743377685547, L1: 10.241514205932617, L3: 25.38591957092285\n",
      "Current prediction:  61.56707763671875 \n",
      "\n",
      "Iteration 5109, Loss: 36.74164581298828, L1: 10.23142147064209, L3: 26.510225296020508\n",
      "Current prediction:  61.57918167114258 \n",
      "\n",
      "Iteration 5110, Loss: 35.66564178466797, L1: 10.201130867004395, L3: 25.464509963989258\n",
      "Current prediction:  61.5793571472168 \n",
      "\n",
      "Iteration 5111, Loss: 36.83476257324219, L1: 10.222000122070312, L3: 26.612762451171875\n",
      "Current prediction:  61.576881408691406 \n",
      "\n",
      "Iteration 5112, Loss: 36.682029724121094, L1: 10.20926284790039, L3: 26.472768783569336\n",
      "Current prediction:  61.57398223876953 \n",
      "\n",
      "Iteration 5113, Loss: 37.498924255371094, L1: 10.221845626831055, L3: 27.277080535888672\n",
      "Current prediction:  61.57024383544922 \n",
      "\n",
      "Iteration 5114, Loss: 37.00370788574219, L1: 10.242046356201172, L3: 26.761659622192383\n",
      "Current prediction:  61.565711975097656 \n",
      "\n",
      "Iteration 5115, Loss: 36.46711730957031, L1: 10.220632553100586, L3: 26.24648666381836\n",
      "Current prediction:  61.561424255371094 \n",
      "\n",
      "Iteration 5116, Loss: 36.99187469482422, L1: 10.244542121887207, L3: 26.747333526611328\n",
      "Current prediction:  61.55636215209961 \n",
      "\n",
      "Iteration 5117, Loss: 37.11990737915039, L1: 10.226714134216309, L3: 26.8931941986084\n",
      "Current prediction:  61.55035400390625 \n",
      "\n",
      "Iteration 5118, Loss: 37.07472610473633, L1: 10.248832702636719, L3: 26.82589340209961\n",
      "Current prediction:  61.5433349609375 \n",
      "\n",
      "Iteration 5119, Loss: 35.62327575683594, L1: 10.239603996276855, L3: 25.3836727142334\n",
      "Current prediction:  61.53607940673828 \n",
      "\n",
      "Iteration 5120, Loss: 36.6856575012207, L1: 10.263727188110352, L3: 26.42193031311035\n",
      "Current prediction:  61.528480529785156 \n",
      "\n",
      "Iteration 5121, Loss: 36.885047912597656, L1: 10.263223648071289, L3: 26.621822357177734\n",
      "Current prediction:  61.520015716552734 \n",
      "\n",
      "Iteration 5122, Loss: 35.81418228149414, L1: 10.279173851013184, L3: 25.535009384155273\n",
      "Current prediction:  61.51133728027344 \n",
      "\n",
      "Iteration 5123, Loss: 36.731544494628906, L1: 10.272969245910645, L3: 26.458574295043945\n",
      "Current prediction:  61.485877990722656 \n",
      "\n",
      "Iteration 5124, Loss: 35.92934799194336, L1: 10.277107238769531, L3: 25.652240753173828\n",
      "Current prediction:  61.428627014160156 \n",
      "\n",
      "Iteration 5125, Loss: 36.48796844482422, L1: 10.25886344909668, L3: 26.229103088378906\n",
      "Current prediction:  61.38676452636719 \n",
      "\n",
      "Iteration 5126, Loss: 36.836280822753906, L1: 10.287618637084961, L3: 26.548660278320312\n",
      "Current prediction:  61.33343505859375 \n",
      "\n",
      "Iteration 5127, Loss: 37.9669075012207, L1: 10.284963607788086, L3: 27.681943893432617\n",
      "Current prediction:  61.190269470214844 \n",
      "\n",
      "Iteration 5128, Loss: 37.397247314453125, L1: 10.27783489227295, L3: 27.119413375854492\n",
      "Current prediction:  60.976470947265625 \n",
      "\n",
      "Iteration 5129, Loss: 35.98200988769531, L1: 10.28642463684082, L3: 25.69558334350586\n",
      "Current prediction:  60.832130432128906 \n",
      "\n",
      "Iteration 5130, Loss: 36.56191635131836, L1: 10.33988094329834, L3: 26.222034454345703\n",
      "Current prediction:  60.79872512817383 \n",
      "\n",
      "Iteration 5131, Loss: 36.342063903808594, L1: 10.34734058380127, L3: 25.994722366333008\n",
      "Current prediction:  60.82306671142578 \n",
      "\n",
      "Iteration 5132, Loss: 35.96357727050781, L1: 10.341583251953125, L3: 25.62199592590332\n",
      "Current prediction:  60.97319412231445 \n",
      "\n",
      "Iteration 5133, Loss: 36.344268798828125, L1: 10.302989959716797, L3: 26.041276931762695\n",
      "Current prediction:  61.24934005737305 \n",
      "\n",
      "Iteration 5134, Loss: 36.82638931274414, L1: 10.28236198425293, L3: 26.54402732849121\n",
      "Current prediction:  61.439674377441406 \n",
      "\n",
      "Iteration 5135, Loss: 36.29364776611328, L1: 10.306055068969727, L3: 25.987594604492188\n",
      "Current prediction:  61.4769287109375 \n",
      "\n",
      "Iteration 5136, Loss: 36.374141693115234, L1: 10.286955833435059, L3: 26.087186813354492\n",
      "Current prediction:  61.48919677734375 \n",
      "\n",
      "Iteration 5137, Loss: 36.201087951660156, L1: 10.286195755004883, L3: 25.914894104003906\n",
      "Current prediction:  61.49638748168945 \n",
      "\n",
      "Iteration 5138, Loss: 36.053096771240234, L1: 10.284883499145508, L3: 25.768213272094727\n",
      "Current prediction:  61.50096893310547 \n",
      "\n",
      "Iteration 5139, Loss: 36.21848678588867, L1: 10.276606559753418, L3: 25.94188117980957\n",
      "Current prediction:  61.50243377685547 \n",
      "\n",
      "Iteration 5140, Loss: 35.691795349121094, L1: 10.264373779296875, L3: 25.427419662475586\n",
      "Current prediction:  61.50539779663086 \n",
      "\n",
      "Iteration 5141, Loss: 35.53910827636719, L1: 10.240154266357422, L3: 25.298954010009766\n",
      "Current prediction:  61.49845504760742 \n",
      "\n",
      "Iteration 5142, Loss: 36.56296920776367, L1: 10.294726371765137, L3: 26.26824378967285\n",
      "Current prediction:  61.49364471435547 \n",
      "\n",
      "Iteration 5143, Loss: 37.13945388793945, L1: 10.269688606262207, L3: 26.869766235351562\n",
      "Current prediction:  61.48088836669922 \n",
      "\n",
      "Iteration 5144, Loss: 35.47565841674805, L1: 10.270108222961426, L3: 25.205549240112305\n",
      "Current prediction:  61.454673767089844 \n",
      "\n",
      "Iteration 5145, Loss: 36.776344299316406, L1: 10.244808197021484, L3: 26.531538009643555\n",
      "Current prediction:  61.314178466796875 \n",
      "\n",
      "Iteration 5146, Loss: 37.075050354003906, L1: 10.244606971740723, L3: 26.8304443359375\n",
      "Current prediction:  61.044158935546875 \n",
      "\n",
      "Iteration 5147, Loss: 35.980106353759766, L1: 10.265809059143066, L3: 25.714296340942383\n",
      "Current prediction:  60.998268127441406 \n",
      "\n",
      "Iteration 5148, Loss: 36.8835334777832, L1: 10.274480819702148, L3: 26.609052658081055\n",
      "Current prediction:  61.00550079345703 \n",
      "\n",
      "Iteration 5149, Loss: 36.758872985839844, L1: 10.284839630126953, L3: 26.474031448364258\n",
      "Current prediction:  61.18278121948242 \n",
      "\n",
      "Iteration 5150, Loss: 36.22669982910156, L1: 10.248939514160156, L3: 25.977758407592773\n",
      "Current prediction:  61.411312103271484 \n",
      "\n",
      "Iteration 5151, Loss: 36.21843338012695, L1: 10.248669624328613, L3: 25.969764709472656\n",
      "Current prediction:  61.44025421142578 \n",
      "\n",
      "Iteration 5152, Loss: 36.40167236328125, L1: 10.22356128692627, L3: 26.178110122680664\n",
      "Current prediction:  61.43430709838867 \n",
      "\n",
      "Iteration 5153, Loss: 37.24418640136719, L1: 10.23351001739502, L3: 27.01067543029785\n",
      "Current prediction:  61.26866149902344 \n",
      "\n",
      "Iteration 5154, Loss: 36.755958557128906, L1: 10.236886024475098, L3: 26.519071578979492\n",
      "Current prediction:  61.08831024169922 \n",
      "\n",
      "Iteration 5155, Loss: 37.14479446411133, L1: 10.232796669006348, L3: 26.911996841430664\n",
      "Current prediction:  60.978267669677734 \n",
      "\n",
      "Iteration 5156, Loss: 36.47929382324219, L1: 10.263139724731445, L3: 26.21615219116211\n",
      "Current prediction:  61.03327560424805 \n",
      "\n",
      "Iteration 5157, Loss: 36.88640594482422, L1: 10.227703094482422, L3: 26.658700942993164\n",
      "Current prediction:  61.19521713256836 \n",
      "\n",
      "Iteration 5158, Loss: 37.271026611328125, L1: 10.218132019042969, L3: 27.052894592285156\n",
      "Current prediction:  61.32655334472656 \n",
      "\n",
      "Iteration 5159, Loss: 37.62025451660156, L1: 10.218480110168457, L3: 27.401775360107422\n",
      "Current prediction:  61.43927764892578 \n",
      "\n",
      "Iteration 5160, Loss: 36.1655387878418, L1: 10.209090232849121, L3: 25.95644760131836\n",
      "Current prediction:  61.50001907348633 \n",
      "\n",
      "Iteration 5161, Loss: 36.66587829589844, L1: 10.214629173278809, L3: 26.451250076293945\n",
      "Current prediction:  61.508697509765625 \n",
      "\n",
      "Iteration 5162, Loss: 35.51285171508789, L1: 10.215332984924316, L3: 25.29751968383789\n",
      "Current prediction:  61.49351119995117 \n",
      "\n",
      "Iteration 5163, Loss: 36.7762451171875, L1: 10.1951265335083, L3: 26.581119537353516\n",
      "Current prediction:  61.43212127685547 \n",
      "\n",
      "Iteration 5164, Loss: 36.96151351928711, L1: 10.234707832336426, L3: 26.726806640625\n",
      "Current prediction:  61.243804931640625 \n",
      "\n",
      "Iteration 5165, Loss: 35.545982360839844, L1: 10.21256160736084, L3: 25.333419799804688\n",
      "Current prediction:  61.09762954711914 \n",
      "\n",
      "Iteration 5166, Loss: 37.71223068237305, L1: 10.21540641784668, L3: 27.496824264526367\n",
      "Current prediction:  61.11825180053711 \n",
      "\n",
      "Iteration 5167, Loss: 37.49280548095703, L1: 10.21680736541748, L3: 27.275997161865234\n",
      "Current prediction:  61.23979187011719 \n",
      "\n",
      "Iteration 5168, Loss: 36.83049774169922, L1: 10.23256778717041, L3: 26.597930908203125\n",
      "Current prediction:  61.28645706176758 \n",
      "\n",
      "Iteration 5169, Loss: 36.3306770324707, L1: 10.249821662902832, L3: 26.080854415893555\n",
      "Current prediction:  61.35020446777344 \n",
      "\n",
      "Iteration 5170, Loss: 36.876434326171875, L1: 10.226638793945312, L3: 26.649797439575195\n",
      "Current prediction:  61.37605667114258 \n",
      "\n",
      "Iteration 5171, Loss: 36.49214172363281, L1: 10.256696701049805, L3: 26.23544692993164\n",
      "Current prediction:  61.39445495605469 \n",
      "\n",
      "Iteration 5172, Loss: 35.77088165283203, L1: 10.23645305633545, L3: 25.5344295501709\n",
      "Current prediction:  61.441829681396484 \n",
      "\n",
      "Iteration 5173, Loss: 35.910194396972656, L1: 10.273046493530273, L3: 25.637147903442383\n",
      "Current prediction:  61.45494079589844 \n",
      "\n",
      "Iteration 5174, Loss: 36.290653228759766, L1: 10.240291595458984, L3: 26.05036163330078\n",
      "Current prediction:  61.47565460205078 \n",
      "\n",
      "Iteration 5175, Loss: 36.57851028442383, L1: 10.238222122192383, L3: 26.340288162231445\n",
      "Current prediction:  61.49066925048828 \n",
      "\n",
      "Iteration 5176, Loss: 36.26115417480469, L1: 10.256261825561523, L3: 26.004894256591797\n",
      "Current prediction:  61.480010986328125 \n",
      "\n",
      "Iteration 5177, Loss: 37.25063705444336, L1: 10.239534378051758, L3: 27.0111026763916\n",
      "Current prediction:  61.39806365966797 \n",
      "\n",
      "Iteration 5178, Loss: 37.46165466308594, L1: 10.248967170715332, L3: 27.212688446044922\n",
      "Current prediction:  61.19059753417969 \n",
      "\n",
      "Iteration 5179, Loss: 35.79936218261719, L1: 10.28376579284668, L3: 25.515596389770508\n",
      "Current prediction:  60.88272476196289 \n",
      "\n",
      "Iteration 5180, Loss: 35.835731506347656, L1: 10.295795440673828, L3: 25.53993797302246\n",
      "Current prediction:  60.803768157958984 \n",
      "\n",
      "Iteration 5181, Loss: 36.40434646606445, L1: 10.296224594116211, L3: 26.108121871948242\n",
      "Current prediction:  60.788490295410156 \n",
      "\n",
      "Iteration 5182, Loss: 36.14820098876953, L1: 10.320283889770508, L3: 25.827919006347656\n",
      "Current prediction:  60.789222717285156 \n",
      "\n",
      "Iteration 5183, Loss: 35.868316650390625, L1: 10.311955451965332, L3: 25.55636215209961\n",
      "Current prediction:  60.80303192138672 \n",
      "\n",
      "Iteration 5184, Loss: 35.898529052734375, L1: 10.28463363647461, L3: 25.613895416259766\n",
      "Current prediction:  60.86427688598633 \n",
      "\n",
      "Iteration 5185, Loss: 36.24454116821289, L1: 10.302841186523438, L3: 25.941699981689453\n",
      "Current prediction:  61.032230377197266 \n",
      "\n",
      "Iteration 5186, Loss: 37.47178268432617, L1: 10.290689468383789, L3: 27.181093215942383\n",
      "Current prediction:  61.31813049316406 \n",
      "\n",
      "Iteration 5187, Loss: 36.07611846923828, L1: 10.268622398376465, L3: 25.8074951171875\n",
      "Current prediction:  61.453121185302734 \n",
      "\n",
      "Iteration 5188, Loss: 36.15926742553711, L1: 10.264826774597168, L3: 25.894441604614258\n",
      "Current prediction:  61.49234390258789 \n",
      "\n",
      "Iteration 5189, Loss: 35.84308624267578, L1: 10.263887405395508, L3: 25.579200744628906\n",
      "Current prediction:  61.50151062011719 \n",
      "\n",
      "Iteration 5190, Loss: 36.57032012939453, L1: 10.250455856323242, L3: 26.319866180419922\n",
      "Current prediction:  61.48575973510742 \n",
      "\n",
      "Iteration 5191, Loss: 36.566951751708984, L1: 10.23987102508545, L3: 26.32708168029785\n",
      "Current prediction:  61.40071105957031 \n",
      "\n",
      "Iteration 5192, Loss: 37.20137023925781, L1: 10.256470680236816, L3: 26.944900512695312\n",
      "Current prediction:  61.212886810302734 \n",
      "\n",
      "Iteration 5193, Loss: 36.19860076904297, L1: 10.25772762298584, L3: 25.940872192382812\n",
      "Current prediction:  61.03400802612305 \n",
      "\n",
      "Iteration 5194, Loss: 36.70646667480469, L1: 10.24803352355957, L3: 26.45843505859375\n",
      "Current prediction:  60.934146881103516 \n",
      "\n",
      "Iteration 5195, Loss: 36.448631286621094, L1: 10.271352767944336, L3: 26.177278518676758\n",
      "Current prediction:  60.883975982666016 \n",
      "\n",
      "Iteration 5196, Loss: 35.98896026611328, L1: 10.258756637573242, L3: 25.730205535888672\n",
      "Current prediction:  60.877342224121094 \n",
      "\n",
      "Iteration 5197, Loss: 36.29252624511719, L1: 10.254268646240234, L3: 26.03825569152832\n",
      "Current prediction:  60.89949417114258 \n",
      "\n",
      "Iteration 5198, Loss: 36.546241760253906, L1: 10.257553100585938, L3: 26.288686752319336\n",
      "Current prediction:  61.03529357910156 \n",
      "\n",
      "Iteration 5199, Loss: 37.029876708984375, L1: 10.249764442443848, L3: 26.780113220214844\n",
      "Current prediction:  61.291378021240234 \n",
      "\n",
      "Iteration 5200, Loss: 36.247413635253906, L1: 10.234463691711426, L3: 26.012950897216797\n",
      "Current prediction:  61.50106430053711 \n",
      "\n",
      "Iteration 5201, Loss: 35.62760925292969, L1: 10.227828025817871, L3: 25.3997802734375\n",
      "Current prediction:  61.541229248046875 \n",
      "\n",
      "Iteration 5202, Loss: 36.72904586791992, L1: 10.232090950012207, L3: 26.4969539642334\n",
      "Current prediction:  61.55059814453125 \n",
      "\n",
      "Iteration 5203, Loss: 36.18769073486328, L1: 10.258713722229004, L3: 25.928977966308594\n",
      "Current prediction:  61.55210494995117 \n",
      "\n",
      "Iteration 5204, Loss: 35.99000549316406, L1: 10.226407051086426, L3: 25.76359748840332\n",
      "Current prediction:  61.552452087402344 \n",
      "\n",
      "Iteration 5205, Loss: 34.88392639160156, L1: 10.210373878479004, L3: 24.673551559448242\n",
      "Current prediction:  61.55247497558594 \n",
      "\n",
      "Iteration 5206, Loss: 36.86178970336914, L1: 10.253750801086426, L3: 26.6080379486084\n",
      "Current prediction:  61.551353454589844 \n",
      "\n",
      "Iteration 5207, Loss: 36.297645568847656, L1: 10.25330638885498, L3: 26.04433822631836\n",
      "Current prediction:  61.55094528198242 \n",
      "\n",
      "Iteration 5208, Loss: 36.38716506958008, L1: 10.22961139678955, L3: 26.15755271911621\n",
      "Current prediction:  61.5507926940918 \n",
      "\n",
      "Iteration 5209, Loss: 36.41553497314453, L1: 10.219616889953613, L3: 26.195919036865234\n",
      "Current prediction:  61.55030059814453 \n",
      "\n",
      "Iteration 5210, Loss: 36.158294677734375, L1: 10.246234893798828, L3: 25.912057876586914\n",
      "Current prediction:  61.549102783203125 \n",
      "\n",
      "Iteration 5211, Loss: 37.33361053466797, L1: 10.255844116210938, L3: 27.077768325805664\n",
      "Current prediction:  61.54697799682617 \n",
      "\n",
      "Iteration 5212, Loss: 36.700889587402344, L1: 10.229759216308594, L3: 26.47113037109375\n",
      "Current prediction:  61.54229736328125 \n",
      "\n",
      "Iteration 5213, Loss: 37.01451873779297, L1: 10.24765682220459, L3: 26.766862869262695\n",
      "Current prediction:  61.51886749267578 \n",
      "\n",
      "Iteration 5214, Loss: 36.257911682128906, L1: 10.232749938964844, L3: 26.025161743164062\n",
      "Current prediction:  61.410343170166016 \n",
      "\n",
      "Iteration 5215, Loss: 35.644874572753906, L1: 10.229072570800781, L3: 25.415803909301758\n",
      "Current prediction:  61.19853210449219 \n",
      "\n",
      "Iteration 5216, Loss: 36.64234161376953, L1: 10.259410858154297, L3: 26.382930755615234\n",
      "Current prediction:  60.943153381347656 \n",
      "\n",
      "Iteration 5217, Loss: 35.85414123535156, L1: 10.278573036193848, L3: 25.57556915283203\n",
      "Current prediction:  60.85035705566406 \n",
      "\n",
      "Iteration 5218, Loss: 36.63227844238281, L1: 10.288896560668945, L3: 26.3433837890625\n",
      "Current prediction:  60.806915283203125 \n",
      "\n",
      "Iteration 5219, Loss: 36.31153106689453, L1: 10.306543350219727, L3: 26.004989624023438\n",
      "Current prediction:  60.79447937011719 \n",
      "\n",
      "Iteration 5220, Loss: 36.35552978515625, L1: 10.29638957977295, L3: 26.059141159057617\n",
      "Current prediction:  60.79582595825195 \n",
      "\n",
      "Iteration 5221, Loss: 37.749549865722656, L1: 10.311776161193848, L3: 27.437772750854492\n",
      "Current prediction:  60.80121994018555 \n",
      "\n",
      "Iteration 5222, Loss: 36.099853515625, L1: 10.291947364807129, L3: 25.807905197143555\n",
      "Current prediction:  60.827064514160156 \n",
      "\n",
      "Iteration 5223, Loss: 36.29314041137695, L1: 10.311285972595215, L3: 25.981853485107422\n",
      "Current prediction:  60.87606430053711 \n",
      "\n",
      "Iteration 5224, Loss: 36.433433532714844, L1: 10.28193187713623, L3: 26.151500701904297\n",
      "Current prediction:  61.12879180908203 \n",
      "\n",
      "Iteration 5225, Loss: 35.580718994140625, L1: 10.266297340393066, L3: 25.314420700073242\n",
      "Current prediction:  61.428001403808594 \n",
      "\n",
      "Iteration 5226, Loss: 36.10081481933594, L1: 10.203446388244629, L3: 25.897369384765625\n",
      "Current prediction:  61.48673629760742 \n",
      "\n",
      "Iteration 5227, Loss: 36.95282745361328, L1: 10.254867553710938, L3: 26.697961807250977\n",
      "Current prediction:  61.50747299194336 \n",
      "\n",
      "Iteration 5228, Loss: 37.1983642578125, L1: 10.244017601013184, L3: 26.954347610473633\n",
      "Current prediction:  61.519500732421875 \n",
      "\n",
      "Iteration 5229, Loss: 36.233543395996094, L1: 10.224973678588867, L3: 26.008567810058594\n",
      "Current prediction:  61.51777267456055 \n",
      "\n",
      "Iteration 5230, Loss: 34.61359405517578, L1: 10.230439186096191, L3: 24.383153915405273\n",
      "Current prediction:  61.50979995727539 \n",
      "\n",
      "Iteration 5231, Loss: 36.32490921020508, L1: 10.221383094787598, L3: 26.103527069091797\n",
      "Current prediction:  61.502716064453125 \n",
      "\n",
      "Iteration 5232, Loss: 35.69255065917969, L1: 10.235525131225586, L3: 25.457027435302734\n",
      "Current prediction:  61.44437789916992 \n",
      "\n",
      "Iteration 5233, Loss: 36.16547393798828, L1: 10.239896774291992, L3: 25.925579071044922\n",
      "Current prediction:  61.135223388671875 \n",
      "\n",
      "Iteration 5234, Loss: 37.150428771972656, L1: 10.24599838256836, L3: 26.904428482055664\n",
      "Current prediction:  60.891021728515625 \n",
      "\n",
      "Iteration 5235, Loss: 36.10417938232422, L1: 10.251838684082031, L3: 25.85234260559082\n",
      "Current prediction:  60.833740234375 \n",
      "\n",
      "Iteration 5236, Loss: 36.72838592529297, L1: 10.255840301513672, L3: 26.472543716430664\n",
      "Current prediction:  60.817935943603516 \n",
      "\n",
      "Iteration 5237, Loss: 36.3802375793457, L1: 10.303059577941895, L3: 26.077178955078125\n",
      "Current prediction:  60.818397521972656 \n",
      "\n",
      "Iteration 5238, Loss: 36.07603454589844, L1: 10.284314155578613, L3: 25.79172134399414\n",
      "Current prediction:  60.82172393798828 \n",
      "\n",
      "Iteration 5239, Loss: 37.534488677978516, L1: 10.305571556091309, L3: 27.228918075561523\n",
      "Current prediction:  60.826759338378906 \n",
      "\n",
      "Iteration 5240, Loss: 37.00748825073242, L1: 10.3027925491333, L3: 26.704696655273438\n",
      "Current prediction:  60.837440490722656 \n",
      "\n",
      "Iteration 5241, Loss: 36.6416130065918, L1: 10.267191886901855, L3: 26.374422073364258\n",
      "Current prediction:  60.930458068847656 \n",
      "\n",
      "Iteration 5242, Loss: 35.554046630859375, L1: 10.250539779663086, L3: 25.30350685119629\n",
      "Current prediction:  61.425899505615234 \n",
      "\n",
      "Iteration 5243, Loss: 36.14497375488281, L1: 10.216634750366211, L3: 25.9283390045166\n",
      "Current prediction:  61.57895278930664 \n",
      "\n",
      "Iteration 5244, Loss: 35.69789505004883, L1: 10.214369773864746, L3: 25.4835262298584\n",
      "Current prediction:  61.59770965576172 \n",
      "\n",
      "Iteration 5245, Loss: 36.0579719543457, L1: 10.19006061553955, L3: 25.867910385131836\n",
      "Current prediction:  61.60371780395508 \n",
      "\n",
      "Iteration 5246, Loss: 36.472042083740234, L1: 10.196054458618164, L3: 26.27598762512207\n",
      "Current prediction:  61.60710525512695 \n",
      "\n",
      "Iteration 5247, Loss: 36.9553337097168, L1: 10.170671463012695, L3: 26.7846622467041\n",
      "Current prediction:  61.60757827758789 \n",
      "\n",
      "Iteration 5248, Loss: 36.95365524291992, L1: 10.187505722045898, L3: 26.766149520874023\n",
      "Current prediction:  61.605987548828125 \n",
      "\n",
      "Iteration 5249, Loss: 37.017967224121094, L1: 10.18392562866211, L3: 26.834043502807617\n",
      "Current prediction:  61.602081298828125 \n",
      "\n",
      "Iteration 5250, Loss: 37.019630432128906, L1: 10.1745023727417, L3: 26.845129013061523\n",
      "Current prediction:  61.59337615966797 \n",
      "\n",
      "Iteration 5251, Loss: 35.98699188232422, L1: 10.201227188110352, L3: 25.785764694213867\n",
      "Current prediction:  61.558929443359375 \n",
      "\n",
      "Iteration 5252, Loss: 36.45000457763672, L1: 10.186349868774414, L3: 26.263652801513672\n",
      "Current prediction:  61.376522064208984 \n",
      "\n",
      "Iteration 5253, Loss: 36.041725158691406, L1: 10.219181060791016, L3: 25.82254409790039\n",
      "Current prediction:  61.05592727661133 \n",
      "\n",
      "Iteration 5254, Loss: 36.929847717285156, L1: 10.223424911499023, L3: 26.706424713134766\n",
      "Current prediction:  60.907920837402344 \n",
      "\n",
      "Iteration 5255, Loss: 36.849090576171875, L1: 10.230302810668945, L3: 26.618789672851562\n",
      "Current prediction:  60.85634994506836 \n",
      "\n",
      "Iteration 5256, Loss: 36.62541961669922, L1: 10.2648344039917, L3: 26.360584259033203\n",
      "Current prediction:  60.89725875854492 \n",
      "\n",
      "Iteration 5257, Loss: 36.46375274658203, L1: 10.238119125366211, L3: 26.225631713867188\n",
      "Current prediction:  61.09846496582031 \n",
      "\n",
      "Iteration 5258, Loss: 37.045005798339844, L1: 10.255594253540039, L3: 26.789409637451172\n",
      "Current prediction:  61.41984558105469 \n",
      "\n",
      "Iteration 5259, Loss: 36.631900787353516, L1: 10.229269027709961, L3: 26.402631759643555\n",
      "Current prediction:  61.51323318481445 \n",
      "\n",
      "Iteration 5260, Loss: 36.985042572021484, L1: 10.239677429199219, L3: 26.745365142822266\n",
      "Current prediction:  61.528076171875 \n",
      "\n",
      "Iteration 5261, Loss: 36.579612731933594, L1: 10.24679183959961, L3: 26.332822799682617\n",
      "Current prediction:  61.528297424316406 \n",
      "\n",
      "Iteration 5262, Loss: 34.90107345581055, L1: 10.239667892456055, L3: 24.661405563354492\n",
      "Current prediction:  61.52545928955078 \n",
      "\n",
      "Iteration 5263, Loss: 36.380821228027344, L1: 10.23806095123291, L3: 26.14276123046875\n",
      "Current prediction:  61.522090911865234 \n",
      "\n",
      "Iteration 5264, Loss: 36.441410064697266, L1: 10.252931594848633, L3: 26.188478469848633\n",
      "Current prediction:  61.5178108215332 \n",
      "\n",
      "Iteration 5265, Loss: 37.39850616455078, L1: 10.27592945098877, L3: 27.122577667236328\n",
      "Current prediction:  61.51136016845703 \n",
      "\n",
      "Iteration 5266, Loss: 36.96012496948242, L1: 10.279487609863281, L3: 26.68063735961914\n",
      "Current prediction:  61.50503921508789 \n",
      "\n",
      "Iteration 5267, Loss: 36.2275505065918, L1: 10.273391723632812, L3: 25.954158782958984\n",
      "Current prediction:  61.499053955078125 \n",
      "\n",
      "Iteration 5268, Loss: 35.70547866821289, L1: 10.275574684143066, L3: 25.42990493774414\n",
      "Current prediction:  61.49402618408203 \n",
      "\n",
      "Iteration 5269, Loss: 36.783905029296875, L1: 10.25324535369873, L3: 26.530658721923828\n",
      "Current prediction:  61.48921585083008 \n",
      "\n",
      "Iteration 5270, Loss: 37.00648498535156, L1: 10.299732208251953, L3: 26.70675277709961\n",
      "Current prediction:  61.48408508300781 \n",
      "\n",
      "Iteration 5271, Loss: 36.1270751953125, L1: 10.271553039550781, L3: 25.85552406311035\n",
      "Current prediction:  61.47072982788086 \n",
      "\n",
      "Iteration 5272, Loss: 37.043006896972656, L1: 10.279095649719238, L3: 26.763912200927734\n",
      "Current prediction:  61.42044448852539 \n",
      "\n",
      "Iteration 5273, Loss: 36.25958251953125, L1: 10.252629280090332, L3: 26.0069522857666\n",
      "Current prediction:  61.20687484741211 \n",
      "\n",
      "Iteration 5274, Loss: 36.02239227294922, L1: 10.284605026245117, L3: 25.73778533935547\n",
      "Current prediction:  60.919124603271484 \n",
      "\n",
      "Iteration 5275, Loss: 36.65625, L1: 10.312259674072266, L3: 26.343990325927734\n",
      "Current prediction:  60.80702209472656 \n",
      "\n",
      "Iteration 5276, Loss: 36.35981750488281, L1: 10.304788589477539, L3: 26.055030822753906\n",
      "Current prediction:  60.806373596191406 \n",
      "\n",
      "Iteration 5277, Loss: 36.72655487060547, L1: 10.320539474487305, L3: 26.406015396118164\n",
      "Current prediction:  60.890560150146484 \n",
      "\n",
      "Iteration 5278, Loss: 36.1861686706543, L1: 10.279569625854492, L3: 25.906599044799805\n",
      "Current prediction:  61.02605056762695 \n",
      "\n",
      "Iteration 5279, Loss: 36.739013671875, L1: 10.268990516662598, L3: 26.470022201538086\n",
      "Current prediction:  61.2170524597168 \n",
      "\n",
      "Iteration 5280, Loss: 36.53960037231445, L1: 10.24437141418457, L3: 26.295228958129883\n",
      "Current prediction:  61.32303237915039 \n",
      "\n",
      "Iteration 5281, Loss: 35.8773078918457, L1: 10.268448829650879, L3: 25.608858108520508\n",
      "Current prediction:  61.40612030029297 \n",
      "\n",
      "Iteration 5282, Loss: 36.50091552734375, L1: 10.217984199523926, L3: 26.282930374145508\n",
      "Current prediction:  61.465545654296875 \n",
      "\n",
      "Iteration 5283, Loss: 35.27600860595703, L1: 10.210569381713867, L3: 25.06543731689453\n",
      "Current prediction:  61.49940490722656 \n",
      "\n",
      "Iteration 5284, Loss: 37.22254943847656, L1: 10.230712890625, L3: 26.991836547851562\n",
      "Current prediction:  61.4937858581543 \n",
      "\n",
      "Iteration 5285, Loss: 35.973392486572266, L1: 10.201000213623047, L3: 25.77239227294922\n",
      "Current prediction:  61.47311019897461 \n",
      "\n",
      "Iteration 5286, Loss: 36.158226013183594, L1: 10.253067016601562, L3: 25.9051570892334\n",
      "Current prediction:  61.4510498046875 \n",
      "\n",
      "Iteration 5287, Loss: 36.797218322753906, L1: 10.200510025024414, L3: 26.596710205078125\n",
      "Current prediction:  61.40988540649414 \n",
      "\n",
      "Iteration 5288, Loss: 36.78898620605469, L1: 10.206527709960938, L3: 26.58245849609375\n",
      "Current prediction:  61.29322052001953 \n",
      "\n",
      "Iteration 5289, Loss: 36.20272445678711, L1: 10.21944522857666, L3: 25.983278274536133\n",
      "Current prediction:  61.19624710083008 \n",
      "\n",
      "Iteration 5290, Loss: 36.85099411010742, L1: 10.23591136932373, L3: 26.615081787109375\n",
      "Current prediction:  61.184940338134766 \n",
      "\n",
      "Iteration 5291, Loss: 36.26516342163086, L1: 10.214068412780762, L3: 26.05109405517578\n",
      "Current prediction:  61.049686431884766 \n",
      "\n",
      "Iteration 5292, Loss: 36.79595947265625, L1: 10.233098983764648, L3: 26.562862396240234\n",
      "Current prediction:  60.929683685302734 \n",
      "\n",
      "Iteration 5293, Loss: 36.66516876220703, L1: 10.241479873657227, L3: 26.423686981201172\n",
      "Current prediction:  60.91506576538086 \n",
      "\n",
      "Iteration 5294, Loss: 36.66133117675781, L1: 10.243460655212402, L3: 26.417871475219727\n",
      "Current prediction:  60.96259689331055 \n",
      "\n",
      "Iteration 5295, Loss: 36.448997497558594, L1: 10.222046852111816, L3: 26.22694969177246\n",
      "Current prediction:  61.127105712890625 \n",
      "\n",
      "Iteration 5296, Loss: 36.60043716430664, L1: 10.232996940612793, L3: 26.367441177368164\n",
      "Current prediction:  61.25309753417969 \n",
      "\n",
      "Iteration 5297, Loss: 36.944732666015625, L1: 10.235699653625488, L3: 26.70903205871582\n",
      "Current prediction:  61.364864349365234 \n",
      "\n",
      "Iteration 5298, Loss: 36.17875671386719, L1: 10.18966293334961, L3: 25.989093780517578\n",
      "Current prediction:  61.47960662841797 \n",
      "\n",
      "Iteration 5299, Loss: 37.03949737548828, L1: 10.209823608398438, L3: 26.829675674438477\n",
      "Current prediction:  61.50083923339844 \n",
      "\n",
      "Iteration 5300, Loss: 37.02968978881836, L1: 10.23145580291748, L3: 26.798234939575195\n",
      "Current prediction:  61.46734619140625 \n",
      "\n",
      "Iteration 5301, Loss: 36.976619720458984, L1: 10.230574607849121, L3: 26.74604606628418\n",
      "Current prediction:  61.43490219116211 \n",
      "\n",
      "Iteration 5302, Loss: 36.046451568603516, L1: 10.232041358947754, L3: 25.814411163330078\n",
      "Current prediction:  61.42222595214844 \n",
      "\n",
      "Iteration 5303, Loss: 36.08705139160156, L1: 10.224956512451172, L3: 25.862096786499023\n",
      "Current prediction:  61.37214660644531 \n",
      "\n",
      "Iteration 5304, Loss: 36.59515380859375, L1: 10.236379623413086, L3: 26.358774185180664\n",
      "Current prediction:  61.22846221923828 \n",
      "\n",
      "Iteration 5305, Loss: 36.44920349121094, L1: 10.243021965026855, L3: 26.206180572509766\n",
      "Current prediction:  61.24724197387695 \n",
      "\n",
      "Iteration 5306, Loss: 37.41930389404297, L1: 10.252046585083008, L3: 27.167255401611328\n",
      "Current prediction:  61.216739654541016 \n",
      "\n",
      "Iteration 5307, Loss: 37.16581344604492, L1: 10.232460021972656, L3: 26.933353424072266\n",
      "Current prediction:  61.172279357910156 \n",
      "\n",
      "Iteration 5308, Loss: 36.782772064208984, L1: 10.238046646118164, L3: 26.54472541809082\n",
      "Current prediction:  61.13727569580078 \n",
      "\n",
      "Iteration 5309, Loss: 37.10617446899414, L1: 10.248893737792969, L3: 26.857280731201172\n",
      "Current prediction:  61.16476058959961 \n",
      "\n",
      "Iteration 5310, Loss: 36.76078414916992, L1: 10.234627723693848, L3: 26.526155471801758\n",
      "Current prediction:  61.08091735839844 \n",
      "\n",
      "Iteration 5311, Loss: 35.47785186767578, L1: 10.244349479675293, L3: 25.233501434326172\n",
      "Current prediction:  60.96788787841797 \n",
      "\n",
      "Iteration 5312, Loss: 36.80199432373047, L1: 10.277297973632812, L3: 26.524696350097656\n",
      "Current prediction:  60.92639923095703 \n",
      "\n",
      "Iteration 5313, Loss: 36.53298568725586, L1: 10.266380310058594, L3: 26.266605377197266\n",
      "Current prediction:  61.03800582885742 \n",
      "\n",
      "Iteration 5314, Loss: 37.087303161621094, L1: 10.237465858459473, L3: 26.849838256835938\n",
      "Current prediction:  61.278099060058594 \n",
      "\n",
      "Iteration 5315, Loss: 35.667964935302734, L1: 10.26515007019043, L3: 25.402814865112305\n",
      "Current prediction:  61.463539123535156 \n",
      "\n",
      "Iteration 5316, Loss: 36.436344146728516, L1: 10.216474533081055, L3: 26.21986961364746\n",
      "Current prediction:  61.51951599121094 \n",
      "\n",
      "Iteration 5317, Loss: 36.28326416015625, L1: 10.244743347167969, L3: 26.03852081298828\n",
      "Current prediction:  61.53270721435547 \n",
      "\n",
      "Iteration 5318, Loss: 36.57718276977539, L1: 10.24624252319336, L3: 26.33094024658203\n",
      "Current prediction:  61.53801727294922 \n",
      "\n",
      "Iteration 5319, Loss: 36.602684020996094, L1: 10.230304718017578, L3: 26.37238121032715\n",
      "Current prediction:  61.5324821472168 \n",
      "\n",
      "Iteration 5320, Loss: 35.97182083129883, L1: 10.231687545776367, L3: 25.74013328552246\n",
      "Current prediction:  61.51563262939453 \n",
      "\n",
      "Iteration 5321, Loss: 36.140235900878906, L1: 10.235172271728516, L3: 25.905061721801758\n",
      "Current prediction:  61.462093353271484 \n",
      "\n",
      "Iteration 5322, Loss: 36.64495849609375, L1: 10.242837905883789, L3: 26.402118682861328\n",
      "Current prediction:  61.326107025146484 \n",
      "\n",
      "Iteration 5323, Loss: 36.02550506591797, L1: 10.241870880126953, L3: 25.783632278442383\n",
      "Current prediction:  61.13546371459961 \n",
      "\n",
      "Iteration 5324, Loss: 36.47438049316406, L1: 10.241690635681152, L3: 26.232690811157227\n",
      "Current prediction:  61.07598876953125 \n",
      "\n",
      "Iteration 5325, Loss: 35.83280944824219, L1: 10.25667667388916, L3: 25.576133728027344\n",
      "Current prediction:  61.06187438964844 \n",
      "\n",
      "Iteration 5326, Loss: 35.828819274902344, L1: 10.254653930664062, L3: 25.574167251586914\n",
      "Current prediction:  61.1235466003418 \n",
      "\n",
      "Iteration 5327, Loss: 36.34486389160156, L1: 10.247068405151367, L3: 26.097797393798828\n",
      "Current prediction:  61.269283294677734 \n",
      "\n",
      "Iteration 5328, Loss: 36.158485412597656, L1: 10.266904830932617, L3: 25.891578674316406\n",
      "Current prediction:  61.31192398071289 \n",
      "\n",
      "Iteration 5329, Loss: 36.80874252319336, L1: 10.262097358703613, L3: 26.54664421081543\n",
      "Current prediction:  61.40488815307617 \n",
      "\n",
      "Iteration 5330, Loss: 35.275970458984375, L1: 10.253069877624512, L3: 25.022899627685547\n",
      "Current prediction:  61.435707092285156 \n",
      "\n",
      "Iteration 5331, Loss: 35.86841583251953, L1: 10.230474472045898, L3: 25.637939453125\n",
      "Current prediction:  61.45983123779297 \n",
      "\n",
      "Iteration 5332, Loss: 37.01120376586914, L1: 10.24442195892334, L3: 26.766780853271484\n",
      "Current prediction:  61.48896789550781 \n",
      "\n",
      "Iteration 5333, Loss: 37.247291564941406, L1: 10.253294944763184, L3: 26.99399757385254\n",
      "Current prediction:  61.48597717285156 \n",
      "\n",
      "Iteration 5334, Loss: 37.24034881591797, L1: 10.241909980773926, L3: 26.998437881469727\n",
      "Current prediction:  61.49884796142578 \n",
      "\n",
      "Iteration 5335, Loss: 36.66899490356445, L1: 10.261313438415527, L3: 26.40768051147461\n",
      "Current prediction:  61.5079231262207 \n",
      "\n",
      "Iteration 5336, Loss: 35.6976432800293, L1: 10.238473892211914, L3: 25.459169387817383\n",
      "Current prediction:  61.505035400390625 \n",
      "\n",
      "Iteration 5337, Loss: 35.59043884277344, L1: 10.226195335388184, L3: 25.36424446105957\n",
      "Current prediction:  61.494422912597656 \n",
      "\n",
      "Iteration 5338, Loss: 35.68346405029297, L1: 10.232712745666504, L3: 25.45075035095215\n",
      "Current prediction:  61.486000061035156 \n",
      "\n",
      "Iteration 5339, Loss: 36.09267044067383, L1: 10.247057914733887, L3: 25.845611572265625\n",
      "Current prediction:  61.481563568115234 \n",
      "\n",
      "Iteration 5340, Loss: 35.81074905395508, L1: 10.226815223693848, L3: 25.583932876586914\n",
      "Current prediction:  61.472965240478516 \n",
      "\n",
      "Iteration 5341, Loss: 36.37139892578125, L1: 10.215818405151367, L3: 26.155582427978516\n",
      "Current prediction:  61.413997650146484 \n",
      "\n",
      "Iteration 5342, Loss: 36.523475646972656, L1: 10.214591979980469, L3: 26.308881759643555\n",
      "Current prediction:  61.4021110534668 \n",
      "\n",
      "Iteration 5343, Loss: 35.6512336730957, L1: 10.24002456665039, L3: 25.411209106445312\n",
      "Current prediction:  61.47153854370117 \n",
      "\n",
      "Iteration 5344, Loss: 36.35820007324219, L1: 10.238308906555176, L3: 26.119890213012695\n",
      "Current prediction:  61.514041900634766 \n",
      "\n",
      "Iteration 5345, Loss: 35.45873260498047, L1: 10.223827362060547, L3: 25.234907150268555\n",
      "Current prediction:  61.540706634521484 \n",
      "\n",
      "Iteration 5346, Loss: 36.405818939208984, L1: 10.229378700256348, L3: 26.176441192626953\n",
      "Current prediction:  61.54792022705078 \n",
      "\n",
      "Iteration 5347, Loss: 35.515228271484375, L1: 10.217313766479492, L3: 25.29791259765625\n",
      "Current prediction:  61.549591064453125 \n",
      "\n",
      "Iteration 5348, Loss: 36.496543884277344, L1: 10.214250564575195, L3: 26.282291412353516\n",
      "Current prediction:  61.545692443847656 \n",
      "\n",
      "Iteration 5349, Loss: 36.88081741333008, L1: 10.193278312683105, L3: 26.687538146972656\n",
      "Current prediction:  61.523502349853516 \n",
      "\n",
      "Iteration 5350, Loss: 37.0714111328125, L1: 10.190435409545898, L3: 26.8809757232666\n",
      "Current prediction:  61.340003967285156 \n",
      "\n",
      "Iteration 5351, Loss: 36.32912063598633, L1: 10.202397346496582, L3: 26.126724243164062\n",
      "Current prediction:  61.125389099121094 \n",
      "\n",
      "Iteration 5352, Loss: 35.00199890136719, L1: 10.224967002868652, L3: 24.77703285217285\n",
      "Current prediction:  60.9713134765625 \n",
      "\n",
      "Iteration 5353, Loss: 36.145328521728516, L1: 10.198938369750977, L3: 25.94639015197754\n",
      "Current prediction:  61.034881591796875 \n",
      "\n",
      "Iteration 5354, Loss: 36.97926330566406, L1: 10.217049598693848, L3: 26.7622127532959\n",
      "Current prediction:  61.14362716674805 \n",
      "\n",
      "Iteration 5355, Loss: 36.95116424560547, L1: 10.218316078186035, L3: 26.732847213745117\n",
      "Current prediction:  61.241207122802734 \n",
      "\n",
      "Iteration 5356, Loss: 38.01654052734375, L1: 10.210015296936035, L3: 27.8065242767334\n",
      "Current prediction:  61.42273712158203 \n",
      "\n",
      "Iteration 5357, Loss: 35.59031677246094, L1: 10.204532623291016, L3: 25.385784149169922\n",
      "Current prediction:  61.50455856323242 \n",
      "\n",
      "Iteration 5358, Loss: 37.10466003417969, L1: 10.204801559448242, L3: 26.899858474731445\n",
      "Current prediction:  61.51057052612305 \n",
      "\n",
      "Iteration 5359, Loss: 36.698055267333984, L1: 10.196192741394043, L3: 26.501861572265625\n",
      "Current prediction:  61.51947784423828 \n",
      "\n",
      "Iteration 5360, Loss: 36.07793045043945, L1: 10.194716453552246, L3: 25.88321304321289\n",
      "Current prediction:  61.526878356933594 \n",
      "\n",
      "Iteration 5361, Loss: 36.4977912902832, L1: 10.211152076721191, L3: 26.286640167236328\n",
      "Current prediction:  61.50886154174805 \n",
      "\n",
      "Iteration 5362, Loss: 35.79423141479492, L1: 10.212719917297363, L3: 25.581510543823242\n",
      "Current prediction:  61.459869384765625 \n",
      "\n",
      "Iteration 5363, Loss: 36.39434051513672, L1: 10.202916145324707, L3: 26.191425323486328\n",
      "Current prediction:  61.23301315307617 \n",
      "\n",
      "Iteration 5364, Loss: 36.06073760986328, L1: 10.207772254943848, L3: 25.85296630859375\n",
      "Current prediction:  60.95583724975586 \n",
      "\n",
      "Iteration 5365, Loss: 36.6002082824707, L1: 10.225802421569824, L3: 26.374406814575195\n",
      "Current prediction:  60.840030670166016 \n",
      "\n",
      "Iteration 5366, Loss: 36.51122283935547, L1: 10.23293399810791, L3: 26.278287887573242\n",
      "Current prediction:  60.81568908691406 \n",
      "\n",
      "Iteration 5367, Loss: 36.182498931884766, L1: 10.253539085388184, L3: 25.9289608001709\n",
      "Current prediction:  60.80543899536133 \n",
      "\n",
      "Iteration 5368, Loss: 35.948036193847656, L1: 10.272917747497559, L3: 25.675119400024414\n",
      "Current prediction:  60.821205139160156 \n",
      "\n",
      "Iteration 5369, Loss: 37.53048324584961, L1: 10.261534690856934, L3: 27.268949508666992\n",
      "Current prediction:  60.940731048583984 \n",
      "\n",
      "Iteration 5370, Loss: 36.883155822753906, L1: 10.250694274902344, L3: 26.632461547851562\n",
      "Current prediction:  61.391197204589844 \n",
      "\n",
      "Iteration 5371, Loss: 36.03594970703125, L1: 10.243090629577637, L3: 25.792858123779297\n",
      "Current prediction:  61.5117301940918 \n",
      "\n",
      "Iteration 5372, Loss: 36.48273468017578, L1: 10.232198715209961, L3: 26.250537872314453\n",
      "Current prediction:  61.518123626708984 \n",
      "\n",
      "Iteration 5373, Loss: 37.08224868774414, L1: 10.24975299835205, L3: 26.832496643066406\n",
      "Current prediction:  61.51456069946289 \n",
      "\n",
      "Iteration 5374, Loss: 36.42996597290039, L1: 10.254103660583496, L3: 26.17586326599121\n",
      "Current prediction:  61.51018142700195 \n",
      "\n",
      "Iteration 5375, Loss: 36.24452590942383, L1: 10.254321098327637, L3: 25.990205764770508\n",
      "Current prediction:  61.506690979003906 \n",
      "\n",
      "Iteration 5376, Loss: 36.15269088745117, L1: 10.287362098693848, L3: 25.86532974243164\n",
      "Current prediction:  61.501773834228516 \n",
      "\n",
      "Iteration 5377, Loss: 36.150360107421875, L1: 10.246563911437988, L3: 25.903797149658203\n",
      "Current prediction:  61.496299743652344 \n",
      "\n",
      "Iteration 5378, Loss: 37.62130355834961, L1: 10.285831451416016, L3: 27.335472106933594\n",
      "Current prediction:  61.489234924316406 \n",
      "\n",
      "Iteration 5379, Loss: 35.957244873046875, L1: 10.298563003540039, L3: 25.658681869506836\n",
      "Current prediction:  61.481597900390625 \n",
      "\n",
      "Iteration 5380, Loss: 37.169586181640625, L1: 10.31609058380127, L3: 26.853496551513672\n",
      "Current prediction:  61.473697662353516 \n",
      "\n",
      "Iteration 5381, Loss: 36.36712646484375, L1: 10.287336349487305, L3: 26.079790115356445\n",
      "Current prediction:  61.46670913696289 \n",
      "\n",
      "Iteration 5382, Loss: 36.10070037841797, L1: 10.338461875915527, L3: 25.762237548828125\n",
      "Current prediction:  61.4606819152832 \n",
      "\n",
      "Iteration 5383, Loss: 35.12149429321289, L1: 10.286528587341309, L3: 24.8349666595459\n",
      "Current prediction:  61.456207275390625 \n",
      "\n",
      "Iteration 5384, Loss: 35.74250793457031, L1: 10.290231704711914, L3: 25.45227813720703\n",
      "Current prediction:  61.44086837768555 \n",
      "\n",
      "Iteration 5385, Loss: 36.38994216918945, L1: 10.296274185180664, L3: 26.09366798400879\n",
      "Current prediction:  61.352989196777344 \n",
      "\n",
      "Iteration 5386, Loss: 37.04338836669922, L1: 10.303001403808594, L3: 26.740386962890625\n",
      "Current prediction:  61.11989212036133 \n",
      "\n",
      "Iteration 5387, Loss: 36.6242790222168, L1: 10.307055473327637, L3: 26.317224502563477\n",
      "Current prediction:  60.87236404418945 \n",
      "\n",
      "Iteration 5388, Loss: 36.84210968017578, L1: 10.322678565979004, L3: 26.519432067871094\n",
      "Current prediction:  60.82522201538086 \n",
      "\n",
      "Iteration 5389, Loss: 35.928531646728516, L1: 10.29712200164795, L3: 25.63140869140625\n",
      "Current prediction:  60.863460540771484 \n",
      "\n",
      "Iteration 5390, Loss: 36.12891387939453, L1: 10.279094696044922, L3: 25.849821090698242\n",
      "Current prediction:  61.014686584472656 \n",
      "\n",
      "Iteration 5391, Loss: 35.1837272644043, L1: 10.29530143737793, L3: 24.888425827026367\n",
      "Current prediction:  61.30332946777344 \n",
      "\n",
      "Iteration 5392, Loss: 37.14866638183594, L1: 10.270341873168945, L3: 26.87832260131836\n",
      "Current prediction:  61.455047607421875 \n",
      "\n",
      "Iteration 5393, Loss: 37.0313720703125, L1: 10.244877815246582, L3: 26.7864933013916\n",
      "Current prediction:  61.47412109375 \n",
      "\n",
      "Iteration 5394, Loss: 36.32912063598633, L1: 10.234618186950684, L3: 26.09450340270996\n",
      "Current prediction:  61.46409225463867 \n",
      "\n",
      "Iteration 5395, Loss: 36.76072311401367, L1: 10.238781929016113, L3: 26.521940231323242\n",
      "Current prediction:  61.3881721496582 \n",
      "\n",
      "Iteration 5396, Loss: 36.99342727661133, L1: 10.235746383666992, L3: 26.757680892944336\n",
      "Current prediction:  61.29990768432617 \n",
      "\n",
      "Iteration 5397, Loss: 36.120872497558594, L1: 10.23002815246582, L3: 25.89084243774414\n",
      "Current prediction:  61.238006591796875 \n",
      "\n",
      "Iteration 5398, Loss: 36.31597900390625, L1: 10.227081298828125, L3: 26.088899612426758\n",
      "Current prediction:  61.1639404296875 \n",
      "\n",
      "Iteration 5399, Loss: 37.14116668701172, L1: 10.244526863098145, L3: 26.89664077758789\n",
      "Current prediction:  61.10811996459961 \n",
      "\n",
      "Iteration 5400, Loss: 36.620643615722656, L1: 10.24519157409668, L3: 26.37545394897461\n",
      "Current prediction:  61.1743049621582 \n",
      "\n",
      "Iteration 5401, Loss: 36.71900177001953, L1: 10.19265079498291, L3: 26.526350021362305\n",
      "Current prediction:  61.122528076171875 \n",
      "\n",
      "Iteration 5402, Loss: 37.50465393066406, L1: 10.215396881103516, L3: 27.28925895690918\n",
      "Current prediction:  61.08136749267578 \n",
      "\n",
      "Iteration 5403, Loss: 36.18222427368164, L1: 10.185462951660156, L3: 25.996761322021484\n",
      "Current prediction:  61.00514602661133 \n",
      "\n",
      "Iteration 5404, Loss: 36.75271987915039, L1: 10.242314338684082, L3: 26.510406494140625\n",
      "Current prediction:  60.87740707397461 \n",
      "\n",
      "Iteration 5405, Loss: 36.658775329589844, L1: 10.230875015258789, L3: 26.427900314331055\n",
      "Current prediction:  60.83796310424805 \n",
      "\n",
      "Iteration 5406, Loss: 36.86070251464844, L1: 10.263290405273438, L3: 26.597412109375\n",
      "Current prediction:  60.833778381347656 \n",
      "\n",
      "Iteration 5407, Loss: 35.93505859375, L1: 10.24302864074707, L3: 25.69202995300293\n",
      "Current prediction:  60.86948013305664 \n",
      "\n",
      "Iteration 5408, Loss: 35.92076110839844, L1: 10.206134796142578, L3: 25.71462631225586\n",
      "Current prediction:  60.953792572021484 \n",
      "\n",
      "Iteration 5409, Loss: 36.28376007080078, L1: 10.229223251342773, L3: 26.054534912109375\n",
      "Current prediction:  61.29133987426758 \n",
      "\n",
      "Iteration 5410, Loss: 35.54889678955078, L1: 10.201075553894043, L3: 25.347822189331055\n",
      "Current prediction:  61.49442672729492 \n",
      "\n",
      "Iteration 5411, Loss: 35.70377731323242, L1: 10.198319435119629, L3: 25.50545883178711\n",
      "Current prediction:  61.55292892456055 \n",
      "\n",
      "Iteration 5412, Loss: 37.16877746582031, L1: 10.166446685791016, L3: 27.002328872680664\n",
      "Current prediction:  61.55809783935547 \n",
      "\n",
      "Iteration 5413, Loss: 36.098106384277344, L1: 10.206644058227539, L3: 25.891462326049805\n",
      "Current prediction:  61.56614685058594 \n",
      "\n",
      "Iteration 5414, Loss: 36.81452941894531, L1: 10.192415237426758, L3: 26.622116088867188\n",
      "Current prediction:  61.572479248046875 \n",
      "\n",
      "Iteration 5415, Loss: 37.11762619018555, L1: 10.185245513916016, L3: 26.93238067626953\n",
      "Current prediction:  61.57875442504883 \n",
      "\n",
      "Iteration 5416, Loss: 36.13452911376953, L1: 10.195660591125488, L3: 25.93886947631836\n",
      "Current prediction:  61.585044860839844 \n",
      "\n",
      "Iteration 5417, Loss: 36.107704162597656, L1: 10.193434715270996, L3: 25.914268493652344\n",
      "Current prediction:  61.586849212646484 \n",
      "\n",
      "Iteration 5418, Loss: 36.7657356262207, L1: 10.16695499420166, L3: 26.59878158569336\n",
      "Current prediction:  61.586341857910156 \n",
      "\n",
      "Iteration 5419, Loss: 36.54597091674805, L1: 10.180092811584473, L3: 26.36587905883789\n",
      "Current prediction:  61.58490753173828 \n",
      "\n",
      "Iteration 5420, Loss: 35.90840148925781, L1: 10.196660995483398, L3: 25.711742401123047\n",
      "Current prediction:  61.5828742980957 \n",
      "\n",
      "Iteration 5421, Loss: 37.35643005371094, L1: 10.201684951782227, L3: 27.154743194580078\n",
      "Current prediction:  61.58052062988281 \n",
      "\n",
      "Iteration 5422, Loss: 35.514923095703125, L1: 10.204019546508789, L3: 25.310903549194336\n",
      "Current prediction:  61.57704544067383 \n",
      "\n",
      "Iteration 5423, Loss: 37.19049835205078, L1: 10.188581466674805, L3: 27.00191879272461\n",
      "Current prediction:  61.57341003417969 \n",
      "\n",
      "Iteration 5424, Loss: 35.861610412597656, L1: 10.234375953674316, L3: 25.627233505249023\n",
      "Current prediction:  61.56941604614258 \n",
      "\n",
      "Iteration 5425, Loss: 36.748741149902344, L1: 10.188092231750488, L3: 26.560649871826172\n",
      "Current prediction:  61.56279754638672 \n",
      "\n",
      "Iteration 5426, Loss: 36.286006927490234, L1: 10.206467628479004, L3: 26.079538345336914\n",
      "Current prediction:  61.55452346801758 \n",
      "\n",
      "Iteration 5427, Loss: 38.184654235839844, L1: 10.225973129272461, L3: 27.958681106567383\n",
      "Current prediction:  61.54112243652344 \n",
      "\n",
      "Iteration 5428, Loss: 36.98210906982422, L1: 10.203178405761719, L3: 26.7789306640625\n",
      "Current prediction:  61.45806121826172 \n",
      "\n",
      "Iteration 5429, Loss: 37.074302673339844, L1: 10.235315322875977, L3: 26.838987350463867\n",
      "Current prediction:  61.11849594116211 \n",
      "\n",
      "Iteration 5430, Loss: 36.36536407470703, L1: 10.229670524597168, L3: 26.13569450378418\n",
      "Current prediction:  60.822601318359375 \n",
      "\n",
      "Iteration 5431, Loss: 36.711212158203125, L1: 10.27527904510498, L3: 26.435932159423828\n",
      "Current prediction:  60.79001235961914 \n",
      "\n",
      "Iteration 5432, Loss: 35.637630462646484, L1: 10.28751277923584, L3: 25.35011863708496\n",
      "Current prediction:  60.78392028808594 \n",
      "\n",
      "Iteration 5433, Loss: 37.61785125732422, L1: 10.28639030456543, L3: 27.331459045410156\n",
      "Current prediction:  60.78642272949219 \n",
      "\n",
      "Iteration 5434, Loss: 37.24884796142578, L1: 10.283323287963867, L3: 26.965524673461914\n",
      "Current prediction:  60.7957763671875 \n",
      "\n",
      "Iteration 5435, Loss: 36.1662483215332, L1: 10.279562950134277, L3: 25.88668441772461\n",
      "Current prediction:  60.828731536865234 \n",
      "\n",
      "Iteration 5436, Loss: 36.37322235107422, L1: 10.267910957336426, L3: 26.10531234741211\n",
      "Current prediction:  61.017662048339844 \n",
      "\n",
      "Iteration 5437, Loss: 36.41064453125, L1: 10.22719955444336, L3: 26.183443069458008\n",
      "Current prediction:  61.436370849609375 \n",
      "\n",
      "Iteration 5438, Loss: 36.137901306152344, L1: 10.232311248779297, L3: 25.905588150024414\n",
      "Current prediction:  61.523582458496094 \n",
      "\n",
      "Iteration 5439, Loss: 37.07807159423828, L1: 10.222455978393555, L3: 26.855615615844727\n",
      "Current prediction:  61.53702163696289 \n",
      "\n",
      "Iteration 5440, Loss: 35.599361419677734, L1: 10.222894668579102, L3: 25.376466751098633\n",
      "Current prediction:  61.539093017578125 \n",
      "\n",
      "Iteration 5441, Loss: 35.548641204833984, L1: 10.20825481414795, L3: 25.34038543701172\n",
      "Current prediction:  61.54001998901367 \n",
      "\n",
      "Iteration 5442, Loss: 36.992393493652344, L1: 10.215275764465332, L3: 26.777118682861328\n",
      "Current prediction:  61.54015350341797 \n",
      "\n",
      "Iteration 5443, Loss: 35.72197341918945, L1: 10.21121883392334, L3: 25.510753631591797\n",
      "Current prediction:  61.5394401550293 \n",
      "\n",
      "Iteration 5444, Loss: 37.23012161254883, L1: 10.201336860656738, L3: 27.028783798217773\n",
      "Current prediction:  61.537200927734375 \n",
      "\n",
      "Iteration 5445, Loss: 37.335819244384766, L1: 10.220616340637207, L3: 27.115203857421875\n",
      "Current prediction:  61.53284454345703 \n",
      "\n",
      "Iteration 5446, Loss: 36.29911422729492, L1: 10.230484008789062, L3: 26.06863021850586\n",
      "Current prediction:  61.530452728271484 \n",
      "\n",
      "Iteration 5447, Loss: 35.821075439453125, L1: 10.237959861755371, L3: 25.583114624023438\n",
      "Current prediction:  61.52788162231445 \n",
      "\n",
      "Iteration 5448, Loss: 36.44847869873047, L1: 10.244100570678711, L3: 26.204376220703125\n",
      "Current prediction:  61.5265007019043 \n",
      "\n",
      "Iteration 5449, Loss: 36.19430923461914, L1: 10.242030143737793, L3: 25.95227813720703\n",
      "Current prediction:  61.521728515625 \n",
      "\n",
      "Iteration 5450, Loss: 36.71272277832031, L1: 10.201780319213867, L3: 26.510944366455078\n",
      "Current prediction:  61.474029541015625 \n",
      "\n",
      "Iteration 5451, Loss: 35.673439025878906, L1: 10.239706039428711, L3: 25.433731079101562\n",
      "Current prediction:  61.091773986816406 \n",
      "\n",
      "Iteration 5452, Loss: 36.92752456665039, L1: 10.239392280578613, L3: 26.68813133239746\n",
      "Current prediction:  60.81461715698242 \n",
      "\n",
      "Iteration 5453, Loss: 36.31229782104492, L1: 10.253697395324707, L3: 26.0585994720459\n",
      "Current prediction:  60.78567886352539 \n",
      "\n",
      "Iteration 5454, Loss: 35.876102447509766, L1: 10.29655933380127, L3: 25.57954216003418\n",
      "Current prediction:  60.787628173828125 \n",
      "\n",
      "Iteration 5455, Loss: 36.95819854736328, L1: 10.292253494262695, L3: 26.665943145751953\n",
      "Current prediction:  60.79936599731445 \n",
      "\n",
      "Iteration 5456, Loss: 36.200714111328125, L1: 10.25582218170166, L3: 25.94489288330078\n",
      "Current prediction:  60.819095611572266 \n",
      "\n",
      "Iteration 5457, Loss: 36.682613372802734, L1: 10.267008781433105, L3: 26.415605545043945\n",
      "Current prediction:  60.894283294677734 \n",
      "\n",
      "Iteration 5458, Loss: 35.61310577392578, L1: 10.230233192443848, L3: 25.382871627807617\n",
      "Current prediction:  61.23170471191406 \n",
      "\n",
      "Iteration 5459, Loss: 36.09362030029297, L1: 10.186148643493652, L3: 25.907470703125\n",
      "Current prediction:  61.512962341308594 \n",
      "\n",
      "Iteration 5460, Loss: 36.24937057495117, L1: 10.180333137512207, L3: 26.06903839111328\n",
      "Current prediction:  61.57400894165039 \n",
      "\n",
      "Iteration 5461, Loss: 36.5062141418457, L1: 10.19483470916748, L3: 26.311378479003906\n",
      "Current prediction:  61.58263397216797 \n",
      "\n",
      "Iteration 5462, Loss: 35.14326477050781, L1: 10.16900634765625, L3: 24.974258422851562\n",
      "Current prediction:  61.58769226074219 \n",
      "\n",
      "Iteration 5463, Loss: 36.46343231201172, L1: 10.174945831298828, L3: 26.288488388061523\n",
      "Current prediction:  61.59098815917969 \n",
      "\n",
      "Iteration 5464, Loss: 36.13768768310547, L1: 10.180702209472656, L3: 25.956987380981445\n",
      "Current prediction:  61.59312057495117 \n",
      "\n",
      "Iteration 5465, Loss: 36.51005554199219, L1: 10.188360214233398, L3: 26.321693420410156\n",
      "Current prediction:  61.593544006347656 \n",
      "\n",
      "Iteration 5466, Loss: 36.82463836669922, L1: 10.183412551879883, L3: 26.641225814819336\n",
      "Current prediction:  61.59251403808594 \n",
      "\n",
      "Iteration 5467, Loss: 35.6883659362793, L1: 10.186907768249512, L3: 25.50145721435547\n",
      "Current prediction:  61.58943557739258 \n",
      "\n",
      "Iteration 5468, Loss: 36.720428466796875, L1: 10.222593307495117, L3: 26.49783706665039\n",
      "Current prediction:  61.58384323120117 \n",
      "\n",
      "Iteration 5469, Loss: 35.3834228515625, L1: 10.175195693969727, L3: 25.208229064941406\n",
      "Current prediction:  61.574771881103516 \n",
      "\n",
      "Iteration 5470, Loss: 36.21095275878906, L1: 10.196438789367676, L3: 26.014514923095703\n",
      "Current prediction:  61.56707763671875 \n",
      "\n",
      "Iteration 5471, Loss: 37.6181640625, L1: 10.200555801391602, L3: 27.417606353759766\n",
      "Current prediction:  61.55550765991211 \n",
      "\n",
      "Iteration 5472, Loss: 37.155372619628906, L1: 10.23499870300293, L3: 26.92037582397461\n",
      "Current prediction:  61.528907775878906 \n",
      "\n",
      "Iteration 5473, Loss: 36.08759689331055, L1: 10.18747329711914, L3: 25.900123596191406\n",
      "Current prediction:  61.347076416015625 \n",
      "\n",
      "Iteration 5474, Loss: 37.40228271484375, L1: 10.237931251525879, L3: 27.164352416992188\n",
      "Current prediction:  60.85295867919922 \n",
      "\n",
      "Iteration 5475, Loss: 35.87032699584961, L1: 10.240323066711426, L3: 25.6300048828125\n",
      "Current prediction:  60.78664779663086 \n",
      "\n",
      "Iteration 5476, Loss: 36.04219436645508, L1: 10.29387378692627, L3: 25.748321533203125\n",
      "Current prediction:  60.77579116821289 \n",
      "\n",
      "Iteration 5477, Loss: 36.95172119140625, L1: 10.307107925415039, L3: 26.644615173339844\n",
      "Current prediction:  60.7720832824707 \n",
      "\n",
      "Iteration 5478, Loss: 36.56825256347656, L1: 10.328537940979004, L3: 26.239715576171875\n",
      "Current prediction:  60.77285385131836 \n",
      "\n",
      "Iteration 5479, Loss: 35.81929016113281, L1: 10.309520721435547, L3: 25.509769439697266\n",
      "Current prediction:  60.781402587890625 \n",
      "\n",
      "Iteration 5480, Loss: 36.69371032714844, L1: 10.292876243591309, L3: 26.400835037231445\n",
      "Current prediction:  60.817779541015625 \n",
      "\n",
      "Iteration 5481, Loss: 35.846466064453125, L1: 10.286967277526855, L3: 25.559499740600586\n",
      "Current prediction:  61.1953239440918 \n",
      "\n",
      "Iteration 5482, Loss: 36.65095138549805, L1: 10.230500221252441, L3: 26.42045021057129\n",
      "Current prediction:  61.50434875488281 \n",
      "\n",
      "Iteration 5483, Loss: 36.682193756103516, L1: 10.225907325744629, L3: 26.456287384033203\n",
      "Current prediction:  61.530216217041016 \n",
      "\n",
      "Iteration 5484, Loss: 36.92135238647461, L1: 10.227334022521973, L3: 26.694019317626953\n",
      "Current prediction:  61.535919189453125 \n",
      "\n",
      "Iteration 5485, Loss: 36.59539794921875, L1: 10.222993850708008, L3: 26.372406005859375\n",
      "Current prediction:  61.53675842285156 \n",
      "\n",
      "Iteration 5486, Loss: 36.36188888549805, L1: 10.206343650817871, L3: 26.155546188354492\n",
      "Current prediction:  61.53340148925781 \n",
      "\n",
      "Iteration 5487, Loss: 36.10230255126953, L1: 10.216553688049316, L3: 25.8857479095459\n",
      "Current prediction:  61.52583694458008 \n",
      "\n",
      "Iteration 5488, Loss: 36.288333892822266, L1: 10.21081829071045, L3: 26.0775146484375\n",
      "Current prediction:  61.47714614868164 \n",
      "\n",
      "Iteration 5489, Loss: 37.38597106933594, L1: 10.219217300415039, L3: 27.1667537689209\n",
      "Current prediction:  61.355857849121094 \n",
      "\n",
      "Iteration 5490, Loss: 36.763973236083984, L1: 10.22473430633545, L3: 26.53923797607422\n",
      "Current prediction:  61.03065490722656 \n",
      "\n",
      "Iteration 5491, Loss: 35.726985931396484, L1: 10.215991020202637, L3: 25.51099395751953\n",
      "Current prediction:  60.84212875366211 \n",
      "\n",
      "Iteration 5492, Loss: 36.4620361328125, L1: 10.246870040893555, L3: 26.215166091918945\n",
      "Current prediction:  60.79907989501953 \n",
      "\n",
      "Iteration 5493, Loss: 36.929046630859375, L1: 10.259819984436035, L3: 26.669225692749023\n",
      "Current prediction:  60.79792022705078 \n",
      "\n",
      "Iteration 5494, Loss: 36.49912643432617, L1: 10.2776517868042, L3: 26.22147560119629\n",
      "Current prediction:  60.805233001708984 \n",
      "\n",
      "Iteration 5495, Loss: 35.82238006591797, L1: 10.267062187194824, L3: 25.55531883239746\n",
      "Current prediction:  60.853214263916016 \n",
      "\n",
      "Iteration 5496, Loss: 36.26996612548828, L1: 10.25853443145752, L3: 26.011430740356445\n",
      "Current prediction:  61.149776458740234 \n",
      "\n",
      "Iteration 5497, Loss: 36.471595764160156, L1: 10.24286937713623, L3: 26.22872543334961\n",
      "Current prediction:  61.424224853515625 \n",
      "\n",
      "Iteration 5498, Loss: 36.99858856201172, L1: 10.223368644714355, L3: 26.775218963623047\n",
      "Current prediction:  61.4874153137207 \n",
      "\n",
      "Iteration 5499, Loss: 36.637725830078125, L1: 10.215953826904297, L3: 26.421772003173828\n",
      "Current prediction:  61.51298904418945 \n",
      "\n",
      "Iteration 5500, Loss: 36.075653076171875, L1: 10.236970901489258, L3: 25.838682174682617\n",
      "Current prediction:  61.519039154052734 \n",
      "\n",
      "Iteration 5501, Loss: 37.578269958496094, L1: 10.23702335357666, L3: 27.34124755859375\n",
      "Current prediction:  61.517642974853516 \n",
      "\n",
      "Iteration 5502, Loss: 35.892059326171875, L1: 10.228469848632812, L3: 25.663591384887695\n",
      "Current prediction:  61.510868072509766 \n",
      "\n",
      "Iteration 5503, Loss: 36.21657180786133, L1: 10.211451530456543, L3: 26.00511932373047\n",
      "Current prediction:  61.391937255859375 \n",
      "\n",
      "Iteration 5504, Loss: 36.68766784667969, L1: 10.209993362426758, L3: 26.477672576904297\n",
      "Current prediction:  61.0783576965332 \n",
      "\n",
      "Iteration 5505, Loss: 36.469825744628906, L1: 10.211751937866211, L3: 26.258071899414062\n",
      "Current prediction:  60.83354949951172 \n",
      "\n",
      "Iteration 5506, Loss: 36.42890930175781, L1: 10.260306358337402, L3: 26.168603897094727\n",
      "Current prediction:  60.8029670715332 \n",
      "\n",
      "Iteration 5507, Loss: 36.29258346557617, L1: 10.255098342895508, L3: 26.037485122680664\n",
      "Current prediction:  60.79600143432617 \n",
      "\n",
      "Iteration 5508, Loss: 36.834068298339844, L1: 10.29835319519043, L3: 26.53571319580078\n",
      "Current prediction:  60.7952995300293 \n",
      "\n",
      "Iteration 5509, Loss: 36.565147399902344, L1: 10.247397422790527, L3: 26.317750930786133\n",
      "Current prediction:  60.82622146606445 \n",
      "\n",
      "Iteration 5510, Loss: 35.88787078857422, L1: 10.255537033081055, L3: 25.63233184814453\n",
      "Current prediction:  61.00396728515625 \n",
      "\n",
      "Iteration 5511, Loss: 36.129146575927734, L1: 10.230639457702637, L3: 25.898508071899414\n",
      "Current prediction:  61.40732955932617 \n",
      "\n",
      "Iteration 5512, Loss: 35.60207748413086, L1: 10.210295677185059, L3: 25.391782760620117\n",
      "Current prediction:  61.522701263427734 \n",
      "\n",
      "Iteration 5513, Loss: 35.32021713256836, L1: 10.210723876953125, L3: 25.109493255615234\n",
      "Current prediction:  61.53852844238281 \n",
      "\n",
      "Iteration 5514, Loss: 35.903602600097656, L1: 10.220378875732422, L3: 25.683225631713867\n",
      "Current prediction:  61.54172897338867 \n",
      "\n",
      "Iteration 5515, Loss: 36.519508361816406, L1: 10.231075286865234, L3: 26.28843116760254\n",
      "Current prediction:  61.54155731201172 \n",
      "\n",
      "Iteration 5516, Loss: 35.70557403564453, L1: 10.219345092773438, L3: 25.486230850219727\n",
      "Current prediction:  61.538238525390625 \n",
      "\n",
      "Iteration 5517, Loss: 36.49519729614258, L1: 10.246877670288086, L3: 26.248319625854492\n",
      "Current prediction:  61.528438568115234 \n",
      "\n",
      "Iteration 5518, Loss: 36.83116149902344, L1: 10.192397117614746, L3: 26.638763427734375\n",
      "Current prediction:  61.49876022338867 \n",
      "\n",
      "Iteration 5519, Loss: 37.14365005493164, L1: 10.205984115600586, L3: 26.937665939331055\n",
      "Current prediction:  61.39681625366211 \n",
      "\n",
      "Iteration 5520, Loss: 36.40203094482422, L1: 10.217682838439941, L3: 26.18434715270996\n",
      "Current prediction:  61.160682678222656 \n",
      "\n",
      "Iteration 5521, Loss: 35.8295783996582, L1: 10.199810981750488, L3: 25.6297664642334\n",
      "Current prediction:  61.07363510131836 \n",
      "\n",
      "Iteration 5522, Loss: 35.94547653198242, L1: 10.224787712097168, L3: 25.72068977355957\n",
      "Current prediction:  61.03886032104492 \n",
      "\n",
      "Iteration 5523, Loss: 36.56657791137695, L1: 10.20999813079834, L3: 26.356578826904297\n",
      "Current prediction:  61.130950927734375 \n",
      "\n",
      "Iteration 5524, Loss: 36.41183853149414, L1: 10.226658821105957, L3: 26.185178756713867\n",
      "Current prediction:  61.29164123535156 \n",
      "\n",
      "Iteration 5525, Loss: 36.78118896484375, L1: 10.212458610534668, L3: 26.5687313079834\n",
      "Current prediction:  61.38166809082031 \n",
      "\n",
      "Iteration 5526, Loss: 36.741966247558594, L1: 10.201757431030273, L3: 26.540206909179688\n",
      "Current prediction:  61.365203857421875 \n",
      "\n",
      "Iteration 5527, Loss: 36.07307052612305, L1: 10.224495887756348, L3: 25.848573684692383\n",
      "Current prediction:  61.38120651245117 \n",
      "\n",
      "Iteration 5528, Loss: 36.16487121582031, L1: 10.179092407226562, L3: 25.985780715942383\n",
      "Current prediction:  61.38969039916992 \n",
      "\n",
      "Iteration 5529, Loss: 36.68431091308594, L1: 10.213432312011719, L3: 26.47088050842285\n",
      "Current prediction:  61.37171936035156 \n",
      "\n",
      "Iteration 5530, Loss: 35.62408447265625, L1: 10.221578598022461, L3: 25.402507781982422\n",
      "Current prediction:  61.33235168457031 \n",
      "\n",
      "Iteration 5531, Loss: 36.63441848754883, L1: 10.23091983795166, L3: 26.40349769592285\n",
      "Current prediction:  61.28768539428711 \n",
      "\n",
      "Iteration 5532, Loss: 36.3934440612793, L1: 10.227571487426758, L3: 26.16587257385254\n",
      "Current prediction:  61.01578903198242 \n",
      "\n",
      "Iteration 5533, Loss: 37.36052703857422, L1: 10.228108406066895, L3: 27.13241958618164\n",
      "Current prediction:  60.904701232910156 \n",
      "\n",
      "Iteration 5534, Loss: 36.386112213134766, L1: 10.24172306060791, L3: 26.14438819885254\n",
      "Current prediction:  60.885372161865234 \n",
      "\n",
      "Iteration 5535, Loss: 36.58717346191406, L1: 10.245858192443848, L3: 26.34131622314453\n",
      "Current prediction:  60.897891998291016 \n",
      "\n",
      "Iteration 5536, Loss: 35.407562255859375, L1: 10.219444274902344, L3: 25.18811798095703\n",
      "Current prediction:  60.932491302490234 \n",
      "\n",
      "Iteration 5537, Loss: 36.00949478149414, L1: 10.21170425415039, L3: 25.79779052734375\n",
      "Current prediction:  61.136207580566406 \n",
      "\n",
      "Iteration 5538, Loss: 35.922386169433594, L1: 10.205663681030273, L3: 25.716724395751953\n",
      "Current prediction:  61.20456314086914 \n",
      "\n",
      "Iteration 5539, Loss: 35.46451187133789, L1: 10.193644523620605, L3: 25.2708683013916\n",
      "Current prediction:  61.35905838012695 \n",
      "\n",
      "Iteration 5540, Loss: 36.11018753051758, L1: 10.16016674041748, L3: 25.950021743774414\n",
      "Current prediction:  61.461830139160156 \n",
      "\n",
      "Iteration 5541, Loss: 35.66272735595703, L1: 10.159008979797363, L3: 25.503719329833984\n",
      "Current prediction:  61.491844177246094 \n",
      "\n",
      "Iteration 5542, Loss: 36.80390548706055, L1: 10.207230567932129, L3: 26.596675872802734\n",
      "Current prediction:  61.47993087768555 \n",
      "\n",
      "Iteration 5543, Loss: 36.345237731933594, L1: 10.170963287353516, L3: 26.174272537231445\n",
      "Current prediction:  61.44105529785156 \n",
      "\n",
      "Iteration 5544, Loss: 36.72008514404297, L1: 10.161659240722656, L3: 26.558425903320312\n",
      "Current prediction:  61.4566764831543 \n",
      "\n",
      "Iteration 5545, Loss: 35.92687225341797, L1: 10.159980773925781, L3: 25.76689338684082\n",
      "Current prediction:  61.411861419677734 \n",
      "\n",
      "Iteration 5546, Loss: 36.74168395996094, L1: 10.157013893127441, L3: 26.584671020507812\n",
      "Current prediction:  61.2891960144043 \n",
      "\n",
      "Iteration 5547, Loss: 37.05332946777344, L1: 10.179996490478516, L3: 26.873334884643555\n",
      "Current prediction:  61.19140625 \n",
      "\n",
      "Iteration 5548, Loss: 35.4547233581543, L1: 10.216809272766113, L3: 25.2379150390625\n",
      "Current prediction:  61.20764923095703 \n",
      "\n",
      "Iteration 5549, Loss: 35.912879943847656, L1: 10.172883987426758, L3: 25.739994049072266\n",
      "Current prediction:  61.27429962158203 \n",
      "\n",
      "Iteration 5550, Loss: 35.64540100097656, L1: 10.173112869262695, L3: 25.472286224365234\n",
      "Current prediction:  61.4192008972168 \n",
      "\n",
      "Iteration 5551, Loss: 36.183753967285156, L1: 10.183915138244629, L3: 25.999839782714844\n",
      "Current prediction:  61.525821685791016 \n",
      "\n",
      "Iteration 5552, Loss: 36.6569938659668, L1: 10.175772666931152, L3: 26.48122215270996\n",
      "Current prediction:  61.5493278503418 \n",
      "\n",
      "Iteration 5553, Loss: 37.281681060791016, L1: 10.185402870178223, L3: 27.096277236938477\n",
      "Current prediction:  61.54789352416992 \n",
      "\n",
      "Iteration 5554, Loss: 35.85697937011719, L1: 10.164937973022461, L3: 25.69204330444336\n",
      "Current prediction:  61.533607482910156 \n",
      "\n",
      "Iteration 5555, Loss: 37.59483337402344, L1: 10.20529556274414, L3: 27.389537811279297\n",
      "Current prediction:  61.499298095703125 \n",
      "\n",
      "Iteration 5556, Loss: 36.549644470214844, L1: 10.219650268554688, L3: 26.329994201660156\n",
      "Current prediction:  61.460296630859375 \n",
      "\n",
      "Iteration 5557, Loss: 36.14117431640625, L1: 10.208415985107422, L3: 25.932756423950195\n",
      "Current prediction:  61.386741638183594 \n",
      "\n",
      "Iteration 5558, Loss: 36.29276657104492, L1: 10.214383125305176, L3: 26.078384399414062\n",
      "Current prediction:  61.2465705871582 \n",
      "\n",
      "Iteration 5559, Loss: 35.60634994506836, L1: 10.2017183303833, L3: 25.404632568359375\n",
      "Current prediction:  61.11213302612305 \n",
      "\n",
      "Iteration 5560, Loss: 37.210670471191406, L1: 10.202774047851562, L3: 27.00789451599121\n",
      "Current prediction:  60.957218170166016 \n",
      "\n",
      "Iteration 5561, Loss: 36.081268310546875, L1: 10.223495483398438, L3: 25.857772827148438\n",
      "Current prediction:  60.87913513183594 \n",
      "\n",
      "Iteration 5562, Loss: 36.715538024902344, L1: 10.239348411560059, L3: 26.4761905670166\n",
      "Current prediction:  60.93680191040039 \n",
      "\n",
      "Iteration 5563, Loss: 36.03214645385742, L1: 10.237021446228027, L3: 25.795124053955078\n",
      "Current prediction:  61.08503341674805 \n",
      "\n",
      "Iteration 5564, Loss: 36.84104919433594, L1: 10.236238479614258, L3: 26.604812622070312\n",
      "Current prediction:  61.38478088378906 \n",
      "\n",
      "Iteration 5565, Loss: 36.30801010131836, L1: 10.211156845092773, L3: 26.096853256225586\n",
      "Current prediction:  61.472198486328125 \n",
      "\n",
      "Iteration 5566, Loss: 36.04692840576172, L1: 10.213334083557129, L3: 25.833593368530273\n",
      "Current prediction:  61.50743103027344 \n",
      "\n",
      "Iteration 5567, Loss: 35.76734161376953, L1: 10.2247953414917, L3: 25.54254722595215\n",
      "Current prediction:  61.507080078125 \n",
      "\n",
      "Iteration 5568, Loss: 35.673805236816406, L1: 10.222004890441895, L3: 25.451801300048828\n",
      "Current prediction:  61.50731658935547 \n",
      "\n",
      "Iteration 5569, Loss: 36.36396026611328, L1: 10.188294410705566, L3: 26.1756649017334\n",
      "Current prediction:  61.49799346923828 \n",
      "\n",
      "Iteration 5570, Loss: 35.756385803222656, L1: 10.227825164794922, L3: 25.5285587310791\n",
      "Current prediction:  61.481380462646484 \n",
      "\n",
      "Iteration 5571, Loss: 35.309288024902344, L1: 10.21059513092041, L3: 25.09869384765625\n",
      "Current prediction:  61.45423126220703 \n",
      "\n",
      "Iteration 5572, Loss: 35.59703063964844, L1: 10.216055870056152, L3: 25.38097381591797\n",
      "Current prediction:  61.38853073120117 \n",
      "\n",
      "Iteration 5573, Loss: 35.89547348022461, L1: 10.22861385345459, L3: 25.666858673095703\n",
      "Current prediction:  61.300941467285156 \n",
      "\n",
      "Iteration 5574, Loss: 36.57725524902344, L1: 10.188408851623535, L3: 26.38884735107422\n",
      "Current prediction:  61.16084289550781 \n",
      "\n",
      "Iteration 5575, Loss: 36.92814254760742, L1: 10.209211349487305, L3: 26.718931198120117\n",
      "Current prediction:  61.10346221923828 \n",
      "\n",
      "Iteration 5576, Loss: 37.07974624633789, L1: 10.215328216552734, L3: 26.864418029785156\n",
      "Current prediction:  61.059532165527344 \n",
      "\n",
      "Iteration 5577, Loss: 36.2559700012207, L1: 10.206147193908691, L3: 26.049821853637695\n",
      "Current prediction:  61.2337646484375 \n",
      "\n",
      "Iteration 5578, Loss: 35.93031692504883, L1: 10.177824974060059, L3: 25.752490997314453\n",
      "Current prediction:  61.34269714355469 \n",
      "\n",
      "Iteration 5579, Loss: 36.786865234375, L1: 10.18747329711914, L3: 26.599390029907227\n",
      "Current prediction:  61.46586990356445 \n",
      "\n",
      "Iteration 5580, Loss: 36.123016357421875, L1: 10.173967361450195, L3: 25.949050903320312\n",
      "Current prediction:  61.51005554199219 \n",
      "\n",
      "Iteration 5581, Loss: 35.78181838989258, L1: 10.17487621307373, L3: 25.606943130493164\n",
      "Current prediction:  61.553733825683594 \n",
      "\n",
      "Iteration 5582, Loss: 36.65529251098633, L1: 10.176159858703613, L3: 26.4791316986084\n",
      "Current prediction:  61.566246032714844 \n",
      "\n",
      "Iteration 5583, Loss: 35.80010223388672, L1: 10.193649291992188, L3: 25.606454849243164\n",
      "Current prediction:  61.57098388671875 \n",
      "\n",
      "Iteration 5584, Loss: 35.68986511230469, L1: 10.196046829223633, L3: 25.493818283081055\n",
      "Current prediction:  61.574092864990234 \n",
      "\n",
      "Iteration 5585, Loss: 35.8505744934082, L1: 10.18459415435791, L3: 25.665979385375977\n",
      "Current prediction:  61.576839447021484 \n",
      "\n",
      "Iteration 5586, Loss: 35.4625244140625, L1: 10.196474075317383, L3: 25.26605224609375\n",
      "Current prediction:  61.574790954589844 \n",
      "\n",
      "Iteration 5587, Loss: 36.62300109863281, L1: 10.181021690368652, L3: 26.441978454589844\n",
      "Current prediction:  61.56761932373047 \n",
      "\n",
      "Iteration 5588, Loss: 36.637603759765625, L1: 10.161079406738281, L3: 26.47652244567871\n",
      "Current prediction:  61.543453216552734 \n",
      "\n",
      "Iteration 5589, Loss: 35.233619689941406, L1: 10.165904998779297, L3: 25.067712783813477\n",
      "Current prediction:  61.4897575378418 \n",
      "\n",
      "Iteration 5590, Loss: 37.05423355102539, L1: 10.16947078704834, L3: 26.884761810302734\n",
      "Current prediction:  61.386722564697266 \n",
      "\n",
      "Iteration 5591, Loss: 36.747501373291016, L1: 10.189906120300293, L3: 26.55759620666504\n",
      "Current prediction:  61.23879623413086 \n",
      "\n",
      "Iteration 5592, Loss: 36.042869567871094, L1: 10.187291145324707, L3: 25.855579376220703\n",
      "Current prediction:  61.05628204345703 \n",
      "\n",
      "Iteration 5593, Loss: 35.85271072387695, L1: 10.175774574279785, L3: 25.676937103271484\n",
      "Current prediction:  60.972660064697266 \n",
      "\n",
      "Iteration 5594, Loss: 36.65552520751953, L1: 10.222646713256836, L3: 26.432878494262695\n",
      "Current prediction:  60.862518310546875 \n",
      "\n",
      "Iteration 5595, Loss: 35.69938278198242, L1: 10.232069969177246, L3: 25.46731185913086\n",
      "Current prediction:  60.81672668457031 \n",
      "\n",
      "Iteration 5596, Loss: 36.90211486816406, L1: 10.275328636169434, L3: 26.626785278320312\n",
      "Current prediction:  60.79100036621094 \n",
      "\n",
      "Iteration 5597, Loss: 36.21243667602539, L1: 10.27855110168457, L3: 25.93388557434082\n",
      "Current prediction:  60.789981842041016 \n",
      "\n",
      "Iteration 5598, Loss: 37.27165222167969, L1: 10.263716697692871, L3: 27.0079345703125\n",
      "Current prediction:  60.79061508178711 \n",
      "\n",
      "Iteration 5599, Loss: 35.3353271484375, L1: 10.24966049194336, L3: 25.085664749145508\n",
      "Current prediction:  60.8457145690918 \n",
      "\n",
      "Iteration 5600, Loss: 36.22681427001953, L1: 10.244120597839355, L3: 25.982694625854492\n",
      "Current prediction:  61.05378341674805 \n",
      "\n",
      "Iteration 5601, Loss: 37.299320220947266, L1: 10.241074562072754, L3: 27.058246612548828\n",
      "Current prediction:  61.378746032714844 \n",
      "\n",
      "Iteration 5602, Loss: 37.00175476074219, L1: 10.219847679138184, L3: 26.781906127929688\n",
      "Current prediction:  61.482093811035156 \n",
      "\n",
      "Iteration 5603, Loss: 36.619346618652344, L1: 10.203991889953613, L3: 26.415355682373047\n",
      "Current prediction:  61.50206756591797 \n",
      "\n",
      "Iteration 5604, Loss: 35.857574462890625, L1: 10.215959548950195, L3: 25.641613006591797\n",
      "Current prediction:  61.51068878173828 \n",
      "\n",
      "Iteration 5605, Loss: 36.43366622924805, L1: 10.20675277709961, L3: 26.226913452148438\n",
      "Current prediction:  61.515743255615234 \n",
      "\n",
      "Iteration 5606, Loss: 36.77568054199219, L1: 10.216625213623047, L3: 26.55905532836914\n",
      "Current prediction:  61.51312255859375 \n",
      "\n",
      "Iteration 5607, Loss: 35.5638427734375, L1: 10.233967781066895, L3: 25.329875946044922\n",
      "Current prediction:  61.511444091796875 \n",
      "\n",
      "Iteration 5608, Loss: 36.197898864746094, L1: 10.207231521606445, L3: 25.99066925048828\n",
      "Current prediction:  61.50721740722656 \n",
      "\n",
      "Iteration 5609, Loss: 36.679237365722656, L1: 10.247922897338867, L3: 26.431316375732422\n",
      "Current prediction:  61.5041618347168 \n",
      "\n",
      "Iteration 5610, Loss: 36.35810470581055, L1: 10.221158027648926, L3: 26.136947631835938\n",
      "Current prediction:  61.498817443847656 \n",
      "\n",
      "Iteration 5611, Loss: 36.34769821166992, L1: 10.241557121276855, L3: 26.106142044067383\n",
      "Current prediction:  61.46803665161133 \n",
      "\n",
      "Iteration 5612, Loss: 36.66469192504883, L1: 10.235871315002441, L3: 26.42881965637207\n",
      "Current prediction:  61.236915588378906 \n",
      "\n",
      "Iteration 5613, Loss: 35.93070983886719, L1: 10.228026390075684, L3: 25.70268440246582\n",
      "Current prediction:  60.875755310058594 \n",
      "\n",
      "Iteration 5614, Loss: 36.07908248901367, L1: 10.226091384887695, L3: 25.852991104125977\n",
      "Current prediction:  60.8629035949707 \n",
      "\n",
      "Iteration 5615, Loss: 36.69380569458008, L1: 10.258484840393066, L3: 26.435321807861328\n",
      "Current prediction:  60.89741897583008 \n",
      "\n",
      "Iteration 5616, Loss: 36.890625, L1: 10.255524635314941, L3: 26.635099411010742\n",
      "Current prediction:  61.000144958496094 \n",
      "\n",
      "Iteration 5617, Loss: 35.92746353149414, L1: 10.242271423339844, L3: 25.685192108154297\n",
      "Current prediction:  61.29422378540039 \n",
      "\n",
      "Iteration 5618, Loss: 36.25886535644531, L1: 10.221975326538086, L3: 26.03689193725586\n",
      "Current prediction:  61.44502639770508 \n",
      "\n",
      "Iteration 5619, Loss: 35.25644302368164, L1: 10.223782539367676, L3: 25.03266143798828\n",
      "Current prediction:  61.49345397949219 \n",
      "\n",
      "Iteration 5620, Loss: 36.923683166503906, L1: 10.229791641235352, L3: 26.693891525268555\n",
      "Current prediction:  61.499664306640625 \n",
      "\n",
      "Iteration 5621, Loss: 36.81718444824219, L1: 10.234395980834961, L3: 26.582786560058594\n",
      "Current prediction:  61.49040985107422 \n",
      "\n",
      "Iteration 5622, Loss: 36.640235900878906, L1: 10.180070877075195, L3: 26.460163116455078\n",
      "Current prediction:  61.46739196777344 \n",
      "\n",
      "Iteration 5623, Loss: 36.46028137207031, L1: 10.197463989257812, L3: 26.262819290161133\n",
      "Current prediction:  61.392669677734375 \n",
      "\n",
      "Iteration 5624, Loss: 37.045806884765625, L1: 10.209529876708984, L3: 26.836275100708008\n",
      "Current prediction:  61.13748550415039 \n",
      "\n",
      "Iteration 5625, Loss: 37.20899200439453, L1: 10.205586433410645, L3: 27.003406524658203\n",
      "Current prediction:  60.898677825927734 \n",
      "\n",
      "Iteration 5626, Loss: 36.44248962402344, L1: 10.23923110961914, L3: 26.203258514404297\n",
      "Current prediction:  60.83866500854492 \n",
      "\n",
      "Iteration 5627, Loss: 35.559165954589844, L1: 10.223044395446777, L3: 25.336122512817383\n",
      "Current prediction:  60.852638244628906 \n",
      "\n",
      "Iteration 5628, Loss: 36.32624053955078, L1: 10.224676132202148, L3: 26.101564407348633\n",
      "Current prediction:  60.9559326171875 \n",
      "\n",
      "Iteration 5629, Loss: 36.548683166503906, L1: 10.225740432739258, L3: 26.32294464111328\n",
      "Current prediction:  61.10334396362305 \n",
      "\n",
      "Iteration 5630, Loss: 36.668678283691406, L1: 10.218854904174805, L3: 26.449825286865234\n",
      "Current prediction:  61.30128860473633 \n",
      "\n",
      "Iteration 5631, Loss: 36.58472442626953, L1: 10.199353218078613, L3: 26.385372161865234\n",
      "Current prediction:  61.39689254760742 \n",
      "\n",
      "Iteration 5632, Loss: 36.91427230834961, L1: 10.215093612670898, L3: 26.69917869567871\n",
      "Current prediction:  61.38545608520508 \n",
      "\n",
      "Iteration 5633, Loss: 35.55790710449219, L1: 10.198715209960938, L3: 25.359193801879883\n",
      "Current prediction:  61.354061126708984 \n",
      "\n",
      "Iteration 5634, Loss: 36.04839324951172, L1: 10.212376594543457, L3: 25.836017608642578\n",
      "Current prediction:  61.230167388916016 \n",
      "\n",
      "Iteration 5635, Loss: 37.43309783935547, L1: 10.18281364440918, L3: 27.25028419494629\n",
      "Current prediction:  60.98460388183594 \n",
      "\n",
      "Iteration 5636, Loss: 35.752525329589844, L1: 10.2007474899292, L3: 25.55177879333496\n",
      "Current prediction:  60.8580322265625 \n",
      "\n",
      "Iteration 5637, Loss: 37.56049728393555, L1: 10.257378578186035, L3: 27.303119659423828\n",
      "Current prediction:  60.80423355102539 \n",
      "\n",
      "Iteration 5638, Loss: 37.07909393310547, L1: 10.238338470458984, L3: 26.840755462646484\n",
      "Current prediction:  60.79851150512695 \n",
      "\n",
      "Iteration 5639, Loss: 36.34771728515625, L1: 10.242030143737793, L3: 26.10568618774414\n",
      "Current prediction:  60.804161071777344 \n",
      "\n",
      "Iteration 5640, Loss: 35.91633224487305, L1: 10.258429527282715, L3: 25.657901763916016\n",
      "Current prediction:  60.817874908447266 \n",
      "\n",
      "Iteration 5641, Loss: 35.95578384399414, L1: 10.242422103881836, L3: 25.713361740112305\n",
      "Current prediction:  60.83424758911133 \n",
      "\n",
      "Iteration 5642, Loss: 37.6793327331543, L1: 10.230280876159668, L3: 27.449050903320312\n",
      "Current prediction:  60.92851257324219 \n",
      "\n",
      "Iteration 5643, Loss: 36.3886604309082, L1: 10.204236030578613, L3: 26.184425354003906\n",
      "Current prediction:  61.12176513671875 \n",
      "\n",
      "Iteration 5644, Loss: 36.65211486816406, L1: 10.174135208129883, L3: 26.477981567382812\n",
      "Current prediction:  61.41374969482422 \n",
      "\n",
      "Iteration 5645, Loss: 36.25257873535156, L1: 10.177753448486328, L3: 26.074825286865234\n",
      "Current prediction:  61.48127746582031 \n",
      "\n",
      "Iteration 5646, Loss: 37.77125549316406, L1: 10.17735481262207, L3: 27.593900680541992\n",
      "Current prediction:  61.505104064941406 \n",
      "\n",
      "Iteration 5647, Loss: 36.46868896484375, L1: 10.190540313720703, L3: 26.278146743774414\n",
      "Current prediction:  61.48128128051758 \n",
      "\n",
      "Iteration 5648, Loss: 35.97686767578125, L1: 10.17713737487793, L3: 25.79973030090332\n",
      "Current prediction:  61.431785583496094 \n",
      "\n",
      "Iteration 5649, Loss: 35.16432571411133, L1: 10.190245628356934, L3: 24.97408103942871\n",
      "Current prediction:  61.273414611816406 \n",
      "\n",
      "Iteration 5650, Loss: 36.065555572509766, L1: 10.20542049407959, L3: 25.86013412475586\n",
      "Current prediction:  61.05601501464844 \n",
      "\n",
      "Iteration 5651, Loss: 36.60335922241211, L1: 10.16957950592041, L3: 26.433780670166016\n",
      "Current prediction:  61.014163970947266 \n",
      "\n",
      "Iteration 5652, Loss: 37.18714141845703, L1: 10.216501235961914, L3: 26.970640182495117\n",
      "Current prediction:  61.11196517944336 \n",
      "\n",
      "Iteration 5653, Loss: 36.31690216064453, L1: 10.1888427734375, L3: 26.1280574798584\n",
      "Current prediction:  61.14873123168945 \n",
      "\n",
      "Iteration 5654, Loss: 37.54143142700195, L1: 10.213637351989746, L3: 27.32779312133789\n",
      "Current prediction:  61.187713623046875 \n",
      "\n",
      "Iteration 5655, Loss: 35.99954605102539, L1: 10.213132858276367, L3: 25.786413192749023\n",
      "Current prediction:  61.28187942504883 \n",
      "\n",
      "Iteration 5656, Loss: 36.86079788208008, L1: 10.198559761047363, L3: 26.6622371673584\n",
      "Current prediction:  61.389095306396484 \n",
      "\n",
      "Iteration 5657, Loss: 36.76592254638672, L1: 10.216876983642578, L3: 26.549047470092773\n",
      "Current prediction:  61.444007873535156 \n",
      "\n",
      "Iteration 5658, Loss: 35.540489196777344, L1: 10.210626602172852, L3: 25.329862594604492\n",
      "Current prediction:  61.49939727783203 \n",
      "\n",
      "Iteration 5659, Loss: 36.251773834228516, L1: 10.244866371154785, L3: 26.006906509399414\n",
      "Current prediction:  61.50302505493164 \n",
      "\n",
      "Iteration 5660, Loss: 36.16294860839844, L1: 10.210092544555664, L3: 25.952856063842773\n",
      "Current prediction:  61.4986572265625 \n",
      "\n",
      "Iteration 5661, Loss: 36.05657196044922, L1: 10.225790977478027, L3: 25.830781936645508\n",
      "Current prediction:  61.49296188354492 \n",
      "\n",
      "Iteration 5662, Loss: 36.6630859375, L1: 10.234430313110352, L3: 26.42865562438965\n",
      "Current prediction:  61.46262741088867 \n",
      "\n",
      "Iteration 5663, Loss: 36.90593338012695, L1: 10.224342346191406, L3: 26.681591033935547\n",
      "Current prediction:  61.32680130004883 \n",
      "\n",
      "Iteration 5664, Loss: 36.64356994628906, L1: 10.223907470703125, L3: 26.419662475585938\n",
      "Current prediction:  60.949886322021484 \n",
      "\n",
      "Iteration 5665, Loss: 36.428672790527344, L1: 10.210232734680176, L3: 26.218441009521484\n",
      "Current prediction:  60.820335388183594 \n",
      "\n",
      "Iteration 5666, Loss: 36.59747314453125, L1: 10.26177978515625, L3: 26.335693359375\n",
      "Current prediction:  60.7921028137207 \n",
      "\n",
      "Iteration 5667, Loss: 36.746219635009766, L1: 10.271917343139648, L3: 26.474302291870117\n",
      "Current prediction:  60.82107162475586 \n",
      "\n",
      "Iteration 5668, Loss: 36.38640594482422, L1: 10.249828338623047, L3: 26.13657569885254\n",
      "Current prediction:  60.940948486328125 \n",
      "\n",
      "Iteration 5669, Loss: 36.72093200683594, L1: 10.232004165649414, L3: 26.488927841186523\n",
      "Current prediction:  61.30916976928711 \n",
      "\n",
      "Iteration 5670, Loss: 36.045658111572266, L1: 10.229996681213379, L3: 25.81566047668457\n",
      "Current prediction:  61.46592712402344 \n",
      "\n",
      "Iteration 5671, Loss: 35.97320556640625, L1: 10.229696273803711, L3: 25.743507385253906\n",
      "Current prediction:  61.510169982910156 \n",
      "\n",
      "Iteration 5672, Loss: 37.62285614013672, L1: 10.215359687805176, L3: 27.40749740600586\n",
      "Current prediction:  61.51969909667969 \n",
      "\n",
      "Iteration 5673, Loss: 37.96586227416992, L1: 10.21899700164795, L3: 27.74686622619629\n",
      "Current prediction:  61.521732330322266 \n",
      "\n",
      "Iteration 5674, Loss: 36.3290901184082, L1: 10.202658653259277, L3: 26.12643051147461\n",
      "Current prediction:  61.52119064331055 \n",
      "\n",
      "Iteration 5675, Loss: 36.06565856933594, L1: 10.205493927001953, L3: 25.860166549682617\n",
      "Current prediction:  61.5094108581543 \n",
      "\n",
      "Iteration 5676, Loss: 37.09209060668945, L1: 10.19521427154541, L3: 26.896875381469727\n",
      "Current prediction:  61.476932525634766 \n",
      "\n",
      "Iteration 5677, Loss: 35.734134674072266, L1: 10.2113676071167, L3: 25.522768020629883\n",
      "Current prediction:  61.35854721069336 \n",
      "\n",
      "Iteration 5678, Loss: 35.702674865722656, L1: 10.186448097229004, L3: 25.51622772216797\n",
      "Current prediction:  61.03559494018555 \n",
      "\n",
      "Iteration 5679, Loss: 37.08142852783203, L1: 10.226731300354004, L3: 26.85469627380371\n",
      "Current prediction:  60.864341735839844 \n",
      "\n",
      "Iteration 5680, Loss: 36.58014678955078, L1: 10.22296142578125, L3: 26.3571834564209\n",
      "Current prediction:  60.8020133972168 \n",
      "\n",
      "Iteration 5681, Loss: 36.2599983215332, L1: 10.251346588134766, L3: 26.008651733398438\n",
      "Current prediction:  60.79222106933594 \n",
      "\n",
      "Iteration 5682, Loss: 35.771141052246094, L1: 10.253218650817871, L3: 25.517921447753906\n",
      "Current prediction:  60.79500198364258 \n",
      "\n",
      "Iteration 5683, Loss: 37.14537048339844, L1: 10.255261421203613, L3: 26.89011001586914\n",
      "Current prediction:  60.79872512817383 \n",
      "\n",
      "Iteration 5684, Loss: 36.926300048828125, L1: 10.255377769470215, L3: 26.670921325683594\n",
      "Current prediction:  60.82817077636719 \n",
      "\n",
      "Iteration 5685, Loss: 36.41143798828125, L1: 10.224445343017578, L3: 26.186992645263672\n",
      "Current prediction:  60.9072265625 \n",
      "\n",
      "Iteration 5686, Loss: 35.75597381591797, L1: 10.19864559173584, L3: 25.557329177856445\n",
      "Current prediction:  61.101715087890625 \n",
      "\n",
      "Iteration 5687, Loss: 36.282752990722656, L1: 10.201478004455566, L3: 26.081274032592773\n",
      "Current prediction:  61.26280975341797 \n",
      "\n",
      "Iteration 5688, Loss: 36.271209716796875, L1: 10.48858642578125, L3: 25.782625198364258\n",
      "Current prediction:  61.433860778808594 \n",
      "\n",
      "Iteration 5689, Loss: 36.85730743408203, L1: 10.180830001831055, L3: 26.676475524902344\n",
      "Current prediction:  61.482940673828125 \n",
      "\n",
      "Iteration 5690, Loss: 35.93281173706055, L1: 10.163352012634277, L3: 25.769460678100586\n",
      "Current prediction:  61.536460876464844 \n",
      "\n",
      "Iteration 5691, Loss: 36.61626052856445, L1: 10.186387062072754, L3: 26.429874420166016\n",
      "Current prediction:  61.5426139831543 \n",
      "\n",
      "Iteration 5692, Loss: 36.043609619140625, L1: 10.181595802307129, L3: 25.86201286315918\n",
      "Current prediction:  61.54431915283203 \n",
      "\n",
      "Iteration 5693, Loss: 37.710205078125, L1: 10.195728302001953, L3: 27.51447868347168\n",
      "Current prediction:  61.53823471069336 \n",
      "\n",
      "Iteration 5694, Loss: 36.250858306884766, L1: 10.184927940368652, L3: 26.065929412841797\n",
      "Current prediction:  61.5302619934082 \n",
      "\n",
      "Iteration 5695, Loss: 36.065155029296875, L1: 10.196420669555664, L3: 25.868736267089844\n",
      "Current prediction:  61.454795837402344 \n",
      "\n",
      "Iteration 5696, Loss: 36.088382720947266, L1: 10.353845596313477, L3: 25.73453712463379\n",
      "Current prediction:  61.327980041503906 \n",
      "\n",
      "Iteration 5697, Loss: 35.28140640258789, L1: 10.191536903381348, L3: 25.089868545532227\n",
      "Current prediction:  61.186256408691406 \n",
      "\n",
      "Iteration 5698, Loss: 36.89080047607422, L1: 10.266982078552246, L3: 26.62381935119629\n",
      "Current prediction:  61.189510345458984 \n",
      "\n",
      "Iteration 5699, Loss: 36.79160690307617, L1: 10.212943077087402, L3: 26.578662872314453\n",
      "Current prediction:  61.315948486328125 \n",
      "\n",
      "Iteration 5700, Loss: 36.28694534301758, L1: 10.228068351745605, L3: 26.05887794494629\n",
      "Current prediction:  61.41844177246094 \n",
      "\n",
      "Iteration 5701, Loss: 36.63916778564453, L1: 10.197872161865234, L3: 26.44129753112793\n",
      "Current prediction:  61.41178512573242 \n",
      "\n",
      "Iteration 5702, Loss: 36.803367614746094, L1: 10.200498580932617, L3: 26.602869033813477\n",
      "Current prediction:  61.316993713378906 \n",
      "\n",
      "Iteration 5703, Loss: 36.84883117675781, L1: 10.211088180541992, L3: 26.63774299621582\n",
      "Current prediction:  61.120365142822266 \n",
      "\n",
      "Iteration 5704, Loss: 35.768455505371094, L1: 10.178565979003906, L3: 25.589887619018555\n",
      "Current prediction:  60.922462463378906 \n",
      "\n",
      "Iteration 5705, Loss: 37.378562927246094, L1: 10.225881576538086, L3: 27.152681350708008\n",
      "Current prediction:  60.82562255859375 \n",
      "\n",
      "Iteration 5706, Loss: 37.39447784423828, L1: 10.23969841003418, L3: 27.15477752685547\n",
      "Current prediction:  60.79513168334961 \n",
      "\n",
      "Iteration 5707, Loss: 36.392879486083984, L1: 10.234909057617188, L3: 26.157970428466797\n",
      "Current prediction:  60.788368225097656 \n",
      "\n",
      "Iteration 5708, Loss: 36.95914840698242, L1: 10.253201484680176, L3: 26.70594596862793\n",
      "Current prediction:  60.78481674194336 \n",
      "\n",
      "Iteration 5709, Loss: 36.28096389770508, L1: 10.353339195251465, L3: 25.92762565612793\n",
      "Current prediction:  60.79480743408203 \n",
      "\n",
      "Iteration 5710, Loss: 36.4262809753418, L1: 10.249189376831055, L3: 26.177091598510742\n",
      "Current prediction:  60.84063720703125 \n",
      "\n",
      "Iteration 5711, Loss: 36.73902893066406, L1: 10.285811424255371, L3: 26.453218460083008\n",
      "Current prediction:  60.995872497558594 \n",
      "\n",
      "Iteration 5712, Loss: 36.043399810791016, L1: 10.288786888122559, L3: 25.75461196899414\n",
      "Current prediction:  61.343441009521484 \n",
      "\n",
      "Iteration 5713, Loss: 36.48149871826172, L1: 10.242684364318848, L3: 26.238815307617188\n",
      "Current prediction:  61.52187728881836 \n",
      "\n",
      "Iteration 5714, Loss: 35.547176361083984, L1: 10.19649600982666, L3: 25.35068130493164\n",
      "Current prediction:  61.54353713989258 \n",
      "\n",
      "Iteration 5715, Loss: 35.856109619140625, L1: 10.199444770812988, L3: 25.65666389465332\n",
      "Current prediction:  61.547027587890625 \n",
      "\n",
      "Iteration 5716, Loss: 35.84911346435547, L1: 10.229415893554688, L3: 25.619699478149414\n",
      "Current prediction:  61.54722595214844 \n",
      "\n",
      "Iteration 5717, Loss: 36.25693130493164, L1: 10.297612190246582, L3: 25.959320068359375\n",
      "Current prediction:  61.54533767700195 \n",
      "\n",
      "Iteration 5718, Loss: 36.735374450683594, L1: 10.235825538635254, L3: 26.499549865722656\n",
      "Current prediction:  61.543338775634766 \n",
      "\n",
      "Iteration 5719, Loss: 35.421512603759766, L1: 10.227993965148926, L3: 25.193519592285156\n",
      "Current prediction:  61.541908264160156 \n",
      "\n",
      "Iteration 5720, Loss: 36.04561233520508, L1: 10.221463203430176, L3: 25.82415008544922\n",
      "Current prediction:  61.54045867919922 \n",
      "\n",
      "Iteration 5721, Loss: 35.85193634033203, L1: 10.21428108215332, L3: 25.637657165527344\n",
      "Current prediction:  61.53586196899414 \n",
      "\n",
      "Iteration 5722, Loss: 35.88425064086914, L1: 10.286847114562988, L3: 25.597402572631836\n",
      "Current prediction:  61.52995681762695 \n",
      "\n",
      "Iteration 5723, Loss: 36.51727294921875, L1: 10.324548721313477, L3: 26.19272232055664\n",
      "Current prediction:  61.52198791503906 \n",
      "\n",
      "Iteration 5724, Loss: 37.15584182739258, L1: 10.199607849121094, L3: 26.956233978271484\n",
      "Current prediction:  61.511043548583984 \n",
      "\n",
      "Iteration 5725, Loss: 36.84042739868164, L1: 10.215472221374512, L3: 26.624956130981445\n",
      "Current prediction:  61.42394256591797 \n",
      "\n",
      "Iteration 5726, Loss: 35.552268981933594, L1: 10.202835083007812, L3: 25.34943389892578\n",
      "Current prediction:  61.094505310058594 \n",
      "\n",
      "Iteration 5727, Loss: 37.27352523803711, L1: 10.222247123718262, L3: 27.051279067993164\n",
      "Current prediction:  60.821163177490234 \n",
      "\n",
      "Iteration 5728, Loss: 36.58230972290039, L1: 10.243151664733887, L3: 26.33915901184082\n",
      "Current prediction:  60.77743911743164 \n",
      "\n",
      "Iteration 5729, Loss: 35.67578125, L1: 10.253507614135742, L3: 25.422273635864258\n",
      "Current prediction:  60.76749801635742 \n",
      "\n",
      "Iteration 5730, Loss: 36.325340270996094, L1: 10.287132263183594, L3: 26.038209915161133\n",
      "Current prediction:  60.77117156982422 \n",
      "\n",
      "Iteration 5731, Loss: 36.90367126464844, L1: 10.263798713684082, L3: 26.63987159729004\n",
      "Current prediction:  60.84344482421875 \n",
      "\n",
      "Iteration 5732, Loss: 36.54257583618164, L1: 10.224225044250488, L3: 26.318349838256836\n",
      "Current prediction:  61.25467300415039 \n",
      "\n",
      "Iteration 5733, Loss: 35.01655578613281, L1: 10.2008638381958, L3: 24.815690994262695\n",
      "Current prediction:  61.476722717285156 \n",
      "\n",
      "Iteration 5734, Loss: 35.95883560180664, L1: 10.342469215393066, L3: 25.61636734008789\n",
      "Current prediction:  61.52631378173828 \n",
      "\n",
      "Iteration 5735, Loss: 35.26702880859375, L1: 10.182538986206055, L3: 25.084487915039062\n",
      "Current prediction:  61.542999267578125 \n",
      "\n",
      "Iteration 5736, Loss: 35.65316390991211, L1: 10.20102596282959, L3: 25.452138900756836\n",
      "Current prediction:  61.550148010253906 \n",
      "\n",
      "Iteration 5737, Loss: 36.85037612915039, L1: 10.202003479003906, L3: 26.648372650146484\n",
      "Current prediction:  61.555667877197266 \n",
      "\n",
      "Iteration 5738, Loss: 36.56338119506836, L1: 10.178833961486816, L3: 26.384546279907227\n",
      "Current prediction:  61.56064224243164 \n",
      "\n",
      "Iteration 5739, Loss: 36.150428771972656, L1: 10.180648803710938, L3: 25.969778060913086\n",
      "Current prediction:  61.56024169921875 \n",
      "\n",
      "Iteration 5740, Loss: 35.28046417236328, L1: 10.183528900146484, L3: 25.096933364868164\n",
      "Current prediction:  61.560611724853516 \n",
      "\n",
      "Iteration 5741, Loss: 35.646583557128906, L1: 10.1961669921875, L3: 25.450416564941406\n",
      "Current prediction:  61.56105422973633 \n",
      "\n",
      "Iteration 5742, Loss: 35.951026916503906, L1: 10.186689376831055, L3: 25.764339447021484\n",
      "Current prediction:  61.559513092041016 \n",
      "\n",
      "Iteration 5743, Loss: 37.237571716308594, L1: 10.179290771484375, L3: 27.058279037475586\n",
      "Current prediction:  61.54865264892578 \n",
      "\n",
      "Iteration 5744, Loss: 35.73631286621094, L1: 10.174603462219238, L3: 25.561710357666016\n",
      "Current prediction:  61.529232025146484 \n",
      "\n",
      "Iteration 5745, Loss: 35.93024826049805, L1: 10.172455787658691, L3: 25.757793426513672\n",
      "Current prediction:  61.411441802978516 \n",
      "\n",
      "Iteration 5746, Loss: 36.6356201171875, L1: 10.175352096557617, L3: 26.46026611328125\n",
      "Current prediction:  61.07682418823242 \n",
      "\n",
      "Iteration 5747, Loss: 35.669403076171875, L1: 10.17940902709961, L3: 25.4899959564209\n",
      "Current prediction:  60.90357971191406 \n",
      "\n",
      "Iteration 5748, Loss: 36.74677276611328, L1: 10.19706916809082, L3: 26.54970359802246\n",
      "Current prediction:  60.81876754760742 \n",
      "\n",
      "Iteration 5749, Loss: 36.456573486328125, L1: 10.233732223510742, L3: 26.222843170166016\n",
      "Current prediction:  60.80183792114258 \n",
      "\n",
      "Iteration 5750, Loss: 36.55156707763672, L1: 10.222433090209961, L3: 26.329132080078125\n",
      "Current prediction:  60.83518600463867 \n",
      "\n",
      "Iteration 5751, Loss: 36.88877868652344, L1: 10.242911338806152, L3: 26.6458683013916\n",
      "Current prediction:  61.053993225097656 \n",
      "\n",
      "Iteration 5752, Loss: 36.07057189941406, L1: 10.173973083496094, L3: 25.89659881591797\n",
      "Current prediction:  61.298828125 \n",
      "\n",
      "Iteration 5753, Loss: 36.374359130859375, L1: 10.19748306274414, L3: 26.1768741607666\n",
      "Current prediction:  61.332603454589844 \n",
      "\n",
      "Iteration 5754, Loss: 36.19220733642578, L1: 10.194622993469238, L3: 25.99758529663086\n",
      "Current prediction:  61.43169403076172 \n",
      "\n",
      "Iteration 5755, Loss: 35.76453399658203, L1: 10.1998929977417, L3: 25.56464195251465\n",
      "Current prediction:  61.44624710083008 \n",
      "\n",
      "Iteration 5756, Loss: 36.53137969970703, L1: 10.182792663574219, L3: 26.348587036132812\n",
      "Current prediction:  61.31937026977539 \n",
      "\n",
      "Iteration 5757, Loss: 36.37458801269531, L1: 10.198095321655273, L3: 26.176490783691406\n",
      "Current prediction:  60.99657440185547 \n",
      "\n",
      "Iteration 5758, Loss: 36.545745849609375, L1: 10.205584526062012, L3: 26.34016227722168\n",
      "Current prediction:  60.85694885253906 \n",
      "\n",
      "Iteration 5759, Loss: 36.771018981933594, L1: 10.25197982788086, L3: 26.519041061401367\n",
      "Current prediction:  60.80010986328125 \n",
      "\n",
      "Iteration 5760, Loss: 36.426631927490234, L1: 10.222574234008789, L3: 26.204057693481445\n",
      "Current prediction:  60.792022705078125 \n",
      "\n",
      "Iteration 5761, Loss: 36.74469757080078, L1: 10.245482444763184, L3: 26.499216079711914\n",
      "Current prediction:  60.78114700317383 \n",
      "\n",
      "Iteration 5762, Loss: 35.577754974365234, L1: 10.24448013305664, L3: 25.333274841308594\n",
      "Current prediction:  60.79867935180664 \n",
      "\n",
      "Iteration 5763, Loss: 36.6915168762207, L1: 10.242915153503418, L3: 26.4486026763916\n",
      "Current prediction:  60.85515213012695 \n",
      "\n",
      "Iteration 5764, Loss: 36.45330810546875, L1: 10.211851119995117, L3: 26.241456985473633\n",
      "Current prediction:  61.042884826660156 \n",
      "\n",
      "Iteration 5765, Loss: 36.46028518676758, L1: 10.223001480102539, L3: 26.23728370666504\n",
      "Current prediction:  61.39759063720703 \n",
      "\n",
      "Iteration 5766, Loss: 37.071014404296875, L1: 10.197948455810547, L3: 26.87306785583496\n",
      "Current prediction:  61.501922607421875 \n",
      "\n",
      "Iteration 5767, Loss: 35.80723571777344, L1: 10.17606258392334, L3: 25.63117218017578\n",
      "Current prediction:  61.50432586669922 \n",
      "\n",
      "Iteration 5768, Loss: 36.64517593383789, L1: 10.20162296295166, L3: 26.443553924560547\n",
      "Current prediction:  61.48400115966797 \n",
      "\n",
      "Iteration 5769, Loss: 37.05494689941406, L1: 10.205039978027344, L3: 26.84990882873535\n",
      "Current prediction:  61.384952545166016 \n",
      "\n",
      "Iteration 5770, Loss: 37.38108825683594, L1: 10.181296348571777, L3: 27.199792861938477\n",
      "Current prediction:  61.17017364501953 \n",
      "\n",
      "Iteration 5771, Loss: 36.17372131347656, L1: 10.176349639892578, L3: 25.997371673583984\n",
      "Current prediction:  60.86092758178711 \n",
      "\n",
      "Iteration 5772, Loss: 36.66241455078125, L1: 10.223283767700195, L3: 26.439130783081055\n",
      "Current prediction:  60.80778884887695 \n",
      "\n",
      "Iteration 5773, Loss: 36.98556137084961, L1: 10.25460433959961, L3: 26.73095703125\n",
      "Current prediction:  60.88832092285156 \n",
      "\n",
      "Iteration 5774, Loss: 36.45111083984375, L1: 10.21714973449707, L3: 26.233963012695312\n",
      "Current prediction:  61.017005920410156 \n",
      "\n",
      "Iteration 5775, Loss: 37.88631057739258, L1: 10.211701393127441, L3: 27.674610137939453\n",
      "Current prediction:  61.3609504699707 \n",
      "\n",
      "Iteration 5776, Loss: 36.56559371948242, L1: 10.199458122253418, L3: 26.36613655090332\n",
      "Current prediction:  61.4492073059082 \n",
      "\n",
      "Iteration 5777, Loss: 36.256370544433594, L1: 10.194442749023438, L3: 26.061925888061523\n",
      "Current prediction:  61.49338912963867 \n",
      "\n",
      "Iteration 5778, Loss: 36.120426177978516, L1: 10.213059425354004, L3: 25.907365798950195\n",
      "Current prediction:  61.51465606689453 \n",
      "\n",
      "Iteration 5779, Loss: 35.97905349731445, L1: 10.195106506347656, L3: 25.783946990966797\n",
      "Current prediction:  61.52043533325195 \n",
      "\n",
      "Iteration 5780, Loss: 36.19283676147461, L1: 10.212141036987305, L3: 25.980695724487305\n",
      "Current prediction:  61.52043533325195 \n",
      "\n",
      "Iteration 5781, Loss: 36.362525939941406, L1: 10.195243835449219, L3: 26.167282104492188\n",
      "Current prediction:  61.516441345214844 \n",
      "\n",
      "Iteration 5782, Loss: 36.466461181640625, L1: 10.226820945739746, L3: 26.239641189575195\n",
      "Current prediction:  61.51374435424805 \n",
      "\n",
      "Iteration 5783, Loss: 37.25035095214844, L1: 10.194381713867188, L3: 27.05596923828125\n",
      "Current prediction:  61.50882339477539 \n",
      "\n",
      "Iteration 5784, Loss: 37.60938262939453, L1: 10.217823028564453, L3: 27.391559600830078\n",
      "Current prediction:  61.50330352783203 \n",
      "\n",
      "Iteration 5785, Loss: 36.5655403137207, L1: 10.192473411560059, L3: 26.37306785583496\n",
      "Current prediction:  61.49243927001953 \n",
      "\n",
      "Iteration 5786, Loss: 36.15008544921875, L1: 10.2060546875, L3: 25.94403076171875\n",
      "Current prediction:  61.4821662902832 \n",
      "\n",
      "Iteration 5787, Loss: 36.19941711425781, L1: 10.226987838745117, L3: 25.972427368164062\n",
      "Current prediction:  61.47856903076172 \n",
      "\n",
      "Iteration 5788, Loss: 36.41754913330078, L1: 10.195520401000977, L3: 26.222030639648438\n",
      "Current prediction:  61.45924758911133 \n",
      "\n",
      "Iteration 5789, Loss: 37.48286437988281, L1: 10.223958969116211, L3: 27.2589054107666\n",
      "Current prediction:  61.35472106933594 \n",
      "\n",
      "Iteration 5790, Loss: 36.5420036315918, L1: 10.221781730651855, L3: 26.320220947265625\n",
      "Current prediction:  61.02057647705078 \n",
      "\n",
      "Iteration 5791, Loss: 35.712799072265625, L1: 10.213754653930664, L3: 25.49904441833496\n",
      "Current prediction:  60.946144104003906 \n",
      "\n",
      "Iteration 5792, Loss: 36.794960021972656, L1: 10.230730056762695, L3: 26.564231872558594\n",
      "Current prediction:  60.96186447143555 \n",
      "\n",
      "Iteration 5793, Loss: 35.76422119140625, L1: 10.242647171020508, L3: 25.521575927734375\n",
      "Current prediction:  61.0960807800293 \n",
      "\n",
      "Iteration 5794, Loss: 35.747833251953125, L1: 10.277965545654297, L3: 25.46986961364746\n",
      "Current prediction:  61.16268539428711 \n",
      "\n",
      "Iteration 5795, Loss: 36.471763610839844, L1: 10.220640182495117, L3: 26.25112533569336\n",
      "Current prediction:  61.02826690673828 \n",
      "\n",
      "Iteration 5796, Loss: 36.730770111083984, L1: 10.254775047302246, L3: 26.475994110107422\n",
      "Current prediction:  61.04281997680664 \n",
      "\n",
      "Iteration 5797, Loss: 36.62632751464844, L1: 10.187593460083008, L3: 26.438732147216797\n",
      "Current prediction:  61.19598388671875 \n",
      "\n",
      "Iteration 5798, Loss: 35.91147994995117, L1: 10.184065818786621, L3: 25.727413177490234\n",
      "Current prediction:  61.423274993896484 \n",
      "\n",
      "Iteration 5799, Loss: 36.519386291503906, L1: 10.186885833740234, L3: 26.332502365112305\n",
      "Current prediction:  61.499881744384766 \n",
      "\n",
      "Iteration 5800, Loss: 36.18513870239258, L1: 10.24868106842041, L3: 25.936458587646484\n",
      "Current prediction:  61.52188491821289 \n",
      "\n",
      "Iteration 5801, Loss: 37.935455322265625, L1: 10.179983139038086, L3: 27.75547218322754\n",
      "Current prediction:  61.52452087402344 \n",
      "\n",
      "Iteration 5802, Loss: 34.662052154541016, L1: 10.212409019470215, L3: 24.449644088745117\n",
      "Current prediction:  61.52554702758789 \n",
      "\n",
      "Iteration 5803, Loss: 36.418434143066406, L1: 10.209512710571289, L3: 26.208919525146484\n",
      "Current prediction:  61.52396011352539 \n",
      "\n",
      "Iteration 5804, Loss: 36.51966094970703, L1: 10.157272338867188, L3: 26.362390518188477\n",
      "Current prediction:  61.47088623046875 \n",
      "\n",
      "Iteration 5805, Loss: 36.89244842529297, L1: 10.1800537109375, L3: 26.7123966217041\n",
      "Current prediction:  61.268028259277344 \n",
      "\n",
      "Iteration 5806, Loss: 36.17193603515625, L1: 10.186775207519531, L3: 25.985158920288086\n",
      "Current prediction:  60.86585998535156 \n",
      "\n",
      "Iteration 5807, Loss: 36.3145751953125, L1: 10.21081256866455, L3: 26.103761672973633\n",
      "Current prediction:  60.80107879638672 \n",
      "\n",
      "Iteration 5808, Loss: 35.93997573852539, L1: 10.234875679016113, L3: 25.70509910583496\n",
      "Current prediction:  60.79356384277344 \n",
      "\n",
      "Iteration 5809, Loss: 37.00376892089844, L1: 10.25184440612793, L3: 26.751922607421875\n",
      "Current prediction:  60.804325103759766 \n",
      "\n",
      "Iteration 5810, Loss: 36.02880096435547, L1: 10.204546928405762, L3: 25.824254989624023\n",
      "Current prediction:  60.87429428100586 \n",
      "\n",
      "Iteration 5811, Loss: 37.17060852050781, L1: 10.203168869018555, L3: 26.967437744140625\n",
      "Current prediction:  61.25716018676758 \n",
      "\n",
      "Iteration 5812, Loss: 36.101009368896484, L1: 10.162644386291504, L3: 25.938365936279297\n",
      "Current prediction:  61.52215576171875 \n",
      "\n",
      "Iteration 5813, Loss: 36.2160530090332, L1: 10.150262832641602, L3: 26.0657901763916\n",
      "Current prediction:  61.551265716552734 \n",
      "\n",
      "Iteration 5814, Loss: 36.84776306152344, L1: 10.1630277633667, L3: 26.684734344482422\n",
      "Current prediction:  61.557594299316406 \n",
      "\n",
      "Iteration 5815, Loss: 36.52485656738281, L1: 10.162345886230469, L3: 26.36250877380371\n",
      "Current prediction:  61.557464599609375 \n",
      "\n",
      "Iteration 5816, Loss: 36.53471374511719, L1: 10.169515609741211, L3: 26.365196228027344\n",
      "Current prediction:  61.55312728881836 \n",
      "\n",
      "Iteration 5817, Loss: 36.722450256347656, L1: 10.157929420471191, L3: 26.56452178955078\n",
      "Current prediction:  61.54682540893555 \n",
      "\n",
      "Iteration 5818, Loss: 35.73702621459961, L1: 10.15623664855957, L3: 25.58078956604004\n",
      "Current prediction:  61.54009246826172 \n",
      "\n",
      "Iteration 5819, Loss: 38.12467575073242, L1: 10.195555686950684, L3: 27.929119110107422\n",
      "Current prediction:  61.53045654296875 \n",
      "\n",
      "Iteration 5820, Loss: 36.39323806762695, L1: 10.207274436950684, L3: 26.185964584350586\n",
      "Current prediction:  61.520259857177734 \n",
      "\n",
      "Iteration 5821, Loss: 36.561397552490234, L1: 10.193784713745117, L3: 26.367612838745117\n",
      "Current prediction:  61.49665451049805 \n",
      "\n",
      "Iteration 5822, Loss: 35.95237350463867, L1: 10.199906349182129, L3: 25.75246810913086\n",
      "Current prediction:  61.23637008666992 \n",
      "\n",
      "Iteration 5823, Loss: 35.34174728393555, L1: 10.216158866882324, L3: 25.125587463378906\n",
      "Current prediction:  60.866172790527344 \n",
      "\n",
      "Iteration 5824, Loss: 36.4850959777832, L1: 10.263535499572754, L3: 26.221559524536133\n",
      "Current prediction:  60.78530502319336 \n",
      "\n",
      "Iteration 5825, Loss: 36.897315979003906, L1: 10.232176780700684, L3: 26.66514015197754\n",
      "Current prediction:  60.7635612487793 \n",
      "\n",
      "Iteration 5826, Loss: 36.40492630004883, L1: 10.267735481262207, L3: 26.137189865112305\n",
      "Current prediction:  60.76349639892578 \n",
      "\n",
      "Iteration 5827, Loss: 35.105140686035156, L1: 10.26827335357666, L3: 24.83686637878418\n",
      "Current prediction:  60.7774658203125 \n",
      "\n",
      "Iteration 5828, Loss: 36.1162223815918, L1: 10.245508193969727, L3: 25.87071418762207\n",
      "Current prediction:  60.91111373901367 \n",
      "\n",
      "Iteration 5829, Loss: 36.237709045410156, L1: 10.195799827575684, L3: 26.04191017150879\n",
      "Current prediction:  61.38386154174805 \n",
      "\n",
      "Iteration 5830, Loss: 36.76696014404297, L1: 10.148824691772461, L3: 26.61813735961914\n",
      "Current prediction:  61.530799865722656 \n",
      "\n",
      "Iteration 5831, Loss: 37.33953857421875, L1: 10.163923263549805, L3: 27.175613403320312\n",
      "Current prediction:  61.54962921142578 \n",
      "\n",
      "Iteration 5832, Loss: 36.146095275878906, L1: 10.191356658935547, L3: 25.954736709594727\n",
      "Current prediction:  61.55547332763672 \n",
      "\n",
      "Iteration 5833, Loss: 36.93483352661133, L1: 10.182527542114258, L3: 26.75230598449707\n",
      "Current prediction:  61.55913543701172 \n",
      "\n",
      "Iteration 5834, Loss: 36.04303741455078, L1: 10.166738510131836, L3: 25.876300811767578\n",
      "Current prediction:  61.558876037597656 \n",
      "\n",
      "Iteration 5835, Loss: 36.821937561035156, L1: 10.209217071533203, L3: 26.612722396850586\n",
      "Current prediction:  61.55835723876953 \n",
      "\n",
      "Iteration 5836, Loss: 36.520286560058594, L1: 10.197517395019531, L3: 26.322769165039062\n",
      "Current prediction:  61.55790328979492 \n",
      "\n",
      "Iteration 5837, Loss: 35.71342086791992, L1: 10.200901985168457, L3: 25.51251792907715\n",
      "Current prediction:  61.55723571777344 \n",
      "\n",
      "Iteration 5838, Loss: 36.444358825683594, L1: 10.206536293029785, L3: 26.237821578979492\n",
      "Current prediction:  61.55390167236328 \n",
      "\n",
      "Iteration 5839, Loss: 36.36164093017578, L1: 10.185078620910645, L3: 26.176563262939453\n",
      "Current prediction:  61.54847717285156 \n",
      "\n",
      "Iteration 5840, Loss: 37.564208984375, L1: 10.237579345703125, L3: 27.326627731323242\n",
      "Current prediction:  61.53972625732422 \n",
      "\n",
      "Iteration 5841, Loss: 35.59980392456055, L1: 10.223898887634277, L3: 25.375905990600586\n",
      "Current prediction:  61.5330924987793 \n",
      "\n",
      "Iteration 5842, Loss: 37.07133483886719, L1: 10.222976684570312, L3: 26.848360061645508\n",
      "Current prediction:  61.521080017089844 \n",
      "\n",
      "Iteration 5843, Loss: 36.80510711669922, L1: 10.251275062561035, L3: 26.553831100463867\n",
      "Current prediction:  61.507606506347656 \n",
      "\n",
      "Iteration 5844, Loss: 36.82768630981445, L1: 10.247554779052734, L3: 26.58013153076172\n",
      "Current prediction:  61.494510650634766 \n",
      "\n",
      "Iteration 5845, Loss: 37.1356315612793, L1: 10.265213966369629, L3: 26.870418548583984\n",
      "Current prediction:  61.48153305053711 \n",
      "\n",
      "Iteration 5846, Loss: 36.64250564575195, L1: 10.260769844055176, L3: 26.38173484802246\n",
      "Current prediction:  61.46773910522461 \n",
      "\n",
      "Iteration 5847, Loss: 36.410911560058594, L1: 10.270748138427734, L3: 26.14016342163086\n",
      "Current prediction:  61.45438003540039 \n",
      "\n",
      "Iteration 5848, Loss: 37.35801696777344, L1: 10.256529808044434, L3: 27.10148811340332\n",
      "Current prediction:  61.43766784667969 \n",
      "\n",
      "Iteration 5849, Loss: 36.12159729003906, L1: 10.271615982055664, L3: 25.84998321533203\n",
      "Current prediction:  61.367549896240234 \n",
      "\n",
      "Iteration 5850, Loss: 35.873779296875, L1: 10.297491073608398, L3: 25.5762882232666\n",
      "Current prediction:  60.99134826660156 \n",
      "\n",
      "Iteration 5851, Loss: 35.182498931884766, L1: 10.265007972717285, L3: 24.917490005493164\n",
      "Current prediction:  60.795047760009766 \n",
      "\n",
      "Iteration 5852, Loss: 35.83889389038086, L1: 10.29033088684082, L3: 25.54856300354004\n",
      "Current prediction:  60.809783935546875 \n",
      "\n",
      "Iteration 5853, Loss: 36.18558883666992, L1: 10.293375968933105, L3: 25.892213821411133\n",
      "Current prediction:  60.78035354614258 \n",
      "\n",
      "Iteration 5854, Loss: 35.86098861694336, L1: 10.278215408325195, L3: 25.582773208618164\n",
      "Current prediction:  60.863487243652344 \n",
      "\n",
      "Iteration 5855, Loss: 36.872352600097656, L1: 10.266277313232422, L3: 26.606077194213867\n",
      "Current prediction:  61.086570739746094 \n",
      "\n",
      "Iteration 5856, Loss: 35.700096130371094, L1: 10.25361442565918, L3: 25.446483612060547\n",
      "Current prediction:  61.238834381103516 \n",
      "\n",
      "Iteration 5857, Loss: 36.30229187011719, L1: 10.260441780090332, L3: 26.041851043701172\n",
      "Current prediction:  61.28582763671875 \n",
      "\n",
      "Iteration 5858, Loss: 36.69860076904297, L1: 10.270015716552734, L3: 26.428586959838867\n",
      "Current prediction:  61.31502151489258 \n",
      "\n",
      "Iteration 5859, Loss: 37.392513275146484, L1: 10.254657745361328, L3: 27.137855529785156\n",
      "Current prediction:  61.32640075683594 \n",
      "\n",
      "Iteration 5860, Loss: 36.18497848510742, L1: 10.232216835021973, L3: 25.952762603759766\n",
      "Current prediction:  61.23687744140625 \n",
      "\n",
      "Iteration 5861, Loss: 36.449161529541016, L1: 10.236397743225098, L3: 26.2127628326416\n",
      "Current prediction:  61.13862228393555 \n",
      "\n",
      "Iteration 5862, Loss: 35.401084899902344, L1: 10.24180793762207, L3: 25.15927505493164\n",
      "Current prediction:  61.17110061645508 \n",
      "\n",
      "Iteration 5863, Loss: 36.524658203125, L1: 10.231160163879395, L3: 26.29349708557129\n",
      "Current prediction:  61.22361373901367 \n",
      "\n",
      "Iteration 5864, Loss: 35.423789978027344, L1: 10.19548511505127, L3: 25.22830581665039\n",
      "Current prediction:  61.19334030151367 \n",
      "\n",
      "Iteration 5865, Loss: 36.77949142456055, L1: 10.210679054260254, L3: 26.56881332397461\n",
      "Current prediction:  61.09892654418945 \n",
      "\n",
      "Iteration 5866, Loss: 35.59345245361328, L1: 10.191313743591309, L3: 25.402137756347656\n",
      "Current prediction:  61.205474853515625 \n",
      "\n",
      "Iteration 5867, Loss: 36.52927780151367, L1: 10.163069725036621, L3: 26.366207122802734\n",
      "Current prediction:  61.355289459228516 \n",
      "\n",
      "Iteration 5868, Loss: 36.46240234375, L1: 10.160566329956055, L3: 26.301837921142578\n",
      "Current prediction:  61.45482635498047 \n",
      "\n",
      "Iteration 5869, Loss: 36.57008361816406, L1: 10.14411449432373, L3: 26.425968170166016\n",
      "Current prediction:  61.55051040649414 \n",
      "\n",
      "Iteration 5870, Loss: 36.427734375, L1: 10.140806198120117, L3: 26.286928176879883\n",
      "Current prediction:  61.57011795043945 \n",
      "\n",
      "Iteration 5871, Loss: 36.50423812866211, L1: 10.107121467590332, L3: 26.39711570739746\n",
      "Current prediction:  61.5451545715332 \n",
      "\n",
      "Iteration 5872, Loss: 36.62789535522461, L1: 10.115569114685059, L3: 26.512325286865234\n",
      "Current prediction:  61.4227409362793 \n",
      "\n",
      "Iteration 5873, Loss: 35.194793701171875, L1: 10.132709503173828, L3: 25.062082290649414\n",
      "Current prediction:  61.040184020996094 \n",
      "\n",
      "Iteration 5874, Loss: 36.3333625793457, L1: 10.130658149719238, L3: 26.20270538330078\n",
      "Current prediction:  60.89633560180664 \n",
      "\n",
      "Iteration 5875, Loss: 35.82529830932617, L1: 10.145975112915039, L3: 25.679323196411133\n",
      "Current prediction:  60.88639450073242 \n",
      "\n",
      "Iteration 5876, Loss: 36.57147216796875, L1: 10.15535831451416, L3: 26.416114807128906\n",
      "Current prediction:  60.942989349365234 \n",
      "\n",
      "Iteration 5877, Loss: 35.69731140136719, L1: 10.137939453125, L3: 25.55937385559082\n",
      "Current prediction:  61.310665130615234 \n",
      "\n",
      "Iteration 5878, Loss: 35.48699951171875, L1: 10.117993354797363, L3: 25.36900520324707\n",
      "Current prediction:  61.549625396728516 \n",
      "\n",
      "Iteration 5879, Loss: 36.82050323486328, L1: 10.108782768249512, L3: 26.711719512939453\n",
      "Current prediction:  61.5848274230957 \n",
      "\n",
      "Iteration 5880, Loss: 35.81464767456055, L1: 10.130521774291992, L3: 25.684125900268555\n",
      "Current prediction:  61.59391784667969 \n",
      "\n",
      "Iteration 5881, Loss: 36.27635955810547, L1: 10.136720657348633, L3: 26.139636993408203\n",
      "Current prediction:  61.59413146972656 \n",
      "\n",
      "Iteration 5882, Loss: 36.21754455566406, L1: 10.13177490234375, L3: 26.085769653320312\n",
      "Current prediction:  61.58951187133789 \n",
      "\n",
      "Iteration 5883, Loss: 36.91690444946289, L1: 10.13770580291748, L3: 26.779199600219727\n",
      "Current prediction:  61.584442138671875 \n",
      "\n",
      "Iteration 5884, Loss: 35.08866882324219, L1: 10.123922348022461, L3: 24.964746475219727\n",
      "Current prediction:  61.57268524169922 \n",
      "\n",
      "Iteration 5885, Loss: 36.59746170043945, L1: 10.140013694763184, L3: 26.457447052001953\n",
      "Current prediction:  61.54238510131836 \n",
      "\n",
      "Iteration 5886, Loss: 35.530029296875, L1: 10.156675338745117, L3: 25.373355865478516\n",
      "Current prediction:  61.44993209838867 \n",
      "\n",
      "Iteration 5887, Loss: 35.69512176513672, L1: 10.14223861694336, L3: 25.552885055541992\n",
      "Current prediction:  61.26166534423828 \n",
      "\n",
      "Iteration 5888, Loss: 36.16993713378906, L1: 10.158019065856934, L3: 26.011917114257812\n",
      "Current prediction:  61.16495132446289 \n",
      "\n",
      "Iteration 5889, Loss: 36.74106216430664, L1: 10.161269187927246, L3: 26.57979393005371\n",
      "Current prediction:  61.06718063354492 \n",
      "\n",
      "Iteration 5890, Loss: 35.952301025390625, L1: 10.189804077148438, L3: 25.762496948242188\n",
      "Current prediction:  60.993743896484375 \n",
      "\n",
      "Iteration 5891, Loss: 36.45705032348633, L1: 10.201163291931152, L3: 26.255887985229492\n",
      "Current prediction:  61.10453796386719 \n",
      "\n",
      "Iteration 5892, Loss: 37.42213439941406, L1: 10.194578170776367, L3: 27.227556228637695\n",
      "Current prediction:  61.300209045410156 \n",
      "\n",
      "Iteration 5893, Loss: 35.8994026184082, L1: 10.189053535461426, L3: 25.710350036621094\n",
      "Current prediction:  61.36774444580078 \n",
      "\n",
      "Iteration 5894, Loss: 35.3796272277832, L1: 10.185744285583496, L3: 25.19388198852539\n",
      "Current prediction:  61.41765213012695 \n",
      "\n",
      "Iteration 5895, Loss: 36.430233001708984, L1: 10.175995826721191, L3: 26.25423812866211\n",
      "Current prediction:  61.4362907409668 \n",
      "\n",
      "Iteration 5896, Loss: 35.3004150390625, L1: 10.147012710571289, L3: 25.153404235839844\n",
      "Current prediction:  61.23579406738281 \n",
      "\n",
      "Iteration 5897, Loss: 35.97093200683594, L1: 10.161504745483398, L3: 25.80942726135254\n",
      "Current prediction:  61.1264762878418 \n",
      "\n",
      "Iteration 5898, Loss: 36.55643081665039, L1: 10.176727294921875, L3: 26.379703521728516\n",
      "Current prediction:  61.03153991699219 \n",
      "\n",
      "Iteration 5899, Loss: 36.702247619628906, L1: 10.149386405944824, L3: 26.552860260009766\n",
      "Current prediction:  60.98976516723633 \n",
      "\n",
      "Iteration 5900, Loss: 35.87094497680664, L1: 10.200909614562988, L3: 25.67003631591797\n",
      "Current prediction:  61.09626007080078 \n",
      "\n",
      "Iteration 5901, Loss: 36.756988525390625, L1: 10.169824600219727, L3: 26.587162017822266\n",
      "Current prediction:  61.41254425048828 \n",
      "\n",
      "Iteration 5902, Loss: 35.502296447753906, L1: 10.30341911315918, L3: 25.198877334594727\n",
      "Current prediction:  61.49137496948242 \n",
      "\n",
      "Iteration 5903, Loss: 35.84882354736328, L1: 10.146316528320312, L3: 25.70250701904297\n",
      "Current prediction:  61.52206802368164 \n",
      "\n",
      "Iteration 5904, Loss: 36.350433349609375, L1: 10.160554885864258, L3: 26.189876556396484\n",
      "Current prediction:  61.54773712158203 \n",
      "\n",
      "Iteration 5905, Loss: 35.86700439453125, L1: 10.149270057678223, L3: 25.717735290527344\n",
      "Current prediction:  61.56175994873047 \n",
      "\n",
      "Iteration 5906, Loss: 36.87684631347656, L1: 10.176409721374512, L3: 26.700437545776367\n",
      "Current prediction:  61.56608581542969 \n",
      "\n",
      "Iteration 5907, Loss: 35.86717224121094, L1: 10.158538818359375, L3: 25.708633422851562\n",
      "Current prediction:  61.56753921508789 \n",
      "\n",
      "Iteration 5908, Loss: 35.28205108642578, L1: 10.133137702941895, L3: 25.148914337158203\n",
      "Current prediction:  61.568424224853516 \n",
      "\n",
      "Iteration 5909, Loss: 36.141632080078125, L1: 10.162541389465332, L3: 25.97909164428711\n",
      "Current prediction:  61.56844711303711 \n",
      "\n",
      "Iteration 5910, Loss: 36.34444808959961, L1: 10.141352653503418, L3: 26.203094482421875\n",
      "Current prediction:  61.56653594970703 \n",
      "\n",
      "Iteration 5911, Loss: 36.307132720947266, L1: 10.136330604553223, L3: 26.170801162719727\n",
      "Current prediction:  61.56061553955078 \n",
      "\n",
      "Iteration 5912, Loss: 36.08648681640625, L1: 10.179798126220703, L3: 25.90669059753418\n",
      "Current prediction:  61.53874588012695 \n",
      "\n",
      "Iteration 5913, Loss: 36.79424285888672, L1: 10.181022644042969, L3: 26.613222122192383\n",
      "Current prediction:  61.4642333984375 \n",
      "\n",
      "Iteration 5914, Loss: 36.00672149658203, L1: 10.167479515075684, L3: 25.839242935180664\n",
      "Current prediction:  61.11362075805664 \n",
      "\n",
      "Iteration 5915, Loss: 37.393062591552734, L1: 10.181239128112793, L3: 27.211822509765625\n",
      "Current prediction:  60.845367431640625 \n",
      "\n",
      "Iteration 5916, Loss: 36.704063415527344, L1: 10.206623077392578, L3: 26.497440338134766\n",
      "Current prediction:  60.79816818237305 \n",
      "\n",
      "Iteration 5917, Loss: 36.20248794555664, L1: 10.211923599243164, L3: 25.990564346313477\n",
      "Current prediction:  60.790504455566406 \n",
      "\n",
      "Iteration 5918, Loss: 36.307395935058594, L1: 10.238298416137695, L3: 26.0690975189209\n",
      "Current prediction:  60.84102249145508 \n",
      "\n",
      "Iteration 5919, Loss: 36.32724380493164, L1: 10.211848258972168, L3: 26.115394592285156\n",
      "Current prediction:  61.19908142089844 \n",
      "\n",
      "Iteration 5920, Loss: 36.50043487548828, L1: 10.182598114013672, L3: 26.317834854125977\n",
      "Current prediction:  61.468231201171875 \n",
      "\n",
      "Iteration 5921, Loss: 36.24549102783203, L1: 10.148393630981445, L3: 26.09709930419922\n",
      "Current prediction:  61.51983642578125 \n",
      "\n",
      "Iteration 5922, Loss: 37.32490921020508, L1: 10.202993392944336, L3: 27.121915817260742\n",
      "Current prediction:  61.521827697753906 \n",
      "\n",
      "Iteration 5923, Loss: 36.16896438598633, L1: 10.144684791564941, L3: 26.024280548095703\n",
      "Current prediction:  61.5155029296875 \n",
      "\n",
      "Iteration 5924, Loss: 36.5257682800293, L1: 10.180744171142578, L3: 26.34502410888672\n",
      "Current prediction:  61.520931243896484 \n",
      "\n",
      "Iteration 5925, Loss: 36.486202239990234, L1: 10.174764633178711, L3: 26.311437606811523\n",
      "Current prediction:  61.52907180786133 \n",
      "\n",
      "Iteration 5926, Loss: 36.49716567993164, L1: 10.186786651611328, L3: 26.310379028320312\n",
      "Current prediction:  61.52703857421875 \n",
      "\n",
      "Iteration 5927, Loss: 35.79967498779297, L1: 10.17446231842041, L3: 25.625213623046875\n",
      "Current prediction:  61.52878189086914 \n",
      "\n",
      "Iteration 5928, Loss: 36.61507034301758, L1: 10.196257591247559, L3: 26.418811798095703\n",
      "Current prediction:  61.5277099609375 \n",
      "\n",
      "Iteration 5929, Loss: 36.79175567626953, L1: 10.178454399108887, L3: 26.61330223083496\n",
      "Current prediction:  61.51601028442383 \n",
      "\n",
      "Iteration 5930, Loss: 36.615421295166016, L1: 10.147822380065918, L3: 26.467599868774414\n",
      "Current prediction:  61.495155334472656 \n",
      "\n",
      "Iteration 5931, Loss: 37.57421112060547, L1: 10.182096481323242, L3: 27.39211654663086\n",
      "Current prediction:  61.44663619995117 \n",
      "\n",
      "Iteration 5932, Loss: 36.70389175415039, L1: 10.144352912902832, L3: 26.559537887573242\n",
      "Current prediction:  61.326416015625 \n",
      "\n",
      "Iteration 5933, Loss: 37.36561584472656, L1: 10.196305274963379, L3: 27.169309616088867\n",
      "Current prediction:  61.26297378540039 \n",
      "\n",
      "Iteration 5934, Loss: 36.46647644042969, L1: 10.162439346313477, L3: 26.304035186767578\n",
      "Current prediction:  61.33311462402344 \n",
      "\n",
      "Iteration 5935, Loss: 35.8451042175293, L1: 10.17717170715332, L3: 25.667932510375977\n",
      "Current prediction:  61.422054290771484 \n",
      "\n",
      "Iteration 5936, Loss: 35.782344818115234, L1: 10.197600364685059, L3: 25.584745407104492\n",
      "Current prediction:  61.483341217041016 \n",
      "\n",
      "Iteration 5937, Loss: 36.566688537597656, L1: 10.155242919921875, L3: 26.411447525024414\n",
      "Current prediction:  61.49655532836914 \n",
      "\n",
      "Iteration 5938, Loss: 36.001953125, L1: 10.184020042419434, L3: 25.81793212890625\n",
      "Current prediction:  61.47067642211914 \n",
      "\n",
      "Iteration 5939, Loss: 36.0464973449707, L1: 10.17379379272461, L3: 25.872703552246094\n",
      "Current prediction:  61.39347457885742 \n",
      "\n",
      "Iteration 5940, Loss: 36.58946990966797, L1: 10.170662879943848, L3: 26.418806076049805\n",
      "Current prediction:  61.338096618652344 \n",
      "\n",
      "Iteration 5941, Loss: 35.63692092895508, L1: 10.156014442443848, L3: 25.480905532836914\n",
      "Current prediction:  61.25200271606445 \n",
      "\n",
      "Iteration 5942, Loss: 35.916297912597656, L1: 10.171730995178223, L3: 25.744565963745117\n",
      "Current prediction:  60.948665618896484 \n",
      "\n",
      "Iteration 5943, Loss: 35.596107482910156, L1: 10.161032676696777, L3: 25.435075759887695\n",
      "Current prediction:  60.801151275634766 \n",
      "\n",
      "Iteration 5944, Loss: 35.53559875488281, L1: 10.205909729003906, L3: 25.329687118530273\n",
      "Current prediction:  60.793704986572266 \n",
      "\n",
      "Iteration 5945, Loss: 36.76192855834961, L1: 10.240341186523438, L3: 26.521587371826172\n",
      "Current prediction:  60.7971076965332 \n",
      "\n",
      "Iteration 5946, Loss: 35.69770812988281, L1: 10.239742279052734, L3: 25.457965850830078\n",
      "Current prediction:  60.806758880615234 \n",
      "\n",
      "Iteration 5947, Loss: 36.073463439941406, L1: 10.208151817321777, L3: 25.865312576293945\n",
      "Current prediction:  60.86103439331055 \n",
      "\n",
      "Iteration 5948, Loss: 37.194183349609375, L1: 10.192327499389648, L3: 27.001855850219727\n",
      "Current prediction:  61.04764175415039 \n",
      "\n",
      "Iteration 5949, Loss: 36.2350959777832, L1: 10.138411521911621, L3: 26.0966854095459\n",
      "Current prediction:  61.40642547607422 \n",
      "\n",
      "Iteration 5950, Loss: 36.26969909667969, L1: 10.117786407470703, L3: 26.151912689208984\n",
      "Current prediction:  61.53349685668945 \n",
      "\n",
      "Iteration 5951, Loss: 35.315391540527344, L1: 10.134075164794922, L3: 25.181316375732422\n",
      "Current prediction:  61.568885803222656 \n",
      "\n",
      "Iteration 5952, Loss: 36.654441833496094, L1: 10.117681503295898, L3: 26.536762237548828\n",
      "Current prediction:  61.579681396484375 \n",
      "\n",
      "Iteration 5953, Loss: 36.676055908203125, L1: 10.135489463806152, L3: 26.540565490722656\n",
      "Current prediction:  61.58185958862305 \n",
      "\n",
      "Iteration 5954, Loss: 37.056129455566406, L1: 10.141758918762207, L3: 26.914369583129883\n",
      "Current prediction:  61.576881408691406 \n",
      "\n",
      "Iteration 5955, Loss: 35.92362594604492, L1: 10.13064193725586, L3: 25.792984008789062\n",
      "Current prediction:  61.56836700439453 \n",
      "\n",
      "Iteration 5956, Loss: 36.55665588378906, L1: 10.12840461730957, L3: 26.428253173828125\n",
      "Current prediction:  61.55801773071289 \n",
      "\n",
      "Iteration 5957, Loss: 34.58572769165039, L1: 10.138742446899414, L3: 24.446985244750977\n",
      "Current prediction:  61.54336166381836 \n",
      "\n",
      "Iteration 5958, Loss: 35.724544525146484, L1: 10.155354499816895, L3: 25.569189071655273\n",
      "Current prediction:  61.50196075439453 \n",
      "\n",
      "Iteration 5959, Loss: 36.038787841796875, L1: 10.14016342163086, L3: 25.898624420166016\n",
      "Current prediction:  61.45737075805664 \n",
      "\n",
      "Iteration 5960, Loss: 36.35546112060547, L1: 10.148094177246094, L3: 26.207368850708008\n",
      "Current prediction:  61.35434341430664 \n",
      "\n",
      "Iteration 5961, Loss: 35.814361572265625, L1: 10.16746711730957, L3: 25.646894454956055\n",
      "Current prediction:  61.2249755859375 \n",
      "\n",
      "Iteration 5962, Loss: 36.013771057128906, L1: 10.173389434814453, L3: 25.840383529663086\n",
      "Current prediction:  61.19588088989258 \n",
      "\n",
      "Iteration 5963, Loss: 35.270851135253906, L1: 10.146316528320312, L3: 25.124536514282227\n",
      "Current prediction:  61.15218734741211 \n",
      "\n",
      "Iteration 5964, Loss: 35.49309539794922, L1: 10.167229652404785, L3: 25.325864791870117\n",
      "Current prediction:  61.272212982177734 \n",
      "\n",
      "Iteration 5965, Loss: 36.35002899169922, L1: 10.158565521240234, L3: 26.191463470458984\n",
      "Current prediction:  61.2860221862793 \n",
      "\n",
      "Iteration 5966, Loss: 36.43918228149414, L1: 10.148995399475098, L3: 26.29018783569336\n",
      "Current prediction:  61.46284866333008 \n",
      "\n",
      "Iteration 5967, Loss: 36.32664489746094, L1: 10.170422554016113, L3: 26.15622329711914\n",
      "Current prediction:  61.505348205566406 \n",
      "\n",
      "Iteration 5968, Loss: 36.95189666748047, L1: 10.162271499633789, L3: 26.78962516784668\n",
      "Current prediction:  61.48256301879883 \n",
      "\n",
      "Iteration 5969, Loss: 36.45330810546875, L1: 10.153206825256348, L3: 26.30010223388672\n",
      "Current prediction:  61.465126037597656 \n",
      "\n",
      "Iteration 5970, Loss: 35.988059997558594, L1: 10.179128646850586, L3: 25.808929443359375\n",
      "Current prediction:  61.390350341796875 \n",
      "\n",
      "Iteration 5971, Loss: 36.08431625366211, L1: 10.167943954467773, L3: 25.916372299194336\n",
      "Current prediction:  61.287593841552734 \n",
      "\n",
      "Iteration 5972, Loss: 36.27476119995117, L1: 10.191076278686523, L3: 26.08368492126465\n",
      "Current prediction:  61.18260192871094 \n",
      "\n",
      "Iteration 5973, Loss: 36.12109375, L1: 10.176080703735352, L3: 25.94501304626465\n",
      "Current prediction:  61.153953552246094 \n",
      "\n",
      "Iteration 5974, Loss: 35.87165451049805, L1: 10.171586036682129, L3: 25.700069427490234\n",
      "Current prediction:  61.20941162109375 \n",
      "\n",
      "Iteration 5975, Loss: 35.61796569824219, L1: 10.17097282409668, L3: 25.446990966796875\n",
      "Current prediction:  61.36967086791992 \n",
      "\n",
      "Iteration 5976, Loss: 36.234275817871094, L1: 10.140995025634766, L3: 26.093278884887695\n",
      "Current prediction:  61.42258071899414 \n",
      "\n",
      "Iteration 5977, Loss: 35.58576965332031, L1: 10.16231632232666, L3: 25.423452377319336\n",
      "Current prediction:  61.46648025512695 \n",
      "\n",
      "Iteration 5978, Loss: 35.622947692871094, L1: 10.188116073608398, L3: 25.434831619262695\n",
      "Current prediction:  61.49224090576172 \n",
      "\n",
      "Iteration 5979, Loss: 36.36692428588867, L1: 10.15012264251709, L3: 26.2168025970459\n",
      "Current prediction:  61.47346496582031 \n",
      "\n",
      "Iteration 5980, Loss: 37.12028503417969, L1: 10.16253662109375, L3: 26.95775032043457\n",
      "Current prediction:  61.342613220214844 \n",
      "\n",
      "Iteration 5981, Loss: 36.57319641113281, L1: 10.16191291809082, L3: 26.411283493041992\n",
      "Current prediction:  61.09919357299805 \n",
      "\n",
      "Iteration 5982, Loss: 36.62327575683594, L1: 10.154679298400879, L3: 26.468597412109375\n",
      "Current prediction:  61.045997619628906 \n",
      "\n",
      "Iteration 5983, Loss: 36.47550964355469, L1: 10.124661445617676, L3: 26.350849151611328\n",
      "Current prediction:  61.260494232177734 \n",
      "\n",
      "Iteration 5984, Loss: 36.31610107421875, L1: 10.139322280883789, L3: 26.17677879333496\n",
      "Current prediction:  61.45055389404297 \n",
      "\n",
      "Iteration 5985, Loss: 36.45103073120117, L1: 10.152595520019531, L3: 26.29843521118164\n",
      "Current prediction:  61.522891998291016 \n",
      "\n",
      "Iteration 5986, Loss: 36.677734375, L1: 10.146345138549805, L3: 26.531391143798828\n",
      "Current prediction:  61.48994445800781 \n",
      "\n",
      "Iteration 5987, Loss: 37.28953552246094, L1: 10.147467613220215, L3: 27.14206886291504\n",
      "Current prediction:  61.22346115112305 \n",
      "\n",
      "Iteration 5988, Loss: 36.04252624511719, L1: 10.132259368896484, L3: 25.91026496887207\n",
      "Current prediction:  60.92900848388672 \n",
      "\n",
      "Iteration 5989, Loss: 36.25121307373047, L1: 10.191364288330078, L3: 26.059850692749023\n",
      "Current prediction:  60.83500671386719 \n",
      "\n",
      "Iteration 5990, Loss: 35.16156005859375, L1: 10.190391540527344, L3: 24.971168518066406\n",
      "Current prediction:  60.85468673706055 \n",
      "\n",
      "Iteration 5991, Loss: 35.43457794189453, L1: 10.178004264831543, L3: 25.256574630737305\n",
      "Current prediction:  60.881752014160156 \n",
      "\n",
      "Iteration 5992, Loss: 36.73295593261719, L1: 10.171574592590332, L3: 26.56138038635254\n",
      "Current prediction:  61.08687973022461 \n",
      "\n",
      "Iteration 5993, Loss: 36.88930892944336, L1: 10.11221981048584, L3: 26.777090072631836\n",
      "Current prediction:  61.42292022705078 \n",
      "\n",
      "Iteration 5994, Loss: 35.438446044921875, L1: 10.156021118164062, L3: 25.28242301940918\n",
      "Current prediction:  61.552162170410156 \n",
      "\n",
      "Iteration 5995, Loss: 36.4864616394043, L1: 10.150471687316895, L3: 26.33599090576172\n",
      "Current prediction:  61.561893463134766 \n",
      "\n",
      "Iteration 5996, Loss: 35.927642822265625, L1: 10.158666610717773, L3: 25.768978118896484\n",
      "Current prediction:  61.560646057128906 \n",
      "\n",
      "Iteration 5997, Loss: 36.16832733154297, L1: 10.1692533493042, L3: 25.999074935913086\n",
      "Current prediction:  61.5572509765625 \n",
      "\n",
      "Iteration 5998, Loss: 37.13627243041992, L1: 10.166752815246582, L3: 26.969518661499023\n",
      "Current prediction:  61.55173873901367 \n",
      "\n",
      "â†³ LR reduced to 5.0e-04 at iteration 6000 \n",
      "\n",
      "Iteration 5999, Loss: 36.16107940673828, L1: 10.189663887023926, L3: 25.971416473388672\n",
      "Current prediction:  61.54544448852539 \n",
      "\n",
      "Iteration 6000, Loss: 36.229923248291016, L1: 10.190756797790527, L3: 26.039167404174805\n",
      "Current prediction:  61.539188385009766 \n",
      "\n",
      "Iteration 6001, Loss: 36.40092468261719, L1: 10.182351112365723, L3: 26.21857261657715\n",
      "Current prediction:  61.53224563598633 \n",
      "\n",
      "Iteration 6002, Loss: 37.25889205932617, L1: 10.216651916503906, L3: 27.042240142822266\n",
      "Current prediction:  61.5238037109375 \n",
      "\n",
      "Iteration 6003, Loss: 36.502960205078125, L1: 10.229157447814941, L3: 26.2738037109375\n",
      "Current prediction:  61.514007568359375 \n",
      "\n",
      "Iteration 6004, Loss: 37.057003021240234, L1: 10.226064682006836, L3: 26.8309383392334\n",
      "Current prediction:  61.50373840332031 \n",
      "\n",
      "Iteration 6005, Loss: 36.78254318237305, L1: 10.224403381347656, L3: 26.55813980102539\n",
      "Current prediction:  61.493202209472656 \n",
      "\n",
      "Iteration 6006, Loss: 36.80448532104492, L1: 10.274420738220215, L3: 26.530065536499023\n",
      "Current prediction:  61.48356628417969 \n",
      "\n",
      "Iteration 6007, Loss: 36.59495544433594, L1: 10.266090393066406, L3: 26.32886505126953\n",
      "Current prediction:  61.47331619262695 \n",
      "\n",
      "Iteration 6008, Loss: 35.50566101074219, L1: 10.226887702941895, L3: 25.27877426147461\n",
      "Current prediction:  61.464576721191406 \n",
      "\n",
      "Iteration 6009, Loss: 36.25513458251953, L1: 10.265647888183594, L3: 25.989484786987305\n",
      "Current prediction:  61.45718765258789 \n",
      "\n",
      "Iteration 6010, Loss: 36.937198638916016, L1: 10.239566802978516, L3: 26.6976318359375\n",
      "Current prediction:  61.449703216552734 \n",
      "\n",
      "Iteration 6011, Loss: 36.08974075317383, L1: 10.221335411071777, L3: 25.868406295776367\n",
      "Current prediction:  61.43700408935547 \n",
      "\n",
      "Iteration 6012, Loss: 35.97296142578125, L1: 10.244819641113281, L3: 25.72814178466797\n",
      "Current prediction:  61.21508026123047 \n",
      "\n",
      "Iteration 6013, Loss: 36.674110412597656, L1: 10.247467041015625, L3: 26.42664337158203\n",
      "Current prediction:  60.7856559753418 \n",
      "\n",
      "Iteration 6014, Loss: 35.93037033081055, L1: 10.250284194946289, L3: 25.680086135864258\n",
      "Current prediction:  60.71363830566406 \n",
      "\n",
      "Iteration 6015, Loss: 36.079105377197266, L1: 10.25578784942627, L3: 25.82331657409668\n",
      "Current prediction:  60.704986572265625 \n",
      "\n",
      "Iteration 6016, Loss: 35.74681091308594, L1: 10.266541481018066, L3: 25.480270385742188\n",
      "Current prediction:  60.71257781982422 \n",
      "\n",
      "Iteration 6017, Loss: 37.338417053222656, L1: 10.286787033081055, L3: 27.05162811279297\n",
      "Current prediction:  60.72588348388672 \n",
      "\n",
      "Iteration 6018, Loss: 37.48263168334961, L1: 10.303150177001953, L3: 27.179481506347656\n",
      "Current prediction:  60.73752212524414 \n",
      "\n",
      "Iteration 6019, Loss: 37.1165657043457, L1: 10.269492149353027, L3: 26.84707260131836\n",
      "Current prediction:  60.75388717651367 \n",
      "\n",
      "Iteration 6020, Loss: 36.42014694213867, L1: 10.248156547546387, L3: 26.1719913482666\n",
      "Current prediction:  60.80803680419922 \n",
      "\n",
      "Iteration 6021, Loss: 35.445377349853516, L1: 10.176271438598633, L3: 25.269105911254883\n",
      "Current prediction:  61.257057189941406 \n",
      "\n",
      "Iteration 6022, Loss: 35.81110382080078, L1: 10.177228927612305, L3: 25.63387680053711\n",
      "Current prediction:  61.52059555053711 \n",
      "\n",
      "Iteration 6023, Loss: 37.06242752075195, L1: 10.171835899353027, L3: 26.89059066772461\n",
      "Current prediction:  61.55010223388672 \n",
      "\n",
      "Iteration 6024, Loss: 37.013282775878906, L1: 10.138448715209961, L3: 26.874834060668945\n",
      "Current prediction:  61.56401824951172 \n",
      "\n",
      "Iteration 6025, Loss: 36.68492889404297, L1: 10.14106559753418, L3: 26.543865203857422\n",
      "Current prediction:  61.570159912109375 \n",
      "\n",
      "Iteration 6026, Loss: 37.01502227783203, L1: 10.146224975585938, L3: 26.868799209594727\n",
      "Current prediction:  61.5732536315918 \n",
      "\n",
      "Iteration 6027, Loss: 37.35426330566406, L1: 10.140289306640625, L3: 27.21397590637207\n",
      "Current prediction:  61.5753173828125 \n",
      "\n",
      "Iteration 6028, Loss: 36.787841796875, L1: 10.138544082641602, L3: 26.64929962158203\n",
      "Current prediction:  61.57207107543945 \n",
      "\n",
      "Iteration 6029, Loss: 36.01701736450195, L1: 10.151603698730469, L3: 25.865413665771484\n",
      "Current prediction:  61.542449951171875 \n",
      "\n",
      "Iteration 6030, Loss: 36.489036560058594, L1: 10.11495590209961, L3: 26.37407875061035\n",
      "Current prediction:  61.35177993774414 \n",
      "\n",
      "Iteration 6031, Loss: 35.649925231933594, L1: 10.144989013671875, L3: 25.504934310913086\n",
      "Current prediction:  61.038997650146484 \n",
      "\n",
      "Iteration 6032, Loss: 36.983951568603516, L1: 10.132346153259277, L3: 26.851606369018555\n",
      "Current prediction:  60.8363151550293 \n",
      "\n",
      "Iteration 6033, Loss: 36.2297477722168, L1: 10.156462669372559, L3: 26.073284149169922\n",
      "Current prediction:  60.80873107910156 \n",
      "\n",
      "Iteration 6034, Loss: 36.84907913208008, L1: 10.191510200500488, L3: 26.657567977905273\n",
      "Current prediction:  60.7934455871582 \n",
      "\n",
      "Iteration 6035, Loss: 36.63639450073242, L1: 10.212041854858398, L3: 26.424352645874023\n",
      "Current prediction:  60.81852722167969 \n",
      "\n",
      "Iteration 6036, Loss: 37.40023422241211, L1: 10.205418586730957, L3: 27.194814682006836\n",
      "Current prediction:  61.12097930908203 \n",
      "\n",
      "Iteration 6037, Loss: 35.74277877807617, L1: 10.192512512207031, L3: 25.55026626586914\n",
      "Current prediction:  61.50238800048828 \n",
      "\n",
      "Iteration 6038, Loss: 36.68489074707031, L1: 10.153711318969727, L3: 26.531177520751953\n",
      "Current prediction:  61.526390075683594 \n",
      "\n",
      "Iteration 6039, Loss: 37.15994644165039, L1: 10.17341136932373, L3: 26.986534118652344\n",
      "Current prediction:  61.526824951171875 \n",
      "\n",
      "Iteration 6040, Loss: 36.5888671875, L1: 10.146452903747559, L3: 26.442415237426758\n",
      "Current prediction:  61.52320861816406 \n",
      "\n",
      "Iteration 6041, Loss: 36.6125373840332, L1: 10.181292533874512, L3: 26.431243896484375\n",
      "Current prediction:  61.51536178588867 \n",
      "\n",
      "Iteration 6042, Loss: 35.907318115234375, L1: 10.203268051147461, L3: 25.70404815673828\n",
      "Current prediction:  61.500335693359375 \n",
      "\n",
      "Iteration 6043, Loss: 36.40867233276367, L1: 10.22249698638916, L3: 26.186174392700195\n",
      "Current prediction:  61.39128875732422 \n",
      "\n",
      "Iteration 6044, Loss: 36.00210952758789, L1: 10.187457084655762, L3: 25.814651489257812\n",
      "Current prediction:  60.994449615478516 \n",
      "\n",
      "Iteration 6045, Loss: 35.687896728515625, L1: 10.178376197814941, L3: 25.509519577026367\n",
      "Current prediction:  60.799800872802734 \n",
      "\n",
      "Iteration 6046, Loss: 35.95672607421875, L1: 10.241127014160156, L3: 25.715599060058594\n",
      "Current prediction:  60.764793395996094 \n",
      "\n",
      "Iteration 6047, Loss: 36.54294204711914, L1: 10.235400199890137, L3: 26.30754280090332\n",
      "Current prediction:  60.7873420715332 \n",
      "\n",
      "Iteration 6048, Loss: 36.64640808105469, L1: 10.241455078125, L3: 26.404953002929688\n",
      "Current prediction:  60.869503021240234 \n",
      "\n",
      "Iteration 6049, Loss: 35.261680603027344, L1: 10.191276550292969, L3: 25.070402145385742\n",
      "Current prediction:  61.162879943847656 \n",
      "\n",
      "Iteration 6050, Loss: 35.88288497924805, L1: 10.176482200622559, L3: 25.706403732299805\n",
      "Current prediction:  61.422908782958984 \n",
      "\n",
      "Iteration 6051, Loss: 36.13063049316406, L1: 10.158809661865234, L3: 25.97182273864746\n",
      "Current prediction:  61.497398376464844 \n",
      "\n",
      "Iteration 6052, Loss: 36.20444107055664, L1: 10.148177146911621, L3: 26.056264877319336\n",
      "Current prediction:  61.51042175292969 \n",
      "\n",
      "Iteration 6053, Loss: 36.3865966796875, L1: 10.175042152404785, L3: 26.21155548095703\n",
      "Current prediction:  61.45600891113281 \n",
      "\n",
      "Iteration 6054, Loss: 36.6651496887207, L1: 10.177029609680176, L3: 26.48811912536621\n",
      "Current prediction:  61.24143600463867 \n",
      "\n",
      "Iteration 6055, Loss: 36.419830322265625, L1: 10.193901062011719, L3: 26.22593116760254\n",
      "Current prediction:  60.92854309082031 \n",
      "\n",
      "Iteration 6056, Loss: 36.00873947143555, L1: 10.161696434020996, L3: 25.847042083740234\n",
      "Current prediction:  60.78744125366211 \n",
      "\n",
      "Iteration 6057, Loss: 36.186580657958984, L1: 10.22917366027832, L3: 25.957406997680664\n",
      "Current prediction:  60.77933120727539 \n",
      "\n",
      "Iteration 6058, Loss: 36.314674377441406, L1: 10.23390007019043, L3: 26.080774307250977\n",
      "Current prediction:  60.842437744140625 \n",
      "\n",
      "Iteration 6059, Loss: 35.40037536621094, L1: 10.202526092529297, L3: 25.197847366333008\n",
      "Current prediction:  61.13198471069336 \n",
      "\n",
      "Iteration 6060, Loss: 35.85490417480469, L1: 10.153631210327148, L3: 25.701271057128906\n",
      "Current prediction:  61.45869827270508 \n",
      "\n",
      "Iteration 6061, Loss: 36.48259735107422, L1: 10.153367042541504, L3: 26.32923126220703\n",
      "Current prediction:  61.47661209106445 \n",
      "\n",
      "Iteration 6062, Loss: 35.62142562866211, L1: 10.162192344665527, L3: 25.4592342376709\n",
      "Current prediction:  61.478607177734375 \n",
      "\n",
      "Iteration 6063, Loss: 36.42739486694336, L1: 10.162347793579102, L3: 26.265047073364258\n",
      "Current prediction:  61.46302032470703 \n",
      "\n",
      "Iteration 6064, Loss: 36.32577896118164, L1: 10.163597106933594, L3: 26.162181854248047\n",
      "Current prediction:  61.30845642089844 \n",
      "\n",
      "Iteration 6065, Loss: 36.32659912109375, L1: 10.162942886352539, L3: 26.163658142089844\n",
      "Current prediction:  61.00682067871094 \n",
      "\n",
      "Iteration 6066, Loss: 36.80113220214844, L1: 10.167732238769531, L3: 26.633399963378906\n",
      "Current prediction:  60.8525390625 \n",
      "\n",
      "Iteration 6067, Loss: 36.38761520385742, L1: 10.173623085021973, L3: 26.213991165161133\n",
      "Current prediction:  60.81727600097656 \n",
      "\n",
      "Iteration 6068, Loss: 35.994049072265625, L1: 10.184804916381836, L3: 25.809246063232422\n",
      "Current prediction:  60.81978225708008 \n",
      "\n",
      "Iteration 6069, Loss: 35.826168060302734, L1: 10.196276664733887, L3: 25.62989044189453\n",
      "Current prediction:  60.97118377685547 \n",
      "\n",
      "Iteration 6070, Loss: 36.013755798339844, L1: 10.152368545532227, L3: 25.86138916015625\n",
      "Current prediction:  61.42308807373047 \n",
      "\n",
      "Iteration 6071, Loss: 36.328975677490234, L1: 10.1545991897583, L3: 26.174375534057617\n",
      "Current prediction:  61.53506088256836 \n",
      "\n",
      "Iteration 6072, Loss: 37.066402435302734, L1: 10.171769142150879, L3: 26.894634246826172\n",
      "Current prediction:  61.54502487182617 \n",
      "\n",
      "Iteration 6073, Loss: 36.14397430419922, L1: 10.149714469909668, L3: 25.994258880615234\n",
      "Current prediction:  61.54277420043945 \n",
      "\n",
      "Iteration 6074, Loss: 35.63496398925781, L1: 10.178882598876953, L3: 25.45608139038086\n",
      "Current prediction:  61.53853225708008 \n",
      "\n",
      "Iteration 6075, Loss: 36.1999397277832, L1: 10.182442665100098, L3: 26.01749610900879\n",
      "Current prediction:  61.53388595581055 \n",
      "\n",
      "Iteration 6076, Loss: 36.08100891113281, L1: 10.17457389831543, L3: 25.906435012817383\n",
      "Current prediction:  61.51766586303711 \n",
      "\n",
      "Iteration 6077, Loss: 37.01150894165039, L1: 10.15833568572998, L3: 26.853174209594727\n",
      "Current prediction:  61.347686767578125 \n",
      "\n",
      "Iteration 6078, Loss: 36.11552429199219, L1: 10.15495491027832, L3: 25.960569381713867\n",
      "Current prediction:  60.834327697753906 \n",
      "\n",
      "Iteration 6079, Loss: 35.74737548828125, L1: 10.191551208496094, L3: 25.555824279785156\n",
      "Current prediction:  60.793731689453125 \n",
      "\n",
      "Iteration 6080, Loss: 36.46145248413086, L1: 10.209846496582031, L3: 26.251605987548828\n",
      "Current prediction:  60.86530685424805 \n",
      "\n",
      "Iteration 6081, Loss: 35.67182922363281, L1: 10.179195404052734, L3: 25.492631912231445\n",
      "Current prediction:  61.153621673583984 \n",
      "\n",
      "Iteration 6082, Loss: 35.312904357910156, L1: 10.16687297821045, L3: 25.14603042602539\n",
      "Current prediction:  61.34452438354492 \n",
      "\n",
      "Iteration 6083, Loss: 36.26445770263672, L1: 10.149055480957031, L3: 26.11540412902832\n",
      "Current prediction:  61.429222106933594 \n",
      "\n",
      "Iteration 6084, Loss: 36.6703987121582, L1: 10.140449523925781, L3: 26.529949188232422\n",
      "Current prediction:  61.383235931396484 \n",
      "\n",
      "Iteration 6085, Loss: 35.752559661865234, L1: 10.150651931762695, L3: 25.60190773010254\n",
      "Current prediction:  61.252010345458984 \n",
      "\n",
      "Iteration 6086, Loss: 35.9556770324707, L1: 10.132145881652832, L3: 25.823532104492188\n",
      "Current prediction:  61.08768844604492 \n",
      "\n",
      "Iteration 6087, Loss: 37.44308090209961, L1: 10.131278038024902, L3: 27.311803817749023\n",
      "Current prediction:  61.03888702392578 \n",
      "\n",
      "Iteration 6088, Loss: 35.97951126098633, L1: 10.137950897216797, L3: 25.84156036376953\n",
      "Current prediction:  60.86106491088867 \n",
      "\n",
      "Iteration 6089, Loss: 35.541419982910156, L1: 10.163753509521484, L3: 25.37766456604004\n",
      "Current prediction:  60.83699035644531 \n",
      "\n",
      "Iteration 6090, Loss: 36.47146987915039, L1: 10.174546241760254, L3: 26.296924591064453\n",
      "Current prediction:  60.953758239746094 \n",
      "\n",
      "Iteration 6091, Loss: 36.88800811767578, L1: 10.134799003601074, L3: 26.753210067749023\n",
      "Current prediction:  61.41728973388672 \n",
      "\n",
      "Iteration 6092, Loss: 36.04218292236328, L1: 10.140380859375, L3: 25.90180206298828\n",
      "Current prediction:  61.5579948425293 \n",
      "\n",
      "Iteration 6093, Loss: 36.08500671386719, L1: 10.139412879943848, L3: 25.945594787597656\n",
      "Current prediction:  61.5703125 \n",
      "\n",
      "Iteration 6094, Loss: 36.32753372192383, L1: 10.131437301635742, L3: 26.196096420288086\n",
      "Current prediction:  61.56916427612305 \n",
      "\n",
      "Iteration 6095, Loss: 36.07134246826172, L1: 10.144766807556152, L3: 25.92657470703125\n",
      "Current prediction:  61.56526565551758 \n",
      "\n",
      "Iteration 6096, Loss: 36.752376556396484, L1: 10.165968894958496, L3: 26.586406707763672\n",
      "Current prediction:  61.55671691894531 \n",
      "\n",
      "Iteration 6097, Loss: 36.88261032104492, L1: 10.132972717285156, L3: 26.749637603759766\n",
      "Current prediction:  61.54576110839844 \n",
      "\n",
      "Iteration 6098, Loss: 35.66227722167969, L1: 10.129478454589844, L3: 25.532798767089844\n",
      "Current prediction:  61.494590759277344 \n",
      "\n",
      "Iteration 6099, Loss: 36.99312973022461, L1: 10.157774925231934, L3: 26.83535385131836\n",
      "Current prediction:  61.123233795166016 \n",
      "\n",
      "Iteration 6100, Loss: 35.487972259521484, L1: 10.18369197845459, L3: 25.30428123474121\n",
      "Current prediction:  60.8062629699707 \n",
      "\n",
      "Iteration 6101, Loss: 36.44719314575195, L1: 10.191778182983398, L3: 26.255414962768555\n",
      "Current prediction:  60.75935745239258 \n",
      "\n",
      "Iteration 6102, Loss: 36.95496368408203, L1: 10.258742332458496, L3: 26.69622230529785\n",
      "Current prediction:  60.75649642944336 \n",
      "\n",
      "Iteration 6103, Loss: 36.02500915527344, L1: 10.247123718261719, L3: 25.777883529663086\n",
      "Current prediction:  60.75790786743164 \n",
      "\n",
      "Iteration 6104, Loss: 36.367637634277344, L1: 10.278932571411133, L3: 26.088703155517578\n",
      "Current prediction:  60.762691497802734 \n",
      "\n",
      "Iteration 6105, Loss: 37.10913848876953, L1: 10.302895545959473, L3: 26.806243896484375\n",
      "Current prediction:  60.769351959228516 \n",
      "\n",
      "Iteration 6106, Loss: 36.56793975830078, L1: 10.246404647827148, L3: 26.321535110473633\n",
      "Current prediction:  60.779972076416016 \n",
      "\n",
      "Iteration 6107, Loss: 35.64840316772461, L1: 10.2290620803833, L3: 25.419340133666992\n",
      "Current prediction:  60.80663299560547 \n",
      "\n",
      "Iteration 6108, Loss: 36.325199127197266, L1: 10.18051528930664, L3: 26.144683837890625\n",
      "Current prediction:  61.00771713256836 \n",
      "\n",
      "Iteration 6109, Loss: 36.14378356933594, L1: 10.145739555358887, L3: 25.998043060302734\n",
      "Current prediction:  61.48943328857422 \n",
      "\n",
      "Iteration 6110, Loss: 36.21514129638672, L1: 10.109273910522461, L3: 26.105867385864258\n",
      "Current prediction:  61.55874252319336 \n",
      "\n",
      "Iteration 6111, Loss: 36.285369873046875, L1: 10.113368034362793, L3: 26.1720027923584\n",
      "Current prediction:  61.580596923828125 \n",
      "\n",
      "Iteration 6112, Loss: 36.65958786010742, L1: 10.088030815124512, L3: 26.571556091308594\n",
      "Current prediction:  61.59549331665039 \n",
      "\n",
      "Iteration 6113, Loss: 36.84904479980469, L1: 10.107450485229492, L3: 26.741594314575195\n",
      "Current prediction:  61.599822998046875 \n",
      "\n",
      "Iteration 6114, Loss: 36.43178176879883, L1: 10.102173805236816, L3: 26.329607009887695\n",
      "Current prediction:  61.60103225708008 \n",
      "\n",
      "Iteration 6115, Loss: 35.58637619018555, L1: 10.113205909729004, L3: 25.47317123413086\n",
      "Current prediction:  61.59716033935547 \n",
      "\n",
      "Iteration 6116, Loss: 36.1873893737793, L1: 10.091456413269043, L3: 26.09593391418457\n",
      "Current prediction:  61.58261489868164 \n",
      "\n",
      "Iteration 6117, Loss: 36.45368957519531, L1: 10.097712516784668, L3: 26.35597801208496\n",
      "Current prediction:  61.549461364746094 \n",
      "\n",
      "Iteration 6118, Loss: 35.64146423339844, L1: 10.08133602142334, L3: 25.56012725830078\n",
      "Current prediction:  61.38212966918945 \n",
      "\n",
      "Iteration 6119, Loss: 36.71376419067383, L1: 10.082379341125488, L3: 26.631383895874023\n",
      "Current prediction:  60.9432258605957 \n",
      "\n",
      "Iteration 6120, Loss: 36.3564338684082, L1: 10.118612289428711, L3: 26.237821578979492\n",
      "Current prediction:  60.838775634765625 \n",
      "\n",
      "Iteration 6121, Loss: 35.67375564575195, L1: 10.14406681060791, L3: 25.529687881469727\n",
      "Current prediction:  60.84191131591797 \n",
      "\n",
      "Iteration 6122, Loss: 35.81112289428711, L1: 10.167927742004395, L3: 25.64319610595703\n",
      "Current prediction:  60.83955764770508 \n",
      "\n",
      "Iteration 6123, Loss: 38.0994758605957, L1: 11.8203763961792, L3: 26.27910041809082\n",
      "Current prediction:  60.869102478027344 \n",
      "\n",
      "Iteration 6124, Loss: 37.45790100097656, L1: 10.134570121765137, L3: 27.323331832885742\n",
      "Current prediction:  60.863162994384766 \n",
      "\n",
      "Iteration 6125, Loss: 35.50945281982422, L1: 10.117600440979004, L3: 25.39185333251953\n",
      "Current prediction:  60.95326614379883 \n",
      "\n",
      "Iteration 6126, Loss: 35.50746154785156, L1: 10.145054817199707, L3: 25.36240577697754\n",
      "Current prediction:  61.43046569824219 \n",
      "\n",
      "Iteration 6127, Loss: 35.66202926635742, L1: 10.119864463806152, L3: 25.542165756225586\n",
      "Current prediction:  61.542076110839844 \n",
      "\n",
      "Iteration 6128, Loss: 36.12773513793945, L1: 10.161903381347656, L3: 25.965831756591797\n",
      "Current prediction:  61.549442291259766 \n",
      "\n",
      "Iteration 6129, Loss: 36.233787536621094, L1: 10.16158390045166, L3: 26.072202682495117\n",
      "Current prediction:  61.547096252441406 \n",
      "\n",
      "Iteration 6130, Loss: 36.18977737426758, L1: 10.1394624710083, L3: 26.050315856933594\n",
      "Current prediction:  61.54262924194336 \n",
      "\n",
      "Iteration 6131, Loss: 36.249385833740234, L1: 10.171280860900879, L3: 26.078105926513672\n",
      "Current prediction:  61.538330078125 \n",
      "\n",
      "Iteration 6132, Loss: 36.889869689941406, L1: 10.189740180969238, L3: 26.700130462646484\n",
      "Current prediction:  61.5318717956543 \n",
      "\n",
      "Iteration 6133, Loss: 36.27265548706055, L1: 10.17959976196289, L3: 26.093055725097656\n",
      "Current prediction:  61.525394439697266 \n",
      "\n",
      "Iteration 6134, Loss: 37.624454498291016, L1: 10.17142391204834, L3: 27.453031539916992\n",
      "Current prediction:  61.518226623535156 \n",
      "\n",
      "Iteration 6135, Loss: 36.34983444213867, L1: 10.18165111541748, L3: 26.168184280395508\n",
      "Current prediction:  61.50308609008789 \n",
      "\n",
      "Iteration 6136, Loss: 37.12617874145508, L1: 10.166098594665527, L3: 26.960081100463867\n",
      "Current prediction:  61.325286865234375 \n",
      "\n",
      "Iteration 6137, Loss: 36.4776611328125, L1: 10.162221908569336, L3: 26.31543731689453\n",
      "Current prediction:  60.810482025146484 \n",
      "\n",
      "Iteration 6138, Loss: 36.19613265991211, L1: 10.202679634094238, L3: 25.993453979492188\n",
      "Current prediction:  60.730987548828125 \n",
      "\n",
      "Iteration 6139, Loss: 36.33010482788086, L1: 10.249614715576172, L3: 26.080490112304688\n",
      "Current prediction:  60.72233581542969 \n",
      "\n",
      "Iteration 6140, Loss: 35.72644805908203, L1: 10.295848846435547, L3: 25.430601119995117\n",
      "Current prediction:  60.721580505371094 \n",
      "\n",
      "Iteration 6141, Loss: 36.131591796875, L1: 10.295995712280273, L3: 25.83559799194336\n",
      "Current prediction:  60.725650787353516 \n",
      "\n",
      "Iteration 6142, Loss: 37.15621566772461, L1: 10.338114738464355, L3: 26.818099975585938\n",
      "Current prediction:  60.73014831542969 \n",
      "\n",
      "Iteration 6143, Loss: 36.0989875793457, L1: 10.29577922821045, L3: 25.80320930480957\n",
      "Current prediction:  60.7401008605957 \n",
      "\n",
      "Iteration 6144, Loss: 35.28069305419922, L1: 10.280170440673828, L3: 25.000524520874023\n",
      "Current prediction:  60.75278854370117 \n",
      "\n",
      "Iteration 6145, Loss: 36.440589904785156, L1: 10.221646308898926, L3: 26.218944549560547\n",
      "Current prediction:  60.78590393066406 \n",
      "\n",
      "Iteration 6146, Loss: 36.11200714111328, L1: 10.187621116638184, L3: 25.92438507080078\n",
      "Current prediction:  61.244720458984375 \n",
      "\n",
      "Iteration 6147, Loss: 35.81177520751953, L1: 10.135852813720703, L3: 25.67592430114746\n",
      "Current prediction:  61.53955078125 \n",
      "\n",
      "Iteration 6148, Loss: 37.4996337890625, L1: 10.166441917419434, L3: 27.333192825317383\n",
      "Current prediction:  61.54993438720703 \n",
      "\n",
      "Iteration 6149, Loss: 35.91236114501953, L1: 10.127790451049805, L3: 25.78457260131836\n",
      "Current prediction:  61.55335998535156 \n",
      "\n",
      "Iteration 6150, Loss: 36.47517395019531, L1: 10.160296440124512, L3: 26.314878463745117\n",
      "Current prediction:  61.55263900756836 \n",
      "\n",
      "Iteration 6151, Loss: 35.769927978515625, L1: 10.157081604003906, L3: 25.61284637451172\n",
      "Current prediction:  61.55129623413086 \n",
      "\n",
      "Iteration 6152, Loss: 36.90834045410156, L1: 10.169068336486816, L3: 26.73927116394043\n",
      "Current prediction:  61.55068588256836 \n",
      "\n",
      "Iteration 6153, Loss: 35.428714752197266, L1: 10.159889221191406, L3: 25.26882553100586\n",
      "Current prediction:  61.54848861694336 \n",
      "\n",
      "Iteration 6154, Loss: 36.32841491699219, L1: 10.184727668762207, L3: 26.143688201904297\n",
      "Current prediction:  61.54273986816406 \n",
      "\n",
      "Iteration 6155, Loss: 36.46077346801758, L1: 10.183436393737793, L3: 26.2773380279541\n",
      "Current prediction:  61.53424835205078 \n",
      "\n",
      "Iteration 6156, Loss: 35.562843322753906, L1: 10.182394981384277, L3: 25.380447387695312\n",
      "Current prediction:  61.52532196044922 \n",
      "\n",
      "Iteration 6157, Loss: 35.73310470581055, L1: 10.214749336242676, L3: 25.518356323242188\n",
      "Current prediction:  61.51544952392578 \n",
      "\n",
      "Iteration 6158, Loss: 35.620262145996094, L1: 10.207021713256836, L3: 25.41324234008789\n",
      "Current prediction:  61.504066467285156 \n",
      "\n",
      "Iteration 6159, Loss: 37.0645637512207, L1: 10.180102348327637, L3: 26.884462356567383\n",
      "Current prediction:  61.489192962646484 \n",
      "\n",
      "Iteration 6160, Loss: 36.55942916870117, L1: 10.173727989196777, L3: 26.385700225830078\n",
      "Current prediction:  61.47400665283203 \n",
      "\n",
      "Iteration 6161, Loss: 36.508399963378906, L1: 10.200480461120605, L3: 26.307918548583984\n",
      "Current prediction:  61.099422454833984 \n",
      "\n",
      "Iteration 6162, Loss: 36.33926773071289, L1: 10.220845222473145, L3: 26.118423461914062\n",
      "Current prediction:  60.693023681640625 \n",
      "\n",
      "Iteration 6163, Loss: 37.27390670776367, L1: 10.26062297821045, L3: 27.01328468322754\n",
      "Current prediction:  60.681461334228516 \n",
      "\n",
      "Iteration 6164, Loss: 36.358421325683594, L1: 10.361618041992188, L3: 25.99680519104004\n",
      "Current prediction:  60.6796760559082 \n",
      "\n",
      "Iteration 6165, Loss: 36.85818862915039, L1: 10.38805866241455, L3: 26.470130920410156\n",
      "Current prediction:  60.67994689941406 \n",
      "\n",
      "Iteration 6166, Loss: 35.98735809326172, L1: 10.44985294342041, L3: 25.537506103515625\n",
      "Current prediction:  60.68572235107422 \n",
      "\n",
      "Iteration 6167, Loss: 37.43950653076172, L1: 10.409808158874512, L3: 27.02969741821289\n",
      "Current prediction:  60.69784164428711 \n",
      "\n",
      "Iteration 6168, Loss: 35.952842712402344, L1: 10.350143432617188, L3: 25.602699279785156\n",
      "Current prediction:  60.71683120727539 \n",
      "\n",
      "Iteration 6169, Loss: 36.58778381347656, L1: 10.244827270507812, L3: 26.34295654296875\n",
      "Current prediction:  61.300289154052734 \n",
      "\n",
      "Iteration 6170, Loss: 36.8663444519043, L1: 10.185259819030762, L3: 26.68108367919922\n",
      "Current prediction:  61.51108932495117 \n",
      "\n",
      "Iteration 6171, Loss: 36.87322235107422, L1: 10.172270774841309, L3: 26.700952529907227\n",
      "Current prediction:  61.53032302856445 \n",
      "\n",
      "Iteration 6172, Loss: 36.2651252746582, L1: 10.175835609436035, L3: 26.08928871154785\n",
      "Current prediction:  61.5429573059082 \n",
      "\n",
      "Iteration 6173, Loss: 36.3450927734375, L1: 10.1983642578125, L3: 26.146730422973633\n",
      "Current prediction:  61.5496711730957 \n",
      "\n",
      "Iteration 6174, Loss: 36.2978515625, L1: 10.17916488647461, L3: 26.11868667602539\n",
      "Current prediction:  61.55414581298828 \n",
      "\n",
      "Iteration 6175, Loss: 35.79664611816406, L1: 10.178251266479492, L3: 25.618392944335938\n",
      "Current prediction:  61.55475616455078 \n",
      "\n",
      "Iteration 6176, Loss: 37.09974670410156, L1: 10.19418716430664, L3: 26.905559539794922\n",
      "Current prediction:  61.54938888549805 \n",
      "\n",
      "Iteration 6177, Loss: 36.34904861450195, L1: 10.207050323486328, L3: 26.141998291015625\n",
      "Current prediction:  61.53956985473633 \n",
      "\n",
      "Iteration 6178, Loss: 36.77470016479492, L1: 10.729557037353516, L3: 26.045143127441406\n",
      "Current prediction:  61.52492141723633 \n",
      "\n",
      "Iteration 6179, Loss: 35.95899200439453, L1: 10.218728065490723, L3: 25.740264892578125\n",
      "Current prediction:  61.51228713989258 \n",
      "\n",
      "Iteration 6180, Loss: 36.47228240966797, L1: 10.22407341003418, L3: 26.248207092285156\n",
      "Current prediction:  61.500755310058594 \n",
      "\n",
      "Iteration 6181, Loss: 36.31409454345703, L1: 10.22140121459961, L3: 26.09269142150879\n",
      "Current prediction:  61.49012756347656 \n",
      "\n",
      "Iteration 6182, Loss: 35.85560607910156, L1: 10.230522155761719, L3: 25.62508201599121\n",
      "Current prediction:  61.479061126708984 \n",
      "\n",
      "Iteration 6183, Loss: 36.501251220703125, L1: 10.201574325561523, L3: 26.29967498779297\n",
      "Current prediction:  61.45909118652344 \n",
      "\n",
      "Iteration 6184, Loss: 37.06310272216797, L1: 10.2261381149292, L3: 26.836965560913086\n",
      "Current prediction:  61.280799865722656 \n",
      "\n",
      "Iteration 6185, Loss: 36.15102767944336, L1: 10.198803901672363, L3: 25.952224731445312\n",
      "Current prediction:  60.718265533447266 \n",
      "\n",
      "Iteration 6186, Loss: 36.043548583984375, L1: 10.252303123474121, L3: 25.791244506835938\n",
      "Current prediction:  60.687042236328125 \n",
      "\n",
      "Iteration 6187, Loss: 36.50059509277344, L1: 10.248265266418457, L3: 26.252328872680664\n",
      "Current prediction:  60.69161605834961 \n",
      "\n",
      "Iteration 6188, Loss: 35.827762603759766, L1: 10.275300025939941, L3: 25.55246353149414\n",
      "Current prediction:  60.70756530761719 \n",
      "\n",
      "Iteration 6189, Loss: 36.99250030517578, L1: 10.229248046875, L3: 26.76325225830078\n",
      "Current prediction:  60.86225891113281 \n",
      "\n",
      "Iteration 6190, Loss: 36.63349151611328, L1: 10.212801933288574, L3: 26.420690536499023\n",
      "Current prediction:  61.36774444580078 \n",
      "\n",
      "Iteration 6191, Loss: 37.027278900146484, L1: 10.186579704284668, L3: 26.840700149536133\n",
      "Current prediction:  61.46562576293945 \n",
      "\n",
      "Iteration 6192, Loss: 36.337337493896484, L1: 10.181938171386719, L3: 26.155399322509766\n",
      "Current prediction:  61.496097564697266 \n",
      "\n",
      "Iteration 6193, Loss: 36.67028045654297, L1: 10.197976112365723, L3: 26.472305297851562\n",
      "Current prediction:  61.50599670410156 \n",
      "\n",
      "Iteration 6194, Loss: 37.535308837890625, L1: 10.16547679901123, L3: 27.369831085205078\n",
      "Current prediction:  61.51188659667969 \n",
      "\n",
      "Iteration 6195, Loss: 37.04188537597656, L1: 10.180253982543945, L3: 26.861631393432617\n",
      "Current prediction:  61.51640319824219 \n",
      "\n",
      "Iteration 6196, Loss: 36.70771789550781, L1: 10.183032989501953, L3: 26.524686813354492\n",
      "Current prediction:  61.516483306884766 \n",
      "\n",
      "Iteration 6197, Loss: 35.531211853027344, L1: 10.164484977722168, L3: 25.366727828979492\n",
      "Current prediction:  61.495723724365234 \n",
      "\n",
      "Iteration 6198, Loss: 36.265892028808594, L1: 10.168915748596191, L3: 26.09697723388672\n",
      "Current prediction:  61.22726058959961 \n",
      "\n",
      "Iteration 6199, Loss: 36.33465576171875, L1: 10.129544258117676, L3: 26.205110549926758\n",
      "Current prediction:  60.83546447753906 \n",
      "\n",
      "Iteration 6200, Loss: 36.58736801147461, L1: 10.181384086608887, L3: 26.40598487854004\n",
      "Current prediction:  60.765533447265625 \n",
      "\n",
      "Iteration 6201, Loss: 37.03011703491211, L1: 10.231361389160156, L3: 26.798755645751953\n",
      "Current prediction:  60.7612190246582 \n",
      "\n",
      "Iteration 6202, Loss: 36.98504638671875, L1: 10.258415222167969, L3: 26.72663116455078\n",
      "Current prediction:  60.762386322021484 \n",
      "\n",
      "Iteration 6203, Loss: 36.56181716918945, L1: 10.246100425720215, L3: 26.315715789794922\n",
      "Current prediction:  60.77135467529297 \n",
      "\n",
      "Iteration 6204, Loss: 36.72102355957031, L1: 10.191362380981445, L3: 26.5296630859375\n",
      "Current prediction:  60.80675506591797 \n",
      "\n",
      "Iteration 6205, Loss: 36.41181945800781, L1: 10.162662506103516, L3: 26.249156951904297\n",
      "Current prediction:  61.056922912597656 \n",
      "\n",
      "Iteration 6206, Loss: 36.786598205566406, L1: 10.141741752624512, L3: 26.644855499267578\n",
      "Current prediction:  61.46986389160156 \n",
      "\n",
      "Iteration 6207, Loss: 36.146888732910156, L1: 10.120882034301758, L3: 26.026004791259766\n",
      "Current prediction:  61.55821990966797 \n",
      "\n",
      "Iteration 6208, Loss: 36.72212600708008, L1: 10.10315990447998, L3: 26.61896514892578\n",
      "Current prediction:  61.57030487060547 \n",
      "\n",
      "Iteration 6209, Loss: 36.097190856933594, L1: 10.125807762145996, L3: 25.97138214111328\n",
      "Current prediction:  61.571598052978516 \n",
      "\n",
      "Iteration 6210, Loss: 37.61825942993164, L1: 10.146092414855957, L3: 27.472166061401367\n",
      "Current prediction:  61.56934356689453 \n",
      "\n",
      "Iteration 6211, Loss: 36.74173355102539, L1: 10.149041175842285, L3: 26.592693328857422\n",
      "Current prediction:  61.564308166503906 \n",
      "\n",
      "Iteration 6212, Loss: 37.42579650878906, L1: 10.146106719970703, L3: 27.27968978881836\n",
      "Current prediction:  61.55508804321289 \n",
      "\n",
      "Iteration 6213, Loss: 36.37690353393555, L1: 10.138331413269043, L3: 26.238571166992188\n",
      "Current prediction:  61.546470642089844 \n",
      "\n",
      "Iteration 6214, Loss: 36.41628646850586, L1: 10.14699649810791, L3: 26.269290924072266\n",
      "Current prediction:  61.5353889465332 \n",
      "\n",
      "Iteration 6215, Loss: 36.4088134765625, L1: 10.192752838134766, L3: 26.2160587310791\n",
      "Current prediction:  61.5276985168457 \n",
      "\n",
      "Iteration 6216, Loss: 35.30330276489258, L1: 10.158942222595215, L3: 25.144359588623047\n",
      "Current prediction:  61.517486572265625 \n",
      "\n",
      "Iteration 6217, Loss: 36.24742889404297, L1: 10.150900840759277, L3: 26.096529006958008\n",
      "Current prediction:  61.447635650634766 \n",
      "\n",
      "Iteration 6218, Loss: 36.546966552734375, L1: 10.131065368652344, L3: 26.415903091430664\n",
      "Current prediction:  61.055782318115234 \n",
      "\n",
      "Iteration 6219, Loss: 36.222591400146484, L1: 10.152361869812012, L3: 26.07023048400879\n",
      "Current prediction:  60.937217712402344 \n",
      "\n",
      "Iteration 6220, Loss: 35.205318450927734, L1: 10.1781005859375, L3: 25.027217864990234\n",
      "Current prediction:  60.900390625 \n",
      "\n",
      "Iteration 6221, Loss: 36.39297866821289, L1: 10.17947006225586, L3: 26.21350860595703\n",
      "Current prediction:  60.905338287353516 \n",
      "\n",
      "Iteration 6222, Loss: 36.370216369628906, L1: 10.146614074707031, L3: 26.223602294921875\n",
      "Current prediction:  60.9018440246582 \n",
      "\n",
      "Iteration 6223, Loss: 36.25339889526367, L1: 10.206916809082031, L3: 26.04648208618164\n",
      "Current prediction:  61.05586624145508 \n",
      "\n",
      "Iteration 6224, Loss: 36.80400085449219, L1: 10.153124809265137, L3: 26.650875091552734\n",
      "Current prediction:  61.42563247680664 \n",
      "\n",
      "Iteration 6225, Loss: 36.419517517089844, L1: 10.129737854003906, L3: 26.289777755737305\n",
      "Current prediction:  61.53380584716797 \n",
      "\n",
      "Iteration 6226, Loss: 36.479896545410156, L1: 10.15260124206543, L3: 26.327293395996094\n",
      "Current prediction:  61.543766021728516 \n",
      "\n",
      "Iteration 6227, Loss: 36.60221862792969, L1: 10.168069839477539, L3: 26.43415069580078\n",
      "Current prediction:  61.547149658203125 \n",
      "\n",
      "Iteration 6228, Loss: 36.57238006591797, L1: 10.14370059967041, L3: 26.428680419921875\n",
      "Current prediction:  61.54785919189453 \n",
      "\n",
      "Iteration 6229, Loss: 36.03752136230469, L1: 10.160002708435059, L3: 25.877519607543945\n",
      "Current prediction:  61.54779052734375 \n",
      "\n",
      "Iteration 6230, Loss: 35.991371154785156, L1: 10.153196334838867, L3: 25.838176727294922\n",
      "Current prediction:  61.54753494262695 \n",
      "\n",
      "Iteration 6231, Loss: 36.151493072509766, L1: 10.170547485351562, L3: 25.980945587158203\n",
      "Current prediction:  61.545352935791016 \n",
      "\n",
      "Iteration 6232, Loss: 35.90427780151367, L1: 10.135445594787598, L3: 25.76883316040039\n",
      "Current prediction:  61.536094665527344 \n",
      "\n",
      "Iteration 6233, Loss: 35.71992111206055, L1: 10.135214805603027, L3: 25.584707260131836\n",
      "Current prediction:  61.43683624267578 \n",
      "\n",
      "Iteration 6234, Loss: 36.17006301879883, L1: 10.127893447875977, L3: 26.04216957092285\n",
      "Current prediction:  60.87840270996094 \n",
      "\n",
      "Iteration 6235, Loss: 37.4102783203125, L1: 10.1635160446167, L3: 27.246763229370117\n",
      "Current prediction:  60.77688217163086 \n",
      "\n",
      "Iteration 6236, Loss: 35.99295425415039, L1: 10.197900772094727, L3: 25.795053482055664\n",
      "Current prediction:  60.77446365356445 \n",
      "\n",
      "Iteration 6237, Loss: 36.75864028930664, L1: 10.236701965332031, L3: 26.52193832397461\n",
      "Current prediction:  60.778804779052734 \n",
      "\n",
      "Iteration 6238, Loss: 36.06257247924805, L1: 10.231525421142578, L3: 25.83104705810547\n",
      "Current prediction:  60.78322982788086 \n",
      "\n",
      "Iteration 6239, Loss: 35.86407470703125, L1: 10.179469108581543, L3: 25.684606552124023\n",
      "Current prediction:  60.788002014160156 \n",
      "\n",
      "Iteration 6240, Loss: 36.610774993896484, L1: 10.179579734802246, L3: 26.431194305419922\n",
      "Current prediction:  60.797183990478516 \n",
      "\n",
      "Iteration 6241, Loss: 36.6495246887207, L1: 10.182806968688965, L3: 26.466718673706055\n",
      "Current prediction:  60.83686828613281 \n",
      "\n",
      "Iteration 6242, Loss: 37.00688171386719, L1: 10.140275955200195, L3: 26.866605758666992\n",
      "Current prediction:  61.02852249145508 \n",
      "\n",
      "Iteration 6243, Loss: 35.5114860534668, L1: 10.105854988098145, L3: 25.405630111694336\n",
      "Current prediction:  61.07967758178711 \n",
      "\n",
      "Iteration 6244, Loss: 36.777740478515625, L1: 10.131999969482422, L3: 26.645740509033203\n",
      "Current prediction:  61.30439758300781 \n",
      "\n",
      "Iteration 6245, Loss: 36.05176544189453, L1: 10.094016075134277, L3: 25.95775032043457\n",
      "Current prediction:  61.47154998779297 \n",
      "\n",
      "Iteration 6246, Loss: 37.205543518066406, L1: 10.085423469543457, L3: 27.120121002197266\n",
      "Current prediction:  61.52191162109375 \n",
      "\n",
      "Iteration 6247, Loss: 36.17801284790039, L1: 10.102261543273926, L3: 26.07575225830078\n",
      "Current prediction:  61.509368896484375 \n",
      "\n",
      "Iteration 6248, Loss: 36.80064010620117, L1: 10.13693618774414, L3: 26.66370391845703\n",
      "Current prediction:  61.4234504699707 \n",
      "\n",
      "Iteration 6249, Loss: 35.61984634399414, L1: 10.11867618560791, L3: 25.501169204711914\n",
      "Current prediction:  61.343017578125 \n",
      "\n",
      "Iteration 6250, Loss: 36.771095275878906, L1: 10.101946830749512, L3: 26.66914939880371\n",
      "Current prediction:  61.00954055786133 \n",
      "\n",
      "Iteration 6251, Loss: 36.50355529785156, L1: 10.113612174987793, L3: 26.389944076538086\n",
      "Current prediction:  60.91025924682617 \n",
      "\n",
      "Iteration 6252, Loss: 35.24591064453125, L1: 10.15197467803955, L3: 25.093935012817383\n",
      "Current prediction:  60.85249328613281 \n",
      "\n",
      "Iteration 6253, Loss: 36.66191101074219, L1: 10.132024765014648, L3: 26.529888153076172\n",
      "Current prediction:  60.80959701538086 \n",
      "\n",
      "Iteration 6254, Loss: 37.12571334838867, L1: 10.163312911987305, L3: 26.962400436401367\n",
      "Current prediction:  60.798583984375 \n",
      "\n",
      "Iteration 6255, Loss: 36.75275421142578, L1: 10.18441104888916, L3: 26.568344116210938\n",
      "Current prediction:  60.8243522644043 \n",
      "\n",
      "Iteration 6256, Loss: 35.69633102416992, L1: 10.161087036132812, L3: 25.53524398803711\n",
      "Current prediction:  60.933414459228516 \n",
      "\n",
      "Iteration 6257, Loss: 37.21879577636719, L1: 10.135309219360352, L3: 27.083484649658203\n",
      "Current prediction:  61.159671783447266 \n",
      "\n",
      "Iteration 6258, Loss: 37.39048767089844, L1: 10.139076232910156, L3: 27.251413345336914\n",
      "Current prediction:  61.48214340209961 \n",
      "\n",
      "Iteration 6259, Loss: 35.665855407714844, L1: 10.118667602539062, L3: 25.54718780517578\n",
      "Current prediction:  61.542415618896484 \n",
      "\n",
      "Iteration 6260, Loss: 36.86640167236328, L1: 10.110725402832031, L3: 26.755674362182617\n",
      "Current prediction:  61.55146789550781 \n",
      "\n",
      "Iteration 6261, Loss: 37.6323356628418, L1: 10.09022331237793, L3: 27.542112350463867\n",
      "Current prediction:  61.52735900878906 \n",
      "\n",
      "Iteration 6262, Loss: 36.84851837158203, L1: 10.148706436157227, L3: 26.699810028076172\n",
      "Current prediction:  61.50407028198242 \n",
      "\n",
      "Iteration 6263, Loss: 35.97685623168945, L1: 10.128485679626465, L3: 25.848371505737305\n",
      "Current prediction:  61.39396286010742 \n",
      "\n",
      "Iteration 6264, Loss: 36.47661590576172, L1: 10.143370628356934, L3: 26.33324432373047\n",
      "Current prediction:  61.28308868408203 \n",
      "\n",
      "Iteration 6265, Loss: 36.06890106201172, L1: 10.172359466552734, L3: 25.896541595458984\n",
      "Current prediction:  61.0952033996582 \n",
      "\n",
      "Iteration 6266, Loss: 36.247528076171875, L1: 10.138236045837402, L3: 26.109291076660156\n",
      "Current prediction:  60.923465728759766 \n",
      "\n",
      "Iteration 6267, Loss: 35.993736267089844, L1: 10.146650314331055, L3: 25.84708595275879\n",
      "Current prediction:  61.08148193359375 \n",
      "\n",
      "Iteration 6268, Loss: 36.744163513183594, L1: 10.17522144317627, L3: 26.568941116333008\n",
      "Current prediction:  61.28799057006836 \n",
      "\n",
      "Iteration 6269, Loss: 35.92498016357422, L1: 10.154342651367188, L3: 25.770639419555664\n",
      "Current prediction:  61.48912811279297 \n",
      "\n",
      "Iteration 6270, Loss: 35.90009307861328, L1: 10.130205154418945, L3: 25.769887924194336\n",
      "Current prediction:  61.523765563964844 \n",
      "\n",
      "Iteration 6271, Loss: 36.873321533203125, L1: 10.153573036193848, L3: 26.719749450683594\n",
      "Current prediction:  61.524757385253906 \n",
      "\n",
      "Iteration 6272, Loss: 36.29022979736328, L1: 10.19063949584961, L3: 26.099592208862305\n",
      "Current prediction:  61.52360534667969 \n",
      "\n",
      "Iteration 6273, Loss: 35.98030090332031, L1: 10.277828216552734, L3: 25.702470779418945\n",
      "Current prediction:  61.52452850341797 \n",
      "\n",
      "Iteration 6274, Loss: 36.445404052734375, L1: 10.340597152709961, L3: 26.104806900024414\n",
      "Current prediction:  61.52266311645508 \n",
      "\n",
      "Iteration 6275, Loss: 37.473968505859375, L1: 10.317081451416016, L3: 27.156885147094727\n",
      "Current prediction:  61.51846694946289 \n",
      "\n",
      "Iteration 6276, Loss: 36.37734603881836, L1: 10.362714767456055, L3: 26.014631271362305\n",
      "Current prediction:  61.51430892944336 \n",
      "\n",
      "Iteration 6277, Loss: 37.83935546875, L1: 10.283690452575684, L3: 27.555665969848633\n",
      "Current prediction:  61.50615692138672 \n",
      "\n",
      "Iteration 6278, Loss: 37.824520111083984, L1: 10.348323822021484, L3: 27.4761962890625\n",
      "Current prediction:  61.49318313598633 \n",
      "\n",
      "Iteration 6279, Loss: 35.905792236328125, L1: 10.302589416503906, L3: 25.60320281982422\n",
      "Current prediction:  61.46417999267578 \n",
      "\n",
      "Iteration 6280, Loss: 36.10409927368164, L1: 10.496393203735352, L3: 25.60770606994629\n",
      "Current prediction:  61.03678512573242 \n",
      "\n",
      "Iteration 6281, Loss: 36.910316467285156, L1: 10.180747032165527, L3: 26.729568481445312\n",
      "Current prediction:  60.71748352050781 \n",
      "\n",
      "Iteration 6282, Loss: 36.258235931396484, L1: 10.223793029785156, L3: 26.034442901611328\n",
      "Current prediction:  60.694496154785156 \n",
      "\n",
      "Iteration 6283, Loss: 37.18022155761719, L1: 10.273229598999023, L3: 26.90699005126953\n",
      "Current prediction:  60.69219970703125 \n",
      "\n",
      "Iteration 6284, Loss: 35.140419006347656, L1: 10.293985366821289, L3: 24.846431732177734\n",
      "Current prediction:  60.694664001464844 \n",
      "\n",
      "Iteration 6285, Loss: 36.14093017578125, L1: 10.286771774291992, L3: 25.854156494140625\n",
      "Current prediction:  60.70065689086914 \n",
      "\n",
      "Iteration 6286, Loss: 37.65861129760742, L1: 10.278486251831055, L3: 27.380125045776367\n",
      "Current prediction:  60.76670837402344 \n",
      "\n",
      "Iteration 6287, Loss: 36.83259582519531, L1: 10.227701187133789, L3: 26.604896545410156\n",
      "Current prediction:  61.3284912109375 \n",
      "\n",
      "Iteration 6288, Loss: 36.615989685058594, L1: 10.135025024414062, L3: 26.480966567993164\n",
      "Current prediction:  61.481929779052734 \n",
      "\n",
      "Iteration 6289, Loss: 35.83908462524414, L1: 10.173529624938965, L3: 25.66555404663086\n",
      "Current prediction:  61.49361801147461 \n",
      "\n",
      "Iteration 6290, Loss: 37.84275817871094, L1: 10.164413452148438, L3: 27.678342819213867\n",
      "Current prediction:  61.49713897705078 \n",
      "\n",
      "Iteration 6291, Loss: 35.96364974975586, L1: 10.181496620178223, L3: 25.782154083251953\n",
      "Current prediction:  61.50186538696289 \n",
      "\n",
      "Iteration 6292, Loss: 36.62965774536133, L1: 10.175005912780762, L3: 26.454652786254883\n",
      "Current prediction:  61.50499725341797 \n",
      "\n",
      "Iteration 6293, Loss: 36.82569122314453, L1: 10.180763244628906, L3: 26.644927978515625\n",
      "Current prediction:  61.5010986328125 \n",
      "\n",
      "Iteration 6294, Loss: 36.958221435546875, L1: 10.17284107208252, L3: 26.785381317138672\n",
      "Current prediction:  61.484500885009766 \n",
      "\n",
      "Iteration 6295, Loss: 35.98025894165039, L1: 10.1317720413208, L3: 25.848485946655273\n",
      "Current prediction:  61.382240295410156 \n",
      "\n",
      "Iteration 6296, Loss: 36.948184967041016, L1: 10.158562660217285, L3: 26.789621353149414\n",
      "Current prediction:  61.098690032958984 \n",
      "\n",
      "Iteration 6297, Loss: 36.3426628112793, L1: 10.153088569641113, L3: 26.1895751953125\n",
      "Current prediction:  60.97879409790039 \n",
      "\n",
      "Iteration 6298, Loss: 35.7774772644043, L1: 10.149765968322754, L3: 25.62771224975586\n",
      "Current prediction:  61.073917388916016 \n",
      "\n",
      "Iteration 6299, Loss: 36.40769958496094, L1: 10.158241271972656, L3: 26.24945640563965\n",
      "Current prediction:  61.23554229736328 \n",
      "\n",
      "Iteration 6300, Loss: 36.2106819152832, L1: 10.169280052185059, L3: 26.041400909423828\n",
      "Current prediction:  61.214962005615234 \n",
      "\n",
      "Iteration 6301, Loss: 36.564849853515625, L1: 10.14714527130127, L3: 26.41770362854004\n",
      "Current prediction:  61.410560607910156 \n",
      "\n",
      "Iteration 6302, Loss: 35.70246505737305, L1: 10.119695663452148, L3: 25.5827693939209\n",
      "Current prediction:  61.373291015625 \n",
      "\n",
      "Iteration 6303, Loss: 36.60188293457031, L1: 10.135245323181152, L3: 26.466636657714844\n",
      "Current prediction:  61.08964157104492 \n",
      "\n",
      "Iteration 6304, Loss: 36.20856475830078, L1: 10.164520263671875, L3: 26.044044494628906\n",
      "Current prediction:  60.90711212158203 \n",
      "\n",
      "Iteration 6305, Loss: 36.9138069152832, L1: 10.159516334533691, L3: 26.754291534423828\n",
      "Current prediction:  60.82335662841797 \n",
      "\n",
      "Iteration 6306, Loss: 35.99235153198242, L1: 10.187874794006348, L3: 25.804475784301758\n",
      "Current prediction:  60.77241516113281 \n",
      "\n",
      "Iteration 6307, Loss: 35.81730651855469, L1: 10.191040992736816, L3: 25.626266479492188\n",
      "Current prediction:  60.75883102416992 \n",
      "\n",
      "Iteration 6308, Loss: 36.08928680419922, L1: 10.177282333374023, L3: 25.912004470825195\n",
      "Current prediction:  60.78612518310547 \n",
      "\n",
      "Iteration 6309, Loss: 36.28348922729492, L1: 10.165698051452637, L3: 26.11779022216797\n",
      "Current prediction:  60.79535675048828 \n",
      "\n",
      "Iteration 6310, Loss: 36.72565460205078, L1: 10.183334350585938, L3: 26.54231834411621\n",
      "Current prediction:  60.876319885253906 \n",
      "\n",
      "Iteration 6311, Loss: 36.450565338134766, L1: 10.174703598022461, L3: 26.275861740112305\n",
      "Current prediction:  61.26200866699219 \n",
      "\n",
      "Iteration 6312, Loss: 37.24869918823242, L1: 10.140226364135742, L3: 27.10847282409668\n",
      "Current prediction:  61.500606536865234 \n",
      "\n",
      "Iteration 6313, Loss: 36.24579620361328, L1: 10.135169982910156, L3: 26.110624313354492\n",
      "Current prediction:  61.52212905883789 \n",
      "\n",
      "Iteration 6314, Loss: 36.34617233276367, L1: 10.134827613830566, L3: 26.211345672607422\n",
      "Current prediction:  61.52973937988281 \n",
      "\n",
      "Iteration 6315, Loss: 36.23104476928711, L1: 10.166962623596191, L3: 26.0640811920166\n",
      "Current prediction:  61.530921936035156 \n",
      "\n",
      "Iteration 6316, Loss: 35.73754119873047, L1: 10.136004447937012, L3: 25.60153579711914\n",
      "Current prediction:  61.52157974243164 \n",
      "\n",
      "Iteration 6317, Loss: 36.62477111816406, L1: 10.123348236083984, L3: 26.50142478942871\n",
      "Current prediction:  61.478370666503906 \n",
      "\n",
      "Iteration 6318, Loss: 35.11729431152344, L1: 10.108072280883789, L3: 25.00922203063965\n",
      "Current prediction:  60.997413635253906 \n",
      "\n",
      "Iteration 6319, Loss: 37.183143615722656, L1: 10.125812530517578, L3: 27.057331085205078\n",
      "Current prediction:  60.78218078613281 \n",
      "\n",
      "Iteration 6320, Loss: 37.889862060546875, L1: 10.206725120544434, L3: 27.683137893676758\n",
      "Current prediction:  60.766868591308594 \n",
      "\n",
      "Iteration 6321, Loss: 35.793479919433594, L1: 10.176798820495605, L3: 25.616680145263672\n",
      "Current prediction:  60.77366256713867 \n",
      "\n",
      "Iteration 6322, Loss: 36.080535888671875, L1: 10.163814544677734, L3: 25.916723251342773\n",
      "Current prediction:  60.946510314941406 \n",
      "\n",
      "Iteration 6323, Loss: 36.58723068237305, L1: 10.127884864807129, L3: 26.459346771240234\n",
      "Current prediction:  61.300018310546875 \n",
      "\n",
      "Iteration 6324, Loss: 36.767398834228516, L1: 10.169184684753418, L3: 26.59821319580078\n",
      "Current prediction:  61.48347854614258 \n",
      "\n",
      "Iteration 6325, Loss: 35.58642578125, L1: 10.157464027404785, L3: 25.42896270751953\n",
      "Current prediction:  61.4989128112793 \n",
      "\n",
      "Iteration 6326, Loss: 37.1826171875, L1: 10.139274597167969, L3: 27.043344497680664\n",
      "Current prediction:  61.50538635253906 \n",
      "\n",
      "Iteration 6327, Loss: 36.320213317871094, L1: 10.173194885253906, L3: 26.147018432617188\n",
      "Current prediction:  61.50196838378906 \n",
      "\n",
      "Iteration 6328, Loss: 36.11014938354492, L1: 10.164868354797363, L3: 25.945281982421875\n",
      "Current prediction:  61.464324951171875 \n",
      "\n",
      "Iteration 6329, Loss: 36.19502258300781, L1: 10.164898872375488, L3: 26.030122756958008\n",
      "Current prediction:  61.420040130615234 \n",
      "\n",
      "Iteration 6330, Loss: 35.93421173095703, L1: 10.099989891052246, L3: 25.8342227935791\n",
      "Current prediction:  61.288238525390625 \n",
      "\n",
      "Iteration 6331, Loss: 35.454246520996094, L1: 10.116847038269043, L3: 25.337400436401367\n",
      "Current prediction:  61.03260803222656 \n",
      "\n",
      "Iteration 6332, Loss: 36.15434265136719, L1: 10.143959045410156, L3: 26.01038360595703\n",
      "Current prediction:  60.7855339050293 \n",
      "\n",
      "Iteration 6333, Loss: 36.66900634765625, L1: 10.181999206542969, L3: 26.48700714111328\n",
      "Current prediction:  60.758934020996094 \n",
      "\n",
      "Iteration 6334, Loss: 35.72291564941406, L1: 10.185013771057129, L3: 25.537900924682617\n",
      "Current prediction:  60.75045394897461 \n",
      "\n",
      "Iteration 6335, Loss: 35.99190902709961, L1: 10.21720027923584, L3: 25.774709701538086\n",
      "Current prediction:  60.751949310302734 \n",
      "\n",
      "Iteration 6336, Loss: 36.036739349365234, L1: 10.252657890319824, L3: 25.784082412719727\n",
      "Current prediction:  60.75883865356445 \n",
      "\n",
      "Iteration 6337, Loss: 35.61830139160156, L1: 10.20864200592041, L3: 25.409658432006836\n",
      "Current prediction:  60.79067611694336 \n",
      "\n",
      "Iteration 6338, Loss: 35.413978576660156, L1: 10.127943992614746, L3: 25.286033630371094\n",
      "Current prediction:  60.88410186767578 \n",
      "\n",
      "Iteration 6339, Loss: 37.28160095214844, L1: 10.128362655639648, L3: 27.15323829650879\n",
      "Current prediction:  61.40887451171875 \n",
      "\n",
      "Iteration 6340, Loss: 36.904945373535156, L1: 10.112465858459473, L3: 26.792478561401367\n",
      "Current prediction:  61.541263580322266 \n",
      "\n",
      "Iteration 6341, Loss: 36.709110260009766, L1: 10.082254409790039, L3: 26.626855850219727\n",
      "Current prediction:  61.57481002807617 \n",
      "\n",
      "Iteration 6342, Loss: 36.05454635620117, L1: 10.092291831970215, L3: 25.962255477905273\n",
      "Current prediction:  61.581356048583984 \n",
      "\n",
      "Iteration 6343, Loss: 36.66675567626953, L1: 10.071842193603516, L3: 26.594911575317383\n",
      "Current prediction:  61.5058479309082 \n",
      "\n",
      "Iteration 6344, Loss: 37.56175994873047, L1: 10.0629301071167, L3: 27.498830795288086\n",
      "Current prediction:  61.27713394165039 \n",
      "\n",
      "Iteration 6345, Loss: 36.127891540527344, L1: 10.069046974182129, L3: 26.0588436126709\n",
      "Current prediction:  61.28209686279297 \n",
      "\n",
      "Iteration 6346, Loss: 35.82661437988281, L1: 10.078978538513184, L3: 25.747636795043945\n",
      "Current prediction:  61.26807403564453 \n",
      "\n",
      "Iteration 6347, Loss: 35.98528289794922, L1: 10.041260719299316, L3: 25.944021224975586\n",
      "Current prediction:  61.220706939697266 \n",
      "\n",
      "Iteration 6348, Loss: 36.4251823425293, L1: 10.068671226501465, L3: 26.35651206970215\n",
      "Current prediction:  61.19401550292969 \n",
      "\n",
      "Iteration 6349, Loss: 36.58595657348633, L1: 10.052533149719238, L3: 26.533422470092773\n",
      "Current prediction:  61.19098663330078 \n",
      "\n",
      "Iteration 6350, Loss: 37.362998962402344, L1: 10.046133041381836, L3: 27.31686782836914\n",
      "Current prediction:  61.24061584472656 \n",
      "\n",
      "Iteration 6351, Loss: 36.46023178100586, L1: 10.042041778564453, L3: 26.418190002441406\n",
      "Current prediction:  61.245140075683594 \n",
      "\n",
      "Iteration 6352, Loss: 36.53717803955078, L1: 10.054000854492188, L3: 26.48317527770996\n",
      "Current prediction:  61.32051086425781 \n",
      "\n",
      "Iteration 6353, Loss: 35.9223518371582, L1: 10.089262962341309, L3: 25.83308982849121\n",
      "Current prediction:  61.44974136352539 \n",
      "\n",
      "Iteration 6354, Loss: 36.748382568359375, L1: 10.06362533569336, L3: 26.68475914001465\n",
      "Current prediction:  61.469783782958984 \n",
      "\n",
      "Iteration 6355, Loss: 36.58760452270508, L1: 10.089371681213379, L3: 26.498231887817383\n",
      "Current prediction:  61.317527770996094 \n",
      "\n",
      "Iteration 6356, Loss: 37.058753967285156, L1: 10.108146667480469, L3: 26.95060920715332\n",
      "Current prediction:  61.01278305053711 \n",
      "\n",
      "Iteration 6357, Loss: 35.775489807128906, L1: 10.127140998840332, L3: 25.648347854614258\n",
      "Current prediction:  60.8399658203125 \n",
      "\n",
      "Iteration 6358, Loss: 35.30415344238281, L1: 10.115046501159668, L3: 25.18910789489746\n",
      "Current prediction:  60.94767761230469 \n",
      "\n",
      "Iteration 6359, Loss: 37.148895263671875, L1: 10.140013694763184, L3: 27.008880615234375\n",
      "Current prediction:  61.059200286865234 \n",
      "\n",
      "Iteration 6360, Loss: 35.896820068359375, L1: 10.12861442565918, L3: 25.768203735351562\n",
      "Current prediction:  61.208946228027344 \n",
      "\n",
      "Iteration 6361, Loss: 36.05807876586914, L1: 10.126648902893066, L3: 25.93143081665039\n",
      "Current prediction:  61.335479736328125 \n",
      "\n",
      "Iteration 6362, Loss: 35.56830596923828, L1: 10.119624137878418, L3: 25.448680877685547\n",
      "Current prediction:  61.46904754638672 \n",
      "\n",
      "Iteration 6363, Loss: 36.98521041870117, L1: 10.133007049560547, L3: 26.852203369140625\n",
      "Current prediction:  61.504966735839844 \n",
      "\n",
      "Iteration 6364, Loss: 36.49421691894531, L1: 10.097668647766113, L3: 26.396549224853516\n",
      "Current prediction:  61.51110076904297 \n",
      "\n",
      "Iteration 6365, Loss: 35.507930755615234, L1: 10.14863109588623, L3: 25.35930061340332\n",
      "Current prediction:  61.492645263671875 \n",
      "\n",
      "Iteration 6366, Loss: 35.86935806274414, L1: 10.109320640563965, L3: 25.760038375854492\n",
      "Current prediction:  61.44279861450195 \n",
      "\n",
      "Iteration 6367, Loss: 35.64319610595703, L1: 10.151697158813477, L3: 25.491497039794922\n",
      "Current prediction:  61.34343719482422 \n",
      "\n",
      "Iteration 6368, Loss: 36.437442779541016, L1: 10.146801948547363, L3: 26.290639877319336\n",
      "Current prediction:  61.16975402832031 \n",
      "\n",
      "Iteration 6369, Loss: 35.38135528564453, L1: 10.129222869873047, L3: 25.252134323120117\n",
      "Current prediction:  60.97050857543945 \n",
      "\n",
      "Iteration 6370, Loss: 35.99192428588867, L1: 10.146408081054688, L3: 25.845516204833984\n",
      "Current prediction:  60.847965240478516 \n",
      "\n",
      "Iteration 6371, Loss: 36.07606506347656, L1: 10.127705574035645, L3: 25.9483585357666\n",
      "Current prediction:  60.844547271728516 \n",
      "\n",
      "Iteration 6372, Loss: 37.02812194824219, L1: 10.165952682495117, L3: 26.86216926574707\n",
      "Current prediction:  60.872257232666016 \n",
      "\n",
      "Iteration 6373, Loss: 35.595054626464844, L1: 10.130387306213379, L3: 25.46466636657715\n",
      "Current prediction:  61.09376907348633 \n",
      "\n",
      "Iteration 6374, Loss: 36.845909118652344, L1: 10.118997573852539, L3: 26.726909637451172\n",
      "Current prediction:  61.46586990356445 \n",
      "\n",
      "Iteration 6375, Loss: 35.785980224609375, L1: 10.089526176452637, L3: 25.696453094482422\n",
      "Current prediction:  61.538211822509766 \n",
      "\n",
      "Iteration 6376, Loss: 37.019813537597656, L1: 10.086085319519043, L3: 26.933727264404297\n",
      "Current prediction:  61.55815505981445 \n",
      "\n",
      "Iteration 6377, Loss: 35.692298889160156, L1: 10.095691680908203, L3: 25.59660530090332\n",
      "Current prediction:  61.56196975708008 \n",
      "\n",
      "Iteration 6378, Loss: 37.214439392089844, L1: 10.12668228149414, L3: 27.087757110595703\n",
      "Current prediction:  61.556522369384766 \n",
      "\n",
      "Iteration 6379, Loss: 35.81719970703125, L1: 10.085162162780762, L3: 25.732036590576172\n",
      "Current prediction:  61.51665496826172 \n",
      "\n",
      "Iteration 6380, Loss: 35.732025146484375, L1: 10.09585189819336, L3: 25.636171340942383\n",
      "Current prediction:  61.42287826538086 \n",
      "\n",
      "Iteration 6381, Loss: 36.04646682739258, L1: 10.149173736572266, L3: 25.897293090820312\n",
      "Current prediction:  61.224674224853516 \n",
      "\n",
      "Iteration 6382, Loss: 36.54622268676758, L1: 10.103185653686523, L3: 26.443037033081055\n",
      "Current prediction:  60.9697380065918 \n",
      "\n",
      "Iteration 6383, Loss: 36.53955841064453, L1: 10.126812934875488, L3: 26.412744522094727\n",
      "Current prediction:  60.9630241394043 \n",
      "\n",
      "Iteration 6384, Loss: 36.598609924316406, L1: 10.091586112976074, L3: 26.50702476501465\n",
      "Current prediction:  61.07491683959961 \n",
      "\n",
      "Iteration 6385, Loss: 37.16242980957031, L1: 10.1065034866333, L3: 27.055925369262695\n",
      "Current prediction:  61.25858688354492 \n",
      "\n",
      "Iteration 6386, Loss: 36.21017837524414, L1: 10.098448753356934, L3: 26.11172866821289\n",
      "Current prediction:  61.50819778442383 \n",
      "\n",
      "Iteration 6387, Loss: 36.790313720703125, L1: 10.121908187866211, L3: 26.66840362548828\n",
      "Current prediction:  61.56397247314453 \n",
      "\n",
      "Iteration 6388, Loss: 35.82797622680664, L1: 10.093964576721191, L3: 25.734012603759766\n",
      "Current prediction:  61.56206512451172 \n",
      "\n",
      "Iteration 6389, Loss: 36.909828186035156, L1: 10.088386535644531, L3: 26.821439743041992\n",
      "Current prediction:  61.506351470947266 \n",
      "\n",
      "Iteration 6390, Loss: 36.36711883544922, L1: 10.06222915649414, L3: 26.304889678955078\n",
      "Current prediction:  61.16058349609375 \n",
      "\n",
      "Iteration 6391, Loss: 35.827335357666016, L1: 10.0734224319458, L3: 25.7539119720459\n",
      "Current prediction:  61.019561767578125 \n",
      "\n",
      "Iteration 6392, Loss: 36.42198181152344, L1: 10.105267524719238, L3: 26.316715240478516\n",
      "Current prediction:  60.906158447265625 \n",
      "\n",
      "Iteration 6393, Loss: 36.37505340576172, L1: 10.1126708984375, L3: 26.262380599975586\n",
      "Current prediction:  61.050655364990234 \n",
      "\n",
      "Iteration 6394, Loss: 35.60717010498047, L1: 10.099568367004395, L3: 25.507600784301758\n",
      "Current prediction:  61.32917404174805 \n",
      "\n",
      "Iteration 6395, Loss: 35.06953811645508, L1: 10.113739013671875, L3: 24.955799102783203\n",
      "Current prediction:  61.387332916259766 \n",
      "\n",
      "Iteration 6396, Loss: 36.225914001464844, L1: 10.078948020935059, L3: 26.1469669342041\n",
      "Current prediction:  61.406883239746094 \n",
      "\n",
      "Iteration 6397, Loss: 36.304256439208984, L1: 10.118826866149902, L3: 26.1854305267334\n",
      "Current prediction:  61.520694732666016 \n",
      "\n",
      "Iteration 6398, Loss: 35.65519714355469, L1: 10.100398063659668, L3: 25.554800033569336\n",
      "Current prediction:  61.54454040527344 \n",
      "\n",
      "Iteration 6399, Loss: 36.2547607421875, L1: 10.095724105834961, L3: 26.159038543701172\n",
      "Current prediction:  61.549400329589844 \n",
      "\n",
      "Iteration 6400, Loss: 36.677146911621094, L1: 10.115204811096191, L3: 26.561941146850586\n",
      "Current prediction:  61.552589416503906 \n",
      "\n",
      "Iteration 6401, Loss: 35.72897720336914, L1: 10.10507869720459, L3: 25.623897552490234\n",
      "Current prediction:  61.54777145385742 \n",
      "\n",
      "Iteration 6402, Loss: 36.65247344970703, L1: 10.103933334350586, L3: 26.548540115356445\n",
      "Current prediction:  61.47416305541992 \n",
      "\n",
      "Iteration 6403, Loss: 36.09318161010742, L1: 10.110301971435547, L3: 25.982879638671875\n",
      "Current prediction:  61.12425231933594 \n",
      "\n",
      "Iteration 6404, Loss: 35.95521926879883, L1: 10.087806701660156, L3: 25.867412567138672\n",
      "Current prediction:  60.8647346496582 \n",
      "\n",
      "Iteration 6405, Loss: 36.413734436035156, L1: 10.100085258483887, L3: 26.313650131225586\n",
      "Current prediction:  60.83732986450195 \n",
      "\n",
      "Iteration 6406, Loss: 36.18524169921875, L1: 10.128610610961914, L3: 26.056629180908203\n",
      "Current prediction:  60.93449020385742 \n",
      "\n",
      "Iteration 6407, Loss: 36.47000503540039, L1: 10.104266166687012, L3: 26.365739822387695\n",
      "Current prediction:  61.22185134887695 \n",
      "\n",
      "Iteration 6408, Loss: 36.827762603759766, L1: 10.06905460357666, L3: 26.75870704650879\n",
      "Current prediction:  61.55390930175781 \n",
      "\n",
      "Iteration 6409, Loss: 36.36460494995117, L1: 10.063383102416992, L3: 26.30122184753418\n",
      "Current prediction:  61.59059143066406 \n",
      "\n",
      "Iteration 6410, Loss: 36.99397659301758, L1: 10.064645767211914, L3: 26.929330825805664\n",
      "Current prediction:  61.59324645996094 \n",
      "\n",
      "Iteration 6411, Loss: 36.84564208984375, L1: 10.115201950073242, L3: 26.730438232421875\n",
      "Current prediction:  61.59279251098633 \n",
      "\n",
      "Iteration 6412, Loss: 36.55076599121094, L1: 10.098933219909668, L3: 26.451831817626953\n",
      "Current prediction:  61.59031295776367 \n",
      "\n",
      "Iteration 6413, Loss: 36.51784133911133, L1: 10.104939460754395, L3: 26.41290283203125\n",
      "Current prediction:  61.58200454711914 \n",
      "\n",
      "Iteration 6414, Loss: 35.50370788574219, L1: 10.130658149719238, L3: 25.373050689697266\n",
      "Current prediction:  61.57539367675781 \n",
      "\n",
      "Iteration 6415, Loss: 35.49085235595703, L1: 10.112251281738281, L3: 25.37860107421875\n",
      "Current prediction:  61.56672668457031 \n",
      "\n",
      "Iteration 6416, Loss: 35.68930435180664, L1: 10.104276657104492, L3: 25.58502769470215\n",
      "Current prediction:  61.5546760559082 \n",
      "\n",
      "Iteration 6417, Loss: 37.302772521972656, L1: 10.105266571044922, L3: 27.197505950927734\n",
      "Current prediction:  61.5376091003418 \n",
      "\n",
      "Iteration 6418, Loss: 35.585227966308594, L1: 10.111693382263184, L3: 25.473533630371094\n",
      "Current prediction:  61.45713424682617 \n",
      "\n",
      "Iteration 6419, Loss: 37.53506088256836, L1: 10.096806526184082, L3: 27.43825340270996\n",
      "Current prediction:  60.96297836303711 \n",
      "\n",
      "Iteration 6420, Loss: 36.75331115722656, L1: 10.124484062194824, L3: 26.628828048706055\n",
      "Current prediction:  60.74348068237305 \n",
      "\n",
      "Iteration 6421, Loss: 36.357940673828125, L1: 10.211309432983398, L3: 26.146631240844727\n",
      "Current prediction:  60.72996139526367 \n",
      "\n",
      "Iteration 6422, Loss: 36.72582244873047, L1: 10.205795288085938, L3: 26.520029067993164\n",
      "Current prediction:  60.77056884765625 \n",
      "\n",
      "Iteration 6423, Loss: 36.5116081237793, L1: 10.182856559753418, L3: 26.328750610351562\n",
      "Current prediction:  60.997039794921875 \n",
      "\n",
      "Iteration 6424, Loss: 36.33203125, L1: 10.160970687866211, L3: 26.171058654785156\n",
      "Current prediction:  61.4683837890625 \n",
      "\n",
      "Iteration 6425, Loss: 35.81267166137695, L1: 10.166138648986816, L3: 25.64653205871582\n",
      "Current prediction:  61.5019645690918 \n",
      "\n",
      "Iteration 6426, Loss: 37.141197204589844, L1: 10.162134170532227, L3: 26.979063034057617\n",
      "Current prediction:  61.510498046875 \n",
      "\n",
      "Iteration 6427, Loss: 36.09327697753906, L1: 10.14574146270752, L3: 25.947534561157227\n",
      "Current prediction:  61.51280975341797 \n",
      "\n",
      "Iteration 6428, Loss: 36.046077728271484, L1: 10.138596534729004, L3: 25.907480239868164\n",
      "Current prediction:  61.51938247680664 \n",
      "\n",
      "Iteration 6429, Loss: 35.19417953491211, L1: 10.147583961486816, L3: 25.046594619750977\n",
      "Current prediction:  61.52047348022461 \n",
      "\n",
      "Iteration 6430, Loss: 36.04746627807617, L1: 10.14291763305664, L3: 25.90454864501953\n",
      "Current prediction:  61.47861099243164 \n",
      "\n",
      "Iteration 6431, Loss: 36.66056823730469, L1: 10.12396240234375, L3: 26.536605834960938\n",
      "Current prediction:  61.27814865112305 \n",
      "\n",
      "Iteration 6432, Loss: 36.41864013671875, L1: 10.115132331848145, L3: 26.303508758544922\n",
      "Current prediction:  60.94710922241211 \n",
      "\n",
      "Iteration 6433, Loss: 36.385860443115234, L1: 10.113673210144043, L3: 26.272188186645508\n",
      "Current prediction:  60.766361236572266 \n",
      "\n",
      "Iteration 6434, Loss: 35.75154113769531, L1: 10.16812515258789, L3: 25.583415985107422\n",
      "Current prediction:  60.75606918334961 \n",
      "\n",
      "Iteration 6435, Loss: 36.47409439086914, L1: 10.184638023376465, L3: 26.28945541381836\n",
      "Current prediction:  60.76101303100586 \n",
      "\n",
      "Iteration 6436, Loss: 37.474979400634766, L1: 10.232072830200195, L3: 27.24290657043457\n",
      "Current prediction:  60.76593017578125 \n",
      "\n",
      "Iteration 6437, Loss: 35.601715087890625, L1: 10.179817199707031, L3: 25.42189598083496\n",
      "Current prediction:  60.77457809448242 \n",
      "\n",
      "Iteration 6438, Loss: 35.50664520263672, L1: 10.180028915405273, L3: 25.326618194580078\n",
      "Current prediction:  60.82184600830078 \n",
      "\n",
      "Iteration 6439, Loss: 35.98870849609375, L1: 10.138376235961914, L3: 25.850332260131836\n",
      "Current prediction:  61.009151458740234 \n",
      "\n",
      "Iteration 6440, Loss: 36.672672271728516, L1: 10.130936622619629, L3: 26.54173469543457\n",
      "Current prediction:  61.22336196899414 \n",
      "\n",
      "Iteration 6441, Loss: 37.39384841918945, L1: 10.205719947814941, L3: 27.188127517700195\n",
      "Current prediction:  61.432518005371094 \n",
      "\n",
      "Iteration 6442, Loss: 36.298851013183594, L1: 10.146993637084961, L3: 26.15185546875\n",
      "Current prediction:  61.42582321166992 \n",
      "\n",
      "Iteration 6443, Loss: 36.331077575683594, L1: 10.115313529968262, L3: 26.21576499938965\n",
      "Current prediction:  61.47980499267578 \n",
      "\n",
      "Iteration 6444, Loss: 36.49345397949219, L1: 10.302717208862305, L3: 26.19073486328125\n",
      "Current prediction:  61.39787292480469 \n",
      "\n",
      "Iteration 6445, Loss: 36.799007415771484, L1: 10.2138032913208, L3: 26.585203170776367\n",
      "Current prediction:  61.29754638671875 \n",
      "\n",
      "Iteration 6446, Loss: 36.09583282470703, L1: 10.27542495727539, L3: 25.820405960083008\n",
      "Current prediction:  60.541419982910156 \n",
      "\n",
      "Iteration 6447, Loss: 36.27124786376953, L1: 10.36850357055664, L3: 25.902746200561523\n",
      "Current prediction:  60.455482482910156 \n",
      "\n",
      "Iteration 6448, Loss: 36.32611846923828, L1: 10.480623245239258, L3: 25.845497131347656\n",
      "Current prediction:  60.401336669921875 \n",
      "\n",
      "Iteration 6449, Loss: 36.26549530029297, L1: 10.588184356689453, L3: 25.67731285095215\n",
      "Current prediction:  60.424434661865234 \n",
      "\n",
      "Iteration 6450, Loss: 37.05631637573242, L1: 10.588091850280762, L3: 26.468225479125977\n",
      "Current prediction:  60.49382400512695 \n",
      "\n",
      "Iteration 6451, Loss: 35.949607849121094, L1: 10.451845169067383, L3: 25.497760772705078\n",
      "Current prediction:  60.55677032470703 \n",
      "\n",
      "Iteration 6452, Loss: 36.77965545654297, L1: 10.31279182434082, L3: 26.466861724853516\n",
      "Current prediction:  60.55598068237305 \n",
      "\n",
      "Iteration 6453, Loss: 36.59568405151367, L1: 10.614506721496582, L3: 25.981176376342773\n",
      "Current prediction:  61.294105529785156 \n",
      "\n",
      "Iteration 6454, Loss: 37.32537078857422, L1: 10.060583114624023, L3: 27.264787673950195\n",
      "Current prediction:  61.73704147338867 \n",
      "\n",
      "Iteration 6455, Loss: 36.83012390136719, L1: 10.037912368774414, L3: 26.792213439941406\n",
      "Current prediction:  61.864952087402344 \n",
      "\n",
      "Iteration 6456, Loss: 36.9268684387207, L1: 9.956259727478027, L3: 26.970609664916992\n",
      "Current prediction:  61.894798278808594 \n",
      "\n",
      "Iteration 6457, Loss: 37.210968017578125, L1: 9.909093856811523, L3: 27.3018741607666\n",
      "Current prediction:  61.88656997680664 \n",
      "\n",
      "Iteration 6458, Loss: 35.99625015258789, L1: 9.892592430114746, L3: 26.103656768798828\n",
      "Current prediction:  61.73386001586914 \n",
      "\n",
      "Iteration 6459, Loss: 36.27668380737305, L1: 9.855876922607422, L3: 26.420806884765625\n",
      "Current prediction:  61.36688995361328 \n",
      "\n",
      "Iteration 6460, Loss: 36.47685241699219, L1: 9.940765380859375, L3: 26.536087036132812\n",
      "Current prediction:  61.206336975097656 \n",
      "\n",
      "Iteration 6461, Loss: 36.2542724609375, L1: 9.93850040435791, L3: 26.315771102905273\n",
      "Current prediction:  61.154083251953125 \n",
      "\n",
      "Iteration 6462, Loss: 35.71681594848633, L1: 10.030085563659668, L3: 25.686731338500977\n",
      "Current prediction:  61.08342361450195 \n",
      "\n",
      "Iteration 6463, Loss: 36.431522369384766, L1: 10.031598091125488, L3: 26.399925231933594\n",
      "Current prediction:  61.07342529296875 \n",
      "\n",
      "Iteration 6464, Loss: 36.49477005004883, L1: 10.000845909118652, L3: 26.493925094604492\n",
      "Current prediction:  61.321170806884766 \n",
      "\n",
      "Iteration 6465, Loss: 36.730499267578125, L1: 9.93726921081543, L3: 26.793228149414062\n",
      "Current prediction:  61.7088623046875 \n",
      "\n",
      "Iteration 6466, Loss: 36.71995162963867, L1: 9.95207691192627, L3: 26.76787567138672\n",
      "Current prediction:  61.693809509277344 \n",
      "\n",
      "Iteration 6467, Loss: 36.08818435668945, L1: 9.985506057739258, L3: 26.102678298950195\n",
      "Current prediction:  61.645416259765625 \n",
      "\n",
      "Iteration 6468, Loss: 36.66324234008789, L1: 9.999605178833008, L3: 26.663637161254883\n",
      "Current prediction:  61.408870697021484 \n",
      "\n",
      "Iteration 6469, Loss: 35.530670166015625, L1: 10.022025108337402, L3: 25.508644104003906\n",
      "Current prediction:  60.97945022583008 \n",
      "\n",
      "Iteration 6470, Loss: 36.09026336669922, L1: 10.054975509643555, L3: 26.03528594970703\n",
      "Current prediction:  60.81550598144531 \n",
      "\n",
      "Iteration 6471, Loss: 36.595951080322266, L1: 10.155390739440918, L3: 26.44055938720703\n",
      "Current prediction:  60.77461242675781 \n",
      "\n",
      "Iteration 6472, Loss: 36.391597747802734, L1: 10.162485122680664, L3: 26.22911262512207\n",
      "Current prediction:  60.80891036987305 \n",
      "\n",
      "Iteration 6473, Loss: 36.9012336730957, L1: 10.198421478271484, L3: 26.70281219482422\n",
      "Current prediction:  61.3651237487793 \n",
      "\n",
      "Iteration 6474, Loss: 36.09832763671875, L1: 10.137331008911133, L3: 25.960996627807617\n",
      "Current prediction:  61.45156478881836 \n",
      "\n",
      "Iteration 6475, Loss: 38.17625427246094, L1: 10.185989379882812, L3: 27.990262985229492\n",
      "Current prediction:  61.444828033447266 \n",
      "\n",
      "Iteration 6476, Loss: 36.05706787109375, L1: 10.20669174194336, L3: 25.850378036499023\n",
      "Current prediction:  61.42924118041992 \n",
      "\n",
      "Iteration 6477, Loss: 36.438358306884766, L1: 10.215441703796387, L3: 26.222915649414062\n",
      "Current prediction:  61.41523742675781 \n",
      "\n",
      "Iteration 6478, Loss: 36.75469970703125, L1: 10.258282661437988, L3: 26.496417999267578\n",
      "Current prediction:  61.40583038330078 \n",
      "\n",
      "Iteration 6479, Loss: 37.04997253417969, L1: 10.236312866210938, L3: 26.81365966796875\n",
      "Current prediction:  61.3986930847168 \n",
      "\n",
      "Iteration 6480, Loss: 36.10255813598633, L1: 10.278817176818848, L3: 25.823740005493164\n",
      "Current prediction:  61.3963737487793 \n",
      "\n",
      "Iteration 6481, Loss: 37.17856216430664, L1: 10.303350448608398, L3: 26.875211715698242\n",
      "Current prediction:  61.39670944213867 \n",
      "\n",
      "Iteration 6482, Loss: 35.782196044921875, L1: 10.231820106506348, L3: 25.55037498474121\n",
      "Current prediction:  61.39850997924805 \n",
      "\n",
      "Iteration 6483, Loss: 37.71961975097656, L1: 10.282561302185059, L3: 27.43705940246582\n",
      "Current prediction:  61.39882278442383 \n",
      "\n",
      "Iteration 6484, Loss: 37.08285140991211, L1: 10.282808303833008, L3: 26.8000431060791\n",
      "Current prediction:  61.398956298828125 \n",
      "\n",
      "Iteration 6485, Loss: 36.851558685302734, L1: 10.300871849060059, L3: 26.550687789916992\n",
      "Current prediction:  61.398685455322266 \n",
      "\n",
      "Iteration 6486, Loss: 35.6126708984375, L1: 10.285435676574707, L3: 25.32723617553711\n",
      "Current prediction:  61.40110397338867 \n",
      "\n",
      "Iteration 6487, Loss: 36.872596740722656, L1: 10.267958641052246, L3: 26.604637145996094\n",
      "Current prediction:  61.40178680419922 \n",
      "\n",
      "Iteration 6488, Loss: 35.8525390625, L1: 10.265974998474121, L3: 25.586565017700195\n",
      "Current prediction:  61.40742111206055 \n",
      "\n",
      "Iteration 6489, Loss: 36.42527770996094, L1: 10.248788833618164, L3: 26.17648696899414\n",
      "Current prediction:  61.41404724121094 \n",
      "\n",
      "Iteration 6490, Loss: 35.43166732788086, L1: 10.228180885314941, L3: 25.203487396240234\n",
      "Current prediction:  61.41370391845703 \n",
      "\n",
      "Iteration 6491, Loss: 36.66766357421875, L1: 10.205657958984375, L3: 26.462005615234375\n",
      "Current prediction:  61.30430221557617 \n",
      "\n",
      "Iteration 6492, Loss: 36.920623779296875, L1: 10.206992149353027, L3: 26.71363067626953\n",
      "Current prediction:  60.813201904296875 \n",
      "\n",
      "Iteration 6493, Loss: 37.071266174316406, L1: 10.2145414352417, L3: 26.856725692749023\n",
      "Current prediction:  60.6763916015625 \n",
      "\n",
      "Iteration 6494, Loss: 35.99671173095703, L1: 10.228739738464355, L3: 25.76797103881836\n",
      "Current prediction:  60.68387985229492 \n",
      "\n",
      "Iteration 6495, Loss: 35.309383392333984, L1: 10.259270668029785, L3: 25.050113677978516\n",
      "Current prediction:  60.70466613769531 \n",
      "\n",
      "Iteration 6496, Loss: 36.222049713134766, L1: 10.313403129577637, L3: 25.908645629882812\n",
      "Current prediction:  60.728511810302734 \n",
      "\n",
      "Iteration 6497, Loss: 35.71860885620117, L1: 10.263415336608887, L3: 25.4551944732666\n",
      "Current prediction:  60.7535400390625 \n",
      "\n",
      "Iteration 6498, Loss: 35.964508056640625, L1: 10.204212188720703, L3: 25.760297775268555\n",
      "Current prediction:  60.7793083190918 \n",
      "\n",
      "Iteration 6499, Loss: 36.01630783081055, L1: 10.153661727905273, L3: 25.862646102905273\n",
      "Current prediction:  60.86376190185547 \n",
      "\n",
      "Iteration 6500, Loss: 36.47205352783203, L1: 10.089353561401367, L3: 26.382699966430664\n",
      "Current prediction:  61.084251403808594 \n",
      "\n",
      "Iteration 6501, Loss: 36.00878143310547, L1: 10.065072059631348, L3: 25.943710327148438\n",
      "Current prediction:  61.344730377197266 \n",
      "\n",
      "Iteration 6502, Loss: 35.39906311035156, L1: 10.033899307250977, L3: 25.36516571044922\n",
      "Current prediction:  61.51573181152344 \n",
      "\n",
      "Iteration 6503, Loss: 37.63518142700195, L1: 10.038166999816895, L3: 27.597015380859375\n",
      "Current prediction:  61.50718307495117 \n",
      "\n",
      "Iteration 6504, Loss: 36.669532775878906, L1: 10.052260398864746, L3: 26.617271423339844\n",
      "Current prediction:  61.413272857666016 \n",
      "\n",
      "Iteration 6505, Loss: 36.43946075439453, L1: 10.038458824157715, L3: 26.401002883911133\n",
      "Current prediction:  61.32630920410156 \n",
      "\n",
      "Iteration 6506, Loss: 35.78404998779297, L1: 10.0564603805542, L3: 25.727590560913086\n",
      "Current prediction:  61.15626525878906 \n",
      "\n",
      "Iteration 6507, Loss: 35.808937072753906, L1: 10.038529396057129, L3: 25.77040672302246\n",
      "Current prediction:  60.99776840209961 \n",
      "\n",
      "Iteration 6508, Loss: 35.20926284790039, L1: 10.043768882751465, L3: 25.16549301147461\n",
      "Current prediction:  61.00779724121094 \n",
      "\n",
      "Iteration 6509, Loss: 36.745567321777344, L1: 10.065933227539062, L3: 26.67963218688965\n",
      "Current prediction:  61.15616226196289 \n",
      "\n",
      "Iteration 6510, Loss: 36.78970718383789, L1: 10.060433387756348, L3: 26.72927474975586\n",
      "Current prediction:  61.48728561401367 \n",
      "\n",
      "Iteration 6511, Loss: 35.95307922363281, L1: 10.060364723205566, L3: 25.89271354675293\n",
      "Current prediction:  61.55609130859375 \n",
      "\n",
      "Iteration 6512, Loss: 36.13218307495117, L1: 10.037285804748535, L3: 26.09489631652832\n",
      "Current prediction:  61.56922149658203 \n",
      "\n",
      "Iteration 6513, Loss: 35.46372604370117, L1: 10.063773155212402, L3: 25.399953842163086\n",
      "Current prediction:  61.51981735229492 \n",
      "\n",
      "Iteration 6514, Loss: 36.68389129638672, L1: 10.05833625793457, L3: 26.62555694580078\n",
      "Current prediction:  61.330257415771484 \n",
      "\n",
      "Iteration 6515, Loss: 35.143394470214844, L1: 10.054550170898438, L3: 25.08884620666504\n",
      "Current prediction:  60.93791198730469 \n",
      "\n",
      "Iteration 6516, Loss: 36.59080505371094, L1: 10.121540069580078, L3: 26.469263076782227\n",
      "Current prediction:  60.844276428222656 \n",
      "\n",
      "Iteration 6517, Loss: 36.49003982543945, L1: 10.101770401000977, L3: 26.388269424438477\n",
      "Current prediction:  60.835662841796875 \n",
      "\n",
      "Iteration 6518, Loss: 36.91606903076172, L1: 10.118185043334961, L3: 26.797882080078125\n",
      "Current prediction:  60.90786361694336 \n",
      "\n",
      "Iteration 6519, Loss: 36.42190933227539, L1: 10.084185600280762, L3: 26.337724685668945\n",
      "Current prediction:  61.14898681640625 \n",
      "\n",
      "Iteration 6520, Loss: 35.65013122558594, L1: 10.07638931274414, L3: 25.573740005493164\n",
      "Current prediction:  60.99245834350586 \n",
      "\n",
      "Iteration 6521, Loss: 35.986053466796875, L1: 10.093496322631836, L3: 25.89255714416504\n",
      "Current prediction:  60.93069839477539 \n",
      "\n",
      "Iteration 6522, Loss: 36.026668548583984, L1: 10.088749885559082, L3: 25.93791961669922\n",
      "Current prediction:  61.0122184753418 \n",
      "\n",
      "Iteration 6523, Loss: 35.88914489746094, L1: 10.094179153442383, L3: 25.794967651367188\n",
      "Current prediction:  61.174354553222656 \n",
      "\n",
      "Iteration 6524, Loss: 35.74721908569336, L1: 10.03287124633789, L3: 25.71434783935547\n",
      "Current prediction:  61.509010314941406 \n",
      "\n",
      "Iteration 6525, Loss: 36.28949737548828, L1: 10.06148910522461, L3: 26.228010177612305\n",
      "Current prediction:  61.5749397277832 \n",
      "\n",
      "Iteration 6526, Loss: 36.33832550048828, L1: 10.089434623718262, L3: 26.248889923095703\n",
      "Current prediction:  61.58012390136719 \n",
      "\n",
      "Iteration 6527, Loss: 36.12611770629883, L1: 10.06806755065918, L3: 26.05805015563965\n",
      "Current prediction:  61.56840133666992 \n",
      "\n",
      "Iteration 6528, Loss: 37.12879943847656, L1: 10.056044578552246, L3: 27.072755813598633\n",
      "Current prediction:  61.403106689453125 \n",
      "\n",
      "Iteration 6529, Loss: 36.82438278198242, L1: 10.0924072265625, L3: 26.731975555419922\n",
      "Current prediction:  60.90758514404297 \n",
      "\n",
      "Iteration 6530, Loss: 35.67786407470703, L1: 10.102388381958008, L3: 25.575477600097656\n",
      "Current prediction:  60.79463577270508 \n",
      "\n",
      "Iteration 6531, Loss: 36.275848388671875, L1: 10.127909660339355, L3: 26.147939682006836\n",
      "Current prediction:  60.781715393066406 \n",
      "\n",
      "Iteration 6532, Loss: 35.63699722290039, L1: 10.19626522064209, L3: 25.440732955932617\n",
      "Current prediction:  60.79072952270508 \n",
      "\n",
      "Iteration 6533, Loss: 36.318206787109375, L1: 10.17136001586914, L3: 26.146848678588867\n",
      "Current prediction:  60.817787170410156 \n",
      "\n",
      "Iteration 6534, Loss: 36.558990478515625, L1: 10.136652946472168, L3: 26.42233657836914\n",
      "Current prediction:  61.02781295776367 \n",
      "\n",
      "Iteration 6535, Loss: 36.442893981933594, L1: 10.096158981323242, L3: 26.34673500061035\n",
      "Current prediction:  61.361671447753906 \n",
      "\n",
      "Iteration 6536, Loss: 35.63105392456055, L1: 10.059901237487793, L3: 25.571151733398438\n",
      "Current prediction:  61.504600524902344 \n",
      "\n",
      "Iteration 6537, Loss: 37.48149490356445, L1: 10.084635734558105, L3: 27.396860122680664\n",
      "Current prediction:  61.48102951049805 \n",
      "\n",
      "Iteration 6538, Loss: 37.06601333618164, L1: 10.087493896484375, L3: 26.978519439697266\n",
      "Current prediction:  61.41508865356445 \n",
      "\n",
      "Iteration 6539, Loss: 36.362979888916016, L1: 10.081622123718262, L3: 26.281356811523438\n",
      "Current prediction:  61.01385498046875 \n",
      "\n",
      "Iteration 6540, Loss: 36.81842041015625, L1: 10.10594367980957, L3: 26.712474822998047\n",
      "Current prediction:  60.83000564575195 \n",
      "\n",
      "Iteration 6541, Loss: 36.23872756958008, L1: 10.114838600158691, L3: 26.123889923095703\n",
      "Current prediction:  60.82420349121094 \n",
      "\n",
      "Iteration 6542, Loss: 36.24990463256836, L1: 10.118134498596191, L3: 26.131771087646484\n",
      "Current prediction:  60.93404769897461 \n",
      "\n",
      "Iteration 6543, Loss: 36.26934051513672, L1: 10.123847961425781, L3: 26.14549446105957\n",
      "Current prediction:  61.33572769165039 \n",
      "\n",
      "Iteration 6544, Loss: 35.9658088684082, L1: 10.085005760192871, L3: 25.88080406188965\n",
      "Current prediction:  61.49033737182617 \n",
      "\n",
      "Iteration 6545, Loss: 35.5483512878418, L1: 10.1084566116333, L3: 25.43989372253418\n",
      "Current prediction:  61.48525619506836 \n",
      "\n",
      "Iteration 6546, Loss: 36.57386779785156, L1: 10.090651512145996, L3: 26.48321533203125\n",
      "Current prediction:  61.44976043701172 \n",
      "\n",
      "Iteration 6547, Loss: 36.40047836303711, L1: 10.094292640686035, L3: 26.30618667602539\n",
      "Current prediction:  61.48720932006836 \n",
      "\n",
      "Iteration 6548, Loss: 35.977333068847656, L1: 10.101459503173828, L3: 25.87587547302246\n",
      "Current prediction:  61.50708770751953 \n",
      "\n",
      "Iteration 6549, Loss: 36.46736145019531, L1: 10.065771102905273, L3: 26.40159034729004\n",
      "Current prediction:  61.39441680908203 \n",
      "\n",
      "Iteration 6550, Loss: 36.1515007019043, L1: 10.098981857299805, L3: 26.052518844604492\n",
      "Current prediction:  61.01445007324219 \n",
      "\n",
      "Iteration 6551, Loss: 35.510189056396484, L1: 10.09970760345459, L3: 25.410480499267578\n",
      "Current prediction:  60.91558074951172 \n",
      "\n",
      "Iteration 6552, Loss: 36.194942474365234, L1: 10.09010124206543, L3: 26.104841232299805\n",
      "Current prediction:  60.90699768066406 \n",
      "\n",
      "Iteration 6553, Loss: 36.19302749633789, L1: 10.087223052978516, L3: 26.105804443359375\n",
      "Current prediction:  61.24899673461914 \n",
      "\n",
      "Iteration 6554, Loss: 36.28116989135742, L1: 10.030616760253906, L3: 26.250553131103516\n",
      "Current prediction:  61.51057052612305 \n",
      "\n",
      "Iteration 6555, Loss: 37.43574523925781, L1: 10.068914413452148, L3: 27.366830825805664\n",
      "Current prediction:  61.55580139160156 \n",
      "\n",
      "Iteration 6556, Loss: 36.58702087402344, L1: 10.092964172363281, L3: 26.494056701660156\n",
      "Current prediction:  61.556556701660156 \n",
      "\n",
      "Iteration 6557, Loss: 36.143272399902344, L1: 10.082818031311035, L3: 26.060453414916992\n",
      "Current prediction:  61.5518798828125 \n",
      "\n",
      "Iteration 6558, Loss: 35.846256256103516, L1: 10.082016944885254, L3: 25.764240264892578\n",
      "Current prediction:  61.546566009521484 \n",
      "\n",
      "Iteration 6559, Loss: 35.52050018310547, L1: 10.094064712524414, L3: 25.426435470581055\n",
      "Current prediction:  61.541839599609375 \n",
      "\n",
      "Iteration 6560, Loss: 36.8417854309082, L1: 10.14096736907959, L3: 26.70081901550293\n",
      "Current prediction:  61.53695297241211 \n",
      "\n",
      "Iteration 6561, Loss: 35.87266159057617, L1: 10.152472496032715, L3: 25.720190048217773\n",
      "Current prediction:  61.533111572265625 \n",
      "\n",
      "Iteration 6562, Loss: 37.04338455200195, L1: 10.192082405090332, L3: 26.851303100585938\n",
      "Current prediction:  61.52592086791992 \n",
      "\n",
      "Iteration 6563, Loss: 36.04035186767578, L1: 10.139076232910156, L3: 25.901275634765625\n",
      "Current prediction:  61.515380859375 \n",
      "\n",
      "Iteration 6564, Loss: 35.883460998535156, L1: 10.136784553527832, L3: 25.746675491333008\n",
      "Current prediction:  61.502559661865234 \n",
      "\n",
      "Iteration 6565, Loss: 36.82182312011719, L1: 10.12380599975586, L3: 26.698015213012695\n",
      "Current prediction:  61.421356201171875 \n",
      "\n",
      "Iteration 6566, Loss: 35.17831039428711, L1: 10.1226167678833, L3: 25.055694580078125\n",
      "Current prediction:  60.90258026123047 \n",
      "\n",
      "Iteration 6567, Loss: 36.60574722290039, L1: 10.183128356933594, L3: 26.422618865966797\n",
      "Current prediction:  60.73442459106445 \n",
      "\n",
      "Iteration 6568, Loss: 36.935462951660156, L1: 10.162832260131836, L3: 26.772628784179688\n",
      "Current prediction:  60.72451400756836 \n",
      "\n",
      "Iteration 6569, Loss: 36.715782165527344, L1: 10.179099082946777, L3: 26.536684036254883\n",
      "Current prediction:  60.804100036621094 \n",
      "\n",
      "Iteration 6570, Loss: 35.99148178100586, L1: 10.17061996459961, L3: 25.82086181640625\n",
      "Current prediction:  61.067596435546875 \n",
      "\n",
      "Iteration 6571, Loss: 36.54705047607422, L1: 10.131815910339355, L3: 26.415233612060547\n",
      "Current prediction:  61.47092819213867 \n",
      "\n",
      "Iteration 6572, Loss: 36.9085578918457, L1: 10.129134178161621, L3: 26.7794246673584\n",
      "Current prediction:  61.531070709228516 \n",
      "\n",
      "Iteration 6573, Loss: 36.34336853027344, L1: 10.11971664428711, L3: 26.22365379333496\n",
      "Current prediction:  61.54793930053711 \n",
      "\n",
      "Iteration 6574, Loss: 36.721473693847656, L1: 10.119635581970215, L3: 26.601837158203125\n",
      "Current prediction:  61.56047439575195 \n",
      "\n",
      "Iteration 6575, Loss: 35.73125076293945, L1: 10.09669017791748, L3: 25.63456153869629\n",
      "Current prediction:  61.56861877441406 \n",
      "\n",
      "Iteration 6576, Loss: 36.46245574951172, L1: 10.111591339111328, L3: 26.350862503051758\n",
      "Current prediction:  61.573692321777344 \n",
      "\n",
      "Iteration 6577, Loss: 36.55986404418945, L1: 10.1265230178833, L3: 26.433340072631836\n",
      "Current prediction:  61.574344635009766 \n",
      "\n",
      "Iteration 6578, Loss: 36.59931564331055, L1: 10.091002464294434, L3: 26.508312225341797\n",
      "Current prediction:  61.57154083251953 \n",
      "\n",
      "Iteration 6579, Loss: 36.31726837158203, L1: 10.11618709564209, L3: 26.201080322265625\n",
      "Current prediction:  61.56839370727539 \n",
      "\n",
      "Iteration 6580, Loss: 35.87580108642578, L1: 10.111783981323242, L3: 25.764015197753906\n",
      "Current prediction:  61.56208419799805 \n",
      "\n",
      "Iteration 6581, Loss: 37.31365966796875, L1: 10.111473083496094, L3: 27.202186584472656\n",
      "Current prediction:  61.52619552612305 \n",
      "\n",
      "Iteration 6582, Loss: 36.718406677246094, L1: 10.078455924987793, L3: 26.639949798583984\n",
      "Current prediction:  60.94508743286133 \n",
      "\n",
      "Iteration 6583, Loss: 36.0721435546875, L1: 10.077163696289062, L3: 25.994977951049805\n",
      "Current prediction:  60.73705291748047 \n",
      "\n",
      "Iteration 6584, Loss: 36.39115905761719, L1: 10.181297302246094, L3: 26.209863662719727\n",
      "Current prediction:  60.72545623779297 \n",
      "\n",
      "Iteration 6585, Loss: 35.60417938232422, L1: 10.25947380065918, L3: 25.344707489013672\n",
      "Current prediction:  60.721824645996094 \n",
      "\n",
      "Iteration 6586, Loss: 36.43408966064453, L1: 10.320603370666504, L3: 26.113487243652344\n",
      "Current prediction:  60.71951675415039 \n",
      "\n",
      "Iteration 6587, Loss: 36.344303131103516, L1: 10.249245643615723, L3: 26.095056533813477\n",
      "Current prediction:  60.724891662597656 \n",
      "\n",
      "Iteration 6588, Loss: 36.277748107910156, L1: 10.232500076293945, L3: 26.045246124267578\n",
      "Current prediction:  60.735111236572266 \n",
      "\n",
      "Iteration 6589, Loss: 36.276824951171875, L1: 10.190767288208008, L3: 26.086055755615234\n",
      "Current prediction:  60.865760803222656 \n",
      "\n",
      "Iteration 6590, Loss: 35.72571563720703, L1: 10.105095863342285, L3: 25.62061882019043\n",
      "Current prediction:  61.48627853393555 \n",
      "\n",
      "Iteration 6591, Loss: 35.5186767578125, L1: 10.096488952636719, L3: 25.422189712524414\n",
      "Current prediction:  61.54486083984375 \n",
      "\n",
      "Iteration 6592, Loss: 35.457862854003906, L1: 10.120881080627441, L3: 25.33698272705078\n",
      "Current prediction:  61.55718994140625 \n",
      "\n",
      "Iteration 6593, Loss: 36.482025146484375, L1: 10.095903396606445, L3: 26.38612174987793\n",
      "Current prediction:  61.56370162963867 \n",
      "\n",
      "Iteration 6594, Loss: 35.53118133544922, L1: 10.115621566772461, L3: 25.41556167602539\n",
      "Current prediction:  61.56614685058594 \n",
      "\n",
      "Iteration 6595, Loss: 36.222816467285156, L1: 10.110780715942383, L3: 26.11203384399414\n",
      "Current prediction:  61.56782150268555 \n",
      "\n",
      "Iteration 6596, Loss: 36.17460250854492, L1: 10.103777885437012, L3: 26.070825576782227\n",
      "Current prediction:  61.56976318359375 \n",
      "\n",
      "Iteration 6597, Loss: 36.824607849121094, L1: 10.11141300201416, L3: 26.713193893432617\n",
      "Current prediction:  61.56369400024414 \n",
      "\n",
      "Iteration 6598, Loss: 36.68292999267578, L1: 10.106010437011719, L3: 26.576921463012695\n",
      "Current prediction:  61.54197311401367 \n",
      "\n",
      "Iteration 6599, Loss: 35.94023132324219, L1: 10.062834739685059, L3: 25.877395629882812\n",
      "Current prediction:  60.89329147338867 \n",
      "\n",
      "Iteration 6600, Loss: 37.0173225402832, L1: 10.067444801330566, L3: 26.949878692626953\n",
      "Current prediction:  60.74613571166992 \n",
      "\n",
      "Iteration 6601, Loss: 37.06899642944336, L1: 10.165711402893066, L3: 26.903284072875977\n",
      "Current prediction:  60.741214752197266 \n",
      "\n",
      "Iteration 6602, Loss: 36.758602142333984, L1: 10.25253677368164, L3: 26.506065368652344\n",
      "Current prediction:  60.74113464355469 \n",
      "\n",
      "Iteration 6603, Loss: 35.94541931152344, L1: 10.228530883789062, L3: 25.716890335083008\n",
      "Current prediction:  60.74055480957031 \n",
      "\n",
      "Iteration 6604, Loss: 37.13289260864258, L1: 10.1967134475708, L3: 26.93617820739746\n",
      "Current prediction:  60.741573333740234 \n",
      "\n",
      "Iteration 6605, Loss: 36.52979278564453, L1: 10.232359886169434, L3: 26.29743194580078\n",
      "Current prediction:  60.74172592163086 \n",
      "\n",
      "Iteration 6606, Loss: 36.16130447387695, L1: 10.1863431930542, L3: 25.97496223449707\n",
      "Current prediction:  60.74827575683594 \n",
      "\n",
      "Iteration 6607, Loss: 35.444854736328125, L1: 10.170191764831543, L3: 25.2746639251709\n",
      "Current prediction:  60.80625915527344 \n",
      "\n",
      "Iteration 6608, Loss: 36.59090805053711, L1: 10.120017051696777, L3: 26.470890045166016\n",
      "Current prediction:  61.08877944946289 \n",
      "\n",
      "Iteration 6609, Loss: 35.703041076660156, L1: 10.116827964782715, L3: 25.586214065551758\n",
      "Current prediction:  61.49605178833008 \n",
      "\n",
      "Iteration 6610, Loss: 35.64027404785156, L1: 10.123126983642578, L3: 25.517147064208984\n",
      "Current prediction:  61.54324722290039 \n",
      "\n",
      "Iteration 6611, Loss: 37.032230377197266, L1: 10.10738468170166, L3: 26.924846649169922\n",
      "Current prediction:  61.54542541503906 \n",
      "\n",
      "Iteration 6612, Loss: 36.0484619140625, L1: 10.096657752990723, L3: 25.951805114746094\n",
      "Current prediction:  61.545677185058594 \n",
      "\n",
      "Iteration 6613, Loss: 36.367191314697266, L1: 10.118155479431152, L3: 26.24903678894043\n",
      "Current prediction:  61.54374313354492 \n",
      "\n",
      "Iteration 6614, Loss: 35.87583541870117, L1: 10.146425247192383, L3: 25.72941017150879\n",
      "Current prediction:  61.54188537597656 \n",
      "\n",
      "Iteration 6615, Loss: 36.35896682739258, L1: 10.15143871307373, L3: 26.207529067993164\n",
      "Current prediction:  61.53909683227539 \n",
      "\n",
      "Iteration 6616, Loss: 36.92302322387695, L1: 10.128390312194824, L3: 26.794633865356445\n",
      "Current prediction:  61.53480529785156 \n",
      "\n",
      "Iteration 6617, Loss: 36.60068130493164, L1: 10.140443801879883, L3: 26.460237503051758\n",
      "Current prediction:  61.5317497253418 \n",
      "\n",
      "Iteration 6618, Loss: 37.09949493408203, L1: 10.118088722229004, L3: 26.981407165527344\n",
      "Current prediction:  61.526615142822266 \n",
      "\n",
      "Iteration 6619, Loss: 37.079193115234375, L1: 10.112667083740234, L3: 26.966524124145508\n",
      "Current prediction:  61.519012451171875 \n",
      "\n",
      "Iteration 6620, Loss: 35.604759216308594, L1: 10.106220245361328, L3: 25.498538970947266\n",
      "Current prediction:  61.5045280456543 \n",
      "\n",
      "Iteration 6621, Loss: 36.555145263671875, L1: 10.113646507263184, L3: 26.441499710083008\n",
      "Current prediction:  61.39601516723633 \n",
      "\n",
      "Iteration 6622, Loss: 36.083065032958984, L1: 10.13474178314209, L3: 25.94832420349121\n",
      "Current prediction:  61.095428466796875 \n",
      "\n",
      "Iteration 6623, Loss: 36.58495330810547, L1: 10.137468338012695, L3: 26.447484970092773\n",
      "Current prediction:  60.92425537109375 \n",
      "\n",
      "Iteration 6624, Loss: 36.36796188354492, L1: 10.156367301940918, L3: 26.211593627929688\n",
      "Current prediction:  61.03471755981445 \n",
      "\n",
      "Iteration 6625, Loss: 36.7132568359375, L1: 10.122085571289062, L3: 26.591171264648438\n",
      "Current prediction:  61.03501510620117 \n",
      "\n",
      "Iteration 6626, Loss: 36.471832275390625, L1: 10.10423755645752, L3: 26.36759376525879\n",
      "Current prediction:  61.07185363769531 \n",
      "\n",
      "Iteration 6627, Loss: 36.67544174194336, L1: 10.10523796081543, L3: 26.57020378112793\n",
      "Current prediction:  61.01185989379883 \n",
      "\n",
      "Iteration 6628, Loss: 35.90617370605469, L1: 10.14699935913086, L3: 25.759172439575195\n",
      "Current prediction:  61.13785934448242 \n",
      "\n",
      "Iteration 6629, Loss: 35.914520263671875, L1: 10.095964431762695, L3: 25.81855583190918\n",
      "Current prediction:  61.300025939941406 \n",
      "\n",
      "Iteration 6630, Loss: 35.468719482421875, L1: 10.126523971557617, L3: 25.342193603515625\n",
      "Current prediction:  61.33826446533203 \n",
      "\n",
      "Iteration 6631, Loss: 36.40034866333008, L1: 10.099020957946777, L3: 26.301328659057617\n",
      "Current prediction:  61.21818542480469 \n",
      "\n",
      "Iteration 6632, Loss: 37.220951080322266, L1: 10.107837677001953, L3: 27.113113403320312\n",
      "Current prediction:  61.38739776611328 \n",
      "\n",
      "Iteration 6633, Loss: 36.64720153808594, L1: 10.122176170349121, L3: 26.5250244140625\n",
      "Current prediction:  61.43511962890625 \n",
      "\n",
      "Iteration 6634, Loss: 36.05031967163086, L1: 10.123417854309082, L3: 25.92690086364746\n",
      "Current prediction:  61.25161361694336 \n",
      "\n",
      "Iteration 6635, Loss: 36.991127014160156, L1: 10.1557035446167, L3: 26.835424423217773\n",
      "Current prediction:  60.91338348388672 \n",
      "\n",
      "Iteration 6636, Loss: 36.76622009277344, L1: 10.143562316894531, L3: 26.622655868530273\n",
      "Current prediction:  60.745059967041016 \n",
      "\n",
      "Iteration 6637, Loss: 36.14891815185547, L1: 10.161911010742188, L3: 25.987009048461914\n",
      "Current prediction:  60.83059310913086 \n",
      "\n",
      "Iteration 6638, Loss: 35.84284973144531, L1: 10.174835205078125, L3: 25.66801643371582\n",
      "Current prediction:  61.2710075378418 \n",
      "\n",
      "Iteration 6639, Loss: 36.86351776123047, L1: 10.149723052978516, L3: 26.713796615600586\n",
      "Current prediction:  61.440467834472656 \n",
      "\n",
      "Iteration 6640, Loss: 37.280731201171875, L1: 10.1406831741333, L3: 27.140047073364258\n",
      "Current prediction:  61.457176208496094 \n",
      "\n",
      "Iteration 6641, Loss: 36.290916442871094, L1: 10.15544319152832, L3: 26.13547134399414\n",
      "Current prediction:  61.45964050292969 \n",
      "\n",
      "Iteration 6642, Loss: 35.79158401489258, L1: 10.13603401184082, L3: 25.655550003051758\n",
      "Current prediction:  61.46809387207031 \n",
      "\n",
      "Iteration 6643, Loss: 36.189815521240234, L1: 10.15999984741211, L3: 26.029815673828125\n",
      "Current prediction:  61.480064392089844 \n",
      "\n",
      "Iteration 6644, Loss: 37.726409912109375, L1: 10.127445220947266, L3: 27.59896469116211\n",
      "Current prediction:  61.48494338989258 \n",
      "\n",
      "Iteration 6645, Loss: 36.823856353759766, L1: 10.139172554016113, L3: 26.68468475341797\n",
      "Current prediction:  61.48808670043945 \n",
      "\n",
      "Iteration 6646, Loss: 36.20475769042969, L1: 10.162734031677246, L3: 26.042024612426758\n",
      "Current prediction:  61.47391891479492 \n",
      "\n",
      "Iteration 6647, Loss: 36.351036071777344, L1: 10.120126724243164, L3: 26.23090934753418\n",
      "Current prediction:  61.31780242919922 \n",
      "\n",
      "Iteration 6648, Loss: 37.3531494140625, L1: 10.111775398254395, L3: 27.241374969482422\n",
      "Current prediction:  60.90443801879883 \n",
      "\n",
      "Iteration 6649, Loss: 36.49452590942383, L1: 10.113863945007324, L3: 26.38066291809082\n",
      "Current prediction:  60.80772018432617 \n",
      "\n",
      "Iteration 6650, Loss: 35.18680953979492, L1: 10.105414390563965, L3: 25.08139419555664\n",
      "Current prediction:  60.7598876953125 \n",
      "\n",
      "Iteration 6651, Loss: 35.706443786621094, L1: 10.177948951721191, L3: 25.528493881225586\n",
      "Current prediction:  60.82527542114258 \n",
      "\n",
      "Iteration 6652, Loss: 37.544105529785156, L1: 10.126752853393555, L3: 27.41735076904297\n",
      "Current prediction:  60.980167388916016 \n",
      "\n",
      "Iteration 6653, Loss: 36.2696418762207, L1: 10.129426002502441, L3: 26.140214920043945\n",
      "Current prediction:  61.18813705444336 \n",
      "\n",
      "Iteration 6654, Loss: 36.196136474609375, L1: 10.067838668823242, L3: 26.1282958984375\n",
      "Current prediction:  61.41828155517578 \n",
      "\n",
      "Iteration 6655, Loss: 37.378326416015625, L1: 10.088860511779785, L3: 27.289464950561523\n",
      "Current prediction:  61.51609802246094 \n",
      "\n",
      "Iteration 6656, Loss: 36.40764617919922, L1: 10.04857063293457, L3: 26.35907554626465\n",
      "Current prediction:  61.52812957763672 \n",
      "\n",
      "Iteration 6657, Loss: 36.648231506347656, L1: 10.058859825134277, L3: 26.589372634887695\n",
      "Current prediction:  61.381126403808594 \n",
      "\n",
      "Iteration 6658, Loss: 35.801273345947266, L1: 10.071017265319824, L3: 25.730257034301758\n",
      "Current prediction:  60.960975646972656 \n",
      "\n",
      "Iteration 6659, Loss: 36.47368240356445, L1: 10.070819854736328, L3: 26.402862548828125\n",
      "Current prediction:  60.842933654785156 \n",
      "\n",
      "Iteration 6660, Loss: 35.566612243652344, L1: 10.07218074798584, L3: 25.494430541992188\n",
      "Current prediction:  60.80945587158203 \n",
      "\n",
      "Iteration 6661, Loss: 36.18601608276367, L1: 10.127080917358398, L3: 26.058935165405273\n",
      "Current prediction:  60.80487060546875 \n",
      "\n",
      "Iteration 6662, Loss: 35.87004089355469, L1: 10.117180824279785, L3: 25.75286102294922\n",
      "Current prediction:  60.83534622192383 \n",
      "\n",
      "Iteration 6663, Loss: 35.97383117675781, L1: 10.067113876342773, L3: 25.906719207763672\n",
      "Current prediction:  61.17551040649414 \n",
      "\n",
      "Iteration 6664, Loss: 36.88017654418945, L1: 10.014589309692383, L3: 26.86558723449707\n",
      "Current prediction:  61.556907653808594 \n",
      "\n",
      "Iteration 6665, Loss: 34.89844512939453, L1: 10.02150821685791, L3: 24.876937866210938\n",
      "Current prediction:  61.59928512573242 \n",
      "\n",
      "Iteration 6666, Loss: 36.923404693603516, L1: 10.053162574768066, L3: 26.870243072509766\n",
      "Current prediction:  61.61039733886719 \n",
      "\n",
      "Iteration 6667, Loss: 35.65896224975586, L1: 10.041106224060059, L3: 25.617856979370117\n",
      "Current prediction:  61.61005401611328 \n",
      "\n",
      "Iteration 6668, Loss: 36.21862030029297, L1: 10.059408187866211, L3: 26.159210205078125\n",
      "Current prediction:  61.6074104309082 \n",
      "\n",
      "Iteration 6669, Loss: 35.799102783203125, L1: 10.076884269714355, L3: 25.722219467163086\n",
      "Current prediction:  61.603153228759766 \n",
      "\n",
      "Iteration 6670, Loss: 36.37967300415039, L1: 10.079239845275879, L3: 26.300434112548828\n",
      "Current prediction:  61.594078063964844 \n",
      "\n",
      "Iteration 6671, Loss: 35.954795837402344, L1: 10.09192180633545, L3: 25.862873077392578\n",
      "Current prediction:  61.584903717041016 \n",
      "\n",
      "Iteration 6672, Loss: 36.29470443725586, L1: 10.079503059387207, L3: 26.215200424194336\n",
      "Current prediction:  61.57398223876953 \n",
      "\n",
      "Iteration 6673, Loss: 35.710487365722656, L1: 10.07312297821045, L3: 25.63736343383789\n",
      "Current prediction:  61.545902252197266 \n",
      "\n",
      "Iteration 6674, Loss: 37.17604446411133, L1: 10.066640853881836, L3: 27.109403610229492\n",
      "Current prediction:  61.32097244262695 \n",
      "\n",
      "Iteration 6675, Loss: 37.020545959472656, L1: 10.056483268737793, L3: 26.964061737060547\n",
      "Current prediction:  60.85960388183594 \n",
      "\n",
      "Iteration 6676, Loss: 35.54822540283203, L1: 10.086324691772461, L3: 25.46190071105957\n",
      "Current prediction:  60.74951171875 \n",
      "\n",
      "Iteration 6677, Loss: 36.094451904296875, L1: 10.142709732055664, L3: 25.95174217224121\n",
      "Current prediction:  60.759788513183594 \n",
      "\n",
      "Iteration 6678, Loss: 35.86540222167969, L1: 10.117382049560547, L3: 25.74802017211914\n",
      "Current prediction:  60.789459228515625 \n",
      "\n",
      "Iteration 6679, Loss: 35.104820251464844, L1: 10.113763809204102, L3: 24.99105453491211\n",
      "Current prediction:  60.95249938964844 \n",
      "\n",
      "Iteration 6680, Loss: 36.01865005493164, L1: 10.098310470581055, L3: 25.920339584350586\n",
      "Current prediction:  61.14746856689453 \n",
      "\n",
      "Iteration 6681, Loss: 35.63642883300781, L1: 10.091129302978516, L3: 25.545297622680664\n",
      "Current prediction:  61.389095306396484 \n",
      "\n",
      "Iteration 6682, Loss: 36.423587799072266, L1: 10.105381965637207, L3: 26.318206787109375\n",
      "Current prediction:  61.473426818847656 \n",
      "\n",
      "Iteration 6683, Loss: 36.28985595703125, L1: 10.09001350402832, L3: 26.199844360351562\n",
      "Current prediction:  61.4990348815918 \n",
      "\n",
      "Iteration 6684, Loss: 35.69029998779297, L1: 10.095580101013184, L3: 25.59471893310547\n",
      "Current prediction:  61.47174072265625 \n",
      "\n",
      "Iteration 6685, Loss: 36.641883850097656, L1: 10.106905937194824, L3: 26.53497886657715\n",
      "Current prediction:  61.47245407104492 \n",
      "\n",
      "Iteration 6686, Loss: 35.611061096191406, L1: 10.089744567871094, L3: 25.52131462097168\n",
      "Current prediction:  61.48291778564453 \n",
      "\n",
      "Iteration 6687, Loss: 37.05326843261719, L1: 10.104191780090332, L3: 26.94907569885254\n",
      "Current prediction:  61.381229400634766 \n",
      "\n",
      "Iteration 6688, Loss: 36.62639236450195, L1: 10.071486473083496, L3: 26.554906845092773\n",
      "Current prediction:  61.09708023071289 \n",
      "\n",
      "Iteration 6689, Loss: 35.44469451904297, L1: 10.101741790771484, L3: 25.342952728271484\n",
      "Current prediction:  61.11326217651367 \n",
      "\n",
      "Iteration 6690, Loss: 35.028438568115234, L1: 10.161938667297363, L3: 24.866500854492188\n",
      "Current prediction:  61.22929382324219 \n",
      "\n",
      "Iteration 6691, Loss: 36.872955322265625, L1: 10.085450172424316, L3: 26.787506103515625\n",
      "Current prediction:  61.186161041259766 \n",
      "\n",
      "Iteration 6692, Loss: 36.24325942993164, L1: 10.086607933044434, L3: 26.156652450561523\n",
      "Current prediction:  61.09328079223633 \n",
      "\n",
      "Iteration 6693, Loss: 36.42900466918945, L1: 10.114800453186035, L3: 26.3142032623291\n",
      "Current prediction:  61.32175064086914 \n",
      "\n",
      "Iteration 6694, Loss: 36.12054443359375, L1: 10.118565559387207, L3: 26.001977920532227\n",
      "Current prediction:  61.210548400878906 \n",
      "\n",
      "Iteration 6695, Loss: 36.29240036010742, L1: 10.108946800231934, L3: 26.183454513549805\n",
      "Current prediction:  61.06564712524414 \n",
      "\n",
      "Iteration 6696, Loss: 35.8764533996582, L1: 10.097895622253418, L3: 25.77855682373047\n",
      "Current prediction:  61.06731033325195 \n",
      "\n",
      "Iteration 6697, Loss: 35.690101623535156, L1: 10.10411548614502, L3: 25.585987091064453\n",
      "Current prediction:  60.96253204345703 \n",
      "\n",
      "Iteration 6698, Loss: 37.16069412231445, L1: 10.067336082458496, L3: 27.09335708618164\n",
      "Current prediction:  60.83429718017578 \n",
      "\n",
      "Iteration 6699, Loss: 35.8294563293457, L1: 10.109705924987793, L3: 25.719749450683594\n",
      "Current prediction:  60.863224029541016 \n",
      "\n",
      "Iteration 6700, Loss: 37.833213806152344, L1: 10.093557357788086, L3: 27.739656448364258\n",
      "Current prediction:  61.217254638671875 \n",
      "\n",
      "Iteration 6701, Loss: 34.5524787902832, L1: 10.068282127380371, L3: 24.48419761657715\n",
      "Current prediction:  61.50869369506836 \n",
      "\n",
      "Iteration 6702, Loss: 36.3250617980957, L1: 10.084259986877441, L3: 26.240800857543945\n",
      "Current prediction:  61.5538215637207 \n",
      "\n",
      "Iteration 6703, Loss: 36.32286834716797, L1: 10.083977699279785, L3: 26.238889694213867\n",
      "Current prediction:  61.5612907409668 \n",
      "\n",
      "Iteration 6704, Loss: 35.920379638671875, L1: 10.135181427001953, L3: 25.785198211669922\n",
      "Current prediction:  61.5662727355957 \n",
      "\n",
      "Iteration 6705, Loss: 36.08268737792969, L1: 10.113737106323242, L3: 25.968950271606445\n",
      "Current prediction:  61.57060623168945 \n",
      "\n",
      "Iteration 6706, Loss: 36.55657196044922, L1: 10.131190299987793, L3: 26.42538070678711\n",
      "Current prediction:  61.57080841064453 \n",
      "\n",
      "Iteration 6707, Loss: 36.110755920410156, L1: 10.146808624267578, L3: 25.963947296142578\n",
      "Current prediction:  61.564849853515625 \n",
      "\n",
      "Iteration 6708, Loss: 36.280853271484375, L1: 10.137194633483887, L3: 26.143657684326172\n",
      "Current prediction:  61.55462646484375 \n",
      "\n",
      "Iteration 6709, Loss: 36.551910400390625, L1: 10.15600872039795, L3: 26.39590072631836\n",
      "Current prediction:  61.53950119018555 \n",
      "\n",
      "Iteration 6710, Loss: 36.22296142578125, L1: 10.175971984863281, L3: 26.04698944091797\n",
      "Current prediction:  61.52039337158203 \n",
      "\n",
      "Iteration 6711, Loss: 35.17491912841797, L1: 10.164664268493652, L3: 25.010255813598633\n",
      "Current prediction:  61.49979019165039 \n",
      "\n",
      "Iteration 6712, Loss: 36.33092498779297, L1: 10.20033073425293, L3: 26.13059425354004\n",
      "Current prediction:  61.480716705322266 \n",
      "\n",
      "Iteration 6713, Loss: 36.43527603149414, L1: 10.248916625976562, L3: 26.186359405517578\n",
      "Current prediction:  61.46522903442383 \n",
      "\n",
      "Iteration 6714, Loss: 36.250946044921875, L1: 10.2106294631958, L3: 26.040315628051758\n",
      "Current prediction:  61.45307159423828 \n",
      "\n",
      "Iteration 6715, Loss: 35.77638244628906, L1: 10.212542533874512, L3: 25.563840866088867\n",
      "Current prediction:  61.443450927734375 \n",
      "\n",
      "Iteration 6716, Loss: 36.18205261230469, L1: 10.209065437316895, L3: 25.97298812866211\n",
      "Current prediction:  61.437171936035156 \n",
      "\n",
      "Iteration 6717, Loss: 35.97981262207031, L1: 10.197035789489746, L3: 25.78277587890625\n",
      "Current prediction:  61.4268798828125 \n",
      "\n",
      "Iteration 6718, Loss: 36.654090881347656, L1: 10.194377899169922, L3: 26.459712982177734\n",
      "Current prediction:  61.32908248901367 \n",
      "\n",
      "Iteration 6719, Loss: 36.4393310546875, L1: 10.161331176757812, L3: 26.277999877929688\n",
      "Current prediction:  60.79414749145508 \n",
      "\n",
      "Iteration 6720, Loss: 36.186180114746094, L1: 10.151976585388184, L3: 26.034204483032227\n",
      "Current prediction:  60.65101623535156 \n",
      "\n",
      "Iteration 6721, Loss: 36.24228286743164, L1: 10.198582649230957, L3: 26.043701171875\n",
      "Current prediction:  60.646461486816406 \n",
      "\n",
      "Iteration 6722, Loss: 37.296417236328125, L1: 10.243228912353516, L3: 27.05318832397461\n",
      "Current prediction:  60.67241668701172 \n",
      "\n",
      "Iteration 6723, Loss: 35.93486404418945, L1: 10.20005989074707, L3: 25.734804153442383\n",
      "Current prediction:  60.88876724243164 \n",
      "\n",
      "Iteration 6724, Loss: 37.259056091308594, L1: 10.145119667053223, L3: 27.113937377929688\n",
      "Current prediction:  61.08260726928711 \n",
      "\n",
      "Iteration 6725, Loss: 36.36552047729492, L1: 10.117512702941895, L3: 26.24800682067871\n",
      "Current prediction:  61.18450927734375 \n",
      "\n",
      "Iteration 6726, Loss: 36.562660217285156, L1: 10.109243392944336, L3: 26.453414916992188\n",
      "Current prediction:  61.24382019042969 \n",
      "\n",
      "Iteration 6727, Loss: 36.221622467041016, L1: 10.069921493530273, L3: 26.151700973510742\n",
      "Current prediction:  61.344268798828125 \n",
      "\n",
      "Iteration 6728, Loss: 36.455718994140625, L1: 10.060076713562012, L3: 26.39564323425293\n",
      "Current prediction:  61.29478454589844 \n",
      "\n",
      "Iteration 6729, Loss: 36.51817321777344, L1: 10.07849407196045, L3: 26.439678192138672\n",
      "Current prediction:  61.05492401123047 \n",
      "\n",
      "Iteration 6730, Loss: 35.94117736816406, L1: 10.084512710571289, L3: 25.856664657592773\n",
      "Current prediction:  60.85856628417969 \n",
      "\n",
      "Iteration 6731, Loss: 35.89038848876953, L1: 10.08247184753418, L3: 25.80791473388672\n",
      "Current prediction:  60.82457733154297 \n",
      "\n",
      "Iteration 6732, Loss: 36.85597610473633, L1: 10.092316627502441, L3: 26.76365852355957\n",
      "Current prediction:  60.84156036376953 \n",
      "\n",
      "Iteration 6733, Loss: 37.00727844238281, L1: 10.073923110961914, L3: 26.9333553314209\n",
      "Current prediction:  60.90324401855469 \n",
      "\n",
      "Iteration 6734, Loss: 36.593475341796875, L1: 10.048537254333496, L3: 26.544937133789062\n",
      "Current prediction:  61.08834457397461 \n",
      "\n",
      "Iteration 6735, Loss: 35.64627456665039, L1: 10.019770622253418, L3: 25.626502990722656\n",
      "Current prediction:  61.26471710205078 \n",
      "\n",
      "Iteration 6736, Loss: 36.87709426879883, L1: 9.985701560974121, L3: 26.891393661499023\n",
      "Current prediction:  61.44314193725586 \n",
      "\n",
      "Iteration 6737, Loss: 36.33850860595703, L1: 10.032846450805664, L3: 26.305662155151367\n",
      "Current prediction:  61.53898239135742 \n",
      "\n",
      "Iteration 6738, Loss: 36.22940444946289, L1: 10.012391090393066, L3: 26.217012405395508\n",
      "Current prediction:  61.58524703979492 \n",
      "\n",
      "Iteration 6739, Loss: 37.31780242919922, L1: 10.04903793334961, L3: 27.268766403198242\n",
      "Current prediction:  61.58873748779297 \n",
      "\n",
      "Iteration 6740, Loss: 35.31350326538086, L1: 10.044477462768555, L3: 25.269025802612305\n",
      "Current prediction:  61.55049133300781 \n",
      "\n",
      "Iteration 6741, Loss: 35.9979362487793, L1: 10.00522232055664, L3: 25.992713928222656\n",
      "Current prediction:  61.36240005493164 \n",
      "\n",
      "Iteration 6742, Loss: 36.924530029296875, L1: 10.006501197814941, L3: 26.918027877807617\n",
      "Current prediction:  60.876346588134766 \n",
      "\n",
      "Iteration 6743, Loss: 36.59246063232422, L1: 10.04779052734375, L3: 26.544668197631836\n",
      "Current prediction:  60.765743255615234 \n",
      "\n",
      "Iteration 6744, Loss: 37.050384521484375, L1: 10.144489288330078, L3: 26.905893325805664\n",
      "Current prediction:  60.750823974609375 \n",
      "\n",
      "Iteration 6745, Loss: 35.89720153808594, L1: 10.158007621765137, L3: 25.739192962646484\n",
      "Current prediction:  60.74732208251953 \n",
      "\n",
      "Iteration 6746, Loss: 36.728294372558594, L1: 10.137453079223633, L3: 26.59084129333496\n",
      "Current prediction:  60.749698638916016 \n",
      "\n",
      "Iteration 6747, Loss: 36.424522399902344, L1: 10.116765975952148, L3: 26.307756423950195\n",
      "Current prediction:  60.798282623291016 \n",
      "\n",
      "Iteration 6748, Loss: 36.07313537597656, L1: 10.104628562927246, L3: 25.968505859375\n",
      "Current prediction:  61.35697555541992 \n",
      "\n",
      "Iteration 6749, Loss: 36.54054260253906, L1: 10.035733222961426, L3: 26.50480842590332\n",
      "Current prediction:  61.539798736572266 \n",
      "\n",
      "Iteration 6750, Loss: 36.71052932739258, L1: 10.066153526306152, L3: 26.64437484741211\n",
      "Current prediction:  61.55203628540039 \n",
      "\n",
      "Iteration 6751, Loss: 36.471923828125, L1: 10.065935134887695, L3: 26.405986785888672\n",
      "Current prediction:  61.549285888671875 \n",
      "\n",
      "Iteration 6752, Loss: 37.382354736328125, L1: 10.074668884277344, L3: 27.30768394470215\n",
      "Current prediction:  61.541622161865234 \n",
      "\n",
      "Iteration 6753, Loss: 35.83791732788086, L1: 10.079583168029785, L3: 25.75833511352539\n",
      "Current prediction:  61.5272216796875 \n",
      "\n",
      "Iteration 6754, Loss: 35.62490463256836, L1: 10.07180118560791, L3: 25.553102493286133\n",
      "Current prediction:  61.47428894042969 \n",
      "\n",
      "Iteration 6755, Loss: 35.87369155883789, L1: 10.078425407409668, L3: 25.795265197753906\n",
      "Current prediction:  61.15617370605469 \n",
      "\n",
      "Iteration 6756, Loss: 37.40617370605469, L1: 10.097610473632812, L3: 27.308565139770508\n",
      "Current prediction:  60.904842376708984 \n",
      "\n",
      "Iteration 6757, Loss: 36.88877868652344, L1: 10.097635269165039, L3: 26.791141510009766\n",
      "Current prediction:  60.792869567871094 \n",
      "\n",
      "Iteration 6758, Loss: 35.941558837890625, L1: 10.108642578125, L3: 25.832918167114258\n",
      "Current prediction:  60.87639236450195 \n",
      "\n",
      "Iteration 6759, Loss: 36.47057342529297, L1: 10.096197128295898, L3: 26.374378204345703\n",
      "Current prediction:  60.84986114501953 \n",
      "\n",
      "Iteration 6760, Loss: 36.12970733642578, L1: 10.08542251586914, L3: 26.044286727905273\n",
      "Current prediction:  61.17684555053711 \n",
      "\n",
      "Iteration 6761, Loss: 37.25800704956055, L1: 10.077496528625488, L3: 27.180509567260742\n",
      "Current prediction:  61.33546829223633 \n",
      "\n",
      "Iteration 6762, Loss: 36.56379699707031, L1: 10.082405090332031, L3: 26.481393814086914\n",
      "Current prediction:  61.29777145385742 \n",
      "\n",
      "Iteration 6763, Loss: 36.564552307128906, L1: 10.067972183227539, L3: 26.496578216552734\n",
      "Current prediction:  61.35940933227539 \n",
      "\n",
      "Iteration 6764, Loss: 36.423912048339844, L1: 10.056694984436035, L3: 26.367218017578125\n",
      "Current prediction:  61.41263961791992 \n",
      "\n",
      "Iteration 6765, Loss: 36.03879165649414, L1: 10.060267448425293, L3: 25.97852325439453\n",
      "Current prediction:  61.46092224121094 \n",
      "\n",
      "Iteration 6766, Loss: 35.76911926269531, L1: 10.07286548614502, L3: 25.69625473022461\n",
      "Current prediction:  61.49798583984375 \n",
      "\n",
      "Iteration 6767, Loss: 35.428863525390625, L1: 10.102636337280273, L3: 25.32622528076172\n",
      "Current prediction:  61.514041900634766 \n",
      "\n",
      "Iteration 6768, Loss: 36.80808639526367, L1: 10.125844955444336, L3: 26.682241439819336\n",
      "Current prediction:  61.512325286865234 \n",
      "\n",
      "Iteration 6769, Loss: 36.73015594482422, L1: 10.12255573272705, L3: 26.60759925842285\n",
      "Current prediction:  61.50566482543945 \n",
      "\n",
      "Iteration 6770, Loss: 37.19822692871094, L1: 10.138641357421875, L3: 27.059587478637695\n",
      "Current prediction:  61.495269775390625 \n",
      "\n",
      "Iteration 6771, Loss: 36.206993103027344, L1: 10.128742218017578, L3: 26.078250885009766\n",
      "Current prediction:  61.447044372558594 \n",
      "\n",
      "Iteration 6772, Loss: 36.91980743408203, L1: 10.086304664611816, L3: 26.83350372314453\n",
      "Current prediction:  61.297176361083984 \n",
      "\n",
      "Iteration 6773, Loss: 37.04850769042969, L1: 10.097481727600098, L3: 26.951025009155273\n",
      "Current prediction:  61.074581146240234 \n",
      "\n",
      "Iteration 6774, Loss: 36.59504318237305, L1: 10.11274242401123, L3: 26.4822998046875\n",
      "Current prediction:  60.81147766113281 \n",
      "\n",
      "Iteration 6775, Loss: 37.25990295410156, L1: 10.128105163574219, L3: 27.131799697875977\n",
      "Current prediction:  60.8364372253418 \n",
      "\n",
      "Iteration 6776, Loss: 36.30304718017578, L1: 10.089803695678711, L3: 26.213241577148438\n",
      "Current prediction:  60.96970748901367 \n",
      "\n",
      "Iteration 6777, Loss: 36.058067321777344, L1: 10.128726959228516, L3: 25.92934226989746\n",
      "Current prediction:  61.17796325683594 \n",
      "\n",
      "Iteration 6778, Loss: 36.373016357421875, L1: 10.058027267456055, L3: 26.314987182617188\n",
      "Current prediction:  61.44710159301758 \n",
      "\n",
      "Iteration 6779, Loss: 36.14787292480469, L1: 10.077081680297852, L3: 26.070791244506836\n",
      "Current prediction:  61.50672149658203 \n",
      "\n",
      "Iteration 6780, Loss: 35.01760482788086, L1: 10.069958686828613, L3: 24.947647094726562\n",
      "Current prediction:  61.50822067260742 \n",
      "\n",
      "Iteration 6781, Loss: 36.97706604003906, L1: 10.074407577514648, L3: 26.90265655517578\n",
      "Current prediction:  61.433380126953125 \n",
      "\n",
      "Iteration 6782, Loss: 35.845489501953125, L1: 10.03943920135498, L3: 25.80605125427246\n",
      "Current prediction:  61.27195739746094 \n",
      "\n",
      "Iteration 6783, Loss: 35.622432708740234, L1: 10.05156421661377, L3: 25.57086944580078\n",
      "Current prediction:  61.20431900024414 \n",
      "\n",
      "Iteration 6784, Loss: 35.89182662963867, L1: 10.069254875183105, L3: 25.822572708129883\n",
      "Current prediction:  61.00368118286133 \n",
      "\n",
      "Iteration 6785, Loss: 36.3272819519043, L1: 10.049776077270508, L3: 26.27750587463379\n",
      "Current prediction:  60.84555435180664 \n",
      "\n",
      "Iteration 6786, Loss: 35.7669677734375, L1: 10.036523818969727, L3: 25.730443954467773\n",
      "Current prediction:  60.78046417236328 \n",
      "\n",
      "Iteration 6787, Loss: 35.5501708984375, L1: 10.118720054626465, L3: 25.43144989013672\n",
      "Current prediction:  60.80156326293945 \n",
      "\n",
      "Iteration 6788, Loss: 35.77318572998047, L1: 10.09727954864502, L3: 25.675907135009766\n",
      "Current prediction:  60.91282272338867 \n",
      "\n",
      "Iteration 6789, Loss: 35.82358169555664, L1: 10.052022933959961, L3: 25.77155876159668\n",
      "Current prediction:  61.16918182373047 \n",
      "\n",
      "Iteration 6790, Loss: 37.4288444519043, L1: 10.035210609436035, L3: 27.393634796142578\n",
      "Current prediction:  61.50519561767578 \n",
      "\n",
      "Iteration 6791, Loss: 37.11293029785156, L1: 10.005455017089844, L3: 27.10747718811035\n",
      "Current prediction:  61.587520599365234 \n",
      "\n",
      "Iteration 6792, Loss: 36.33332061767578, L1: 10.012402534484863, L3: 26.320919036865234\n",
      "Current prediction:  61.61370849609375 \n",
      "\n",
      "Iteration 6793, Loss: 37.504859924316406, L1: 9.999040603637695, L3: 27.50581932067871\n",
      "Current prediction:  61.60328674316406 \n",
      "\n",
      "Iteration 6794, Loss: 36.5919189453125, L1: 10.014969825744629, L3: 26.576950073242188\n",
      "Current prediction:  61.52054214477539 \n",
      "\n",
      "Iteration 6795, Loss: 35.05872344970703, L1: 10.019011497497559, L3: 25.039710998535156\n",
      "Current prediction:  61.26380920410156 \n",
      "\n",
      "Iteration 6796, Loss: 36.771453857421875, L1: 10.028205871582031, L3: 26.743249893188477\n",
      "Current prediction:  61.045875549316406 \n",
      "\n",
      "Iteration 6797, Loss: 36.265602111816406, L1: 10.006160736083984, L3: 26.259443283081055\n",
      "Current prediction:  61.19440460205078 \n",
      "\n",
      "Iteration 6798, Loss: 36.39402770996094, L1: 10.053890228271484, L3: 26.340137481689453\n",
      "Current prediction:  61.13191223144531 \n",
      "\n",
      "Iteration 6799, Loss: 37.31114959716797, L1: 10.051238059997559, L3: 27.259912490844727\n",
      "Current prediction:  61.04835510253906 \n",
      "\n",
      "Iteration 6800, Loss: 36.586402893066406, L1: 10.053686141967773, L3: 26.53271484375\n",
      "Current prediction:  61.02206039428711 \n",
      "\n",
      "Iteration 6801, Loss: 35.83992385864258, L1: 10.055594444274902, L3: 25.784330368041992\n",
      "Current prediction:  61.331974029541016 \n",
      "\n",
      "Iteration 6802, Loss: 36.055152893066406, L1: 10.063434600830078, L3: 25.99172019958496\n",
      "Current prediction:  61.50194549560547 \n",
      "\n",
      "Iteration 6803, Loss: 36.484004974365234, L1: 10.074519157409668, L3: 26.40948486328125\n",
      "Current prediction:  61.51472854614258 \n",
      "\n",
      "Iteration 6804, Loss: 36.87186813354492, L1: 10.034131050109863, L3: 26.837738037109375\n",
      "Current prediction:  61.51917266845703 \n",
      "\n",
      "Iteration 6805, Loss: 35.78486633300781, L1: 10.073646545410156, L3: 25.711219787597656\n",
      "Current prediction:  61.4414176940918 \n",
      "\n",
      "Iteration 6806, Loss: 36.10628128051758, L1: 10.108343124389648, L3: 25.99793815612793\n",
      "Current prediction:  61.08573913574219 \n",
      "\n",
      "Iteration 6807, Loss: 35.68793869018555, L1: 10.070940017700195, L3: 25.61699867248535\n",
      "Current prediction:  61.216285705566406 \n",
      "\n",
      "Iteration 6808, Loss: 37.311641693115234, L1: 10.0993070602417, L3: 27.21233367919922\n",
      "Current prediction:  61.27351379394531 \n",
      "\n",
      "Iteration 6809, Loss: 37.02595520019531, L1: 10.066896438598633, L3: 26.959056854248047\n",
      "Current prediction:  61.282676696777344 \n",
      "\n",
      "Iteration 6810, Loss: 36.41786193847656, L1: 10.104382514953613, L3: 26.313480377197266\n",
      "Current prediction:  61.09587478637695 \n",
      "\n",
      "Iteration 6811, Loss: 36.169559478759766, L1: 10.102819442749023, L3: 26.066740036010742\n",
      "Current prediction:  60.89273452758789 \n",
      "\n",
      "Iteration 6812, Loss: 36.11225509643555, L1: 10.102803230285645, L3: 26.00945281982422\n",
      "Current prediction:  60.85966873168945 \n",
      "\n",
      "Iteration 6813, Loss: 35.82568359375, L1: 10.103455543518066, L3: 25.722227096557617\n",
      "Current prediction:  61.16127395629883 \n",
      "\n",
      "Iteration 6814, Loss: 37.28873825073242, L1: 10.061318397521973, L3: 27.227418899536133\n",
      "Current prediction:  61.32011413574219 \n",
      "\n",
      "Iteration 6815, Loss: 35.691078186035156, L1: 10.061344146728516, L3: 25.629732131958008\n",
      "Current prediction:  61.39156723022461 \n",
      "\n",
      "Iteration 6816, Loss: 36.372154235839844, L1: 10.044294357299805, L3: 26.327861785888672\n",
      "Current prediction:  61.36504364013672 \n",
      "\n",
      "Iteration 6817, Loss: 36.11496353149414, L1: 10.039538383483887, L3: 26.075424194335938\n",
      "Current prediction:  61.26716232299805 \n",
      "\n",
      "Iteration 6818, Loss: 36.13644027709961, L1: 10.04297924041748, L3: 26.093461990356445\n",
      "Current prediction:  61.01389694213867 \n",
      "\n",
      "Iteration 6819, Loss: 35.64063262939453, L1: 10.045764923095703, L3: 25.594867706298828\n",
      "Current prediction:  60.980751037597656 \n",
      "\n",
      "Iteration 6820, Loss: 36.74881362915039, L1: 10.026508331298828, L3: 26.722305297851562\n",
      "Current prediction:  61.123077392578125 \n",
      "\n",
      "Iteration 6821, Loss: 36.537986755371094, L1: 10.032451629638672, L3: 26.505535125732422\n",
      "Current prediction:  61.51401901245117 \n",
      "\n",
      "Iteration 6822, Loss: 36.74707794189453, L1: 10.008167266845703, L3: 26.73891258239746\n",
      "Current prediction:  61.573612213134766 \n",
      "\n",
      "Iteration 6823, Loss: 37.056968688964844, L1: 10.063301086425781, L3: 26.993669509887695\n",
      "Current prediction:  61.57045364379883 \n",
      "\n",
      "Iteration 6824, Loss: 36.560394287109375, L1: 10.029358863830566, L3: 26.531034469604492\n",
      "Current prediction:  61.561824798583984 \n",
      "\n",
      "Iteration 6825, Loss: 36.76782989501953, L1: 10.078768730163574, L3: 26.689062118530273\n",
      "Current prediction:  61.5513916015625 \n",
      "\n",
      "Iteration 6826, Loss: 36.622703552246094, L1: 10.0677490234375, L3: 26.55495262145996\n",
      "Current prediction:  61.541263580322266 \n",
      "\n",
      "Iteration 6827, Loss: 37.4983024597168, L1: 10.07967758178711, L3: 27.418624877929688\n",
      "Current prediction:  61.524269104003906 \n",
      "\n",
      "Iteration 6828, Loss: 35.703548431396484, L1: 10.087057113647461, L3: 25.616491317749023\n",
      "Current prediction:  61.50210189819336 \n",
      "\n",
      "Iteration 6829, Loss: 36.352237701416016, L1: 10.1177978515625, L3: 26.234439849853516\n",
      "Current prediction:  61.46847152709961 \n",
      "\n",
      "Iteration 6830, Loss: 36.42308807373047, L1: 10.116328239440918, L3: 26.306758880615234\n",
      "Current prediction:  61.34309768676758 \n",
      "\n",
      "Iteration 6831, Loss: 36.34798049926758, L1: 10.083386421203613, L3: 26.26459312438965\n",
      "Current prediction:  61.18082046508789 \n",
      "\n",
      "Iteration 6832, Loss: 36.37656784057617, L1: 10.101540565490723, L3: 26.275028228759766\n",
      "Current prediction:  61.0191650390625 \n",
      "\n",
      "Iteration 6833, Loss: 35.64349365234375, L1: 10.095952987670898, L3: 25.54754066467285\n",
      "Current prediction:  61.273216247558594 \n",
      "\n",
      "Iteration 6834, Loss: 35.68903732299805, L1: 10.052433967590332, L3: 25.6366024017334\n",
      "Current prediction:  61.1978874206543 \n",
      "\n",
      "Iteration 6835, Loss: 37.127464294433594, L1: 10.055442810058594, L3: 27.072019577026367\n",
      "Current prediction:  61.24025344848633 \n",
      "\n",
      "Iteration 6836, Loss: 36.22117614746094, L1: 10.054841041564941, L3: 26.16633415222168\n",
      "Current prediction:  61.277061462402344 \n",
      "\n",
      "Iteration 6837, Loss: 35.89851760864258, L1: 10.01413631439209, L3: 25.884382247924805\n",
      "Current prediction:  61.254478454589844 \n",
      "\n",
      "Iteration 6838, Loss: 37.02796936035156, L1: 10.068145751953125, L3: 26.95982551574707\n",
      "Current prediction:  61.11061096191406 \n",
      "\n",
      "Iteration 6839, Loss: 36.79548645019531, L1: 10.040903091430664, L3: 26.754581451416016\n",
      "Current prediction:  61.039913177490234 \n",
      "\n",
      "Iteration 6840, Loss: 36.36078643798828, L1: 10.052618980407715, L3: 26.308168411254883\n",
      "Current prediction:  61.05752944946289 \n",
      "\n",
      "Iteration 6841, Loss: 36.70949935913086, L1: 9.988327026367188, L3: 26.721172332763672\n",
      "Current prediction:  60.9637336730957 \n",
      "\n",
      "Iteration 6842, Loss: 35.0808219909668, L1: 10.055901527404785, L3: 25.024919509887695\n",
      "Current prediction:  60.802066802978516 \n",
      "\n",
      "Iteration 6843, Loss: 37.04011917114258, L1: 10.050774574279785, L3: 26.989343643188477\n",
      "Current prediction:  60.75679397583008 \n",
      "\n",
      "Iteration 6844, Loss: 36.82524108886719, L1: 10.118721961975098, L3: 26.706518173217773\n",
      "Current prediction:  60.75130844116211 \n",
      "\n",
      "Iteration 6845, Loss: 36.551998138427734, L1: 10.201766014099121, L3: 26.350231170654297\n",
      "Current prediction:  60.74592971801758 \n",
      "\n",
      "Iteration 6846, Loss: 37.30175018310547, L1: 10.164151191711426, L3: 27.13759994506836\n",
      "Current prediction:  60.74349594116211 \n",
      "\n",
      "Iteration 6847, Loss: 36.836360931396484, L1: 10.176351547241211, L3: 26.660009384155273\n",
      "Current prediction:  60.74909591674805 \n",
      "\n",
      "Iteration 6848, Loss: 36.56264877319336, L1: 10.131976127624512, L3: 26.43067169189453\n",
      "Current prediction:  60.84393310546875 \n",
      "\n",
      "Iteration 6849, Loss: 35.89199447631836, L1: 10.086262702941895, L3: 25.80573081970215\n",
      "Current prediction:  61.37617111206055 \n",
      "\n",
      "Iteration 6850, Loss: 36.07208251953125, L1: 10.072277069091797, L3: 25.999807357788086\n",
      "Current prediction:  61.56686782836914 \n",
      "\n",
      "Iteration 6851, Loss: 37.2218017578125, L1: 10.046812057495117, L3: 27.17498779296875\n",
      "Current prediction:  61.57792663574219 \n",
      "\n",
      "Iteration 6852, Loss: 36.09931945800781, L1: 10.069467544555664, L3: 26.02985382080078\n",
      "Current prediction:  61.57930374145508 \n",
      "\n",
      "Iteration 6853, Loss: 36.914283752441406, L1: 10.067228317260742, L3: 26.84705352783203\n",
      "Current prediction:  61.57946014404297 \n",
      "\n",
      "Iteration 6854, Loss: 35.71678924560547, L1: 10.076803207397461, L3: 25.639986038208008\n",
      "Current prediction:  61.57743453979492 \n",
      "\n",
      "Iteration 6855, Loss: 35.453678131103516, L1: 10.07632064819336, L3: 25.377357482910156\n",
      "Current prediction:  61.572723388671875 \n",
      "\n",
      "Iteration 6856, Loss: 36.47911071777344, L1: 10.106060028076172, L3: 26.373050689697266\n",
      "Current prediction:  61.56654739379883 \n",
      "\n",
      "Iteration 6857, Loss: 36.79213333129883, L1: 10.033446311950684, L3: 26.75868797302246\n",
      "Current prediction:  61.55369567871094 \n",
      "\n",
      "Iteration 6858, Loss: 37.51286315917969, L1: 10.0379638671875, L3: 27.474899291992188\n",
      "Current prediction:  61.40262222290039 \n",
      "\n",
      "Iteration 6859, Loss: 36.82252883911133, L1: 10.058735847473145, L3: 26.7637939453125\n",
      "Current prediction:  60.8768424987793 \n",
      "\n",
      "Iteration 6860, Loss: 36.579559326171875, L1: 10.06530475616455, L3: 26.51425552368164\n",
      "Current prediction:  60.72706985473633 \n",
      "\n",
      "Iteration 6861, Loss: 37.361026763916016, L1: 10.156539916992188, L3: 27.204486846923828\n",
      "Current prediction:  60.719688415527344 \n",
      "\n",
      "Iteration 6862, Loss: 35.78812789916992, L1: 10.14738941192627, L3: 25.640737533569336\n",
      "Current prediction:  60.718997955322266 \n",
      "\n",
      "Iteration 6863, Loss: 36.92810821533203, L1: 10.159324645996094, L3: 26.768781661987305\n",
      "Current prediction:  60.75160217285156 \n",
      "\n",
      "Iteration 6864, Loss: 35.144981384277344, L1: 10.10279655456543, L3: 25.042186737060547\n",
      "Current prediction:  61.19829177856445 \n",
      "\n",
      "Iteration 6865, Loss: 36.484188079833984, L1: 10.046442985534668, L3: 26.437744140625\n",
      "Current prediction:  61.55072784423828 \n",
      "\n",
      "Iteration 6866, Loss: 36.72236251831055, L1: 10.031550407409668, L3: 26.690813064575195\n",
      "Current prediction:  61.566246032714844 \n",
      "\n",
      "Iteration 6867, Loss: 36.296852111816406, L1: 10.084065437316895, L3: 26.212785720825195\n",
      "Current prediction:  61.57001495361328 \n",
      "\n",
      "Iteration 6868, Loss: 35.911006927490234, L1: 10.085247993469238, L3: 25.82575798034668\n",
      "Current prediction:  61.57067108154297 \n",
      "\n",
      "Iteration 6869, Loss: 36.34499740600586, L1: 10.075276374816895, L3: 26.26972198486328\n",
      "Current prediction:  61.56603240966797 \n",
      "\n",
      "Iteration 6870, Loss: 36.86642074584961, L1: 10.103270530700684, L3: 26.76314926147461\n",
      "Current prediction:  61.5533447265625 \n",
      "\n",
      "Iteration 6871, Loss: 36.78630065917969, L1: 10.075347900390625, L3: 26.71095085144043\n",
      "Current prediction:  61.46345138549805 \n",
      "\n",
      "Iteration 6872, Loss: 35.49071502685547, L1: 10.04527473449707, L3: 25.445438385009766\n",
      "Current prediction:  61.00579071044922 \n",
      "\n",
      "Iteration 6873, Loss: 35.79920959472656, L1: 10.05705738067627, L3: 25.742151260375977\n",
      "Current prediction:  60.744564056396484 \n",
      "\n",
      "Iteration 6874, Loss: 36.95036315917969, L1: 10.105059623718262, L3: 26.84530258178711\n",
      "Current prediction:  60.72903823852539 \n",
      "\n",
      "Iteration 6875, Loss: 36.30644989013672, L1: 10.092790603637695, L3: 26.213661193847656\n",
      "Current prediction:  60.76290512084961 \n",
      "\n",
      "Iteration 6876, Loss: 36.36145782470703, L1: 10.106959342956543, L3: 26.254497528076172\n",
      "Current prediction:  60.987098693847656 \n",
      "\n",
      "Iteration 6877, Loss: 37.032066345214844, L1: 10.09385871887207, L3: 26.938209533691406\n",
      "Current prediction:  61.33401870727539 \n",
      "\n",
      "Iteration 6878, Loss: 35.95796585083008, L1: 10.078536033630371, L3: 25.87942886352539\n",
      "Current prediction:  61.52670669555664 \n",
      "\n",
      "Iteration 6879, Loss: 36.05567169189453, L1: 10.075946807861328, L3: 25.979724884033203\n",
      "Current prediction:  61.54178237915039 \n",
      "\n",
      "Iteration 6880, Loss: 36.18415832519531, L1: 10.058082580566406, L3: 26.126075744628906\n",
      "Current prediction:  61.54848098754883 \n",
      "\n",
      "Iteration 6881, Loss: 36.037574768066406, L1: 10.098808288574219, L3: 25.938764572143555\n",
      "Current prediction:  61.55567169189453 \n",
      "\n",
      "Iteration 6882, Loss: 36.765865325927734, L1: 10.073470115661621, L3: 26.692394256591797\n",
      "Current prediction:  61.55961608886719 \n",
      "\n",
      "Iteration 6883, Loss: 36.12444305419922, L1: 10.072437286376953, L3: 26.052003860473633\n",
      "Current prediction:  61.556480407714844 \n",
      "\n",
      "Iteration 6884, Loss: 36.85422897338867, L1: 10.089873313903809, L3: 26.76435661315918\n",
      "Current prediction:  61.535804748535156 \n",
      "\n",
      "Iteration 6885, Loss: 36.90008544921875, L1: 10.029544830322266, L3: 26.870540618896484\n",
      "Current prediction:  61.288917541503906 \n",
      "\n",
      "Iteration 6886, Loss: 36.07836151123047, L1: 10.10474967956543, L3: 25.973609924316406\n",
      "Current prediction:  60.904869079589844 \n",
      "\n",
      "Iteration 6887, Loss: 37.197933197021484, L1: 10.066788673400879, L3: 27.13114356994629\n",
      "Current prediction:  60.799190521240234 \n",
      "\n",
      "Iteration 6888, Loss: 35.47834014892578, L1: 10.102218627929688, L3: 25.376121520996094\n",
      "Current prediction:  60.773990631103516 \n",
      "\n",
      "Iteration 6889, Loss: 35.33897018432617, L1: 10.108067512512207, L3: 25.23090171813965\n",
      "Current prediction:  60.88644790649414 \n",
      "\n",
      "Iteration 6890, Loss: 35.59577178955078, L1: 10.06811237335205, L3: 25.527660369873047\n",
      "Current prediction:  61.152164459228516 \n",
      "\n",
      "Iteration 6891, Loss: 35.61098861694336, L1: 10.032358169555664, L3: 25.578630447387695\n",
      "Current prediction:  61.47853469848633 \n",
      "\n",
      "Iteration 6892, Loss: 36.119258880615234, L1: 10.008295059204102, L3: 26.110963821411133\n",
      "Current prediction:  61.556793212890625 \n",
      "\n",
      "Iteration 6893, Loss: 36.410892486572266, L1: 10.025609016418457, L3: 26.385284423828125\n",
      "Current prediction:  61.577796936035156 \n",
      "\n",
      "Iteration 6894, Loss: 35.032470703125, L1: 10.01976203918457, L3: 25.01270866394043\n",
      "Current prediction:  61.5537109375 \n",
      "\n",
      "Iteration 6895, Loss: 36.01264953613281, L1: 10.020406723022461, L3: 25.992244720458984\n",
      "Current prediction:  61.45672607421875 \n",
      "\n",
      "Iteration 6896, Loss: 36.12378692626953, L1: 10.00524616241455, L3: 26.118539810180664\n",
      "Current prediction:  61.362735748291016 \n",
      "\n",
      "Iteration 6897, Loss: 37.00109100341797, L1: 9.978185653686523, L3: 27.022905349731445\n",
      "Current prediction:  61.3393440246582 \n",
      "\n",
      "Iteration 6898, Loss: 35.66929244995117, L1: 9.966115951538086, L3: 25.703176498413086\n",
      "Current prediction:  61.138206481933594 \n",
      "\n",
      "Iteration 6899, Loss: 35.42546081542969, L1: 9.955002784729004, L3: 25.470457077026367\n",
      "Current prediction:  60.82636260986328 \n",
      "\n",
      "Iteration 6900, Loss: 36.30999755859375, L1: 10.082469940185547, L3: 26.227527618408203\n",
      "Current prediction:  60.80144119262695 \n",
      "\n",
      "Iteration 6901, Loss: 35.3035774230957, L1: 10.097574234008789, L3: 25.206003189086914\n",
      "Current prediction:  60.79197692871094 \n",
      "\n",
      "Iteration 6902, Loss: 36.15540313720703, L1: 10.061729431152344, L3: 26.09367561340332\n",
      "Current prediction:  60.805641174316406 \n",
      "\n",
      "Iteration 6903, Loss: 36.266090393066406, L1: 10.077725410461426, L3: 26.188364028930664\n",
      "Current prediction:  60.897830963134766 \n",
      "\n",
      "Iteration 6904, Loss: 37.229915618896484, L1: 10.04015827178955, L3: 27.189756393432617\n",
      "Current prediction:  61.31490707397461 \n",
      "\n",
      "Iteration 6905, Loss: 35.86191177368164, L1: 9.992648124694824, L3: 25.869264602661133\n",
      "Current prediction:  61.605613708496094 \n",
      "\n",
      "Iteration 6906, Loss: 35.78156280517578, L1: 9.998716354370117, L3: 25.782846450805664\n",
      "Current prediction:  61.62345504760742 \n",
      "\n",
      "Iteration 6907, Loss: 37.123661041259766, L1: 9.9989595413208, L3: 27.12470245361328\n",
      "Current prediction:  61.62260055541992 \n",
      "\n",
      "Iteration 6908, Loss: 35.850257873535156, L1: 10.019213676452637, L3: 25.831043243408203\n",
      "Current prediction:  61.61363983154297 \n",
      "\n",
      "Iteration 6909, Loss: 35.41091537475586, L1: 10.040226936340332, L3: 25.37068748474121\n",
      "Current prediction:  61.603275299072266 \n",
      "\n",
      "Iteration 6910, Loss: 36.8866081237793, L1: 10.02531909942627, L3: 26.86128807067871\n",
      "Current prediction:  61.589115142822266 \n",
      "\n",
      "Iteration 6911, Loss: 35.38810348510742, L1: 10.014477729797363, L3: 25.373624801635742\n",
      "Current prediction:  61.57334518432617 \n",
      "\n",
      "Iteration 6912, Loss: 36.79500961303711, L1: 10.086945533752441, L3: 26.70806312561035\n",
      "Current prediction:  61.56186294555664 \n",
      "\n",
      "Iteration 6913, Loss: 36.826171875, L1: 10.05504322052002, L3: 26.771127700805664\n",
      "Current prediction:  61.526634216308594 \n",
      "\n",
      "Iteration 6914, Loss: 36.94205856323242, L1: 10.05528450012207, L3: 26.88677406311035\n",
      "Current prediction:  60.94912338256836 \n",
      "\n",
      "Iteration 6915, Loss: 36.351219177246094, L1: 10.091687202453613, L3: 26.259531021118164\n",
      "Current prediction:  60.72624969482422 \n",
      "\n",
      "Iteration 6916, Loss: 36.2404899597168, L1: 10.115021705627441, L3: 26.12546730041504\n",
      "Current prediction:  60.6978759765625 \n",
      "\n",
      "Iteration 6917, Loss: 36.08607864379883, L1: 10.128935813903809, L3: 25.957143783569336\n",
      "Current prediction:  60.69258117675781 \n",
      "\n",
      "Iteration 6918, Loss: 37.548736572265625, L1: 10.142836570739746, L3: 27.405899047851562\n",
      "Current prediction:  60.69031524658203 \n",
      "\n",
      "Iteration 6919, Loss: 36.111534118652344, L1: 10.20577621459961, L3: 25.9057559967041\n",
      "Current prediction:  60.69422912597656 \n",
      "\n",
      "Iteration 6920, Loss: 36.69375991821289, L1: 10.190741539001465, L3: 26.503019332885742\n",
      "Current prediction:  60.74132537841797 \n",
      "\n",
      "Iteration 6921, Loss: 35.81794738769531, L1: 10.103010177612305, L3: 25.714935302734375\n",
      "Current prediction:  61.22099304199219 \n",
      "\n",
      "Iteration 6922, Loss: 36.772361755371094, L1: 10.072875022888184, L3: 26.699487686157227\n",
      "Current prediction:  61.52017593383789 \n",
      "\n",
      "Iteration 6923, Loss: 36.302555084228516, L1: 10.047624588012695, L3: 26.25493049621582\n",
      "Current prediction:  61.54177474975586 \n",
      "\n",
      "Iteration 6924, Loss: 35.56768035888672, L1: 10.081291198730469, L3: 25.486391067504883\n",
      "Current prediction:  61.548213958740234 \n",
      "\n",
      "Iteration 6925, Loss: 36.74713134765625, L1: 10.074766159057617, L3: 26.67236328125\n",
      "Current prediction:  61.5484619140625 \n",
      "\n",
      "Iteration 6926, Loss: 36.56273651123047, L1: 10.086991310119629, L3: 26.475744247436523\n",
      "Current prediction:  61.5441780090332 \n",
      "\n",
      "Iteration 6927, Loss: 36.12489318847656, L1: 10.083084106445312, L3: 26.04180908203125\n",
      "Current prediction:  61.5426139831543 \n",
      "\n",
      "Iteration 6928, Loss: 36.444549560546875, L1: 10.087790489196777, L3: 26.356760025024414\n",
      "Current prediction:  61.54147720336914 \n",
      "\n",
      "Iteration 6929, Loss: 37.40473556518555, L1: 10.081242561340332, L3: 27.32349395751953\n",
      "Current prediction:  61.53858947753906 \n",
      "\n",
      "Iteration 6930, Loss: 36.29258728027344, L1: 10.10297966003418, L3: 26.18960952758789\n",
      "Current prediction:  61.521522521972656 \n",
      "\n",
      "Iteration 6931, Loss: 36.13677978515625, L1: 10.041532516479492, L3: 26.095245361328125\n",
      "Current prediction:  61.38783645629883 \n",
      "\n",
      "Iteration 6932, Loss: 36.49958801269531, L1: 10.029061317443848, L3: 26.47052764892578\n",
      "Current prediction:  60.951297760009766 \n",
      "\n",
      "Iteration 6933, Loss: 36.86043930053711, L1: 10.044126510620117, L3: 26.816312789916992\n",
      "Current prediction:  60.7560920715332 \n",
      "\n",
      "Iteration 6934, Loss: 35.9257698059082, L1: 10.084729194641113, L3: 25.841039657592773\n",
      "Current prediction:  60.76158905029297 \n",
      "\n",
      "Iteration 6935, Loss: 35.50672912597656, L1: 10.060901641845703, L3: 25.445825576782227\n",
      "Current prediction:  60.77872085571289 \n",
      "\n",
      "Iteration 6936, Loss: 37.04582214355469, L1: 10.089241027832031, L3: 26.956581115722656\n",
      "Current prediction:  61.153480529785156 \n",
      "\n",
      "Iteration 6937, Loss: 35.790008544921875, L1: 10.076964378356934, L3: 25.713043212890625\n",
      "Current prediction:  61.51692581176758 \n",
      "\n",
      "Iteration 6938, Loss: 36.742401123046875, L1: 10.063875198364258, L3: 26.67852783203125\n",
      "Current prediction:  61.55607604980469 \n",
      "\n",
      "Iteration 6939, Loss: 36.01775360107422, L1: 10.016645431518555, L3: 26.001110076904297\n",
      "Current prediction:  61.56262969970703 \n",
      "\n",
      "Iteration 6940, Loss: 36.57854461669922, L1: 10.034887313842773, L3: 26.543659210205078\n",
      "Current prediction:  61.55856704711914 \n",
      "\n",
      "Iteration 6941, Loss: 36.673912048339844, L1: 9.989666938781738, L3: 26.684246063232422\n",
      "Current prediction:  61.54824447631836 \n",
      "\n",
      "Iteration 6942, Loss: 35.77535629272461, L1: 10.053552627563477, L3: 25.721803665161133\n",
      "Current prediction:  61.5212516784668 \n",
      "\n",
      "Iteration 6943, Loss: 36.30067825317383, L1: 10.091660499572754, L3: 26.20901870727539\n",
      "Current prediction:  61.395713806152344 \n",
      "\n",
      "Iteration 6944, Loss: 35.76417541503906, L1: 10.04120922088623, L3: 25.72296714782715\n",
      "Current prediction:  60.96060562133789 \n",
      "\n",
      "Iteration 6945, Loss: 36.43410110473633, L1: 10.039331436157227, L3: 26.3947696685791\n",
      "Current prediction:  60.769317626953125 \n",
      "\n",
      "Iteration 6946, Loss: 36.420166015625, L1: 10.090959548950195, L3: 26.329206466674805\n",
      "Current prediction:  60.774688720703125 \n",
      "\n",
      "Iteration 6947, Loss: 37.361839294433594, L1: 10.080050468444824, L3: 27.281787872314453\n",
      "Current prediction:  60.924224853515625 \n",
      "\n",
      "Iteration 6948, Loss: 36.132469177246094, L1: 10.11374282836914, L3: 26.018728256225586\n",
      "Current prediction:  61.41394805908203 \n",
      "\n",
      "Iteration 6949, Loss: 35.83606719970703, L1: 10.036385536193848, L3: 25.799680709838867\n",
      "Current prediction:  61.529808044433594 \n",
      "\n",
      "Iteration 6950, Loss: 36.054664611816406, L1: 10.022876739501953, L3: 26.031787872314453\n",
      "Current prediction:  61.54512405395508 \n",
      "\n",
      "Iteration 6951, Loss: 35.41835021972656, L1: 10.082242965698242, L3: 25.33610725402832\n",
      "Current prediction:  61.55252456665039 \n",
      "\n",
      "Iteration 6952, Loss: 36.20158004760742, L1: 10.05910587310791, L3: 26.142475128173828\n",
      "Current prediction:  61.5487174987793 \n",
      "\n",
      "Iteration 6953, Loss: 36.966007232666016, L1: 10.03857135772705, L3: 26.92743682861328\n",
      "Current prediction:  61.50971984863281 \n",
      "\n",
      "Iteration 6954, Loss: 35.73811340332031, L1: 10.011319160461426, L3: 25.726795196533203\n",
      "Current prediction:  61.367576599121094 \n",
      "\n",
      "Iteration 6955, Loss: 36.10840606689453, L1: 9.99515438079834, L3: 26.113250732421875\n",
      "Current prediction:  60.890140533447266 \n",
      "\n",
      "Iteration 6956, Loss: 36.35704803466797, L1: 10.036088943481445, L3: 26.320959091186523\n",
      "Current prediction:  60.75151824951172 \n",
      "\n",
      "Iteration 6957, Loss: 35.257965087890625, L1: 10.086713790893555, L3: 25.171249389648438\n",
      "Current prediction:  60.74903106689453 \n",
      "\n",
      "Iteration 6958, Loss: 37.244903564453125, L1: 10.122071266174316, L3: 27.122831344604492\n",
      "Current prediction:  60.755027770996094 \n",
      "\n",
      "Iteration 6959, Loss: 36.858848571777344, L1: 10.103960037231445, L3: 26.75489044189453\n",
      "Current prediction:  60.7619514465332 \n",
      "\n",
      "Iteration 6960, Loss: 36.13853454589844, L1: 10.100648880004883, L3: 26.037885665893555\n",
      "Current prediction:  60.83784866333008 \n",
      "\n",
      "Iteration 6961, Loss: 36.10729217529297, L1: 10.042116165161133, L3: 26.06517791748047\n",
      "Current prediction:  61.45588302612305 \n",
      "\n",
      "Iteration 6962, Loss: 37.04658126831055, L1: 9.98438835144043, L3: 27.062192916870117\n",
      "Current prediction:  61.574859619140625 \n",
      "\n",
      "Iteration 6963, Loss: 36.8480110168457, L1: 9.98674488067627, L3: 26.861265182495117\n",
      "Current prediction:  61.58243179321289 \n",
      "\n",
      "Iteration 6964, Loss: 36.49413299560547, L1: 10.006820678710938, L3: 26.48731231689453\n",
      "Current prediction:  61.57084655761719 \n",
      "\n",
      "Iteration 6965, Loss: 35.70478057861328, L1: 10.016845703125, L3: 25.687936782836914\n",
      "Current prediction:  61.247928619384766 \n",
      "\n",
      "Iteration 6966, Loss: 36.156978607177734, L1: 9.97591781616211, L3: 26.181060791015625\n",
      "Current prediction:  60.98244857788086 \n",
      "\n",
      "Iteration 6967, Loss: 35.536014556884766, L1: 9.981233596801758, L3: 25.554780960083008\n",
      "Current prediction:  60.85847473144531 \n",
      "\n",
      "Iteration 6968, Loss: 35.96723556518555, L1: 10.019161224365234, L3: 25.948074340820312\n",
      "Current prediction:  60.81625747680664 \n",
      "\n",
      "Iteration 6969, Loss: 36.24121856689453, L1: 10.060726165771484, L3: 26.180492401123047\n",
      "Current prediction:  60.86270523071289 \n",
      "\n",
      "Iteration 6970, Loss: 36.65968704223633, L1: 10.008575439453125, L3: 26.651111602783203\n",
      "Current prediction:  61.03989028930664 \n",
      "\n",
      "Iteration 6971, Loss: 35.493404388427734, L1: 10.014078140258789, L3: 25.479326248168945\n",
      "Current prediction:  61.34696578979492 \n",
      "\n",
      "Iteration 6972, Loss: 36.24000549316406, L1: 10.025883674621582, L3: 26.214122772216797\n",
      "Current prediction:  61.451271057128906 \n",
      "\n",
      "Iteration 6973, Loss: 36.047298431396484, L1: 10.036344528198242, L3: 26.010953903198242\n",
      "Current prediction:  61.49253845214844 \n",
      "\n",
      "Iteration 6974, Loss: 35.72669982910156, L1: 9.986169815063477, L3: 25.74053192138672\n",
      "Current prediction:  61.482662200927734 \n",
      "\n",
      "Iteration 6975, Loss: 36.16551971435547, L1: 10.003057479858398, L3: 26.162460327148438\n",
      "Current prediction:  61.43217849731445 \n",
      "\n",
      "Iteration 6976, Loss: 36.48700714111328, L1: 9.962532997131348, L3: 26.52447509765625\n",
      "Current prediction:  61.05335235595703 \n",
      "\n",
      "Iteration 6977, Loss: 36.33130645751953, L1: 9.965401649475098, L3: 26.36590576171875\n",
      "Current prediction:  60.819278717041016 \n",
      "\n",
      "Iteration 6978, Loss: 35.5311279296875, L1: 10.060073852539062, L3: 25.47105598449707\n",
      "Current prediction:  60.77927017211914 \n",
      "\n",
      "Iteration 6979, Loss: 36.348175048828125, L1: 10.077055931091309, L3: 26.2711181640625\n",
      "Current prediction:  60.77357482910156 \n",
      "\n",
      "Iteration 6980, Loss: 37.016868591308594, L1: 10.052082061767578, L3: 26.964784622192383\n",
      "Current prediction:  60.768123626708984 \n",
      "\n",
      "Iteration 6981, Loss: 36.46773910522461, L1: 10.040745735168457, L3: 26.42699432373047\n",
      "Current prediction:  60.90202331542969 \n",
      "\n",
      "Iteration 6982, Loss: 35.5980224609375, L1: 10.034308433532715, L3: 25.5637149810791\n",
      "Current prediction:  61.52851486206055 \n",
      "\n",
      "Iteration 6983, Loss: 36.023189544677734, L1: 10.015276908874512, L3: 26.007911682128906\n",
      "Current prediction:  61.561920166015625 \n",
      "\n",
      "Iteration 6984, Loss: 36.713096618652344, L1: 10.045978546142578, L3: 26.667118072509766\n",
      "Current prediction:  61.559818267822266 \n",
      "\n",
      "Iteration 6985, Loss: 36.808250427246094, L1: 10.054377555847168, L3: 26.75387191772461\n",
      "Current prediction:  61.55038070678711 \n",
      "\n",
      "Iteration 6986, Loss: 35.927120208740234, L1: 10.086414337158203, L3: 25.84070587158203\n",
      "Current prediction:  61.54022979736328 \n",
      "\n",
      "Iteration 6987, Loss: 37.20557403564453, L1: 10.081271171569824, L3: 27.124303817749023\n",
      "Current prediction:  61.530670166015625 \n",
      "\n",
      "Iteration 6988, Loss: 36.6365966796875, L1: 10.063021659851074, L3: 26.57357406616211\n",
      "Current prediction:  61.52252197265625 \n",
      "\n",
      "Iteration 6989, Loss: 37.502197265625, L1: 10.100170135498047, L3: 27.40202522277832\n",
      "Current prediction:  61.51493453979492 \n",
      "\n",
      "Iteration 6990, Loss: 36.350486755371094, L1: 10.109796524047852, L3: 26.24068832397461\n",
      "Current prediction:  61.51136016845703 \n",
      "\n",
      "Iteration 6991, Loss: 36.20567321777344, L1: 10.109840393066406, L3: 26.0958309173584\n",
      "Current prediction:  61.49455261230469 \n",
      "\n",
      "Iteration 6992, Loss: 36.68753433227539, L1: 10.05351734161377, L3: 26.634016036987305\n",
      "Current prediction:  61.0782356262207 \n",
      "\n",
      "Iteration 6993, Loss: 35.808258056640625, L1: 10.042407035827637, L3: 25.765851974487305\n",
      "Current prediction:  60.6942253112793 \n",
      "\n",
      "Iteration 6994, Loss: 36.19611740112305, L1: 10.134604454040527, L3: 26.061511993408203\n",
      "Current prediction:  60.67897033691406 \n",
      "\n",
      "Iteration 6995, Loss: 36.10000228881836, L1: 10.15129566192627, L3: 25.948707580566406\n",
      "Current prediction:  60.68503952026367 \n",
      "\n",
      "Iteration 6996, Loss: 36.24216842651367, L1: 10.14961051940918, L3: 26.092557907104492\n",
      "Current prediction:  60.6998291015625 \n",
      "\n",
      "Iteration 6997, Loss: 36.789833068847656, L1: 10.171561241149902, L3: 26.61827278137207\n",
      "Current prediction:  60.71416091918945 \n",
      "\n",
      "Iteration 6998, Loss: 36.56672668457031, L1: 10.236993789672852, L3: 26.329730987548828\n",
      "Current prediction:  60.73410415649414 \n",
      "\n",
      "â†³ LR reduced to 5.0e-04 at iteration 7000 \n",
      "\n",
      "Iteration 6999, Loss: 37.6314697265625, L1: 10.152898788452148, L3: 27.47856903076172\n",
      "Current prediction:  60.75133514404297 \n",
      "\n",
      "Iteration 7000, Loss: 35.73134994506836, L1: 10.119091033935547, L3: 25.612258911132812\n",
      "Current prediction:  60.78007888793945 \n",
      "\n",
      "Iteration 7001, Loss: 37.7208251953125, L1: 10.072173118591309, L3: 27.648653030395508\n",
      "Current prediction:  61.17766571044922 \n",
      "\n",
      "Iteration 7002, Loss: 36.512725830078125, L1: 9.948989868164062, L3: 26.56373405456543\n",
      "Current prediction:  61.62324523925781 \n",
      "\n",
      "Iteration 7003, Loss: 36.6544075012207, L1: 10.002058982849121, L3: 26.6523494720459\n",
      "Current prediction:  61.63475036621094 \n",
      "\n",
      "Iteration 7004, Loss: 35.77872085571289, L1: 10.035901069641113, L3: 25.74281883239746\n",
      "Current prediction:  61.63834762573242 \n",
      "\n",
      "Iteration 7005, Loss: 36.429630279541016, L1: 10.036450386047363, L3: 26.393178939819336\n",
      "Current prediction:  61.63832473754883 \n",
      "\n",
      "Iteration 7006, Loss: 36.83274459838867, L1: 10.03034496307373, L3: 26.802400588989258\n",
      "Current prediction:  61.630943298339844 \n",
      "\n",
      "Iteration 7007, Loss: 36.95039367675781, L1: 10.031026840209961, L3: 26.91936492919922\n",
      "Current prediction:  61.6231803894043 \n",
      "\n",
      "Iteration 7008, Loss: 36.01993942260742, L1: 10.014065742492676, L3: 26.00587272644043\n",
      "Current prediction:  61.61265563964844 \n",
      "\n",
      "Iteration 7009, Loss: 36.653594970703125, L1: 10.061422348022461, L3: 26.592174530029297\n",
      "Current prediction:  61.59941101074219 \n",
      "\n",
      "Iteration 7010, Loss: 35.925254821777344, L1: 10.035550117492676, L3: 25.88970375061035\n",
      "Current prediction:  61.5825080871582 \n",
      "\n",
      "Iteration 7011, Loss: 35.508121490478516, L1: 10.01883316040039, L3: 25.489288330078125\n",
      "Current prediction:  61.53689193725586 \n",
      "\n",
      "Iteration 7012, Loss: 36.035545349121094, L1: 10.013168334960938, L3: 26.02237892150879\n",
      "Current prediction:  60.948036193847656 \n",
      "\n",
      "Iteration 7013, Loss: 37.07005310058594, L1: 10.056632995605469, L3: 27.013418197631836\n",
      "Current prediction:  60.69023132324219 \n",
      "\n",
      "Iteration 7014, Loss: 35.3061637878418, L1: 10.141342163085938, L3: 25.16482162475586\n",
      "Current prediction:  60.66896438598633 \n",
      "\n",
      "Iteration 7015, Loss: 35.96240234375, L1: 10.179023742675781, L3: 25.78338050842285\n",
      "Current prediction:  60.65629959106445 \n",
      "\n",
      "Iteration 7016, Loss: 37.820499420166016, L1: 10.22884464263916, L3: 27.591655731201172\n",
      "Current prediction:  60.64657211303711 \n",
      "\n",
      "Iteration 7017, Loss: 36.73242950439453, L1: 10.192861557006836, L3: 26.539567947387695\n",
      "Current prediction:  60.64968490600586 \n",
      "\n",
      "Iteration 7018, Loss: 37.650726318359375, L1: 10.182194709777832, L3: 27.468530654907227\n",
      "Current prediction:  60.67732238769531 \n",
      "\n",
      "Iteration 7019, Loss: 35.621192932128906, L1: 10.146159172058105, L3: 25.475034713745117\n",
      "Current prediction:  60.95945358276367 \n",
      "\n",
      "Iteration 7020, Loss: 35.89948654174805, L1: 10.099867820739746, L3: 25.799617767333984\n",
      "Current prediction:  61.460567474365234 \n",
      "\n",
      "Iteration 7021, Loss: 35.756813049316406, L1: 10.09665298461914, L3: 25.6601619720459\n",
      "Current prediction:  61.52182388305664 \n",
      "\n",
      "Iteration 7022, Loss: 35.803436279296875, L1: 10.078323364257812, L3: 25.725112915039062\n",
      "Current prediction:  61.54092788696289 \n",
      "\n",
      "Iteration 7023, Loss: 36.29719543457031, L1: 10.0438814163208, L3: 26.253314971923828\n",
      "Current prediction:  61.553287506103516 \n",
      "\n",
      "Iteration 7024, Loss: 35.5496940612793, L1: 10.081536293029785, L3: 25.468156814575195\n",
      "Current prediction:  61.5665168762207 \n",
      "\n",
      "Iteration 7025, Loss: 36.428462982177734, L1: 10.072084426879883, L3: 26.35637855529785\n",
      "Current prediction:  61.57774353027344 \n",
      "\n",
      "Iteration 7026, Loss: 36.55596923828125, L1: 10.057064056396484, L3: 26.4989070892334\n",
      "Current prediction:  61.58470916748047 \n",
      "\n",
      "Iteration 7027, Loss: 35.22108840942383, L1: 10.06322193145752, L3: 25.157865524291992\n",
      "Current prediction:  61.58745193481445 \n",
      "\n",
      "Iteration 7028, Loss: 36.27128982543945, L1: 10.01494312286377, L3: 26.25634765625\n",
      "Current prediction:  61.584808349609375 \n",
      "\n",
      "Iteration 7029, Loss: 35.44923782348633, L1: 10.033295631408691, L3: 25.41594123840332\n",
      "Current prediction:  61.57710647583008 \n",
      "\n",
      "Iteration 7030, Loss: 35.74324035644531, L1: 10.013930320739746, L3: 25.72930908203125\n",
      "Current prediction:  61.38066864013672 \n",
      "\n",
      "Iteration 7031, Loss: 35.492366790771484, L1: 10.011547088623047, L3: 25.480819702148438\n",
      "Current prediction:  60.79268264770508 \n",
      "\n",
      "Iteration 7032, Loss: 35.42669677734375, L1: 10.048382759094238, L3: 25.378314971923828\n",
      "Current prediction:  60.75172805786133 \n",
      "\n",
      "Iteration 7033, Loss: 37.57541275024414, L1: 10.126863479614258, L3: 27.448549270629883\n",
      "Current prediction:  60.78323745727539 \n",
      "\n",
      "Iteration 7034, Loss: 36.999141693115234, L1: 10.061887741088867, L3: 26.937253952026367\n",
      "Current prediction:  61.04054641723633 \n",
      "\n",
      "Iteration 7035, Loss: 36.16783142089844, L1: 10.022987365722656, L3: 26.14484405517578\n",
      "Current prediction:  61.55428695678711 \n",
      "\n",
      "Iteration 7036, Loss: 35.67425537109375, L1: 9.997936248779297, L3: 25.676321029663086\n",
      "Current prediction:  61.58580017089844 \n",
      "\n",
      "Iteration 7037, Loss: 35.26533126831055, L1: 10.014439582824707, L3: 25.250890731811523\n",
      "Current prediction:  61.584716796875 \n",
      "\n",
      "Iteration 7038, Loss: 35.67964172363281, L1: 10.046059608459473, L3: 25.633581161499023\n",
      "Current prediction:  61.579620361328125 \n",
      "\n",
      "Iteration 7039, Loss: 35.981903076171875, L1: 10.047426223754883, L3: 25.93447494506836\n",
      "Current prediction:  61.57028579711914 \n",
      "\n",
      "Iteration 7040, Loss: 36.14566421508789, L1: 10.053413391113281, L3: 26.09225082397461\n",
      "Current prediction:  61.556396484375 \n",
      "\n",
      "Iteration 7041, Loss: 35.325164794921875, L1: 10.067697525024414, L3: 25.25746726989746\n",
      "Current prediction:  61.4969482421875 \n",
      "\n",
      "Iteration 7042, Loss: 37.12830352783203, L1: 9.994370460510254, L3: 27.133934020996094\n",
      "Current prediction:  60.94148635864258 \n",
      "\n",
      "Iteration 7043, Loss: 36.15935516357422, L1: 10.049825668334961, L3: 26.10953140258789\n",
      "Current prediction:  60.697792053222656 \n",
      "\n",
      "Iteration 7044, Loss: 36.37944793701172, L1: 10.125219345092773, L3: 26.254230499267578\n",
      "Current prediction:  60.68854522705078 \n",
      "\n",
      "Iteration 7045, Loss: 37.14004898071289, L1: 10.192561149597168, L3: 26.947486877441406\n",
      "Current prediction:  60.68593978881836 \n",
      "\n",
      "Iteration 7046, Loss: 36.339752197265625, L1: 10.221504211425781, L3: 26.11824607849121\n",
      "Current prediction:  60.68490982055664 \n",
      "\n",
      "Iteration 7047, Loss: 35.88482666015625, L1: 10.176277160644531, L3: 25.70854949951172\n",
      "Current prediction:  60.70296859741211 \n",
      "\n",
      "Iteration 7048, Loss: 36.96240234375, L1: 10.121376991271973, L3: 26.84102439880371\n",
      "Current prediction:  60.921775817871094 \n",
      "\n",
      "Iteration 7049, Loss: 37.036983489990234, L1: 10.052628517150879, L3: 26.984355926513672\n",
      "Current prediction:  61.5211296081543 \n",
      "\n",
      "Iteration 7050, Loss: 36.85107421875, L1: 10.059226036071777, L3: 26.79184913635254\n",
      "Current prediction:  61.5466194152832 \n",
      "\n",
      "Iteration 7051, Loss: 35.492435455322266, L1: 10.057144165039062, L3: 25.435291290283203\n",
      "Current prediction:  61.551883697509766 \n",
      "\n",
      "Iteration 7052, Loss: 35.78193664550781, L1: 10.130290985107422, L3: 25.651647567749023\n",
      "Current prediction:  61.553138732910156 \n",
      "\n",
      "Iteration 7053, Loss: 36.226600646972656, L1: 10.121274948120117, L3: 26.10532569885254\n",
      "Current prediction:  61.55142593383789 \n",
      "\n",
      "Iteration 7054, Loss: 35.857295989990234, L1: 10.092733383178711, L3: 25.764562606811523\n",
      "Current prediction:  61.546512603759766 \n",
      "\n",
      "Iteration 7055, Loss: 36.34822463989258, L1: 10.105204582214355, L3: 26.24302101135254\n",
      "Current prediction:  61.533382415771484 \n",
      "\n",
      "Iteration 7056, Loss: 36.34483337402344, L1: 10.078914642333984, L3: 26.265918731689453\n",
      "Current prediction:  61.48183059692383 \n",
      "\n",
      "Iteration 7057, Loss: 36.00715637207031, L1: 10.053580284118652, L3: 25.953575134277344\n",
      "Current prediction:  60.795555114746094 \n",
      "\n",
      "Iteration 7058, Loss: 37.399295806884766, L1: 10.074780464172363, L3: 27.32451629638672\n",
      "Current prediction:  60.67885971069336 \n",
      "\n",
      "Iteration 7059, Loss: 36.113792419433594, L1: 10.115006446838379, L3: 25.9987850189209\n",
      "Current prediction:  60.668373107910156 \n",
      "\n",
      "Iteration 7060, Loss: 36.29685974121094, L1: 10.150899887084961, L3: 26.145957946777344\n",
      "Current prediction:  60.670467376708984 \n",
      "\n",
      "Iteration 7061, Loss: 36.10067367553711, L1: 10.110238075256348, L3: 25.990434646606445\n",
      "Current prediction:  60.69269561767578 \n",
      "\n",
      "Iteration 7062, Loss: 36.33226013183594, L1: 10.120867729187012, L3: 26.21139144897461\n",
      "Current prediction:  60.843528747558594 \n",
      "\n",
      "Iteration 7063, Loss: 36.001617431640625, L1: 10.053696632385254, L3: 25.947919845581055\n",
      "Current prediction:  61.4929084777832 \n",
      "\n",
      "Iteration 7064, Loss: 37.202293395996094, L1: 10.04189395904541, L3: 27.160400390625\n",
      "Current prediction:  61.54518508911133 \n",
      "\n",
      "Iteration 7065, Loss: 36.68775939941406, L1: 10.04284381866455, L3: 26.644916534423828\n",
      "Current prediction:  61.55171203613281 \n",
      "\n",
      "Iteration 7066, Loss: 36.259071350097656, L1: 10.08125114440918, L3: 26.17782211303711\n",
      "Current prediction:  61.54910659790039 \n",
      "\n",
      "Iteration 7067, Loss: 36.103355407714844, L1: 10.093886375427246, L3: 26.00946807861328\n",
      "Current prediction:  61.55127716064453 \n",
      "\n",
      "Iteration 7068, Loss: 35.217689514160156, L1: 10.087214469909668, L3: 25.130475997924805\n",
      "Current prediction:  61.55424880981445 \n",
      "\n",
      "Iteration 7069, Loss: 36.149940490722656, L1: 10.077513694763184, L3: 26.07242774963379\n",
      "Current prediction:  61.550453186035156 \n",
      "\n",
      "Iteration 7070, Loss: 36.35132598876953, L1: 10.088309288024902, L3: 26.263015747070312\n",
      "Current prediction:  61.54228210449219 \n",
      "\n",
      "Iteration 7071, Loss: 36.76646423339844, L1: 10.102348327636719, L3: 26.66411781311035\n",
      "Current prediction:  61.53279495239258 \n",
      "\n",
      "Iteration 7072, Loss: 35.973297119140625, L1: 10.059401512145996, L3: 25.913894653320312\n",
      "Current prediction:  61.44449996948242 \n",
      "\n",
      "Iteration 7073, Loss: 35.0765380859375, L1: 10.004014015197754, L3: 25.07252311706543\n",
      "Current prediction:  60.823089599609375 \n",
      "\n",
      "Iteration 7074, Loss: 36.40665054321289, L1: 10.063898086547852, L3: 26.34275245666504\n",
      "Current prediction:  60.71540069580078 \n",
      "\n",
      "Iteration 7075, Loss: 36.53563690185547, L1: 10.102373123168945, L3: 26.433265686035156\n",
      "Current prediction:  60.72144317626953 \n",
      "\n",
      "Iteration 7076, Loss: 35.74081802368164, L1: 10.071764945983887, L3: 25.669052124023438\n",
      "Current prediction:  60.84141159057617 \n",
      "\n",
      "Iteration 7077, Loss: 36.08600997924805, L1: 10.029877662658691, L3: 26.056133270263672\n",
      "Current prediction:  61.25800323486328 \n",
      "\n",
      "Iteration 7078, Loss: 36.345455169677734, L1: 10.003954887390137, L3: 26.34149932861328\n",
      "Current prediction:  61.51830291748047 \n",
      "\n",
      "Iteration 7079, Loss: 36.3918342590332, L1: 10.043022155761719, L3: 26.348812103271484\n",
      "Current prediction:  61.551239013671875 \n",
      "\n",
      "Iteration 7080, Loss: 36.227203369140625, L1: 10.026993751525879, L3: 26.200210571289062\n",
      "Current prediction:  61.560791015625 \n",
      "\n",
      "Iteration 7081, Loss: 37.884498596191406, L1: 10.05545711517334, L3: 27.82904052734375\n",
      "Current prediction:  61.53584671020508 \n",
      "\n",
      "Iteration 7082, Loss: 36.154998779296875, L1: 9.968544006347656, L3: 26.18645477294922\n",
      "Current prediction:  61.45115661621094 \n",
      "\n",
      "Iteration 7083, Loss: 34.97348403930664, L1: 9.964373588562012, L3: 25.009109497070312\n",
      "Current prediction:  60.97496032714844 \n",
      "\n",
      "Iteration 7084, Loss: 36.050933837890625, L1: 10.015424728393555, L3: 26.035507202148438\n",
      "Current prediction:  60.77921676635742 \n",
      "\n",
      "Iteration 7085, Loss: 36.544273376464844, L1: 10.039142608642578, L3: 26.505128860473633\n",
      "Current prediction:  60.74054718017578 \n",
      "\n",
      "Iteration 7086, Loss: 35.867469787597656, L1: 10.059172630310059, L3: 25.80829620361328\n",
      "Current prediction:  60.73476791381836 \n",
      "\n",
      "Iteration 7087, Loss: 36.622989654541016, L1: 10.10163402557373, L3: 26.5213565826416\n",
      "Current prediction:  60.75214767456055 \n",
      "\n",
      "Iteration 7088, Loss: 35.59549331665039, L1: 10.040080070495605, L3: 25.55541229248047\n",
      "Current prediction:  60.949031829833984 \n",
      "\n",
      "Iteration 7089, Loss: 35.53472900390625, L1: 10.00654411315918, L3: 25.52818489074707\n",
      "Current prediction:  61.46805953979492 \n",
      "\n",
      "Iteration 7090, Loss: 36.685646057128906, L1: 10.014486312866211, L3: 26.671159744262695\n",
      "Current prediction:  61.56007766723633 \n",
      "\n",
      "Iteration 7091, Loss: 36.13498306274414, L1: 10.02786922454834, L3: 26.107114791870117\n",
      "Current prediction:  61.55292892456055 \n",
      "\n",
      "Iteration 7092, Loss: 36.98805236816406, L1: 10.049138069152832, L3: 26.938913345336914\n",
      "Current prediction:  61.54364013671875 \n",
      "\n",
      "Iteration 7093, Loss: 36.152076721191406, L1: 10.102448463439941, L3: 26.04962730407715\n",
      "Current prediction:  61.53240966796875 \n",
      "\n",
      "Iteration 7094, Loss: 36.620887756347656, L1: 10.07256031036377, L3: 26.548328399658203\n",
      "Current prediction:  61.51780700683594 \n",
      "\n",
      "Iteration 7095, Loss: 36.5797233581543, L1: 10.09813404083252, L3: 26.48158836364746\n",
      "Current prediction:  61.50462341308594 \n",
      "\n",
      "Iteration 7096, Loss: 35.82328796386719, L1: 10.115837097167969, L3: 25.70745086669922\n",
      "Current prediction:  61.491600036621094 \n",
      "\n",
      "Iteration 7097, Loss: 35.84759521484375, L1: 10.067330360412598, L3: 25.78026580810547\n",
      "Current prediction:  61.47029113769531 \n",
      "\n",
      "Iteration 7098, Loss: 35.75282669067383, L1: 10.120469093322754, L3: 25.63235855102539\n",
      "Current prediction:  61.33224868774414 \n",
      "\n",
      "Iteration 7099, Loss: 36.165191650390625, L1: 10.06738567352295, L3: 26.097806930541992\n",
      "Current prediction:  60.69319152832031 \n",
      "\n",
      "Iteration 7100, Loss: 36.486656188964844, L1: 10.120514869689941, L3: 26.366140365600586\n",
      "Current prediction:  60.63751983642578 \n",
      "\n",
      "Iteration 7101, Loss: 36.4061393737793, L1: 10.17902946472168, L3: 26.227109909057617\n",
      "Current prediction:  60.64484786987305 \n",
      "\n",
      "Iteration 7102, Loss: 36.146671295166016, L1: 10.195958137512207, L3: 25.950714111328125\n",
      "Current prediction:  60.6612663269043 \n",
      "\n",
      "Iteration 7103, Loss: 37.20235824584961, L1: 10.223788261413574, L3: 26.97856903076172\n",
      "Current prediction:  60.67581558227539 \n",
      "\n",
      "Iteration 7104, Loss: 37.25353240966797, L1: 10.153942108154297, L3: 27.09958839416504\n",
      "Current prediction:  60.72739028930664 \n",
      "\n",
      "Iteration 7105, Loss: 35.745452880859375, L1: 10.066356658935547, L3: 25.679094314575195\n",
      "Current prediction:  61.212982177734375 \n",
      "\n",
      "Iteration 7106, Loss: 37.17648696899414, L1: 10.00810718536377, L3: 27.168380737304688\n",
      "Current prediction:  61.52766418457031 \n",
      "\n",
      "Iteration 7107, Loss: 36.94780731201172, L1: 10.051464080810547, L3: 26.896343231201172\n",
      "Current prediction:  61.54801940917969 \n",
      "\n",
      "Iteration 7108, Loss: 35.83876037597656, L1: 10.027471542358398, L3: 25.811290740966797\n",
      "Current prediction:  61.55494689941406 \n",
      "\n",
      "Iteration 7109, Loss: 36.187339782714844, L1: 10.049323081970215, L3: 26.138017654418945\n",
      "Current prediction:  61.558467864990234 \n",
      "\n",
      "Iteration 7110, Loss: 37.11365509033203, L1: 10.07720947265625, L3: 27.036447525024414\n",
      "Current prediction:  61.557315826416016 \n",
      "\n",
      "Iteration 7111, Loss: 34.573341369628906, L1: 10.042208671569824, L3: 24.5311336517334\n",
      "Current prediction:  61.55393981933594 \n",
      "\n",
      "Iteration 7112, Loss: 36.067108154296875, L1: 10.038562774658203, L3: 26.02854347229004\n",
      "Current prediction:  61.55202102661133 \n",
      "\n",
      "Iteration 7113, Loss: 37.973838806152344, L1: 10.002144813537598, L3: 27.97169303894043\n",
      "Current prediction:  61.51959228515625 \n",
      "\n",
      "Iteration 7114, Loss: 35.762874603271484, L1: 10.071131706237793, L3: 25.691743850708008\n",
      "Current prediction:  60.96693801879883 \n",
      "\n",
      "Iteration 7115, Loss: 35.993568420410156, L1: 10.001914024353027, L3: 25.991655349731445\n",
      "Current prediction:  60.69367599487305 \n",
      "\n",
      "Iteration 7116, Loss: 36.13801574707031, L1: 10.132457733154297, L3: 26.005556106567383\n",
      "Current prediction:  60.678733825683594 \n",
      "\n",
      "Iteration 7117, Loss: 36.14849853515625, L1: 10.144462585449219, L3: 26.00403594970703\n",
      "Current prediction:  60.671875 \n",
      "\n",
      "Iteration 7118, Loss: 36.33168029785156, L1: 10.166922569274902, L3: 26.164758682250977\n",
      "Current prediction:  60.674198150634766 \n",
      "\n",
      "Iteration 7119, Loss: 35.90511703491211, L1: 10.16801929473877, L3: 25.737096786499023\n",
      "Current prediction:  60.76290512084961 \n",
      "\n",
      "Iteration 7120, Loss: 35.98619842529297, L1: 10.058479309082031, L3: 25.92772102355957\n",
      "Current prediction:  61.492393493652344 \n",
      "\n",
      "Iteration 7121, Loss: 36.21971130371094, L1: 10.036931037902832, L3: 26.182781219482422\n",
      "Current prediction:  61.53834533691406 \n",
      "\n",
      "Iteration 7122, Loss: 37.194854736328125, L1: 10.065030097961426, L3: 27.129825592041016\n",
      "Current prediction:  61.54536056518555 \n",
      "\n",
      "Iteration 7123, Loss: 36.247779846191406, L1: 10.053970336914062, L3: 26.193809509277344\n",
      "Current prediction:  61.5453987121582 \n",
      "\n",
      "Iteration 7124, Loss: 35.6993408203125, L1: 10.059731483459473, L3: 25.639610290527344\n",
      "Current prediction:  61.54559326171875 \n",
      "\n",
      "Iteration 7125, Loss: 36.54063034057617, L1: 10.055852890014648, L3: 26.484777450561523\n",
      "Current prediction:  61.54191970825195 \n",
      "\n",
      "Iteration 7126, Loss: 35.62739944458008, L1: 10.094226837158203, L3: 25.533172607421875\n",
      "Current prediction:  61.537845611572266 \n",
      "\n",
      "Iteration 7127, Loss: 35.963077545166016, L1: 10.087254524230957, L3: 25.875822067260742\n",
      "Current prediction:  61.53539276123047 \n",
      "\n",
      "Iteration 7128, Loss: 37.2884635925293, L1: 10.102400779724121, L3: 27.18606185913086\n",
      "Current prediction:  61.525489807128906 \n",
      "\n",
      "Iteration 7129, Loss: 36.51502227783203, L1: 10.078597068786621, L3: 26.436424255371094\n",
      "Current prediction:  61.51993179321289 \n",
      "\n",
      "Iteration 7130, Loss: 36.411781311035156, L1: 10.07691764831543, L3: 26.33486557006836\n",
      "Current prediction:  61.5152473449707 \n",
      "\n",
      "Iteration 7131, Loss: 36.62223434448242, L1: 10.105399131774902, L3: 26.516834259033203\n",
      "Current prediction:  61.49153137207031 \n",
      "\n",
      "Iteration 7132, Loss: 36.094154357910156, L1: 10.077194213867188, L3: 26.01696014404297\n",
      "Current prediction:  60.817405700683594 \n",
      "\n",
      "Iteration 7133, Loss: 36.47950744628906, L1: 10.062112808227539, L3: 26.417396545410156\n",
      "Current prediction:  60.650238037109375 \n",
      "\n",
      "Iteration 7134, Loss: 36.8216552734375, L1: 10.150468826293945, L3: 26.671188354492188\n",
      "Current prediction:  60.64482116699219 \n",
      "\n",
      "Iteration 7135, Loss: 36.005977630615234, L1: 10.163796424865723, L3: 25.842182159423828\n",
      "Current prediction:  60.64684295654297 \n",
      "\n",
      "Iteration 7136, Loss: 36.33467483520508, L1: 10.241609573364258, L3: 26.09306526184082\n",
      "Current prediction:  60.65134811401367 \n",
      "\n",
      "Iteration 7137, Loss: 36.25574493408203, L1: 10.191346168518066, L3: 26.06439781188965\n",
      "Current prediction:  60.66756820678711 \n",
      "\n",
      "Iteration 7138, Loss: 36.56768798828125, L1: 10.135887145996094, L3: 26.431800842285156\n",
      "Current prediction:  61.067806243896484 \n",
      "\n",
      "Iteration 7139, Loss: 36.80858612060547, L1: 10.0391263961792, L3: 26.769458770751953\n",
      "Current prediction:  61.52641296386719 \n",
      "\n",
      "Iteration 7140, Loss: 36.256900787353516, L1: 10.063529014587402, L3: 26.19337272644043\n",
      "Current prediction:  61.54228591918945 \n",
      "\n",
      "Iteration 7141, Loss: 35.74778747558594, L1: 10.094962120056152, L3: 25.6528263092041\n",
      "Current prediction:  61.55258560180664 \n",
      "\n",
      "Iteration 7142, Loss: 35.916038513183594, L1: 10.065107345581055, L3: 25.850929260253906\n",
      "Current prediction:  61.559715270996094 \n",
      "\n",
      "Iteration 7143, Loss: 37.32358169555664, L1: 10.108521461486816, L3: 27.215059280395508\n",
      "Current prediction:  61.55897521972656 \n",
      "\n",
      "Iteration 7144, Loss: 36.78384017944336, L1: 10.140549659729004, L3: 26.643291473388672\n",
      "Current prediction:  61.55314636230469 \n",
      "\n",
      "Iteration 7145, Loss: 37.18247985839844, L1: 10.154356002807617, L3: 27.028121948242188\n",
      "Current prediction:  61.53778839111328 \n",
      "\n",
      "Iteration 7146, Loss: 36.281131744384766, L1: 10.147368431091309, L3: 26.13376235961914\n",
      "Current prediction:  61.51964569091797 \n",
      "\n",
      "Iteration 7147, Loss: 35.83713912963867, L1: 10.200889587402344, L3: 25.636249542236328\n",
      "Current prediction:  61.505191802978516 \n",
      "\n",
      "Iteration 7148, Loss: 36.70733642578125, L1: 10.208044052124023, L3: 26.49929428100586\n",
      "Current prediction:  61.49055862426758 \n",
      "\n",
      "Iteration 7149, Loss: 36.328529357910156, L1: 10.216039657592773, L3: 26.112491607666016\n",
      "Current prediction:  61.476314544677734 \n",
      "\n",
      "Iteration 7150, Loss: 36.976806640625, L1: 10.18171501159668, L3: 26.795093536376953\n",
      "Current prediction:  61.45912551879883 \n",
      "\n",
      "Iteration 7151, Loss: 36.98752975463867, L1: 10.19906997680664, L3: 26.78845977783203\n",
      "Current prediction:  61.44221496582031 \n",
      "\n",
      "Iteration 7152, Loss: 36.58367156982422, L1: 10.160451889038086, L3: 26.423219680786133\n",
      "Current prediction:  61.42942810058594 \n",
      "\n",
      "Iteration 7153, Loss: 36.53089904785156, L1: 10.168722152709961, L3: 26.362178802490234\n",
      "Current prediction:  61.366111755371094 \n",
      "\n",
      "Iteration 7154, Loss: 36.15268325805664, L1: 10.16910457611084, L3: 25.983579635620117\n",
      "Current prediction:  60.64310073852539 \n",
      "\n",
      "Iteration 7155, Loss: 36.14324188232422, L1: 10.153313636779785, L3: 25.989927291870117\n",
      "Current prediction:  60.566036224365234 \n",
      "\n",
      "Iteration 7156, Loss: 36.60598373413086, L1: 10.227749824523926, L3: 26.37823486328125\n",
      "Current prediction:  60.57522964477539 \n",
      "\n",
      "Iteration 7157, Loss: 36.41867446899414, L1: 10.29220199584961, L3: 26.12647247314453\n",
      "Current prediction:  60.59125518798828 \n",
      "\n",
      "Iteration 7158, Loss: 36.80853271484375, L1: 10.304327011108398, L3: 26.50420379638672\n",
      "Current prediction:  60.613525390625 \n",
      "\n",
      "Iteration 7159, Loss: 36.95680236816406, L1: 10.2783842086792, L3: 26.67841911315918\n",
      "Current prediction:  60.637325286865234 \n",
      "\n",
      "Iteration 7160, Loss: 36.882328033447266, L1: 10.176995277404785, L3: 26.705331802368164\n",
      "Current prediction:  60.68510437011719 \n",
      "\n",
      "Iteration 7161, Loss: 36.86467742919922, L1: 10.10002613067627, L3: 26.764650344848633\n",
      "Current prediction:  61.218013763427734 \n",
      "\n",
      "Iteration 7162, Loss: 35.24608612060547, L1: 10.03330135345459, L3: 25.212783813476562\n",
      "Current prediction:  61.55150604248047 \n",
      "\n",
      "Iteration 7163, Loss: 35.279083251953125, L1: 10.055228233337402, L3: 25.22385597229004\n",
      "Current prediction:  61.593223571777344 \n",
      "\n",
      "Iteration 7164, Loss: 35.80473327636719, L1: 10.022607803344727, L3: 25.782123565673828\n",
      "Current prediction:  61.617610931396484 \n",
      "\n",
      "Iteration 7165, Loss: 36.93692398071289, L1: 10.01810359954834, L3: 26.918819427490234\n",
      "Current prediction:  61.6322135925293 \n",
      "\n",
      "Iteration 7166, Loss: 36.15670394897461, L1: 10.012541770935059, L3: 26.144161224365234\n",
      "Current prediction:  61.638702392578125 \n",
      "\n",
      "Iteration 7167, Loss: 37.177059173583984, L1: 10.025795936584473, L3: 27.151262283325195\n",
      "Current prediction:  61.63674545288086 \n",
      "\n",
      "Iteration 7168, Loss: 37.817378997802734, L1: 9.986222267150879, L3: 27.83115577697754\n",
      "Current prediction:  61.6237907409668 \n",
      "\n",
      "Iteration 7169, Loss: 37.02876663208008, L1: 9.991625785827637, L3: 27.037139892578125\n",
      "Current prediction:  61.52022933959961 \n",
      "\n",
      "Iteration 7170, Loss: 37.456787109375, L1: 9.962127685546875, L3: 27.494659423828125\n",
      "Current prediction:  60.875457763671875 \n",
      "\n",
      "Iteration 7171, Loss: 35.79234313964844, L1: 10.002782821655273, L3: 25.789562225341797\n",
      "Current prediction:  60.73774337768555 \n",
      "\n",
      "Iteration 7172, Loss: 36.849754333496094, L1: 10.075504302978516, L3: 26.774250030517578\n",
      "Current prediction:  60.72121047973633 \n",
      "\n",
      "Iteration 7173, Loss: 37.35972213745117, L1: 10.095388412475586, L3: 27.264333724975586\n",
      "Current prediction:  60.71057891845703 \n",
      "\n",
      "Iteration 7174, Loss: 36.26247024536133, L1: 10.116436004638672, L3: 26.146034240722656\n",
      "Current prediction:  60.708621978759766 \n",
      "\n",
      "Iteration 7175, Loss: 37.06046676635742, L1: 10.092554092407227, L3: 26.967912673950195\n",
      "Current prediction:  60.75141906738281 \n",
      "\n",
      "Iteration 7176, Loss: 35.54718017578125, L1: 10.033636093139648, L3: 25.51354217529297\n",
      "Current prediction:  60.99054718017578 \n",
      "\n",
      "Iteration 7177, Loss: 35.9962158203125, L1: 10.052671432495117, L3: 25.943546295166016\n",
      "Current prediction:  61.48137283325195 \n",
      "\n",
      "Iteration 7178, Loss: 35.824119567871094, L1: 10.024685859680176, L3: 25.799434661865234\n",
      "Current prediction:  61.52680969238281 \n",
      "\n",
      "Iteration 7179, Loss: 36.00374984741211, L1: 10.008026123046875, L3: 25.995723724365234\n",
      "Current prediction:  61.520469665527344 \n",
      "\n",
      "Iteration 7180, Loss: 35.94488525390625, L1: 10.030467987060547, L3: 25.91441535949707\n",
      "Current prediction:  61.50882339477539 \n",
      "\n",
      "Iteration 7181, Loss: 36.53485107421875, L1: 10.06501293182373, L3: 26.469837188720703\n",
      "Current prediction:  61.449317932128906 \n",
      "\n",
      "Iteration 7182, Loss: 36.3515739440918, L1: 10.019155502319336, L3: 26.33241844177246\n",
      "Current prediction:  61.33445358276367 \n",
      "\n",
      "Iteration 7183, Loss: 36.546016693115234, L1: 10.02806568145752, L3: 26.5179500579834\n",
      "Current prediction:  60.88093566894531 \n",
      "\n",
      "Iteration 7184, Loss: 37.91435623168945, L1: 10.016383171081543, L3: 27.897972106933594\n",
      "Current prediction:  60.695865631103516 \n",
      "\n",
      "Iteration 7185, Loss: 36.819725036621094, L1: 10.115880966186523, L3: 26.70384407043457\n",
      "Current prediction:  60.67452621459961 \n",
      "\n",
      "Iteration 7186, Loss: 35.806053161621094, L1: 10.098052024841309, L3: 25.70800018310547\n",
      "Current prediction:  60.68189239501953 \n",
      "\n",
      "Iteration 7187, Loss: 36.65345764160156, L1: 10.111688613891602, L3: 26.54176902770996\n",
      "Current prediction:  60.8608283996582 \n",
      "\n",
      "Iteration 7188, Loss: 35.272926330566406, L1: 10.048145294189453, L3: 25.224782943725586\n",
      "Current prediction:  61.52227020263672 \n",
      "\n",
      "Iteration 7189, Loss: 36.061458587646484, L1: 10.020752906799316, L3: 26.040706634521484\n",
      "Current prediction:  61.55046081542969 \n",
      "\n",
      "Iteration 7190, Loss: 35.266761779785156, L1: 10.070648193359375, L3: 25.196115493774414\n",
      "Current prediction:  61.55830383300781 \n",
      "\n",
      "Iteration 7191, Loss: 36.54220962524414, L1: 10.05029582977295, L3: 26.491912841796875\n",
      "Current prediction:  61.56300354003906 \n",
      "\n",
      "Iteration 7192, Loss: 36.059906005859375, L1: 10.055404663085938, L3: 26.004499435424805\n",
      "Current prediction:  61.5608024597168 \n",
      "\n",
      "Iteration 7193, Loss: 36.531890869140625, L1: 10.077812194824219, L3: 26.45408058166504\n",
      "Current prediction:  61.5557975769043 \n",
      "\n",
      "Iteration 7194, Loss: 36.4509391784668, L1: 10.064570426940918, L3: 26.386367797851562\n",
      "Current prediction:  61.54792404174805 \n",
      "\n",
      "Iteration 7195, Loss: 35.24114227294922, L1: 10.078344345092773, L3: 25.162797927856445\n",
      "Current prediction:  61.536354064941406 \n",
      "\n",
      "Iteration 7196, Loss: 36.73323059082031, L1: 10.054264068603516, L3: 26.67896842956543\n",
      "Current prediction:  61.50270462036133 \n",
      "\n",
      "Iteration 7197, Loss: 36.59498596191406, L1: 10.039715766906738, L3: 26.55527114868164\n",
      "Current prediction:  60.9719123840332 \n",
      "\n",
      "Iteration 7198, Loss: 36.2891960144043, L1: 10.045674324035645, L3: 26.24352264404297\n",
      "Current prediction:  60.68280792236328 \n",
      "\n",
      "Iteration 7199, Loss: 35.76447677612305, L1: 10.116129875183105, L3: 25.648345947265625\n",
      "Current prediction:  60.66653060913086 \n",
      "\n",
      "Iteration 7200, Loss: 35.374061584472656, L1: 10.12959098815918, L3: 25.244468688964844\n",
      "Current prediction:  60.67133331298828 \n",
      "\n",
      "Iteration 7201, Loss: 36.54763412475586, L1: 10.148161888122559, L3: 26.399471282958984\n",
      "Current prediction:  60.682106018066406 \n",
      "\n",
      "Iteration 7202, Loss: 36.53233337402344, L1: 10.093306541442871, L3: 26.43902587890625\n",
      "Current prediction:  60.7111930847168 \n",
      "\n",
      "Iteration 7203, Loss: 36.014488220214844, L1: 10.046762466430664, L3: 25.96772575378418\n",
      "Current prediction:  60.95482635498047 \n",
      "\n",
      "Iteration 7204, Loss: 35.92626953125, L1: 10.042779922485352, L3: 25.88348960876465\n",
      "Current prediction:  61.414466857910156 \n",
      "\n",
      "Iteration 7205, Loss: 36.107078552246094, L1: 9.985441207885742, L3: 26.12163734436035\n",
      "Current prediction:  61.53446960449219 \n",
      "\n",
      "Iteration 7206, Loss: 36.22859573364258, L1: 9.997481346130371, L3: 26.23111343383789\n",
      "Current prediction:  61.54203414916992 \n",
      "\n",
      "Iteration 7207, Loss: 36.68488693237305, L1: 10.015876770019531, L3: 26.669010162353516\n",
      "Current prediction:  61.25922775268555 \n",
      "\n",
      "Iteration 7208, Loss: 36.181461334228516, L1: 9.969988822937012, L3: 26.211471557617188\n",
      "Current prediction:  60.893924713134766 \n",
      "\n",
      "Iteration 7209, Loss: 36.162349700927734, L1: 9.947484970092773, L3: 26.21486473083496\n",
      "Current prediction:  60.88877487182617 \n",
      "\n",
      "Iteration 7210, Loss: 36.01512145996094, L1: 9.99612808227539, L3: 26.018993377685547\n",
      "Current prediction:  60.8389892578125 \n",
      "\n",
      "Iteration 7211, Loss: 36.35354232788086, L1: 9.986845970153809, L3: 26.366697311401367\n",
      "Current prediction:  60.78030014038086 \n",
      "\n",
      "Iteration 7212, Loss: 42.184547424316406, L1: 15.391690254211426, L3: 26.792856216430664\n",
      "Current prediction:  60.76076126098633 \n",
      "\n",
      "Iteration 7213, Loss: 37.008995056152344, L1: 10.060279846191406, L3: 26.948715209960938\n",
      "Current prediction:  60.83586502075195 \n",
      "\n",
      "Iteration 7214, Loss: 36.445369720458984, L1: 10.017167091369629, L3: 26.428203582763672\n",
      "Current prediction:  60.94826889038086 \n",
      "\n",
      "Iteration 7215, Loss: 36.108795166015625, L1: 10.040214538574219, L3: 26.068578720092773\n",
      "Current prediction:  60.954437255859375 \n",
      "\n",
      "Iteration 7216, Loss: 36.156028747558594, L1: 10.401384353637695, L3: 25.7546443939209\n",
      "Current prediction:  61.04788589477539 \n",
      "\n",
      "Iteration 7217, Loss: 37.731666564941406, L1: 11.509695053100586, L3: 26.22197151184082\n",
      "Current prediction:  60.8756217956543 \n",
      "\n",
      "Iteration 7218, Loss: 37.758148193359375, L1: 11.138460159301758, L3: 26.619688034057617\n",
      "Current prediction:  60.844024658203125 \n",
      "\n",
      "Iteration 7219, Loss: 36.969913482666016, L1: 10.942261695861816, L3: 26.027652740478516\n",
      "Current prediction:  60.80683898925781 \n",
      "\n",
      "Iteration 7220, Loss: 36.76848602294922, L1: 10.679931640625, L3: 26.08855628967285\n",
      "Current prediction:  60.83658218383789 \n",
      "\n",
      "Iteration 7221, Loss: 36.69075393676758, L1: 10.379319190979004, L3: 26.311433792114258\n",
      "Current prediction:  61.17813491821289 \n",
      "\n",
      "Iteration 7222, Loss: 36.9036750793457, L1: 10.448400497436523, L3: 26.45527458190918\n",
      "Current prediction:  60.94105911254883 \n",
      "\n",
      "Iteration 7223, Loss: 37.4381217956543, L1: 10.472424507141113, L3: 26.965696334838867\n",
      "Current prediction:  60.22516632080078 \n",
      "\n",
      "Iteration 7224, Loss: 37.08256912231445, L1: 10.622145652770996, L3: 26.460424423217773\n",
      "Current prediction:  60.32461166381836 \n",
      "\n",
      "Iteration 7225, Loss: 37.2117919921875, L1: 10.792397499084473, L3: 26.41939353942871\n",
      "Current prediction:  60.44368362426758 \n",
      "\n",
      "Iteration 7226, Loss: 37.82644271850586, L1: 10.856792449951172, L3: 26.969650268554688\n",
      "Current prediction:  60.57807922363281 \n",
      "\n",
      "Iteration 7227, Loss: 38.02266311645508, L1: 10.979645729064941, L3: 27.043018341064453\n",
      "Current prediction:  60.718345642089844 \n",
      "\n",
      "Iteration 7228, Loss: 36.84069061279297, L1: 11.055316925048828, L3: 25.78537368774414\n",
      "Current prediction:  60.867733001708984 \n",
      "\n",
      "Iteration 7229, Loss: 38.18925857543945, L1: 11.156798362731934, L3: 27.032459259033203\n",
      "Current prediction:  61.00807189941406 \n",
      "\n",
      "Iteration 7230, Loss: 37.55901336669922, L1: 11.298624992370605, L3: 26.260387420654297\n",
      "Current prediction:  61.15010070800781 \n",
      "\n",
      "Iteration 7231, Loss: 37.98112487792969, L1: 11.074045181274414, L3: 26.907081604003906\n",
      "Current prediction:  61.28046417236328 \n",
      "\n",
      "Iteration 7232, Loss: 38.47508239746094, L1: 11.126395225524902, L3: 27.34868812561035\n",
      "Current prediction:  61.398014068603516 \n",
      "\n",
      "Iteration 7233, Loss: 38.4089241027832, L1: 11.383027076721191, L3: 27.025896072387695\n",
      "Current prediction:  61.4942512512207 \n",
      "\n",
      "Iteration 7234, Loss: 38.656036376953125, L1: 11.32783317565918, L3: 27.328203201293945\n",
      "Current prediction:  61.573280334472656 \n",
      "\n",
      "Iteration 7235, Loss: 37.200660705566406, L1: 10.798234939575195, L3: 26.40242576599121\n",
      "Current prediction:  61.635799407958984 \n",
      "\n",
      "Iteration 7236, Loss: 37.96510696411133, L1: 10.628131866455078, L3: 27.33697509765625\n",
      "Current prediction:  61.67708969116211 \n",
      "\n",
      "Iteration 7237, Loss: 36.84897994995117, L1: 10.522095680236816, L3: 26.326885223388672\n",
      "Current prediction:  61.706336975097656 \n",
      "\n",
      "Iteration 7238, Loss: 37.34125518798828, L1: 10.65976333618164, L3: 26.681493759155273\n",
      "Current prediction:  61.716121673583984 \n",
      "\n",
      "Iteration 7239, Loss: 35.3895263671875, L1: 10.172344207763672, L3: 25.217180252075195\n",
      "Current prediction:  61.715389251708984 \n",
      "\n",
      "Iteration 7240, Loss: 36.68423080444336, L1: 10.901532173156738, L3: 25.782699584960938\n",
      "Current prediction:  61.676448822021484 \n",
      "\n",
      "Iteration 7241, Loss: 37.536930084228516, L1: 10.869181632995605, L3: 26.667749404907227\n",
      "Current prediction:  60.85212707519531 \n",
      "\n",
      "Iteration 7242, Loss: 36.326133728027344, L1: 9.950154304504395, L3: 26.375980377197266\n",
      "Current prediction:  60.81068420410156 \n",
      "\n",
      "Iteration 7243, Loss: 36.31669235229492, L1: 10.033621788024902, L3: 26.283069610595703\n",
      "Current prediction:  60.800289154052734 \n",
      "\n",
      "Iteration 7244, Loss: 37.113319396972656, L1: 10.150829315185547, L3: 26.962488174438477\n",
      "Current prediction:  60.789676666259766 \n",
      "\n",
      "Iteration 7245, Loss: 37.12632751464844, L1: 10.184025764465332, L3: 26.94230079650879\n",
      "Current prediction:  60.77626037597656 \n",
      "\n",
      "Iteration 7246, Loss: 35.942291259765625, L1: 10.186640739440918, L3: 25.75564956665039\n",
      "Current prediction:  60.76508712768555 \n",
      "\n",
      "Iteration 7247, Loss: 36.579673767089844, L1: 10.192412376403809, L3: 26.38726234436035\n",
      "Current prediction:  60.75447463989258 \n",
      "\n",
      "Iteration 7248, Loss: 35.9224853515625, L1: 10.136909484863281, L3: 25.785573959350586\n",
      "Current prediction:  60.75040817260742 \n",
      "\n",
      "Iteration 7249, Loss: 36.82886505126953, L1: 10.056445121765137, L3: 26.772418975830078\n",
      "Current prediction:  60.93214797973633 \n",
      "\n",
      "Iteration 7250, Loss: 36.60803985595703, L1: 9.940038681030273, L3: 26.66800308227539\n",
      "Current prediction:  61.59614562988281 \n",
      "\n",
      "Iteration 7251, Loss: 35.72859191894531, L1: 9.93703556060791, L3: 25.79155731201172\n",
      "Current prediction:  61.59706115722656 \n",
      "\n",
      "Iteration 7252, Loss: 37.01198196411133, L1: 10.046046257019043, L3: 26.9659366607666\n",
      "Current prediction:  61.586463928222656 \n",
      "\n",
      "Iteration 7253, Loss: 36.35112762451172, L1: 10.041568756103516, L3: 26.309558868408203\n",
      "Current prediction:  61.574462890625 \n",
      "\n",
      "Iteration 7254, Loss: 37.20222091674805, L1: 10.058865547180176, L3: 27.143356323242188\n",
      "Current prediction:  61.56133270263672 \n",
      "\n",
      "Iteration 7255, Loss: 35.38800811767578, L1: 10.034042358398438, L3: 25.353967666625977\n",
      "Current prediction:  61.549137115478516 \n",
      "\n",
      "Iteration 7256, Loss: 35.459259033203125, L1: 10.048216819763184, L3: 25.411041259765625\n",
      "Current prediction:  61.53333282470703 \n",
      "\n",
      "Iteration 7257, Loss: 36.04990768432617, L1: 10.078033447265625, L3: 25.971874237060547\n",
      "Current prediction:  61.321163177490234 \n",
      "\n",
      "Iteration 7258, Loss: 36.79913330078125, L1: 10.004456520080566, L3: 26.794675827026367\n",
      "Current prediction:  60.79399871826172 \n",
      "\n",
      "Iteration 7259, Loss: 36.96383285522461, L1: 10.009733200073242, L3: 26.954099655151367\n",
      "Current prediction:  60.68465805053711 \n",
      "\n",
      "Iteration 7260, Loss: 36.808685302734375, L1: 10.04406452178955, L3: 26.76462173461914\n",
      "Current prediction:  60.66909408569336 \n",
      "\n",
      "Iteration 7261, Loss: 36.706764221191406, L1: 10.089584350585938, L3: 26.6171817779541\n",
      "Current prediction:  60.669002532958984 \n",
      "\n",
      "Iteration 7262, Loss: 35.47593688964844, L1: 10.141461372375488, L3: 25.334474563598633\n",
      "Current prediction:  60.67693328857422 \n",
      "\n",
      "Iteration 7263, Loss: 36.41636657714844, L1: 10.144012451171875, L3: 26.272356033325195\n",
      "Current prediction:  60.69462203979492 \n",
      "\n",
      "Iteration 7264, Loss: 35.59019470214844, L1: 10.086966514587402, L3: 25.50322723388672\n",
      "Current prediction:  60.78106689453125 \n",
      "\n",
      "Iteration 7265, Loss: 36.02401351928711, L1: 10.039080619812012, L3: 25.984933853149414\n",
      "Current prediction:  61.47378158569336 \n",
      "\n",
      "Iteration 7266, Loss: 36.33927917480469, L1: 9.95098876953125, L3: 26.388290405273438\n",
      "Current prediction:  61.587650299072266 \n",
      "\n",
      "Iteration 7267, Loss: 35.450313568115234, L1: 9.978076934814453, L3: 25.47223663330078\n",
      "Current prediction:  61.597816467285156 \n",
      "\n",
      "Iteration 7268, Loss: 37.1773681640625, L1: 10.017428398132324, L3: 27.159940719604492\n",
      "Current prediction:  61.59599304199219 \n",
      "\n",
      "Iteration 7269, Loss: 35.2285270690918, L1: 9.993111610412598, L3: 25.235416412353516\n",
      "Current prediction:  61.59202194213867 \n",
      "\n",
      "Iteration 7270, Loss: 36.06127166748047, L1: 10.002605438232422, L3: 26.05866813659668\n",
      "Current prediction:  61.58380889892578 \n",
      "\n",
      "Iteration 7271, Loss: 35.55898666381836, L1: 9.988358497619629, L3: 25.570627212524414\n",
      "Current prediction:  61.56758117675781 \n",
      "\n",
      "Iteration 7272, Loss: 35.9776725769043, L1: 9.985888481140137, L3: 25.991785049438477\n",
      "Current prediction:  61.13931655883789 \n",
      "\n",
      "Iteration 7273, Loss: 35.518157958984375, L1: 9.96219539642334, L3: 25.55596160888672\n",
      "Current prediction:  60.72052764892578 \n",
      "\n",
      "Iteration 7274, Loss: 36.553443908691406, L1: 10.017810821533203, L3: 26.535634994506836\n",
      "Current prediction:  60.6989631652832 \n",
      "\n",
      "Iteration 7275, Loss: 36.049476623535156, L1: 10.10823917388916, L3: 25.941238403320312\n",
      "Current prediction:  60.695640563964844 \n",
      "\n",
      "Iteration 7276, Loss: 35.16866683959961, L1: 10.111062049865723, L3: 25.057605743408203\n",
      "Current prediction:  60.70196533203125 \n",
      "\n",
      "Iteration 7277, Loss: 35.695579528808594, L1: 10.093538284301758, L3: 25.602041244506836\n",
      "Current prediction:  60.727088928222656 \n",
      "\n",
      "Iteration 7278, Loss: 36.69813919067383, L1: 10.030268669128418, L3: 26.667869567871094\n",
      "Current prediction:  61.036617279052734 \n",
      "\n",
      "Iteration 7279, Loss: 36.60315704345703, L1: 9.937795639038086, L3: 26.665363311767578\n",
      "Current prediction:  61.4957160949707 \n",
      "\n",
      "Iteration 7280, Loss: 35.83350372314453, L1: 9.98819351196289, L3: 25.84531021118164\n",
      "Current prediction:  61.56135177612305 \n",
      "\n",
      "Iteration 7281, Loss: 38.12120056152344, L1: 9.988587379455566, L3: 28.132612228393555\n",
      "Current prediction:  61.5770378112793 \n",
      "\n",
      "Iteration 7282, Loss: 37.008358001708984, L1: 9.96274185180664, L3: 27.045616149902344\n",
      "Current prediction:  61.477901458740234 \n",
      "\n",
      "Iteration 7283, Loss: 35.908119201660156, L1: 9.963595390319824, L3: 25.94452476501465\n",
      "Current prediction:  61.22800827026367 \n",
      "\n",
      "Iteration 7284, Loss: 36.183494567871094, L1: 9.939949989318848, L3: 26.24354362487793\n",
      "Current prediction:  61.1557502746582 \n",
      "\n",
      "Iteration 7285, Loss: 35.990516662597656, L1: 9.972925186157227, L3: 26.01759147644043\n",
      "Current prediction:  61.29266357421875 \n",
      "\n",
      "Iteration 7286, Loss: 35.899505615234375, L1: 9.972372055053711, L3: 25.92713165283203\n",
      "Current prediction:  61.41093063354492 \n",
      "\n",
      "Iteration 7287, Loss: 37.36643600463867, L1: 10.03729248046875, L3: 27.329143524169922\n",
      "Current prediction:  61.46494674682617 \n",
      "\n",
      "Iteration 7288, Loss: 35.222381591796875, L1: 9.98022174835205, L3: 25.24216079711914\n",
      "Current prediction:  61.280452728271484 \n",
      "\n",
      "Iteration 7289, Loss: 36.303531646728516, L1: 9.956437110900879, L3: 26.34709358215332\n",
      "Current prediction:  61.29306411743164 \n",
      "\n",
      "Iteration 7290, Loss: 36.61793518066406, L1: 9.985611915588379, L3: 26.63232421875\n",
      "Current prediction:  61.02988815307617 \n",
      "\n",
      "Iteration 7291, Loss: 35.86457443237305, L1: 10.000008583068848, L3: 25.864564895629883\n",
      "Current prediction:  60.75723648071289 \n",
      "\n",
      "Iteration 7292, Loss: 36.2542724609375, L1: 9.960969924926758, L3: 26.29330062866211\n",
      "Current prediction:  60.747955322265625 \n",
      "\n",
      "Iteration 7293, Loss: 36.29838180541992, L1: 10.061301231384277, L3: 26.23708152770996\n",
      "Current prediction:  60.76108169555664 \n",
      "\n",
      "Iteration 7294, Loss: 35.96721649169922, L1: 9.973589897155762, L3: 25.99362564086914\n",
      "Current prediction:  61.14909744262695 \n",
      "\n",
      "Iteration 7295, Loss: 35.953575134277344, L1: 9.985799789428711, L3: 25.967777252197266\n",
      "Current prediction:  61.62262725830078 \n",
      "\n",
      "Iteration 7296, Loss: 35.91648483276367, L1: 9.95776081085205, L3: 25.958723068237305\n",
      "Current prediction:  61.642303466796875 \n",
      "\n",
      "Iteration 7297, Loss: 36.54345703125, L1: 9.968465805053711, L3: 26.57499122619629\n",
      "Current prediction:  61.64676284790039 \n",
      "\n",
      "Iteration 7298, Loss: 36.08233642578125, L1: 9.983272552490234, L3: 26.099063873291016\n",
      "Current prediction:  61.644710540771484 \n",
      "\n",
      "Iteration 7299, Loss: 36.08140182495117, L1: 10.005398750305176, L3: 26.07600212097168\n",
      "Current prediction:  61.6324577331543 \n",
      "\n",
      "Iteration 7300, Loss: 36.96508026123047, L1: 10.015745162963867, L3: 26.949337005615234\n",
      "Current prediction:  61.613101959228516 \n",
      "\n",
      "Iteration 7301, Loss: 36.54235076904297, L1: 10.003316879272461, L3: 26.539031982421875\n",
      "Current prediction:  61.59248352050781 \n",
      "\n",
      "Iteration 7302, Loss: 36.34991455078125, L1: 10.013457298278809, L3: 26.336456298828125\n",
      "Current prediction:  61.55449295043945 \n",
      "\n",
      "Iteration 7303, Loss: 35.73398208618164, L1: 10.021201133728027, L3: 25.71278190612793\n",
      "Current prediction:  60.900489807128906 \n",
      "\n",
      "Iteration 7304, Loss: 35.507911682128906, L1: 9.998079299926758, L3: 25.509830474853516\n",
      "Current prediction:  60.68323516845703 \n",
      "\n",
      "Iteration 7305, Loss: 36.66242218017578, L1: 10.088591575622559, L3: 26.57383155822754\n",
      "Current prediction:  60.65751647949219 \n",
      "\n",
      "Iteration 7306, Loss: 35.3797607421875, L1: 10.13053035736084, L3: 25.249229431152344\n",
      "Current prediction:  60.64924240112305 \n",
      "\n",
      "Iteration 7307, Loss: 36.115882873535156, L1: 10.15502643585205, L3: 25.960857391357422\n",
      "Current prediction:  60.64936065673828 \n",
      "\n",
      "Iteration 7308, Loss: 37.000850677490234, L1: 10.10399055480957, L3: 26.896860122680664\n",
      "Current prediction:  60.649330139160156 \n",
      "\n",
      "Iteration 7309, Loss: 37.014495849609375, L1: 10.081258773803711, L3: 26.933237075805664\n",
      "Current prediction:  60.64651870727539 \n",
      "\n",
      "Iteration 7310, Loss: 35.637939453125, L1: 10.170318603515625, L3: 25.467618942260742\n",
      "Current prediction:  60.654319763183594 \n",
      "\n",
      "Iteration 7311, Loss: 36.89363098144531, L1: 10.089755058288574, L3: 26.803874969482422\n",
      "Current prediction:  60.70237731933594 \n",
      "\n",
      "Iteration 7312, Loss: 35.62186050415039, L1: 10.026001930236816, L3: 25.59585952758789\n",
      "Current prediction:  61.4170036315918 \n",
      "\n",
      "Iteration 7313, Loss: 36.078086853027344, L1: 9.981090545654297, L3: 26.09699821472168\n",
      "Current prediction:  61.54985427856445 \n",
      "\n",
      "Iteration 7314, Loss: 36.5726318359375, L1: 10.02053451538086, L3: 26.552099227905273\n",
      "Current prediction:  61.559593200683594 \n",
      "\n",
      "Iteration 7315, Loss: 36.156558990478516, L1: 9.999058723449707, L3: 26.157499313354492\n",
      "Current prediction:  61.568214416503906 \n",
      "\n",
      "Iteration 7316, Loss: 37.21205520629883, L1: 10.03364086151123, L3: 27.178415298461914\n",
      "Current prediction:  61.56758117675781 \n",
      "\n",
      "Iteration 7317, Loss: 37.04816436767578, L1: 10.034740447998047, L3: 27.013425827026367\n",
      "Current prediction:  61.559749603271484 \n",
      "\n",
      "Iteration 7318, Loss: 36.28579330444336, L1: 9.97869873046875, L3: 26.30709457397461\n",
      "Current prediction:  61.52228927612305 \n",
      "\n",
      "Iteration 7319, Loss: 34.94114303588867, L1: 9.956270217895508, L3: 24.984872817993164\n",
      "Current prediction:  61.161590576171875 \n",
      "\n",
      "Iteration 7320, Loss: 36.3838005065918, L1: 10.004677772521973, L3: 26.379121780395508\n",
      "Current prediction:  60.76346206665039 \n",
      "\n",
      "Iteration 7321, Loss: 36.35710144042969, L1: 10.02664566040039, L3: 26.330455780029297\n",
      "Current prediction:  60.687652587890625 \n",
      "\n",
      "Iteration 7322, Loss: 35.8588752746582, L1: 10.075066566467285, L3: 25.783809661865234\n",
      "Current prediction:  60.686439514160156 \n",
      "\n",
      "Iteration 7323, Loss: 35.67411422729492, L1: 10.108704566955566, L3: 25.565410614013672\n",
      "Current prediction:  60.69469451904297 \n",
      "\n",
      "Iteration 7324, Loss: 36.112510681152344, L1: 10.105484008789062, L3: 26.00702476501465\n",
      "Current prediction:  60.70552444458008 \n",
      "\n",
      "Iteration 7325, Loss: 35.2900390625, L1: 10.057014465332031, L3: 25.2330265045166\n",
      "Current prediction:  60.74018859863281 \n",
      "\n",
      "Iteration 7326, Loss: 37.0047721862793, L1: 10.039684295654297, L3: 26.965087890625\n",
      "Current prediction:  61.15986633300781 \n",
      "\n",
      "Iteration 7327, Loss: 36.09135055541992, L1: 9.983870506286621, L3: 26.107479095458984\n",
      "Current prediction:  61.55263900756836 \n",
      "\n",
      "Iteration 7328, Loss: 36.66913986206055, L1: 9.917102813720703, L3: 26.752037048339844\n",
      "Current prediction:  61.59215545654297 \n",
      "\n",
      "Iteration 7329, Loss: 36.81642150878906, L1: 9.927517890930176, L3: 26.88890266418457\n",
      "Current prediction:  61.58635330200195 \n",
      "\n",
      "Iteration 7330, Loss: 36.50754165649414, L1: 9.932516098022461, L3: 26.57502555847168\n",
      "Current prediction:  61.50244140625 \n",
      "\n",
      "Iteration 7331, Loss: 35.511940002441406, L1: 9.952770233154297, L3: 25.559167861938477\n",
      "Current prediction:  61.05665588378906 \n",
      "\n",
      "Iteration 7332, Loss: 37.048770904541016, L1: 9.955391883850098, L3: 27.0933780670166\n",
      "Current prediction:  60.78272247314453 \n",
      "\n",
      "Iteration 7333, Loss: 36.77522277832031, L1: 10.020872116088867, L3: 26.754350662231445\n",
      "Current prediction:  60.72831344604492 \n",
      "\n",
      "Iteration 7334, Loss: 35.95451736450195, L1: 10.038439750671387, L3: 25.916078567504883\n",
      "Current prediction:  60.715721130371094 \n",
      "\n",
      "Iteration 7335, Loss: 36.23588562011719, L1: 9.991670608520508, L3: 26.244213104248047\n",
      "Current prediction:  60.93015670776367 \n",
      "\n",
      "Iteration 7336, Loss: 36.28087615966797, L1: 9.965628623962402, L3: 26.31524658203125\n",
      "Current prediction:  61.54183578491211 \n",
      "\n",
      "Iteration 7337, Loss: 35.83417892456055, L1: 9.982135772705078, L3: 25.85204315185547\n",
      "Current prediction:  61.55842208862305 \n",
      "\n",
      "Iteration 7338, Loss: 37.209083557128906, L1: 10.047669410705566, L3: 27.161413192749023\n",
      "Current prediction:  61.548133850097656 \n",
      "\n",
      "Iteration 7339, Loss: 36.35584259033203, L1: 10.067666053771973, L3: 26.288177490234375\n",
      "Current prediction:  61.535526275634766 \n",
      "\n",
      "Iteration 7340, Loss: 36.70356750488281, L1: 10.061711311340332, L3: 26.641857147216797\n",
      "Current prediction:  61.521644592285156 \n",
      "\n",
      "Iteration 7341, Loss: 35.59397506713867, L1: 10.086419105529785, L3: 25.50755500793457\n",
      "Current prediction:  61.51009750366211 \n",
      "\n",
      "Iteration 7342, Loss: 37.08094024658203, L1: 10.088701248168945, L3: 26.99224090576172\n",
      "Current prediction:  61.4951286315918 \n",
      "\n",
      "Iteration 7343, Loss: 35.46987533569336, L1: 10.11853313446045, L3: 25.351341247558594\n",
      "Current prediction:  61.48250198364258 \n",
      "\n",
      "Iteration 7344, Loss: 36.39433288574219, L1: 10.09334945678711, L3: 26.300981521606445\n",
      "Current prediction:  61.46974182128906 \n",
      "\n",
      "Iteration 7345, Loss: 36.085350036621094, L1: 10.09168815612793, L3: 25.99365997314453\n",
      "Current prediction:  61.451271057128906 \n",
      "\n",
      "Iteration 7346, Loss: 36.39923095703125, L1: 10.081988334655762, L3: 26.317243576049805\n",
      "Current prediction:  61.26298522949219 \n",
      "\n",
      "Iteration 7347, Loss: 35.58109664916992, L1: 10.0114107131958, L3: 25.569684982299805\n",
      "Current prediction:  60.60081100463867 \n",
      "\n",
      "Iteration 7348, Loss: 37.10806655883789, L1: 10.121459007263184, L3: 26.98660659790039\n",
      "Current prediction:  60.58409118652344 \n",
      "\n",
      "Iteration 7349, Loss: 37.20528793334961, L1: 10.238594055175781, L3: 26.966693878173828\n",
      "Current prediction:  60.587425231933594 \n",
      "\n",
      "Iteration 7350, Loss: 36.87153625488281, L1: 10.199758529663086, L3: 26.671777725219727\n",
      "Current prediction:  60.598506927490234 \n",
      "\n",
      "Iteration 7351, Loss: 36.366539001464844, L1: 10.210010528564453, L3: 26.156530380249023\n",
      "Current prediction:  60.61395263671875 \n",
      "\n",
      "Iteration 7352, Loss: 37.213626861572266, L1: 10.197781562805176, L3: 27.015844345092773\n",
      "Current prediction:  60.638248443603516 \n",
      "\n",
      "Iteration 7353, Loss: 36.485870361328125, L1: 10.143781661987305, L3: 26.342090606689453\n",
      "Current prediction:  60.99721908569336 \n",
      "\n",
      "Iteration 7354, Loss: 35.621299743652344, L1: 9.974732398986816, L3: 25.646568298339844\n",
      "Current prediction:  61.54606246948242 \n",
      "\n",
      "Iteration 7355, Loss: 36.39116668701172, L1: 10.020267486572266, L3: 26.370899200439453\n",
      "Current prediction:  61.57376480102539 \n",
      "\n",
      "Iteration 7356, Loss: 35.76673889160156, L1: 10.045878410339355, L3: 25.720861434936523\n",
      "Current prediction:  61.589599609375 \n",
      "\n",
      "Iteration 7357, Loss: 35.993892669677734, L1: 10.067302703857422, L3: 25.926589965820312\n",
      "Current prediction:  61.59651565551758 \n",
      "\n",
      "Iteration 7358, Loss: 36.203304290771484, L1: 10.040790557861328, L3: 26.162513732910156\n",
      "Current prediction:  61.594703674316406 \n",
      "\n",
      "Iteration 7359, Loss: 37.00904083251953, L1: 10.099227905273438, L3: 26.909812927246094\n",
      "Current prediction:  61.58857727050781 \n",
      "\n",
      "Iteration 7360, Loss: 35.739009857177734, L1: 10.115594863891602, L3: 25.623414993286133\n",
      "Current prediction:  61.57915496826172 \n",
      "\n",
      "Iteration 7361, Loss: 35.451148986816406, L1: 10.130276679992676, L3: 25.320871353149414\n",
      "Current prediction:  61.563209533691406 \n",
      "\n",
      "Iteration 7362, Loss: 36.25953674316406, L1: 10.19909381866455, L3: 26.060441970825195\n",
      "Current prediction:  61.537315368652344 \n",
      "\n",
      "Iteration 7363, Loss: 36.26486587524414, L1: 10.196885108947754, L3: 26.06797981262207\n",
      "Current prediction:  61.506656646728516 \n",
      "\n",
      "Iteration 7364, Loss: 36.29880142211914, L1: 10.197416305541992, L3: 26.10138511657715\n",
      "Current prediction:  61.47246551513672 \n",
      "\n",
      "Iteration 7365, Loss: 36.07645034790039, L1: 10.227710723876953, L3: 25.848739624023438\n",
      "Current prediction:  61.436519622802734 \n",
      "\n",
      "Iteration 7366, Loss: 36.733890533447266, L1: 10.229498863220215, L3: 26.504390716552734\n",
      "Current prediction:  61.400630950927734 \n",
      "\n",
      "Iteration 7367, Loss: 36.34814453125, L1: 10.26032829284668, L3: 26.08781623840332\n",
      "Current prediction:  61.3663330078125 \n",
      "\n",
      "Iteration 7368, Loss: 36.817115783691406, L1: 10.261539459228516, L3: 26.555574417114258\n",
      "Current prediction:  61.33899688720703 \n",
      "\n",
      "Iteration 7369, Loss: 35.829551696777344, L1: 10.213691711425781, L3: 25.615859985351562\n",
      "Current prediction:  61.31086349487305 \n",
      "\n",
      "Iteration 7370, Loss: 36.743614196777344, L1: 10.162666320800781, L3: 26.580949783325195\n",
      "Current prediction:  60.50827407836914 \n",
      "\n",
      "Iteration 7371, Loss: 36.17725372314453, L1: 10.198116302490234, L3: 25.979135513305664\n",
      "Current prediction:  60.441749572753906 \n",
      "\n",
      "Iteration 7372, Loss: 36.845577239990234, L1: 10.29386043548584, L3: 26.551715850830078\n",
      "Current prediction:  60.46157455444336 \n",
      "\n",
      "Iteration 7373, Loss: 35.82057571411133, L1: 10.37943172454834, L3: 25.441144943237305\n",
      "Current prediction:  60.49341583251953 \n",
      "\n",
      "Iteration 7374, Loss: 36.41392135620117, L1: 10.373333930969238, L3: 26.040586471557617\n",
      "Current prediction:  60.53677749633789 \n",
      "\n",
      "Iteration 7375, Loss: 36.582435607910156, L1: 10.348061561584473, L3: 26.234375\n",
      "Current prediction:  60.58956527709961 \n",
      "\n",
      "Iteration 7376, Loss: 36.264244079589844, L1: 10.247974395751953, L3: 26.01626968383789\n",
      "Current prediction:  60.65807342529297 \n",
      "\n",
      "Iteration 7377, Loss: 36.3468132019043, L1: 10.059459686279297, L3: 26.287353515625\n",
      "Current prediction:  61.207035064697266 \n",
      "\n",
      "Iteration 7378, Loss: 36.22581481933594, L1: 9.979826927185059, L3: 26.245986938476562\n",
      "Current prediction:  61.59572219848633 \n",
      "\n",
      "Iteration 7379, Loss: 37.580482482910156, L1: 9.963762283325195, L3: 27.616718292236328\n",
      "Current prediction:  61.6510124206543 \n",
      "\n",
      "Iteration 7380, Loss: 36.56568145751953, L1: 9.921090126037598, L3: 26.644590377807617\n",
      "Current prediction:  61.6727180480957 \n",
      "\n",
      "Iteration 7381, Loss: 36.24676513671875, L1: 9.870878219604492, L3: 26.375886917114258\n",
      "Current prediction:  61.67681884765625 \n",
      "\n",
      "Iteration 7382, Loss: 36.50724792480469, L1: 9.871600151062012, L3: 26.63564682006836\n",
      "Current prediction:  61.48820495605469 \n",
      "\n",
      "Iteration 7383, Loss: 36.2853889465332, L1: 9.852740287780762, L3: 26.432649612426758\n",
      "Current prediction:  60.91477966308594 \n",
      "\n",
      "Iteration 7384, Loss: 37.53530502319336, L1: 9.901347160339355, L3: 27.633956909179688\n",
      "Current prediction:  60.818973541259766 \n",
      "\n",
      "Iteration 7385, Loss: 35.537628173828125, L1: 9.907440185546875, L3: 25.630189895629883\n",
      "Current prediction:  60.807830810546875 \n",
      "\n",
      "Iteration 7386, Loss: 37.03805923461914, L1: 9.965459823608398, L3: 27.072599411010742\n",
      "Current prediction:  60.7973747253418 \n",
      "\n",
      "Iteration 7387, Loss: 36.62870788574219, L1: 10.035344123840332, L3: 26.59336280822754\n",
      "Current prediction:  60.790950775146484 \n",
      "\n",
      "Iteration 7388, Loss: 35.36756134033203, L1: 9.97684383392334, L3: 25.390716552734375\n",
      "Current prediction:  60.78954315185547 \n",
      "\n",
      "Iteration 7389, Loss: 36.47634506225586, L1: 9.960317611694336, L3: 26.516027450561523\n",
      "Current prediction:  60.8929328918457 \n",
      "\n",
      "Iteration 7390, Loss: 35.86228561401367, L1: 9.886643409729004, L3: 25.975643157958984\n",
      "Current prediction:  61.4118766784668 \n",
      "\n",
      "Iteration 7391, Loss: 37.334930419921875, L1: 9.914691925048828, L3: 27.42024040222168\n",
      "Current prediction:  61.566219329833984 \n",
      "\n",
      "Iteration 7392, Loss: 35.08576965332031, L1: 9.913084030151367, L3: 25.172683715820312\n",
      "Current prediction:  61.55128860473633 \n",
      "\n",
      "Iteration 7393, Loss: 35.597042083740234, L1: 9.948410987854004, L3: 25.648632049560547\n",
      "Current prediction:  61.547828674316406 \n",
      "\n",
      "Iteration 7394, Loss: 36.71078109741211, L1: 9.958381652832031, L3: 26.752399444580078\n",
      "Current prediction:  61.51006317138672 \n",
      "\n",
      "Iteration 7395, Loss: 35.60380554199219, L1: 9.966401100158691, L3: 25.63740348815918\n",
      "Current prediction:  61.37614059448242 \n",
      "\n",
      "Iteration 7396, Loss: 37.02500915527344, L1: 9.970766067504883, L3: 27.054243087768555\n",
      "Current prediction:  61.130741119384766 \n",
      "\n",
      "Iteration 7397, Loss: 36.494625091552734, L1: 9.997117042541504, L3: 26.497509002685547\n",
      "Current prediction:  61.083499908447266 \n",
      "\n",
      "Iteration 7398, Loss: 35.56184768676758, L1: 10.00869083404541, L3: 25.553157806396484\n",
      "Current prediction:  60.859107971191406 \n",
      "\n",
      "Iteration 7399, Loss: 34.78140640258789, L1: 10.049328804016113, L3: 24.73207664489746\n",
      "Current prediction:  60.80024719238281 \n",
      "\n",
      "Iteration 7400, Loss: 36.13819885253906, L1: 10.016411781311035, L3: 26.12178611755371\n",
      "Current prediction:  60.853370666503906 \n",
      "\n",
      "Iteration 7401, Loss: 36.43436050415039, L1: 10.017830848693848, L3: 26.416528701782227\n",
      "Current prediction:  60.89696502685547 \n",
      "\n",
      "Iteration 7402, Loss: 35.91569900512695, L1: 10.044354438781738, L3: 25.8713436126709\n",
      "Current prediction:  60.911102294921875 \n",
      "\n",
      "Iteration 7403, Loss: 35.8033561706543, L1: 9.992852210998535, L3: 25.810503005981445\n",
      "Current prediction:  61.140193939208984 \n",
      "\n",
      "Iteration 7404, Loss: 35.77714538574219, L1: 10.014437675476074, L3: 25.762706756591797\n",
      "Current prediction:  61.29542922973633 \n",
      "\n",
      "Iteration 7405, Loss: 36.46662521362305, L1: 10.064263343811035, L3: 26.402362823486328\n",
      "Current prediction:  61.28327560424805 \n",
      "\n",
      "Iteration 7406, Loss: 35.34764862060547, L1: 10.02507495880127, L3: 25.322572708129883\n",
      "Current prediction:  61.17225646972656 \n",
      "\n",
      "Iteration 7407, Loss: 35.99578857421875, L1: 10.010293960571289, L3: 25.985492706298828\n",
      "Current prediction:  60.854461669921875 \n",
      "\n",
      "Iteration 7408, Loss: 35.77847671508789, L1: 9.999719619750977, L3: 25.778757095336914\n",
      "Current prediction:  60.703094482421875 \n",
      "\n",
      "Iteration 7409, Loss: 36.164283752441406, L1: 10.029351234436035, L3: 26.134931564331055\n",
      "Current prediction:  60.68152618408203 \n",
      "\n",
      "Iteration 7410, Loss: 37.0679931640625, L1: 10.019266128540039, L3: 27.048728942871094\n",
      "Current prediction:  60.70284652709961 \n",
      "\n",
      "Iteration 7411, Loss: 36.82902145385742, L1: 10.008305549621582, L3: 26.820714950561523\n",
      "Current prediction:  60.835636138916016 \n",
      "\n",
      "Iteration 7412, Loss: 36.08747482299805, L1: 9.943800926208496, L3: 26.143672943115234\n",
      "Current prediction:  61.45344543457031 \n",
      "\n",
      "Iteration 7413, Loss: 35.39430618286133, L1: 9.98511791229248, L3: 25.40918731689453\n",
      "Current prediction:  61.56201171875 \n",
      "\n",
      "Iteration 7414, Loss: 36.72472381591797, L1: 9.977505683898926, L3: 26.74721908569336\n",
      "Current prediction:  61.56805419921875 \n",
      "\n",
      "Iteration 7415, Loss: 36.385467529296875, L1: 9.961690902709961, L3: 26.423778533935547\n",
      "Current prediction:  61.564971923828125 \n",
      "\n",
      "Iteration 7416, Loss: 36.146949768066406, L1: 10.004076957702637, L3: 26.142873764038086\n",
      "Current prediction:  61.5642204284668 \n",
      "\n",
      "Iteration 7417, Loss: 36.01314926147461, L1: 10.00559139251709, L3: 26.007558822631836\n",
      "Current prediction:  61.55725860595703 \n",
      "\n",
      "Iteration 7418, Loss: 35.76531219482422, L1: 10.013529777526855, L3: 25.75178337097168\n",
      "Current prediction:  61.51974868774414 \n",
      "\n",
      "Iteration 7419, Loss: 36.705902099609375, L1: 9.985725402832031, L3: 26.72017478942871\n",
      "Current prediction:  61.168949127197266 \n",
      "\n",
      "Iteration 7420, Loss: 36.118324279785156, L1: 9.979814529418945, L3: 26.138511657714844\n",
      "Current prediction:  60.99164962768555 \n",
      "\n",
      "Iteration 7421, Loss: 36.87900924682617, L1: 9.93660831451416, L3: 26.942399978637695\n",
      "Current prediction:  60.76881408691406 \n",
      "\n",
      "Iteration 7422, Loss: 36.9638557434082, L1: 10.018911361694336, L3: 26.944944381713867\n",
      "Current prediction:  60.736202239990234 \n",
      "\n",
      "Iteration 7423, Loss: 35.47343444824219, L1: 10.01839828491211, L3: 25.45503807067871\n",
      "Current prediction:  60.783084869384766 \n",
      "\n",
      "Iteration 7424, Loss: 37.380096435546875, L1: 9.98339557647705, L3: 27.396699905395508\n",
      "Current prediction:  60.95035934448242 \n",
      "\n",
      "Iteration 7425, Loss: 36.95829391479492, L1: 9.975605964660645, L3: 26.98268699645996\n",
      "Current prediction:  61.227943420410156 \n",
      "\n",
      "Iteration 7426, Loss: 35.564300537109375, L1: 9.991718292236328, L3: 25.57258415222168\n",
      "Current prediction:  61.497859954833984 \n",
      "\n",
      "Iteration 7427, Loss: 36.45966339111328, L1: 9.940570831298828, L3: 26.519094467163086\n",
      "Current prediction:  61.436588287353516 \n",
      "\n",
      "Iteration 7428, Loss: 36.49021911621094, L1: 9.936330795288086, L3: 26.55388832092285\n",
      "Current prediction:  61.20388412475586 \n",
      "\n",
      "Iteration 7429, Loss: 36.798255920410156, L1: 9.951322555541992, L3: 26.84693145751953\n",
      "Current prediction:  61.06247329711914 \n",
      "\n",
      "Iteration 7430, Loss: 37.198486328125, L1: 9.9566068649292, L3: 27.241880416870117\n",
      "Current prediction:  60.804359436035156 \n",
      "\n",
      "Iteration 7431, Loss: 36.44965744018555, L1: 9.969220161437988, L3: 26.480438232421875\n",
      "Current prediction:  60.79558563232422 \n",
      "\n",
      "Iteration 7432, Loss: 36.34882354736328, L1: 10.005585670471191, L3: 26.343236923217773\n",
      "Current prediction:  61.140228271484375 \n",
      "\n",
      "Iteration 7433, Loss: 36.76998519897461, L1: 9.9375581741333, L3: 26.832427978515625\n",
      "Current prediction:  61.456207275390625 \n",
      "\n",
      "Iteration 7434, Loss: 36.67604446411133, L1: 9.947770118713379, L3: 26.728275299072266\n",
      "Current prediction:  61.537315368652344 \n",
      "\n",
      "Iteration 7435, Loss: 35.60714340209961, L1: 9.974418640136719, L3: 25.63272476196289\n",
      "Current prediction:  61.53633499145508 \n",
      "\n",
      "Iteration 7436, Loss: 35.28734588623047, L1: 9.976509094238281, L3: 25.31083869934082\n",
      "Current prediction:  61.46037292480469 \n",
      "\n",
      "Iteration 7437, Loss: 37.101478576660156, L1: 9.94494342803955, L3: 27.15653419494629\n",
      "Current prediction:  60.963626861572266 \n",
      "\n",
      "Iteration 7438, Loss: 36.40107727050781, L1: 9.973456382751465, L3: 26.427621841430664\n",
      "Current prediction:  60.740604400634766 \n",
      "\n",
      "Iteration 7439, Loss: 36.770843505859375, L1: 10.018258094787598, L3: 26.75258445739746\n",
      "Current prediction:  60.73143005371094 \n",
      "\n",
      "Iteration 7440, Loss: 36.64702224731445, L1: 10.036373138427734, L3: 26.61064910888672\n",
      "Current prediction:  60.721736907958984 \n",
      "\n",
      "Iteration 7441, Loss: 35.80671691894531, L1: 9.98967170715332, L3: 25.81704330444336\n",
      "Current prediction:  60.733314514160156 \n",
      "\n",
      "Iteration 7442, Loss: 36.0359001159668, L1: 9.954059600830078, L3: 26.08184051513672\n",
      "Current prediction:  60.82801818847656 \n",
      "\n",
      "Iteration 7443, Loss: 35.86859893798828, L1: 9.9452543258667, L3: 25.923343658447266\n",
      "Current prediction:  61.39960479736328 \n",
      "\n",
      "Iteration 7444, Loss: 36.508209228515625, L1: 9.927962303161621, L3: 26.580245971679688\n",
      "Current prediction:  61.510589599609375 \n",
      "\n",
      "Iteration 7445, Loss: 35.82756042480469, L1: 9.869874954223633, L3: 25.957683563232422\n",
      "Current prediction:  61.313880920410156 \n",
      "\n",
      "Iteration 7446, Loss: 36.636993408203125, L1: 9.94190502166748, L3: 26.695087432861328\n",
      "Current prediction:  61.28303909301758 \n",
      "\n",
      "Iteration 7447, Loss: 36.29192352294922, L1: 9.926897048950195, L3: 26.365028381347656\n",
      "Current prediction:  61.3078727722168 \n",
      "\n",
      "Iteration 7448, Loss: 36.25146484375, L1: 9.905983924865723, L3: 26.345481872558594\n",
      "Current prediction:  61.38550567626953 \n",
      "\n",
      "Iteration 7449, Loss: 36.4930419921875, L1: 9.939947128295898, L3: 26.553096771240234\n",
      "Current prediction:  61.24691390991211 \n",
      "\n",
      "Iteration 7450, Loss: 36.92338943481445, L1: 9.919036865234375, L3: 27.004352569580078\n",
      "Current prediction:  61.087093353271484 \n",
      "\n",
      "Iteration 7451, Loss: 35.8670768737793, L1: 10.0069580078125, L3: 25.860118865966797\n",
      "Current prediction:  61.061954498291016 \n",
      "\n",
      "Iteration 7452, Loss: 36.46589279174805, L1: 9.92743968963623, L3: 26.538454055786133\n",
      "Current prediction:  61.097084045410156 \n",
      "\n",
      "Iteration 7453, Loss: 36.94481658935547, L1: 9.973615646362305, L3: 26.971200942993164\n",
      "Current prediction:  61.13227081298828 \n",
      "\n",
      "Iteration 7454, Loss: 36.82658386230469, L1: 10.015826225280762, L3: 26.81075668334961\n",
      "Current prediction:  61.198734283447266 \n",
      "\n",
      "Iteration 7455, Loss: 36.234031677246094, L1: 9.983541488647461, L3: 26.25048828125\n",
      "Current prediction:  61.226478576660156 \n",
      "\n",
      "Iteration 7456, Loss: 36.23910140991211, L1: 9.99796199798584, L3: 26.241140365600586\n",
      "Current prediction:  61.22275924682617 \n",
      "\n",
      "Iteration 7457, Loss: 35.746150970458984, L1: 10.038797378540039, L3: 25.707353591918945\n",
      "Current prediction:  61.31519317626953 \n",
      "\n",
      "Iteration 7458, Loss: 36.37504196166992, L1: 10.030011177062988, L3: 26.34503173828125\n",
      "Current prediction:  61.37371826171875 \n",
      "\n",
      "Iteration 7459, Loss: 36.3121337890625, L1: 10.019133567810059, L3: 26.292999267578125\n",
      "Current prediction:  61.428409576416016 \n",
      "\n",
      "Iteration 7460, Loss: 36.579227447509766, L1: 10.007786750793457, L3: 26.571439743041992\n",
      "Current prediction:  61.41754150390625 \n",
      "\n",
      "Iteration 7461, Loss: 36.63284683227539, L1: 10.054154396057129, L3: 26.578693389892578\n",
      "Current prediction:  61.39910888671875 \n",
      "\n",
      "Iteration 7462, Loss: 35.76134490966797, L1: 10.052875518798828, L3: 25.70846939086914\n",
      "Current prediction:  61.32170104980469 \n",
      "\n",
      "Iteration 7463, Loss: 36.02997589111328, L1: 10.037042617797852, L3: 25.992931365966797\n",
      "Current prediction:  61.40144348144531 \n",
      "\n",
      "Iteration 7464, Loss: 35.71202087402344, L1: 10.010083198547363, L3: 25.70193862915039\n",
      "Current prediction:  61.45579147338867 \n",
      "\n",
      "Iteration 7465, Loss: 35.56202697753906, L1: 9.947877883911133, L3: 25.61414909362793\n",
      "Current prediction:  61.479026794433594 \n",
      "\n",
      "Iteration 7466, Loss: 36.30419921875, L1: 10.011390686035156, L3: 26.29280662536621\n",
      "Current prediction:  61.49568176269531 \n",
      "\n",
      "Iteration 7467, Loss: 36.327117919921875, L1: 9.97863483428955, L3: 26.34848403930664\n",
      "Current prediction:  61.437870025634766 \n",
      "\n",
      "Iteration 7468, Loss: 35.82090377807617, L1: 9.956121444702148, L3: 25.864782333374023\n",
      "Current prediction:  61.50598907470703 \n",
      "\n",
      "Iteration 7469, Loss: 35.817108154296875, L1: 9.936866760253906, L3: 25.8802433013916\n",
      "Current prediction:  61.50872802734375 \n",
      "\n",
      "Iteration 7470, Loss: 35.989418029785156, L1: 9.95061206817627, L3: 26.038806915283203\n",
      "Current prediction:  61.47602081298828 \n",
      "\n",
      "Iteration 7471, Loss: 36.169490814208984, L1: 9.943791389465332, L3: 26.22570037841797\n",
      "Current prediction:  61.3294563293457 \n",
      "\n",
      "Iteration 7472, Loss: 36.78156280517578, L1: 9.882621765136719, L3: 26.898942947387695\n",
      "Current prediction:  61.15772247314453 \n",
      "\n",
      "Iteration 7473, Loss: 35.88129425048828, L1: 9.897346496582031, L3: 25.98394775390625\n",
      "Current prediction:  61.10187530517578 \n",
      "\n",
      "Iteration 7474, Loss: 36.2301139831543, L1: 9.915513038635254, L3: 26.314599990844727\n",
      "Current prediction:  61.08080291748047 \n",
      "\n",
      "Iteration 7475, Loss: 36.71367263793945, L1: 9.903204917907715, L3: 26.810468673706055\n",
      "Current prediction:  60.84656524658203 \n",
      "\n",
      "Iteration 7476, Loss: 36.22406768798828, L1: 9.935980796813965, L3: 26.288087844848633\n",
      "Current prediction:  61.01457595825195 \n",
      "\n",
      "Iteration 7477, Loss: 36.82186508178711, L1: 9.908024787902832, L3: 26.913841247558594\n",
      "Current prediction:  61.37368392944336 \n",
      "\n",
      "Iteration 7478, Loss: 35.355106353759766, L1: 9.928669929504395, L3: 25.426437377929688\n",
      "Current prediction:  61.55072784423828 \n",
      "\n",
      "Iteration 7479, Loss: 36.1203727722168, L1: 9.931548118591309, L3: 26.188825607299805\n",
      "Current prediction:  61.592708587646484 \n",
      "\n",
      "Iteration 7480, Loss: 36.086341857910156, L1: 9.852912902832031, L3: 26.233430862426758\n",
      "Current prediction:  61.59695816040039 \n",
      "\n",
      "Iteration 7481, Loss: 35.86273193359375, L1: 9.94139575958252, L3: 25.921337127685547\n",
      "Current prediction:  61.59355926513672 \n",
      "\n",
      "Iteration 7482, Loss: 36.92170715332031, L1: 9.934203147888184, L3: 26.987503051757812\n",
      "Current prediction:  61.54226303100586 \n",
      "\n",
      "Iteration 7483, Loss: 36.58766174316406, L1: 9.930156707763672, L3: 26.657506942749023\n",
      "Current prediction:  61.024539947509766 \n",
      "\n",
      "Iteration 7484, Loss: 35.88640213012695, L1: 9.937627792358398, L3: 25.948774337768555\n",
      "Current prediction:  60.72429656982422 \n",
      "\n",
      "Iteration 7485, Loss: 35.97377014160156, L1: 10.013587951660156, L3: 25.960180282592773\n",
      "Current prediction:  60.704185485839844 \n",
      "\n",
      "Iteration 7486, Loss: 36.24735641479492, L1: 10.0166654586792, L3: 26.230690002441406\n",
      "Current prediction:  60.70453643798828 \n",
      "\n",
      "Iteration 7487, Loss: 35.63813781738281, L1: 10.053359031677246, L3: 25.58477783203125\n",
      "Current prediction:  60.69862747192383 \n",
      "\n",
      "Iteration 7488, Loss: 36.514320373535156, L1: 10.035202026367188, L3: 26.47911834716797\n",
      "Current prediction:  60.70082473754883 \n",
      "\n",
      "Iteration 7489, Loss: 36.87432098388672, L1: 10.067825317382812, L3: 26.806493759155273\n",
      "Current prediction:  60.70811462402344 \n",
      "\n",
      "Iteration 7490, Loss: 36.854896545410156, L1: 10.042478561401367, L3: 26.81241798400879\n",
      "Current prediction:  60.7705078125 \n",
      "\n",
      "Iteration 7491, Loss: 36.04338455200195, L1: 9.964852333068848, L3: 26.078533172607422\n",
      "Current prediction:  61.36334991455078 \n",
      "\n",
      "Iteration 7492, Loss: 36.37240982055664, L1: 10.197050094604492, L3: 26.17535972595215\n",
      "Current prediction:  61.598785400390625 \n",
      "\n",
      "Iteration 7493, Loss: 36.21540451049805, L1: 9.926180839538574, L3: 26.28922462463379\n",
      "Current prediction:  61.61652755737305 \n",
      "\n",
      "Iteration 7494, Loss: 35.471134185791016, L1: 9.915075302124023, L3: 25.556058883666992\n",
      "Current prediction:  61.61854553222656 \n",
      "\n",
      "Iteration 7495, Loss: 37.16272735595703, L1: 9.945161819458008, L3: 27.217565536499023\n",
      "Current prediction:  61.613502502441406 \n",
      "\n",
      "Iteration 7496, Loss: 35.6100959777832, L1: 9.94431209564209, L3: 25.66578483581543\n",
      "Current prediction:  61.6035041809082 \n",
      "\n",
      "Iteration 7497, Loss: 37.49830627441406, L1: 9.947397232055664, L3: 27.5509090423584\n",
      "Current prediction:  61.523216247558594 \n",
      "\n",
      "Iteration 7498, Loss: 37.17156219482422, L1: 9.917945861816406, L3: 27.253616333007812\n",
      "Current prediction:  61.0554313659668 \n",
      "\n",
      "Iteration 7499, Loss: 35.4932975769043, L1: 9.925802230834961, L3: 25.567495346069336\n",
      "Current prediction:  60.744083404541016 \n",
      "\n",
      "Iteration 7500, Loss: 36.392337799072266, L1: 9.942863464355469, L3: 26.449474334716797\n",
      "Current prediction:  60.67313766479492 \n",
      "\n",
      "Iteration 7501, Loss: 35.73633575439453, L1: 10.009465217590332, L3: 25.726869583129883\n",
      "Current prediction:  60.665863037109375 \n",
      "\n",
      "Iteration 7502, Loss: 37.07754898071289, L1: 10.022994041442871, L3: 27.054553985595703\n",
      "Current prediction:  60.667083740234375 \n",
      "\n",
      "Iteration 7503, Loss: 35.94939422607422, L1: 10.055231094360352, L3: 25.894163131713867\n",
      "Current prediction:  60.67335891723633 \n",
      "\n",
      "Iteration 7504, Loss: 36.12847137451172, L1: 10.05052375793457, L3: 26.07794761657715\n",
      "Current prediction:  60.69534683227539 \n",
      "\n",
      "Iteration 7505, Loss: 35.76866912841797, L1: 9.957696914672852, L3: 25.81097412109375\n",
      "Current prediction:  60.925785064697266 \n",
      "\n",
      "Iteration 7506, Loss: 36.374752044677734, L1: 9.926998138427734, L3: 26.44775390625\n",
      "Current prediction:  61.26267623901367 \n",
      "\n",
      "Iteration 7507, Loss: 35.128089904785156, L1: 9.945840835571289, L3: 25.1822509765625\n",
      "Current prediction:  61.26063537597656 \n",
      "\n",
      "Iteration 7508, Loss: 35.445858001708984, L1: 9.878390312194824, L3: 25.567466735839844\n",
      "Current prediction:  61.06354522705078 \n",
      "\n",
      "Iteration 7509, Loss: 35.933258056640625, L1: 9.89579963684082, L3: 26.037460327148438\n",
      "Current prediction:  61.11190414428711 \n",
      "\n",
      "Iteration 7510, Loss: 35.06523132324219, L1: 9.923072814941406, L3: 25.14215850830078\n",
      "Current prediction:  61.04361343383789 \n",
      "\n",
      "Iteration 7511, Loss: 36.98582458496094, L1: 9.900700569152832, L3: 27.085124969482422\n",
      "Current prediction:  61.299659729003906 \n",
      "\n",
      "Iteration 7512, Loss: 35.56023406982422, L1: 9.886423110961914, L3: 25.673810958862305\n",
      "Current prediction:  61.528587341308594 \n",
      "\n",
      "Iteration 7513, Loss: 35.79948806762695, L1: 9.873856544494629, L3: 25.925630569458008\n",
      "Current prediction:  61.62080383300781 \n",
      "\n",
      "Iteration 7514, Loss: 35.964080810546875, L1: 9.868778228759766, L3: 26.095304489135742\n",
      "Current prediction:  61.62069320678711 \n",
      "\n",
      "Iteration 7515, Loss: 36.34988021850586, L1: 9.906350135803223, L3: 26.443531036376953\n",
      "Current prediction:  61.60490417480469 \n",
      "\n",
      "Iteration 7516, Loss: 35.73768615722656, L1: 9.921204566955566, L3: 25.81648063659668\n",
      "Current prediction:  61.58549880981445 \n",
      "\n",
      "Iteration 7517, Loss: 35.87295150756836, L1: 9.933000564575195, L3: 25.939950942993164\n",
      "Current prediction:  61.495567321777344 \n",
      "\n",
      "Iteration 7518, Loss: 36.13926696777344, L1: 9.928762435913086, L3: 26.21050453186035\n",
      "Current prediction:  60.72661209106445 \n",
      "\n",
      "Iteration 7519, Loss: 35.55408477783203, L1: 9.988470077514648, L3: 25.565614700317383\n",
      "Current prediction:  60.65385437011719 \n",
      "\n",
      "Iteration 7520, Loss: 36.65628433227539, L1: 10.09791088104248, L3: 26.558372497558594\n",
      "Current prediction:  60.64396667480469 \n",
      "\n",
      "Iteration 7521, Loss: 36.31021499633789, L1: 10.196200370788574, L3: 26.114015579223633\n",
      "Current prediction:  60.64462661743164 \n",
      "\n",
      "Iteration 7522, Loss: 36.153968811035156, L1: 10.243671417236328, L3: 25.910295486450195\n",
      "Current prediction:  60.65669631958008 \n",
      "\n",
      "Iteration 7523, Loss: 36.048892974853516, L1: 10.202519416809082, L3: 25.84637451171875\n",
      "Current prediction:  60.67583465576172 \n",
      "\n",
      "Iteration 7524, Loss: 37.008087158203125, L1: 10.189472198486328, L3: 26.818613052368164\n",
      "Current prediction:  60.696842193603516 \n",
      "\n",
      "Iteration 7525, Loss: 36.52851486206055, L1: 10.068307876586914, L3: 26.460206985473633\n",
      "Current prediction:  60.736446380615234 \n",
      "\n",
      "Iteration 7526, Loss: 36.51823425292969, L1: 9.993358612060547, L3: 26.524877548217773\n",
      "Current prediction:  61.477806091308594 \n",
      "\n",
      "Iteration 7527, Loss: 37.484169006347656, L1: 9.84200668334961, L3: 27.642162322998047\n",
      "Current prediction:  61.641963958740234 \n",
      "\n",
      "Iteration 7528, Loss: 36.66478729248047, L1: 9.883713722229004, L3: 26.78107452392578\n",
      "Current prediction:  61.64369583129883 \n",
      "\n",
      "Iteration 7529, Loss: 36.51948547363281, L1: 9.941017150878906, L3: 26.578466415405273\n",
      "Current prediction:  61.63457489013672 \n",
      "\n",
      "Iteration 7530, Loss: 35.908203125, L1: 9.989713668823242, L3: 25.91849136352539\n",
      "Current prediction:  61.61720657348633 \n",
      "\n",
      "Iteration 7531, Loss: 35.79398727416992, L1: 9.994121551513672, L3: 25.79986572265625\n",
      "Current prediction:  61.5932731628418 \n",
      "\n",
      "Iteration 7532, Loss: 35.73215103149414, L1: 10.01222038269043, L3: 25.71993064880371\n",
      "Current prediction:  61.56684875488281 \n",
      "\n",
      "Iteration 7533, Loss: 35.634674072265625, L1: 10.01529312133789, L3: 25.6193790435791\n",
      "Current prediction:  61.54252243041992 \n",
      "\n",
      "Iteration 7534, Loss: 35.60752868652344, L1: 10.020374298095703, L3: 25.5871524810791\n",
      "Current prediction:  61.522037506103516 \n",
      "\n",
      "Iteration 7535, Loss: 36.49701690673828, L1: 10.058662414550781, L3: 26.438356399536133\n",
      "Current prediction:  61.50000762939453 \n",
      "\n",
      "Iteration 7536, Loss: 36.448848724365234, L1: 10.027195930480957, L3: 26.421653747558594\n",
      "Current prediction:  61.46737289428711 \n",
      "\n",
      "Iteration 7537, Loss: 36.128055572509766, L1: 10.055150032043457, L3: 26.072904586791992\n",
      "Current prediction:  60.9384651184082 \n",
      "\n",
      "Iteration 7538, Loss: 36.80784225463867, L1: 10.010285377502441, L3: 26.797555923461914\n",
      "Current prediction:  60.6736946105957 \n",
      "\n",
      "Iteration 7539, Loss: 36.82270050048828, L1: 10.06399917602539, L3: 26.758699417114258\n",
      "Current prediction:  60.89025115966797 \n",
      "\n",
      "Iteration 7540, Loss: 35.549617767333984, L1: 10.04165267944336, L3: 25.507965087890625\n",
      "Current prediction:  61.258853912353516 \n",
      "\n",
      "Iteration 7541, Loss: 36.12810516357422, L1: 10.059946060180664, L3: 26.068159103393555\n",
      "Current prediction:  61.42854690551758 \n",
      "\n",
      "Iteration 7542, Loss: 36.20592498779297, L1: 10.016650199890137, L3: 26.189273834228516\n",
      "Current prediction:  61.47998809814453 \n",
      "\n",
      "Iteration 7543, Loss: 37.13990020751953, L1: 9.987739562988281, L3: 27.152162551879883\n",
      "Current prediction:  61.49342727661133 \n",
      "\n",
      "Iteration 7544, Loss: 36.42192077636719, L1: 10.026104927062988, L3: 26.395814895629883\n",
      "Current prediction:  61.495849609375 \n",
      "\n",
      "Iteration 7545, Loss: 37.454559326171875, L1: 10.033451080322266, L3: 27.421106338500977\n",
      "Current prediction:  61.4704704284668 \n",
      "\n",
      "Iteration 7546, Loss: 36.56683349609375, L1: 9.990543365478516, L3: 26.576290130615234\n",
      "Current prediction:  61.27391815185547 \n",
      "\n",
      "Iteration 7547, Loss: 36.40130615234375, L1: 9.945011138916016, L3: 26.4562931060791\n",
      "Current prediction:  60.683170318603516 \n",
      "\n",
      "Iteration 7548, Loss: 36.4044303894043, L1: 10.015886306762695, L3: 26.3885440826416\n",
      "Current prediction:  60.62058639526367 \n",
      "\n",
      "Iteration 7549, Loss: 35.823829650878906, L1: 10.043779373168945, L3: 25.780052185058594\n",
      "Current prediction:  60.62959671020508 \n",
      "\n",
      "Iteration 7550, Loss: 34.95969772338867, L1: 10.009214401245117, L3: 24.950483322143555\n",
      "Current prediction:  60.71833419799805 \n",
      "\n",
      "Iteration 7551, Loss: 36.1358642578125, L1: 9.996783256530762, L3: 26.139080047607422\n",
      "Current prediction:  61.22887420654297 \n",
      "\n",
      "Iteration 7552, Loss: 37.01543426513672, L1: 9.947486877441406, L3: 27.067949295043945\n",
      "Current prediction:  61.50215148925781 \n",
      "\n",
      "Iteration 7553, Loss: 36.26036071777344, L1: 9.968765258789062, L3: 26.291593551635742\n",
      "Current prediction:  61.540122985839844 \n",
      "\n",
      "Iteration 7554, Loss: 37.53370666503906, L1: 9.96601676940918, L3: 27.56768798828125\n",
      "Current prediction:  61.51142883300781 \n",
      "\n",
      "Iteration 7555, Loss: 36.724090576171875, L1: 9.942363739013672, L3: 26.781728744506836\n",
      "Current prediction:  61.24879455566406 \n",
      "\n",
      "Iteration 7556, Loss: 35.750343322753906, L1: 9.98666000366211, L3: 25.763681411743164\n",
      "Current prediction:  60.728004455566406 \n",
      "\n",
      "Iteration 7557, Loss: 36.967403411865234, L1: 9.96877384185791, L3: 26.99863052368164\n",
      "Current prediction:  60.700225830078125 \n",
      "\n",
      "Iteration 7558, Loss: 34.835365295410156, L1: 10.000563621520996, L3: 24.834802627563477\n",
      "Current prediction:  60.73161697387695 \n",
      "\n",
      "Iteration 7559, Loss: 35.99822998046875, L1: 9.969110488891602, L3: 26.029117584228516\n",
      "Current prediction:  61.24604034423828 \n",
      "\n",
      "Iteration 7560, Loss: 36.6336784362793, L1: 9.904892921447754, L3: 26.728784561157227\n",
      "Current prediction:  61.53825378417969 \n",
      "\n",
      "Iteration 7561, Loss: 36.460391998291016, L1: 9.887033462524414, L3: 26.5733585357666\n",
      "Current prediction:  61.569549560546875 \n",
      "\n",
      "Iteration 7562, Loss: 35.557464599609375, L1: 9.899630546569824, L3: 25.657833099365234\n",
      "Current prediction:  61.39600372314453 \n",
      "\n",
      "Iteration 7563, Loss: 36.65526580810547, L1: 9.870709419250488, L3: 26.784555435180664\n",
      "Current prediction:  61.13475799560547 \n",
      "\n",
      "Iteration 7564, Loss: 35.299354553222656, L1: 9.901409149169922, L3: 25.397947311401367\n",
      "Current prediction:  60.83199691772461 \n",
      "\n",
      "Iteration 7565, Loss: 35.186767578125, L1: 9.90992546081543, L3: 25.27684211730957\n",
      "Current prediction:  60.7459602355957 \n",
      "\n",
      "Iteration 7566, Loss: 35.61957550048828, L1: 9.902740478515625, L3: 25.71683692932129\n",
      "Current prediction:  60.74535369873047 \n",
      "\n",
      "Iteration 7567, Loss: 37.03115463256836, L1: 9.915569305419922, L3: 27.115585327148438\n",
      "Current prediction:  60.76081466674805 \n",
      "\n",
      "Iteration 7568, Loss: 35.622066497802734, L1: 9.946943283081055, L3: 25.67512321472168\n",
      "Current prediction:  61.36037063598633 \n",
      "\n",
      "Iteration 7569, Loss: 36.08237838745117, L1: 9.889386177062988, L3: 26.192991256713867\n",
      "Current prediction:  61.602237701416016 \n",
      "\n",
      "Iteration 7570, Loss: 37.04753112792969, L1: 9.928178787231445, L3: 27.119352340698242\n",
      "Current prediction:  61.600074768066406 \n",
      "\n",
      "Iteration 7571, Loss: 36.29584503173828, L1: 9.955846786499023, L3: 26.339996337890625\n",
      "Current prediction:  61.5869255065918 \n",
      "\n",
      "Iteration 7572, Loss: 36.614044189453125, L1: 9.962132453918457, L3: 26.651912689208984\n",
      "Current prediction:  61.571659088134766 \n",
      "\n",
      "Iteration 7573, Loss: 35.92817306518555, L1: 10.015769004821777, L3: 25.912405014038086\n",
      "Current prediction:  61.55439376831055 \n",
      "\n",
      "Iteration 7574, Loss: 36.735496520996094, L1: 10.028621673583984, L3: 26.70687484741211\n",
      "Current prediction:  61.537330627441406 \n",
      "\n",
      "Iteration 7575, Loss: 37.14480209350586, L1: 10.057753562927246, L3: 27.087047576904297\n",
      "Current prediction:  61.522220611572266 \n",
      "\n",
      "Iteration 7576, Loss: 36.58821487426758, L1: 9.987156867980957, L3: 26.601057052612305\n",
      "Current prediction:  61.506317138671875 \n",
      "\n",
      "Iteration 7577, Loss: 35.35725784301758, L1: 10.005646705627441, L3: 25.35161018371582\n",
      "Current prediction:  61.42280960083008 \n",
      "\n",
      "Iteration 7578, Loss: 36.01088333129883, L1: 9.974979400634766, L3: 26.035903930664062\n",
      "Current prediction:  60.69157791137695 \n",
      "\n",
      "Iteration 7579, Loss: 36.201236724853516, L1: 10.026557922363281, L3: 26.174678802490234\n",
      "Current prediction:  60.5900764465332 \n",
      "\n",
      "Iteration 7580, Loss: 35.89964294433594, L1: 10.051695823669434, L3: 25.847946166992188\n",
      "Current prediction:  60.589759826660156 \n",
      "\n",
      "Iteration 7581, Loss: 36.599937438964844, L1: 10.143228530883789, L3: 26.456707000732422\n",
      "Current prediction:  60.603580474853516 \n",
      "\n",
      "Iteration 7582, Loss: 34.32136917114258, L1: 10.05525016784668, L3: 24.2661190032959\n",
      "Current prediction:  60.631553649902344 \n",
      "\n",
      "Iteration 7583, Loss: 35.611793518066406, L1: 10.030401229858398, L3: 25.58139419555664\n",
      "Current prediction:  60.76596450805664 \n",
      "\n",
      "Iteration 7584, Loss: 36.43651580810547, L1: 9.95897102355957, L3: 26.477542877197266\n",
      "Current prediction:  61.49623489379883 \n",
      "\n",
      "Iteration 7585, Loss: 35.98373031616211, L1: 9.951302528381348, L3: 26.032428741455078\n",
      "Current prediction:  61.576499938964844 \n",
      "\n",
      "Iteration 7586, Loss: 35.85393524169922, L1: 9.960494041442871, L3: 25.89344024658203\n",
      "Current prediction:  61.59211730957031 \n",
      "\n",
      "Iteration 7587, Loss: 35.64031982421875, L1: 9.996343612670898, L3: 25.64397430419922\n",
      "Current prediction:  61.598724365234375 \n",
      "\n",
      "Iteration 7588, Loss: 36.46085739135742, L1: 9.991995811462402, L3: 26.468862533569336\n",
      "Current prediction:  61.59804153442383 \n",
      "\n",
      "Iteration 7589, Loss: 36.2919807434082, L1: 9.986108779907227, L3: 26.305871963500977\n",
      "Current prediction:  61.59561538696289 \n",
      "\n",
      "Iteration 7590, Loss: 36.89645004272461, L1: 9.927896499633789, L3: 26.96855354309082\n",
      "Current prediction:  61.58408737182617 \n",
      "\n",
      "Iteration 7591, Loss: 36.10274887084961, L1: 9.981063842773438, L3: 26.121685028076172\n",
      "Current prediction:  61.522186279296875 \n",
      "\n",
      "Iteration 7592, Loss: 35.21649169921875, L1: 9.885140419006348, L3: 25.331350326538086\n",
      "Current prediction:  61.07683181762695 \n",
      "\n",
      "Iteration 7593, Loss: 36.35013198852539, L1: 9.920225143432617, L3: 26.429906845092773\n",
      "Current prediction:  60.71181106567383 \n",
      "\n",
      "Iteration 7594, Loss: 36.162208557128906, L1: 9.96351432800293, L3: 26.19869613647461\n",
      "Current prediction:  60.65483856201172 \n",
      "\n",
      "Iteration 7595, Loss: 36.974159240722656, L1: 10.0169095993042, L3: 26.957250595092773\n",
      "Current prediction:  60.65214538574219 \n",
      "\n",
      "Iteration 7596, Loss: 37.05811309814453, L1: 10.076005935668945, L3: 26.982107162475586\n",
      "Current prediction:  60.71004867553711 \n",
      "\n",
      "Iteration 7597, Loss: 35.976234436035156, L1: 9.992612838745117, L3: 25.983619689941406\n",
      "Current prediction:  61.11576461791992 \n",
      "\n",
      "Iteration 7598, Loss: 36.114505767822266, L1: 9.955060005187988, L3: 26.15944480895996\n",
      "Current prediction:  61.5125732421875 \n",
      "\n",
      "Iteration 7599, Loss: 36.36141586303711, L1: 9.9686861038208, L3: 26.392730712890625\n",
      "Current prediction:  61.531463623046875 \n",
      "\n",
      "Iteration 7600, Loss: 36.4341926574707, L1: 10.010762214660645, L3: 26.423431396484375\n",
      "Current prediction:  61.53266525268555 \n",
      "\n",
      "Iteration 7601, Loss: 37.155975341796875, L1: 9.98567008972168, L3: 27.170303344726562\n",
      "Current prediction:  61.52457809448242 \n",
      "\n",
      "Iteration 7602, Loss: 35.978477478027344, L1: 10.053459167480469, L3: 25.925016403198242\n",
      "Current prediction:  61.52507400512695 \n",
      "\n",
      "Iteration 7603, Loss: 35.99263381958008, L1: 9.99548625946045, L3: 25.997148513793945\n",
      "Current prediction:  61.525108337402344 \n",
      "\n",
      "Iteration 7604, Loss: 36.4571647644043, L1: 10.034757614135742, L3: 26.422407150268555\n",
      "Current prediction:  61.5067253112793 \n",
      "\n",
      "Iteration 7605, Loss: 36.812583923339844, L1: 9.98105239868164, L3: 26.831533432006836\n",
      "Current prediction:  61.39009094238281 \n",
      "\n",
      "Iteration 7606, Loss: 36.155181884765625, L1: 9.983091354370117, L3: 26.17209243774414\n",
      "Current prediction:  60.911216735839844 \n",
      "\n",
      "Iteration 7607, Loss: 36.05645751953125, L1: 9.99073314666748, L3: 26.065723419189453\n",
      "Current prediction:  60.847694396972656 \n",
      "\n",
      "Iteration 7608, Loss: 36.6507682800293, L1: 10.008630752563477, L3: 26.64213752746582\n",
      "Current prediction:  60.72484588623047 \n",
      "\n",
      "Iteration 7609, Loss: 35.876827239990234, L1: 9.977522850036621, L3: 25.899303436279297\n",
      "Current prediction:  60.67465591430664 \n",
      "\n",
      "Iteration 7610, Loss: 36.61049270629883, L1: 10.019366264343262, L3: 26.591127395629883\n",
      "Current prediction:  60.85284423828125 \n",
      "\n",
      "Iteration 7611, Loss: 36.582496643066406, L1: 9.960220336914062, L3: 26.622278213500977\n",
      "Current prediction:  61.297264099121094 \n",
      "\n",
      "Iteration 7612, Loss: 35.71078872680664, L1: 9.992003440856934, L3: 25.71878433227539\n",
      "Current prediction:  61.46617126464844 \n",
      "\n",
      "Iteration 7613, Loss: 36.42975616455078, L1: 9.95075798034668, L3: 26.47899627685547\n",
      "Current prediction:  61.49495315551758 \n",
      "\n",
      "Iteration 7614, Loss: 36.5155143737793, L1: 9.971444129943848, L3: 26.544071197509766\n",
      "Current prediction:  61.42904281616211 \n",
      "\n",
      "Iteration 7615, Loss: 35.8965950012207, L1: 9.930949211120605, L3: 25.96564483642578\n",
      "Current prediction:  61.253257751464844 \n",
      "\n",
      "Iteration 7616, Loss: 34.62400817871094, L1: 9.958701133728027, L3: 24.665306091308594\n",
      "Current prediction:  60.727481842041016 \n",
      "\n",
      "Iteration 7617, Loss: 36.510963439941406, L1: 9.982855796813965, L3: 26.528106689453125\n",
      "Current prediction:  60.652366638183594 \n",
      "\n",
      "Iteration 7618, Loss: 36.24238967895508, L1: 10.003118515014648, L3: 26.23927116394043\n",
      "Current prediction:  60.66026306152344 \n",
      "\n",
      "Iteration 7619, Loss: 36.88072204589844, L1: 9.99839973449707, L3: 26.882322311401367\n",
      "Current prediction:  60.666011810302734 \n",
      "\n",
      "Iteration 7620, Loss: 35.70954513549805, L1: 10.035253524780273, L3: 25.674291610717773\n",
      "Current prediction:  60.80534744262695 \n",
      "\n",
      "Iteration 7621, Loss: 36.538063049316406, L1: 9.962015151977539, L3: 26.576045989990234\n",
      "Current prediction:  61.4802131652832 \n",
      "\n",
      "Iteration 7622, Loss: 36.4024658203125, L1: 9.928170204162598, L3: 26.474294662475586\n",
      "Current prediction:  61.55350875854492 \n",
      "\n",
      "Iteration 7623, Loss: 36.10210418701172, L1: 9.941790580749512, L3: 26.16031265258789\n",
      "Current prediction:  61.575775146484375 \n",
      "\n",
      "Iteration 7624, Loss: 36.48093795776367, L1: 9.957168579101562, L3: 26.52376937866211\n",
      "Current prediction:  61.58390808105469 \n",
      "\n",
      "Iteration 7625, Loss: 36.9421501159668, L1: 9.958662033081055, L3: 26.983488082885742\n",
      "Current prediction:  61.58269500732422 \n",
      "\n",
      "Iteration 7626, Loss: 36.97653579711914, L1: 9.947966575622559, L3: 27.028568267822266\n",
      "Current prediction:  61.576847076416016 \n",
      "\n",
      "Iteration 7627, Loss: 36.103153228759766, L1: 9.963282585144043, L3: 26.139869689941406\n",
      "Current prediction:  61.568119049072266 \n",
      "\n",
      "Iteration 7628, Loss: 36.26734161376953, L1: 9.906753540039062, L3: 26.36058807373047\n",
      "Current prediction:  61.5330810546875 \n",
      "\n",
      "Iteration 7629, Loss: 36.129981994628906, L1: 9.924201965332031, L3: 26.205781936645508\n",
      "Current prediction:  61.25360107421875 \n",
      "\n",
      "Iteration 7630, Loss: 35.77585983276367, L1: 9.912141799926758, L3: 25.863718032836914\n",
      "Current prediction:  60.821285247802734 \n",
      "\n",
      "Iteration 7631, Loss: 36.092933654785156, L1: 9.974343299865723, L3: 26.11859130859375\n",
      "Current prediction:  60.699581146240234 \n",
      "\n",
      "Iteration 7632, Loss: 36.33956527709961, L1: 9.963828086853027, L3: 26.3757381439209\n",
      "Current prediction:  60.7115478515625 \n",
      "\n",
      "Iteration 7633, Loss: 36.579002380371094, L1: 9.973671913146973, L3: 26.605331420898438\n",
      "Current prediction:  61.24352264404297 \n",
      "\n",
      "Iteration 7634, Loss: 35.66455078125, L1: 9.936115264892578, L3: 25.728435516357422\n",
      "Current prediction:  61.44334411621094 \n",
      "\n",
      "Iteration 7635, Loss: 36.14390182495117, L1: 9.920326232910156, L3: 26.223575592041016\n",
      "Current prediction:  61.448326110839844 \n",
      "\n",
      "Iteration 7636, Loss: 36.57546615600586, L1: 9.972092628479004, L3: 26.60337257385254\n",
      "Current prediction:  61.23584747314453 \n",
      "\n",
      "Iteration 7637, Loss: 36.38083267211914, L1: 9.99085521697998, L3: 26.389978408813477\n",
      "Current prediction:  60.67152404785156 \n",
      "\n",
      "Iteration 7638, Loss: 35.932682037353516, L1: 10.031798362731934, L3: 25.900882720947266\n",
      "Current prediction:  60.62355041503906 \n",
      "\n",
      "Iteration 7639, Loss: 35.647396087646484, L1: 10.043478965759277, L3: 25.603918075561523\n",
      "Current prediction:  60.63533401489258 \n",
      "\n",
      "Iteration 7640, Loss: 36.35325622558594, L1: 10.080650329589844, L3: 26.272605895996094\n",
      "Current prediction:  60.656002044677734 \n",
      "\n",
      "Iteration 7641, Loss: 36.250186920166016, L1: 10.011029243469238, L3: 26.239158630371094\n",
      "Current prediction:  60.7032470703125 \n",
      "\n",
      "Iteration 7642, Loss: 34.96282196044922, L1: 9.962602615356445, L3: 25.000221252441406\n",
      "Current prediction:  61.41704177856445 \n",
      "\n",
      "Iteration 7643, Loss: 36.029109954833984, L1: 9.859938621520996, L3: 26.169170379638672\n",
      "Current prediction:  61.640708923339844 \n",
      "\n",
      "Iteration 7644, Loss: 36.41203689575195, L1: 9.887721061706543, L3: 26.524314880371094\n",
      "Current prediction:  61.6685905456543 \n",
      "\n",
      "Iteration 7645, Loss: 35.30500793457031, L1: 9.87631607055664, L3: 25.428693771362305\n",
      "Current prediction:  61.68428039550781 \n",
      "\n",
      "Iteration 7646, Loss: 36.98665237426758, L1: 9.862661361694336, L3: 27.123991012573242\n",
      "Current prediction:  61.68912124633789 \n",
      "\n",
      "Iteration 7647, Loss: 36.205841064453125, L1: 9.824103355407715, L3: 26.381736755371094\n",
      "Current prediction:  61.62900924682617 \n",
      "\n",
      "Iteration 7648, Loss: 36.316036224365234, L1: 9.875741004943848, L3: 26.440296173095703\n",
      "Current prediction:  60.898738861083984 \n",
      "\n",
      "Iteration 7649, Loss: 35.87451171875, L1: 9.821523666381836, L3: 26.052989959716797\n",
      "Current prediction:  60.77276611328125 \n",
      "\n",
      "Iteration 7650, Loss: 36.22937774658203, L1: 9.879034042358398, L3: 26.350343704223633\n",
      "Current prediction:  60.75771713256836 \n",
      "\n",
      "Iteration 7651, Loss: 36.712867736816406, L1: 9.928229331970215, L3: 26.784637451171875\n",
      "Current prediction:  60.74862289428711 \n",
      "\n",
      "Iteration 7652, Loss: 36.509090423583984, L1: 9.908075332641602, L3: 26.601015090942383\n",
      "Current prediction:  60.77266311645508 \n",
      "\n",
      "Iteration 7653, Loss: 35.95302200317383, L1: 9.913999557495117, L3: 26.03902244567871\n",
      "Current prediction:  61.1160888671875 \n",
      "\n",
      "Iteration 7654, Loss: 36.50440216064453, L1: 9.875059127807617, L3: 26.629343032836914\n",
      "Current prediction:  61.54193115234375 \n",
      "\n",
      "Iteration 7655, Loss: 35.830814361572266, L1: 9.882939338684082, L3: 25.9478759765625\n",
      "Current prediction:  61.58454513549805 \n",
      "\n",
      "Iteration 7656, Loss: 36.35765075683594, L1: 9.93050479888916, L3: 26.427146911621094\n",
      "Current prediction:  61.575504302978516 \n",
      "\n",
      "Iteration 7657, Loss: 35.9498176574707, L1: 9.922633171081543, L3: 26.027185440063477\n",
      "Current prediction:  61.56217956542969 \n",
      "\n",
      "Iteration 7658, Loss: 35.888099670410156, L1: 9.962515830993652, L3: 25.92558479309082\n",
      "Current prediction:  61.54924774169922 \n",
      "\n",
      "Iteration 7659, Loss: 36.65260696411133, L1: 9.940098762512207, L3: 26.712509155273438\n",
      "Current prediction:  61.52865982055664 \n",
      "\n",
      "Iteration 7660, Loss: 37.26814651489258, L1: 9.973508834838867, L3: 27.29463768005371\n",
      "Current prediction:  61.48888397216797 \n",
      "\n",
      "Iteration 7661, Loss: 37.31779479980469, L1: 9.963951110839844, L3: 27.353843688964844\n",
      "Current prediction:  61.24934005737305 \n",
      "\n",
      "Iteration 7662, Loss: 36.09955978393555, L1: 9.964072227478027, L3: 26.135486602783203\n",
      "Current prediction:  60.85171127319336 \n",
      "\n",
      "Iteration 7663, Loss: 35.6439094543457, L1: 9.98972225189209, L3: 25.65418815612793\n",
      "Current prediction:  60.94342041015625 \n",
      "\n",
      "Iteration 7664, Loss: 36.841766357421875, L1: 9.938854217529297, L3: 26.902912139892578\n",
      "Current prediction:  61.2053108215332 \n",
      "\n",
      "Iteration 7665, Loss: 36.13751220703125, L1: 9.957714080810547, L3: 26.17979621887207\n",
      "Current prediction:  61.36930465698242 \n",
      "\n",
      "Iteration 7666, Loss: 35.8123893737793, L1: 9.93506145477295, L3: 25.87732696533203\n",
      "Current prediction:  61.45411682128906 \n",
      "\n",
      "Iteration 7667, Loss: 36.760658264160156, L1: 9.907699584960938, L3: 26.85295867919922\n",
      "Current prediction:  61.430458068847656 \n",
      "\n",
      "Iteration 7668, Loss: 36.35184860229492, L1: 9.943835258483887, L3: 26.40801239013672\n",
      "Current prediction:  61.32097244262695 \n",
      "\n",
      "Iteration 7669, Loss: 36.56344223022461, L1: 9.937554359436035, L3: 26.625886917114258\n",
      "Current prediction:  61.023651123046875 \n",
      "\n",
      "Iteration 7670, Loss: 36.92765808105469, L1: 9.935202598571777, L3: 26.992456436157227\n",
      "Current prediction:  60.97469711303711 \n",
      "\n",
      "Iteration 7671, Loss: 36.833255767822266, L1: 9.91344928741455, L3: 26.9198055267334\n",
      "Current prediction:  61.00967788696289 \n",
      "\n",
      "Iteration 7672, Loss: 35.1004638671875, L1: 9.939857482910156, L3: 25.16060447692871\n",
      "Current prediction:  61.33787536621094 \n",
      "\n",
      "Iteration 7673, Loss: 36.79608154296875, L1: 9.890570640563965, L3: 26.90550994873047\n",
      "Current prediction:  61.38883972167969 \n",
      "\n",
      "Iteration 7674, Loss: 35.79190444946289, L1: 9.902307510375977, L3: 25.889596939086914\n",
      "Current prediction:  61.42757797241211 \n",
      "\n",
      "Iteration 7675, Loss: 35.49766159057617, L1: 9.89976692199707, L3: 25.5978946685791\n",
      "Current prediction:  61.46710205078125 \n",
      "\n",
      "Iteration 7676, Loss: 36.28211212158203, L1: 9.909873008728027, L3: 26.37224006652832\n",
      "Current prediction:  61.35163497924805 \n",
      "\n",
      "Iteration 7677, Loss: 36.5570068359375, L1: 9.932795524597168, L3: 26.62421226501465\n",
      "Current prediction:  61.20064163208008 \n",
      "\n",
      "Iteration 7678, Loss: 35.924129486083984, L1: 9.820504188537598, L3: 26.103626251220703\n",
      "Current prediction:  61.06016159057617 \n",
      "\n",
      "Iteration 7679, Loss: 37.03141784667969, L1: 9.89893627166748, L3: 27.13248062133789\n",
      "Current prediction:  61.377986907958984 \n",
      "\n",
      "Iteration 7680, Loss: 36.41619873046875, L1: 9.91630744934082, L3: 26.499893188476562\n",
      "Current prediction:  61.48365020751953 \n",
      "\n",
      "Iteration 7681, Loss: 35.946372985839844, L1: 9.842613220214844, L3: 26.103759765625\n",
      "Current prediction:  61.456687927246094 \n",
      "\n",
      "Iteration 7682, Loss: 36.574974060058594, L1: 9.891210556030273, L3: 26.683761596679688\n",
      "Current prediction:  61.56766128540039 \n",
      "\n",
      "Iteration 7683, Loss: 36.22543716430664, L1: 9.878525733947754, L3: 26.346912384033203\n",
      "Current prediction:  61.59151077270508 \n",
      "\n",
      "Iteration 7684, Loss: 35.987152099609375, L1: 9.860540390014648, L3: 26.12661361694336\n",
      "Current prediction:  61.58578109741211 \n",
      "\n",
      "Iteration 7685, Loss: 36.51097106933594, L1: 9.922466278076172, L3: 26.588502883911133\n",
      "Current prediction:  61.5339241027832 \n",
      "\n",
      "Iteration 7686, Loss: 36.928165435791016, L1: 9.8608980178833, L3: 27.06726837158203\n",
      "Current prediction:  60.93550109863281 \n",
      "\n",
      "Iteration 7687, Loss: 36.254150390625, L1: 9.899131774902344, L3: 26.355018615722656\n",
      "Current prediction:  60.714141845703125 \n",
      "\n",
      "Iteration 7688, Loss: 35.60073471069336, L1: 9.943562507629395, L3: 25.65717124938965\n",
      "Current prediction:  60.670623779296875 \n",
      "\n",
      "Iteration 7689, Loss: 36.00657653808594, L1: 9.957890510559082, L3: 26.048686981201172\n",
      "Current prediction:  60.69399642944336 \n",
      "\n",
      "Iteration 7690, Loss: 36.80194854736328, L1: 9.931018829345703, L3: 26.87093162536621\n",
      "Current prediction:  60.9862060546875 \n",
      "\n",
      "Iteration 7691, Loss: 35.943214416503906, L1: 9.921920776367188, L3: 26.02129554748535\n",
      "Current prediction:  61.48173522949219 \n",
      "\n",
      "Iteration 7692, Loss: 35.69281005859375, L1: 9.853300094604492, L3: 25.83951187133789\n",
      "Current prediction:  61.54830551147461 \n",
      "\n",
      "Iteration 7693, Loss: 36.43525695800781, L1: 9.84981632232666, L3: 26.585439682006836\n",
      "Current prediction:  61.472267150878906 \n",
      "\n",
      "Iteration 7694, Loss: 36.81659698486328, L1: 9.868684768676758, L3: 26.947912216186523\n",
      "Current prediction:  60.75028991699219 \n",
      "\n",
      "Iteration 7695, Loss: 36.53278732299805, L1: 9.947518348693848, L3: 26.585268020629883\n",
      "Current prediction:  60.66951370239258 \n",
      "\n",
      "Iteration 7696, Loss: 36.35745620727539, L1: 10.034074783325195, L3: 26.323381423950195\n",
      "Current prediction:  60.66850662231445 \n",
      "\n",
      "Iteration 7697, Loss: 36.556068420410156, L1: 10.073418617248535, L3: 26.482650756835938\n",
      "Current prediction:  60.67083740234375 \n",
      "\n",
      "Iteration 7698, Loss: 36.27806854248047, L1: 10.056406021118164, L3: 26.221662521362305\n",
      "Current prediction:  60.67920684814453 \n",
      "\n",
      "Iteration 7699, Loss: 35.77552795410156, L1: 9.998847961425781, L3: 25.776681900024414\n",
      "Current prediction:  60.88880157470703 \n",
      "\n",
      "Iteration 7700, Loss: 35.65704345703125, L1: 9.912205696105957, L3: 25.744836807250977\n",
      "Current prediction:  61.58633041381836 \n",
      "\n",
      "Iteration 7701, Loss: 36.36125564575195, L1: 9.929469108581543, L3: 26.431787490844727\n",
      "Current prediction:  61.60346221923828 \n",
      "\n",
      "Iteration 7702, Loss: 35.90290451049805, L1: 9.954298973083496, L3: 25.948606491088867\n",
      "Current prediction:  61.596473693847656 \n",
      "\n",
      "Iteration 7703, Loss: 37.18809127807617, L1: 10.007941246032715, L3: 27.18014907836914\n",
      "Current prediction:  61.58788299560547 \n",
      "\n",
      "Iteration 7704, Loss: 36.309715270996094, L1: 10.048484802246094, L3: 26.261228561401367\n",
      "Current prediction:  61.56980895996094 \n",
      "\n",
      "Iteration 7705, Loss: 36.35437774658203, L1: 10.046079635620117, L3: 26.30829620361328\n",
      "Current prediction:  61.549320220947266 \n",
      "\n",
      "Iteration 7706, Loss: 36.00324630737305, L1: 10.030048370361328, L3: 25.97319793701172\n",
      "Current prediction:  61.532684326171875 \n",
      "\n",
      "Iteration 7707, Loss: 36.4404411315918, L1: 10.04620361328125, L3: 26.394237518310547\n",
      "Current prediction:  61.51359939575195 \n",
      "\n",
      "Iteration 7708, Loss: 36.376731872558594, L1: 10.051204681396484, L3: 26.325525283813477\n",
      "Current prediction:  61.494728088378906 \n",
      "\n",
      "Iteration 7709, Loss: 36.13731384277344, L1: 10.015052795410156, L3: 26.12225914001465\n",
      "Current prediction:  61.47499465942383 \n",
      "\n",
      "Iteration 7710, Loss: 35.52949523925781, L1: 10.035593032836914, L3: 25.4939022064209\n",
      "Current prediction:  61.391231536865234 \n",
      "\n",
      "Iteration 7711, Loss: 36.39939880371094, L1: 9.984975814819336, L3: 26.41442108154297\n",
      "Current prediction:  60.573524475097656 \n",
      "\n",
      "Iteration 7712, Loss: 35.339576721191406, L1: 10.050362586975098, L3: 25.289213180541992\n",
      "Current prediction:  60.515625 \n",
      "\n",
      "Iteration 7713, Loss: 35.494407653808594, L1: 10.134742736816406, L3: 25.359663009643555\n",
      "Current prediction:  60.51288604736328 \n",
      "\n",
      "Iteration 7714, Loss: 36.6667594909668, L1: 10.119576454162598, L3: 26.547183990478516\n",
      "Current prediction:  60.52460861206055 \n",
      "\n",
      "Iteration 7715, Loss: 37.66780090332031, L1: 10.090450286865234, L3: 27.577350616455078\n",
      "Current prediction:  60.568138122558594 \n",
      "\n",
      "Iteration 7716, Loss: 36.668800354003906, L1: 10.090693473815918, L3: 26.578105926513672\n",
      "Current prediction:  60.83531951904297 \n",
      "\n",
      "Iteration 7717, Loss: 36.481990814208984, L1: 10.00849723815918, L3: 26.473493576049805\n",
      "Current prediction:  61.353668212890625 \n",
      "\n",
      "Iteration 7718, Loss: 35.426910400390625, L1: 9.933822631835938, L3: 25.493087768554688\n",
      "Current prediction:  61.48704147338867 \n",
      "\n",
      "Iteration 7719, Loss: 36.38857650756836, L1: 9.979780197143555, L3: 26.408796310424805\n",
      "Current prediction:  61.50414276123047 \n",
      "\n",
      "Iteration 7720, Loss: 36.40322494506836, L1: 9.963274002075195, L3: 26.439950942993164\n",
      "Current prediction:  61.50166702270508 \n",
      "\n",
      "Iteration 7721, Loss: 36.05195236206055, L1: 9.910292625427246, L3: 26.141660690307617\n",
      "Current prediction:  61.504066467285156 \n",
      "\n",
      "Iteration 7722, Loss: 36.41600799560547, L1: 9.902158737182617, L3: 26.51384735107422\n",
      "Current prediction:  61.22265625 \n",
      "\n",
      "Iteration 7723, Loss: 36.34377670288086, L1: 9.907598495483398, L3: 26.43617820739746\n",
      "Current prediction:  60.90757751464844 \n",
      "\n",
      "Iteration 7724, Loss: 36.179473876953125, L1: 9.900175094604492, L3: 26.279300689697266\n",
      "Current prediction:  60.97935485839844 \n",
      "\n",
      "Iteration 7725, Loss: 35.950416564941406, L1: 9.912931442260742, L3: 26.037485122680664\n",
      "Current prediction:  61.32064437866211 \n",
      "\n",
      "Iteration 7726, Loss: 35.716880798339844, L1: 9.853199005126953, L3: 25.863679885864258\n",
      "Current prediction:  61.5448112487793 \n",
      "\n",
      "Iteration 7727, Loss: 35.54016876220703, L1: 9.847199440002441, L3: 25.692968368530273\n",
      "Current prediction:  61.57392883300781 \n",
      "\n",
      "Iteration 7728, Loss: 36.54722595214844, L1: 9.899873733520508, L3: 26.64735221862793\n",
      "Current prediction:  61.43795394897461 \n",
      "\n",
      "Iteration 7729, Loss: 37.19032669067383, L1: 9.902533531188965, L3: 27.28779411315918\n",
      "Current prediction:  60.92853546142578 \n",
      "\n",
      "Iteration 7730, Loss: 34.52647018432617, L1: 9.903329849243164, L3: 24.623140335083008\n",
      "Current prediction:  60.69833755493164 \n",
      "\n",
      "Iteration 7731, Loss: 36.66684341430664, L1: 9.979113578796387, L3: 26.687728881835938\n",
      "Current prediction:  60.6887092590332 \n",
      "\n",
      "Iteration 7732, Loss: 36.56033706665039, L1: 10.00268840789795, L3: 26.557649612426758\n",
      "Current prediction:  60.6922721862793 \n",
      "\n",
      "Iteration 7733, Loss: 36.596893310546875, L1: 10.018630981445312, L3: 26.578262329101562\n",
      "Current prediction:  60.70076370239258 \n",
      "\n",
      "Iteration 7734, Loss: 35.74044418334961, L1: 10.045729637145996, L3: 25.69471549987793\n",
      "Current prediction:  60.71031951904297 \n",
      "\n",
      "Iteration 7735, Loss: 36.73407745361328, L1: 10.0670166015625, L3: 26.667062759399414\n",
      "Current prediction:  60.71818542480469 \n",
      "\n",
      "Iteration 7736, Loss: 36.583126068115234, L1: 9.972460746765137, L3: 26.610666275024414\n",
      "Current prediction:  60.72822189331055 \n",
      "\n",
      "Iteration 7737, Loss: 37.758392333984375, L1: 9.953352928161621, L3: 27.805038452148438\n",
      "Current prediction:  60.74541091918945 \n",
      "\n",
      "Iteration 7738, Loss: 35.40705490112305, L1: 9.931543350219727, L3: 25.47551155090332\n",
      "Current prediction:  61.40837097167969 \n",
      "\n",
      "Iteration 7739, Loss: 35.85512924194336, L1: 9.823732376098633, L3: 26.031396865844727\n",
      "Current prediction:  61.66099548339844 \n",
      "\n",
      "Iteration 7740, Loss: 36.117713928222656, L1: 9.825925827026367, L3: 26.29178810119629\n",
      "Current prediction:  61.6587028503418 \n",
      "\n",
      "Iteration 7741, Loss: 35.02017593383789, L1: 9.903075218200684, L3: 25.117101669311523\n",
      "Current prediction:  61.64839553833008 \n",
      "\n",
      "Iteration 7742, Loss: 36.02544021606445, L1: 9.870349884033203, L3: 26.15509033203125\n",
      "Current prediction:  61.633663177490234 \n",
      "\n",
      "Iteration 7743, Loss: 36.15353012084961, L1: 9.882543563842773, L3: 26.270986557006836\n",
      "Current prediction:  61.609397888183594 \n",
      "\n",
      "Iteration 7744, Loss: 36.811256408691406, L1: 9.921987533569336, L3: 26.88926887512207\n",
      "Current prediction:  61.48287582397461 \n",
      "\n",
      "Iteration 7745, Loss: 35.71470260620117, L1: 9.8973970413208, L3: 25.817306518554688\n",
      "Current prediction:  60.72953796386719 \n",
      "\n",
      "Iteration 7746, Loss: 35.977745056152344, L1: 9.923662185668945, L3: 26.0540828704834\n",
      "Current prediction:  60.61558532714844 \n",
      "\n",
      "Iteration 7747, Loss: 35.88483810424805, L1: 9.999581336975098, L3: 25.885257720947266\n",
      "Current prediction:  60.59184646606445 \n",
      "\n",
      "Iteration 7748, Loss: 36.19833755493164, L1: 10.092364311218262, L3: 26.105972290039062\n",
      "Current prediction:  60.58058547973633 \n",
      "\n",
      "Iteration 7749, Loss: 35.76123809814453, L1: 10.030206680297852, L3: 25.731029510498047\n",
      "Current prediction:  60.66299819946289 \n",
      "\n",
      "Iteration 7750, Loss: 36.53026580810547, L1: 9.983953475952148, L3: 26.54631233215332\n",
      "Current prediction:  61.45405960083008 \n",
      "\n",
      "Iteration 7751, Loss: 36.695716857910156, L1: 9.890135765075684, L3: 26.80558204650879\n",
      "Current prediction:  61.4962158203125 \n",
      "\n",
      "Iteration 7752, Loss: 36.154483795166016, L1: 9.996126174926758, L3: 26.158357620239258\n",
      "Current prediction:  61.50172805786133 \n",
      "\n",
      "Iteration 7753, Loss: 36.945980072021484, L1: 10.04478931427002, L3: 26.90118980407715\n",
      "Current prediction:  61.51216506958008 \n",
      "\n",
      "Iteration 7754, Loss: 37.18019104003906, L1: 10.018437385559082, L3: 27.161752700805664\n",
      "Current prediction:  61.520286560058594 \n",
      "\n",
      "Iteration 7755, Loss: 36.17150115966797, L1: 10.032500267028809, L3: 26.138999938964844\n",
      "Current prediction:  61.528114318847656 \n",
      "\n",
      "Iteration 7756, Loss: 36.19363021850586, L1: 9.981846809387207, L3: 26.211782455444336\n",
      "Current prediction:  61.532508850097656 \n",
      "\n",
      "Iteration 7757, Loss: 36.54990768432617, L1: 9.999712944030762, L3: 26.550193786621094\n",
      "Current prediction:  61.53582763671875 \n",
      "\n",
      "Iteration 7758, Loss: 36.44733810424805, L1: 9.948932647705078, L3: 26.49840545654297\n",
      "Current prediction:  61.5325813293457 \n",
      "\n",
      "Iteration 7759, Loss: 35.846561431884766, L1: 10.003880500793457, L3: 25.842679977416992\n",
      "Current prediction:  61.409915924072266 \n",
      "\n",
      "Iteration 7760, Loss: 35.65065002441406, L1: 9.932596206665039, L3: 25.718055725097656\n",
      "Current prediction:  60.7383918762207 \n",
      "\n",
      "Iteration 7761, Loss: 36.18964385986328, L1: 9.967535972595215, L3: 26.22210693359375\n",
      "Current prediction:  60.60697555541992 \n",
      "\n",
      "Iteration 7762, Loss: 36.09391784667969, L1: 9.994890213012695, L3: 26.09902572631836\n",
      "Current prediction:  60.612693786621094 \n",
      "\n",
      "Iteration 7763, Loss: 36.75008010864258, L1: 10.038403511047363, L3: 26.71167755126953\n",
      "Current prediction:  60.6438102722168 \n",
      "\n",
      "Iteration 7764, Loss: 37.20973587036133, L1: 9.959412574768066, L3: 27.250322341918945\n",
      "Current prediction:  61.003028869628906 \n",
      "\n",
      "Iteration 7765, Loss: 36.07145309448242, L1: 9.921340942382812, L3: 26.15011215209961\n",
      "Current prediction:  61.56381607055664 \n",
      "\n",
      "Iteration 7766, Loss: 36.73930358886719, L1: 9.896870613098145, L3: 26.84243392944336\n",
      "Current prediction:  61.60254669189453 \n",
      "\n",
      "Iteration 7767, Loss: 35.796630859375, L1: 9.897287368774414, L3: 25.899343490600586\n",
      "Current prediction:  61.62013626098633 \n",
      "\n",
      "Iteration 7768, Loss: 36.43328094482422, L1: 9.899377822875977, L3: 26.533903121948242\n",
      "Current prediction:  61.62865447998047 \n",
      "\n",
      "Iteration 7769, Loss: 36.529537200927734, L1: 9.925046920776367, L3: 26.604490280151367\n",
      "Current prediction:  61.63228225708008 \n",
      "\n",
      "Iteration 7770, Loss: 35.86233139038086, L1: 9.893521308898926, L3: 25.968809127807617\n",
      "Current prediction:  61.62997817993164 \n",
      "\n",
      "Iteration 7771, Loss: 35.76063537597656, L1: 9.910868644714355, L3: 25.849767684936523\n",
      "Current prediction:  61.6234130859375 \n",
      "\n",
      "Iteration 7772, Loss: 35.107025146484375, L1: 9.897871971130371, L3: 25.209152221679688\n",
      "Current prediction:  61.576316833496094 \n",
      "\n",
      "Iteration 7773, Loss: 35.70650863647461, L1: 9.823582649230957, L3: 25.882925033569336\n",
      "Current prediction:  60.92036437988281 \n",
      "\n",
      "Iteration 7774, Loss: 36.20439147949219, L1: 9.84572982788086, L3: 26.358659744262695\n",
      "Current prediction:  60.687320709228516 \n",
      "\n",
      "Iteration 7775, Loss: 36.18742752075195, L1: 9.93494701385498, L3: 26.25248146057129\n",
      "Current prediction:  60.68617248535156 \n",
      "\n",
      "Iteration 7776, Loss: 36.380531311035156, L1: 9.991909980773926, L3: 26.388622283935547\n",
      "Current prediction:  60.690223693847656 \n",
      "\n",
      "Iteration 7777, Loss: 34.7972297668457, L1: 9.97441291809082, L3: 24.822816848754883\n",
      "Current prediction:  60.71132278442383 \n",
      "\n",
      "Iteration 7778, Loss: 35.71546936035156, L1: 9.898887634277344, L3: 25.81658363342285\n",
      "Current prediction:  60.79775619506836 \n",
      "\n",
      "Iteration 7779, Loss: 35.10435485839844, L1: 9.845754623413086, L3: 25.25859832763672\n",
      "Current prediction:  61.20530319213867 \n",
      "\n",
      "Iteration 7780, Loss: 36.735069274902344, L1: 9.791964530944824, L3: 26.943103790283203\n",
      "Current prediction:  61.537105560302734 \n",
      "\n",
      "Iteration 7781, Loss: 36.20457077026367, L1: 9.8216552734375, L3: 26.382915496826172\n",
      "Current prediction:  61.53426742553711 \n",
      "\n",
      "Iteration 7782, Loss: 35.985103607177734, L1: 9.83602523803711, L3: 26.149078369140625\n",
      "Current prediction:  61.45257568359375 \n",
      "\n",
      "Iteration 7783, Loss: 36.553749084472656, L1: 9.7886381149292, L3: 26.76511001586914\n",
      "Current prediction:  61.038734436035156 \n",
      "\n",
      "Iteration 7784, Loss: 36.151607513427734, L1: 9.842780113220215, L3: 26.308828353881836\n",
      "Current prediction:  60.8603515625 \n",
      "\n",
      "Iteration 7785, Loss: 35.333709716796875, L1: 9.864486694335938, L3: 25.469221115112305\n",
      "Current prediction:  60.76959991455078 \n",
      "\n",
      "Iteration 7786, Loss: 37.35509490966797, L1: 9.873130798339844, L3: 27.481962203979492\n",
      "Current prediction:  60.7379150390625 \n",
      "\n",
      "Iteration 7787, Loss: 37.54590606689453, L1: 9.889446258544922, L3: 27.656461715698242\n",
      "Current prediction:  60.76980209350586 \n",
      "\n",
      "Iteration 7788, Loss: 36.50362014770508, L1: 9.892909049987793, L3: 26.6107120513916\n",
      "Current prediction:  60.841590881347656 \n",
      "\n",
      "Iteration 7789, Loss: 35.94572448730469, L1: 9.867106437683105, L3: 26.078617095947266\n",
      "Current prediction:  61.24966812133789 \n",
      "\n",
      "Iteration 7790, Loss: 36.287139892578125, L1: 9.875438690185547, L3: 26.411699295043945\n",
      "Current prediction:  61.50468826293945 \n",
      "\n",
      "Iteration 7791, Loss: 36.51796340942383, L1: 9.885905265808105, L3: 26.632057189941406\n",
      "Current prediction:  61.479244232177734 \n",
      "\n",
      "Iteration 7792, Loss: 35.731056213378906, L1: 9.894279479980469, L3: 25.836776733398438\n",
      "Current prediction:  61.35814666748047 \n",
      "\n",
      "Iteration 7793, Loss: 37.292572021484375, L1: 9.842470169067383, L3: 27.45009994506836\n",
      "Current prediction:  60.935951232910156 \n",
      "\n",
      "Iteration 7794, Loss: 35.63511657714844, L1: 9.931159019470215, L3: 25.703956604003906\n",
      "Current prediction:  61.01017379760742 \n",
      "\n",
      "Iteration 7795, Loss: 36.47005844116211, L1: 9.887928009033203, L3: 26.582130432128906\n",
      "Current prediction:  61.27180862426758 \n",
      "\n",
      "Iteration 7796, Loss: 35.68455505371094, L1: 9.912062644958496, L3: 25.772493362426758\n",
      "Current prediction:  61.084625244140625 \n",
      "\n",
      "Iteration 7797, Loss: 35.77021789550781, L1: 9.885251998901367, L3: 25.884965896606445\n",
      "Current prediction:  60.73027038574219 \n",
      "\n",
      "Iteration 7798, Loss: 36.15842056274414, L1: 9.968894004821777, L3: 26.18952751159668\n",
      "Current prediction:  60.66228485107422 \n",
      "\n",
      "Iteration 7799, Loss: 36.05784225463867, L1: 9.957900047302246, L3: 26.09994125366211\n",
      "Current prediction:  60.691253662109375 \n",
      "\n",
      "Iteration 7800, Loss: 36.2452507019043, L1: 9.911377906799316, L3: 26.333871841430664\n",
      "Current prediction:  60.68326187133789 \n",
      "\n",
      "Iteration 7801, Loss: 35.469200134277344, L1: 9.91763687133789, L3: 25.551563262939453\n",
      "Current prediction:  60.8475227355957 \n",
      "\n",
      "Iteration 7802, Loss: 35.92856216430664, L1: 9.88647174835205, L3: 26.042091369628906\n",
      "Current prediction:  61.48591995239258 \n",
      "\n",
      "Iteration 7803, Loss: 36.62501525878906, L1: 9.834375381469727, L3: 26.790637969970703\n",
      "Current prediction:  61.54949188232422 \n",
      "\n",
      "Iteration 7804, Loss: 35.9377326965332, L1: 9.819260597229004, L3: 26.118473052978516\n",
      "Current prediction:  61.54227828979492 \n",
      "\n",
      "Iteration 7805, Loss: 35.94896697998047, L1: 9.841493606567383, L3: 26.10747528076172\n",
      "Current prediction:  61.42178726196289 \n",
      "\n",
      "Iteration 7806, Loss: 37.55695343017578, L1: 9.80348014831543, L3: 27.75347137451172\n",
      "Current prediction:  61.195343017578125 \n",
      "\n",
      "Iteration 7807, Loss: 36.09413146972656, L1: 9.848992347717285, L3: 26.24513816833496\n",
      "Current prediction:  61.08881378173828 \n",
      "\n",
      "Iteration 7808, Loss: 35.62281799316406, L1: 9.803141593933105, L3: 25.819677352905273\n",
      "Current prediction:  61.04909896850586 \n",
      "\n",
      "Iteration 7809, Loss: 36.0396728515625, L1: 9.850943565368652, L3: 26.18872833251953\n",
      "Current prediction:  61.278079986572266 \n",
      "\n",
      "Iteration 7810, Loss: 35.70854949951172, L1: 9.848400115966797, L3: 25.860149383544922\n",
      "Current prediction:  61.382625579833984 \n",
      "\n",
      "Iteration 7811, Loss: 35.99876022338867, L1: 9.839944839477539, L3: 26.158815383911133\n",
      "Current prediction:  61.496700286865234 \n",
      "\n",
      "Iteration 7812, Loss: 36.70683670043945, L1: 9.819899559020996, L3: 26.886938095092773\n",
      "Current prediction:  61.19802474975586 \n",
      "\n",
      "Iteration 7813, Loss: 36.566184997558594, L1: 9.830879211425781, L3: 26.73530387878418\n",
      "Current prediction:  60.97215270996094 \n",
      "\n",
      "Iteration 7814, Loss: 36.26201248168945, L1: 9.856247901916504, L3: 26.405765533447266\n",
      "Current prediction:  60.81279373168945 \n",
      "\n",
      "Iteration 7815, Loss: 36.38933563232422, L1: 9.897553443908691, L3: 26.49178123474121\n",
      "Current prediction:  60.82078170776367 \n",
      "\n",
      "Iteration 7816, Loss: 36.47057342529297, L1: 9.820840835571289, L3: 26.649734497070312\n",
      "Current prediction:  60.815673828125 \n",
      "\n",
      "Iteration 7817, Loss: 36.33768844604492, L1: 9.940464973449707, L3: 26.39722442626953\n",
      "Current prediction:  61.08453369140625 \n",
      "\n",
      "Iteration 7818, Loss: 34.68028259277344, L1: 9.897257804870605, L3: 24.78302574157715\n",
      "Current prediction:  61.399715423583984 \n",
      "\n",
      "Iteration 7819, Loss: 36.97367477416992, L1: 9.854045867919922, L3: 27.11962890625\n",
      "Current prediction:  61.394657135009766 \n",
      "\n",
      "Iteration 7820, Loss: 36.21343231201172, L1: 9.911993026733398, L3: 26.301437377929688\n",
      "Current prediction:  61.3567008972168 \n",
      "\n",
      "Iteration 7821, Loss: 36.38438415527344, L1: 9.910996437072754, L3: 26.473386764526367\n",
      "Current prediction:  61.38304138183594 \n",
      "\n",
      "Iteration 7822, Loss: 36.37944412231445, L1: 9.897644996643066, L3: 26.481800079345703\n",
      "Current prediction:  61.184967041015625 \n",
      "\n",
      "Iteration 7823, Loss: 36.0321044921875, L1: 9.909579277038574, L3: 26.12252426147461\n",
      "Current prediction:  60.6666374206543 \n",
      "\n",
      "Iteration 7824, Loss: 35.734859466552734, L1: 9.916825294494629, L3: 25.81803321838379\n",
      "Current prediction:  60.58766555786133 \n",
      "\n",
      "Iteration 7825, Loss: 35.50749206542969, L1: 10.015776634216309, L3: 25.491714477539062\n",
      "Current prediction:  60.58165740966797 \n",
      "\n",
      "Iteration 7826, Loss: 36.941062927246094, L1: 10.045116424560547, L3: 26.895946502685547\n",
      "Current prediction:  60.59627151489258 \n",
      "\n",
      "Iteration 7827, Loss: 35.68732452392578, L1: 10.022199630737305, L3: 25.665122985839844\n",
      "Current prediction:  60.86366271972656 \n",
      "\n",
      "Iteration 7828, Loss: 35.0136833190918, L1: 9.881345748901367, L3: 25.13233757019043\n",
      "Current prediction:  61.50374221801758 \n",
      "\n",
      "Iteration 7829, Loss: 35.81132888793945, L1: 9.915772438049316, L3: 25.89555549621582\n",
      "Current prediction:  61.550838470458984 \n",
      "\n",
      "Iteration 7830, Loss: 35.34918212890625, L1: 9.94546127319336, L3: 25.403718948364258\n",
      "Current prediction:  61.56234359741211 \n",
      "\n",
      "Iteration 7831, Loss: 34.7972412109375, L1: 9.948867797851562, L3: 24.84837532043457\n",
      "Current prediction:  61.58384704589844 \n",
      "\n",
      "Iteration 7832, Loss: 37.256587982177734, L1: 9.876423835754395, L3: 27.380163192749023\n",
      "Current prediction:  61.605098724365234 \n",
      "\n",
      "Iteration 7833, Loss: 37.12877655029297, L1: 9.904362678527832, L3: 27.224414825439453\n",
      "Current prediction:  61.619014739990234 \n",
      "\n",
      "Iteration 7834, Loss: 35.79951858520508, L1: 9.909746170043945, L3: 25.889772415161133\n",
      "Current prediction:  61.62845230102539 \n",
      "\n",
      "Iteration 7835, Loss: 36.338356018066406, L1: 9.852041244506836, L3: 26.48631477355957\n",
      "Current prediction:  61.61448287963867 \n",
      "\n",
      "Iteration 7836, Loss: 36.31431579589844, L1: 9.87635326385498, L3: 26.43796157836914\n",
      "Current prediction:  61.471405029296875 \n",
      "\n",
      "Iteration 7837, Loss: 36.644996643066406, L1: 9.857304573059082, L3: 26.78769302368164\n",
      "Current prediction:  60.85264205932617 \n",
      "\n",
      "Iteration 7838, Loss: 36.678287506103516, L1: 9.8515625, L3: 26.826725006103516\n",
      "Current prediction:  60.70433044433594 \n",
      "\n",
      "Iteration 7839, Loss: 36.266021728515625, L1: 9.87629222869873, L3: 26.389728546142578\n",
      "Current prediction:  60.70111846923828 \n",
      "\n",
      "Iteration 7840, Loss: 37.840293884277344, L1: 9.941598892211914, L3: 27.898693084716797\n",
      "Current prediction:  60.85282897949219 \n",
      "\n",
      "Iteration 7841, Loss: 36.725521087646484, L1: 9.871360778808594, L3: 26.85416030883789\n",
      "Current prediction:  61.375328063964844 \n",
      "\n",
      "Iteration 7842, Loss: 37.07258987426758, L1: 9.859827041625977, L3: 27.2127628326416\n",
      "Current prediction:  61.554359436035156 \n",
      "\n",
      "Iteration 7843, Loss: 36.75544738769531, L1: 9.869510650634766, L3: 26.885936737060547\n",
      "Current prediction:  61.5624885559082 \n",
      "\n",
      "Iteration 7844, Loss: 35.44402313232422, L1: 9.87641716003418, L3: 25.56760597229004\n",
      "Current prediction:  61.542701721191406 \n",
      "\n",
      "Iteration 7845, Loss: 35.82685470581055, L1: 9.911117553710938, L3: 25.91573715209961\n",
      "Current prediction:  61.4986572265625 \n",
      "\n",
      "Iteration 7846, Loss: 35.775390625, L1: 9.873774528503418, L3: 25.9016170501709\n",
      "Current prediction:  61.2901496887207 \n",
      "\n",
      "Iteration 7847, Loss: 36.69103240966797, L1: 9.900699615478516, L3: 26.79033088684082\n",
      "Current prediction:  60.81742477416992 \n",
      "\n",
      "Iteration 7848, Loss: 36.18151092529297, L1: 9.917098045349121, L3: 26.26441192626953\n",
      "Current prediction:  60.679908752441406 \n",
      "\n",
      "Iteration 7849, Loss: 35.855712890625, L1: 9.927848815917969, L3: 25.927865982055664\n",
      "Current prediction:  60.63775634765625 \n",
      "\n",
      "Iteration 7850, Loss: 35.917999267578125, L1: 9.969634056091309, L3: 25.948366165161133\n",
      "Current prediction:  60.64952850341797 \n",
      "\n",
      "Iteration 7851, Loss: 35.919349670410156, L1: 10.009889602661133, L3: 25.909460067749023\n",
      "Current prediction:  60.759273529052734 \n",
      "\n",
      "Iteration 7852, Loss: 35.64014434814453, L1: 9.926855087280273, L3: 25.713287353515625\n",
      "Current prediction:  61.11925506591797 \n",
      "\n",
      "Iteration 7853, Loss: 35.384681701660156, L1: 9.860928535461426, L3: 25.523754119873047\n",
      "Current prediction:  61.561859130859375 \n",
      "\n",
      "Iteration 7854, Loss: 36.518272399902344, L1: 9.825239181518555, L3: 26.693031311035156\n",
      "Current prediction:  61.62263488769531 \n",
      "\n",
      "Iteration 7855, Loss: 36.733638763427734, L1: 9.874859809875488, L3: 26.85877799987793\n",
      "Current prediction:  61.63768005371094 \n",
      "\n",
      "Iteration 7856, Loss: 35.79439926147461, L1: 9.911700248718262, L3: 25.882699966430664\n",
      "Current prediction:  61.638710021972656 \n",
      "\n",
      "Iteration 7857, Loss: 36.0612678527832, L1: 9.90708065032959, L3: 26.154186248779297\n",
      "Current prediction:  61.63444900512695 \n",
      "\n",
      "Iteration 7858, Loss: 36.146663665771484, L1: 9.886797904968262, L3: 26.259864807128906\n",
      "Current prediction:  61.627105712890625 \n",
      "\n",
      "Iteration 7859, Loss: 36.454734802246094, L1: 9.900630950927734, L3: 26.554105758666992\n",
      "Current prediction:  61.60292053222656 \n",
      "\n",
      "Iteration 7860, Loss: 35.753604888916016, L1: 9.92284107208252, L3: 25.830764770507812\n",
      "Current prediction:  61.49568176269531 \n",
      "\n",
      "Iteration 7861, Loss: 36.96177291870117, L1: 9.862715721130371, L3: 27.099056243896484\n",
      "Current prediction:  60.847164154052734 \n",
      "\n",
      "Iteration 7862, Loss: 36.095672607421875, L1: 9.915887832641602, L3: 26.179784774780273\n",
      "Current prediction:  60.63031005859375 \n",
      "\n",
      "Iteration 7863, Loss: 35.48409652709961, L1: 9.920958518981934, L3: 25.56313705444336\n",
      "Current prediction:  60.623146057128906 \n",
      "\n",
      "Iteration 7864, Loss: 36.3395881652832, L1: 9.964851379394531, L3: 26.374736785888672\n",
      "Current prediction:  60.63576126098633 \n",
      "\n",
      "Iteration 7865, Loss: 36.087955474853516, L1: 9.950628280639648, L3: 26.137327194213867\n",
      "Current prediction:  60.84431076049805 \n",
      "\n",
      "Iteration 7866, Loss: 36.57017517089844, L1: 9.865898132324219, L3: 26.704275131225586\n",
      "Current prediction:  61.524024963378906 \n",
      "\n",
      "Iteration 7867, Loss: 35.4670524597168, L1: 9.855162620544434, L3: 25.611888885498047\n",
      "Current prediction:  61.57731628417969 \n",
      "\n",
      "Iteration 7868, Loss: 36.02018356323242, L1: 9.86475944519043, L3: 26.155424118041992\n",
      "Current prediction:  61.586952209472656 \n",
      "\n",
      "Iteration 7869, Loss: 36.073822021484375, L1: 9.863170623779297, L3: 26.210651397705078\n",
      "Current prediction:  61.55644989013672 \n",
      "\n",
      "Iteration 7870, Loss: 36.202552795410156, L1: 9.838698387145996, L3: 26.363855361938477\n",
      "Current prediction:  61.016265869140625 \n",
      "\n",
      "Iteration 7871, Loss: 36.77116394042969, L1: 9.828163146972656, L3: 26.943002700805664\n",
      "Current prediction:  60.89612579345703 \n",
      "\n",
      "Iteration 7872, Loss: 35.391380310058594, L1: 9.828161239624023, L3: 25.563220977783203\n",
      "Current prediction:  61.16208267211914 \n",
      "\n",
      "Iteration 7873, Loss: 35.60946273803711, L1: 9.842269897460938, L3: 25.767192840576172\n",
      "Current prediction:  61.436214447021484 \n",
      "\n",
      "Iteration 7874, Loss: 36.73299789428711, L1: 9.820634841918945, L3: 26.912363052368164\n",
      "Current prediction:  61.46500778198242 \n",
      "\n",
      "Iteration 7875, Loss: 35.7453727722168, L1: 9.851201057434082, L3: 25.89417266845703\n",
      "Current prediction:  61.253807067871094 \n",
      "\n",
      "Iteration 7876, Loss: 34.88566589355469, L1: 9.834450721740723, L3: 25.05121612548828\n",
      "Current prediction:  60.87845993041992 \n",
      "\n",
      "Iteration 7877, Loss: 35.76310348510742, L1: 9.813324928283691, L3: 25.949779510498047\n",
      "Current prediction:  60.701683044433594 \n",
      "\n",
      "Iteration 7878, Loss: 34.88124084472656, L1: 9.866926193237305, L3: 25.014314651489258\n",
      "Current prediction:  60.68314743041992 \n",
      "\n",
      "Iteration 7879, Loss: 35.88627624511719, L1: 9.869293212890625, L3: 26.016984939575195\n",
      "Current prediction:  60.68183898925781 \n",
      "\n",
      "Iteration 7880, Loss: 36.900848388671875, L1: 9.920759201049805, L3: 26.980091094970703\n",
      "Current prediction:  60.71144104003906 \n",
      "\n",
      "Iteration 7881, Loss: 35.26499938964844, L1: 9.875896453857422, L3: 25.389101028442383\n",
      "Current prediction:  61.096126556396484 \n",
      "\n",
      "Iteration 7882, Loss: 36.840938568115234, L1: 9.852107048034668, L3: 26.98883056640625\n",
      "Current prediction:  61.56242370605469 \n",
      "\n",
      "Iteration 7883, Loss: 35.388954162597656, L1: 9.85400104522705, L3: 25.53495216369629\n",
      "Current prediction:  61.6232795715332 \n",
      "\n",
      "Iteration 7884, Loss: 36.04865264892578, L1: 9.84916877746582, L3: 26.199485778808594\n",
      "Current prediction:  61.626522064208984 \n",
      "\n",
      "Iteration 7885, Loss: 35.75201416015625, L1: 9.844905853271484, L3: 25.907106399536133\n",
      "Current prediction:  61.617881774902344 \n",
      "\n",
      "Iteration 7886, Loss: 35.76918029785156, L1: 9.819389343261719, L3: 25.949790954589844\n",
      "Current prediction:  61.53511428833008 \n",
      "\n",
      "Iteration 7887, Loss: 36.67873001098633, L1: 9.829752922058105, L3: 26.84897804260254\n",
      "Current prediction:  61.120849609375 \n",
      "\n",
      "Iteration 7888, Loss: 36.56317901611328, L1: 9.79625129699707, L3: 26.76692771911621\n",
      "Current prediction:  60.7227783203125 \n",
      "\n",
      "Iteration 7889, Loss: 35.1054801940918, L1: 9.839268684387207, L3: 25.266210556030273\n",
      "Current prediction:  60.8428955078125 \n",
      "\n",
      "Iteration 7890, Loss: 35.557655334472656, L1: 9.836347579956055, L3: 25.72130584716797\n",
      "Current prediction:  61.38986587524414 \n",
      "\n",
      "Iteration 7891, Loss: 36.482425689697266, L1: 9.844551086425781, L3: 26.637874603271484\n",
      "Current prediction:  61.574378967285156 \n",
      "\n",
      "Iteration 7892, Loss: 35.93830490112305, L1: 9.861584663391113, L3: 26.07672119140625\n",
      "Current prediction:  61.58319091796875 \n",
      "\n",
      "Iteration 7893, Loss: 36.70318603515625, L1: 9.91753101348877, L3: 26.785654067993164\n",
      "Current prediction:  61.582374572753906 \n",
      "\n",
      "Iteration 7894, Loss: 37.04389190673828, L1: 9.884296417236328, L3: 27.15959358215332\n",
      "Current prediction:  61.57170867919922 \n",
      "\n",
      "Iteration 7895, Loss: 35.970821380615234, L1: 9.881298065185547, L3: 26.089523315429688\n",
      "Current prediction:  61.54591751098633 \n",
      "\n",
      "Iteration 7896, Loss: 36.50017547607422, L1: 9.871615409851074, L3: 26.628559112548828\n",
      "Current prediction:  61.48751449584961 \n",
      "\n",
      "Iteration 7897, Loss: 37.01835250854492, L1: 9.898467063903809, L3: 27.119884490966797\n",
      "Current prediction:  61.34891128540039 \n",
      "\n",
      "Iteration 7898, Loss: 34.74681854248047, L1: 9.810859680175781, L3: 24.93596076965332\n",
      "Current prediction:  61.294151306152344 \n",
      "\n",
      "Iteration 7899, Loss: 35.79961013793945, L1: 9.874818801879883, L3: 25.92479133605957\n",
      "Current prediction:  61.18362808227539 \n",
      "\n",
      "Iteration 7900, Loss: 36.11655044555664, L1: 9.810281753540039, L3: 26.3062686920166\n",
      "Current prediction:  60.92583084106445 \n",
      "\n",
      "Iteration 7901, Loss: 35.223297119140625, L1: 9.825324058532715, L3: 25.397974014282227\n",
      "Current prediction:  61.12851333618164 \n",
      "\n",
      "Iteration 7902, Loss: 36.239078521728516, L1: 9.824882507324219, L3: 26.414196014404297\n",
      "Current prediction:  61.329036712646484 \n",
      "\n",
      "Iteration 7903, Loss: 35.496803283691406, L1: 9.85214900970459, L3: 25.6446533203125\n",
      "Current prediction:  61.36650466918945 \n",
      "\n",
      "Iteration 7904, Loss: 37.174007415771484, L1: 9.886639595031738, L3: 27.287368774414062\n",
      "Current prediction:  61.12526321411133 \n",
      "\n",
      "Iteration 7905, Loss: 35.92128372192383, L1: 9.826533317565918, L3: 26.094751358032227\n",
      "Current prediction:  60.77623748779297 \n",
      "\n",
      "Iteration 7906, Loss: 35.74266052246094, L1: 9.921734809875488, L3: 25.820924758911133\n",
      "Current prediction:  60.63020324707031 \n",
      "\n",
      "Iteration 7907, Loss: 36.377525329589844, L1: 9.926321983337402, L3: 26.451202392578125\n",
      "Current prediction:  60.64106750488281 \n",
      "\n",
      "Iteration 7908, Loss: 36.302669525146484, L1: 9.90306282043457, L3: 26.399606704711914\n",
      "Current prediction:  60.699195861816406 \n",
      "\n",
      "Iteration 7909, Loss: 36.616539001464844, L1: 9.893482208251953, L3: 26.72305679321289\n",
      "Current prediction:  60.87617492675781 \n",
      "\n",
      "Iteration 7910, Loss: 36.67882537841797, L1: 9.838783264160156, L3: 26.84004020690918\n",
      "Current prediction:  61.53007888793945 \n",
      "\n",
      "Iteration 7911, Loss: 36.71355438232422, L1: 9.83999252319336, L3: 26.873559951782227\n",
      "Current prediction:  61.596405029296875 \n",
      "\n",
      "Iteration 7912, Loss: 35.8380012512207, L1: 9.871172904968262, L3: 25.966827392578125\n",
      "Current prediction:  61.59681701660156 \n",
      "\n",
      "Iteration 7913, Loss: 35.59993362426758, L1: 9.849562644958496, L3: 25.7503719329834\n",
      "Current prediction:  61.585906982421875 \n",
      "\n",
      "Iteration 7914, Loss: 35.28178024291992, L1: 9.848004341125488, L3: 25.43377685546875\n",
      "Current prediction:  61.547523498535156 \n",
      "\n",
      "Iteration 7915, Loss: 37.3424072265625, L1: 9.86966609954834, L3: 27.472740173339844\n",
      "Current prediction:  61.213279724121094 \n",
      "\n",
      "Iteration 7916, Loss: 34.9643440246582, L1: 9.845747947692871, L3: 25.118595123291016\n",
      "Current prediction:  60.69109344482422 \n",
      "\n",
      "Iteration 7917, Loss: 35.956565856933594, L1: 9.91991138458252, L3: 26.03665542602539\n",
      "Current prediction:  60.76498031616211 \n",
      "\n",
      "Iteration 7918, Loss: 35.59475326538086, L1: 9.896568298339844, L3: 25.698184967041016\n",
      "Current prediction:  61.16875457763672 \n",
      "\n",
      "Iteration 7919, Loss: 36.11105728149414, L1: 9.855841636657715, L3: 26.25521469116211\n",
      "Current prediction:  61.55401611328125 \n",
      "\n",
      "Iteration 7920, Loss: 35.538734436035156, L1: 9.89167308807373, L3: 25.64706039428711\n",
      "Current prediction:  61.5831184387207 \n",
      "\n",
      "Iteration 7921, Loss: 35.75293731689453, L1: 9.889945030212402, L3: 25.862993240356445\n",
      "Current prediction:  61.58852767944336 \n",
      "\n",
      "Iteration 7922, Loss: 35.57853317260742, L1: 9.877153396606445, L3: 25.701379776000977\n",
      "Current prediction:  61.595882415771484 \n",
      "\n",
      "Iteration 7923, Loss: 36.518917083740234, L1: 9.934699058532715, L3: 26.584218978881836\n",
      "Current prediction:  61.595726013183594 \n",
      "\n",
      "Iteration 7924, Loss: 36.55322265625, L1: 9.899646759033203, L3: 26.65357780456543\n",
      "Current prediction:  61.58949661254883 \n",
      "\n",
      "Iteration 7925, Loss: 35.60490036010742, L1: 9.861322402954102, L3: 25.74357795715332\n",
      "Current prediction:  61.57316207885742 \n",
      "\n",
      "Iteration 7926, Loss: 36.194580078125, L1: 9.877853393554688, L3: 26.31672477722168\n",
      "Current prediction:  61.521671295166016 \n",
      "\n",
      "Iteration 7927, Loss: 36.44329071044922, L1: 9.915820121765137, L3: 26.527469635009766\n",
      "Current prediction:  61.21023941040039 \n",
      "\n",
      "Iteration 7928, Loss: 36.666114807128906, L1: 9.822050094604492, L3: 26.844064712524414\n",
      "Current prediction:  60.777801513671875 \n",
      "\n",
      "Iteration 7929, Loss: 35.81226348876953, L1: 9.86532974243164, L3: 25.94693374633789\n",
      "Current prediction:  60.62574005126953 \n",
      "\n",
      "Iteration 7930, Loss: 36.099952697753906, L1: 9.985160827636719, L3: 26.11479377746582\n",
      "Current prediction:  60.61455154418945 \n",
      "\n",
      "Iteration 7931, Loss: 35.90318298339844, L1: 9.890174865722656, L3: 26.013010025024414\n",
      "Current prediction:  60.75566101074219 \n",
      "\n",
      "Iteration 7932, Loss: 36.219661712646484, L1: 9.942401885986328, L3: 26.277259826660156\n",
      "Current prediction:  61.075408935546875 \n",
      "\n",
      "Iteration 7933, Loss: 35.51691436767578, L1: 9.849189758300781, L3: 25.667722702026367\n",
      "Current prediction:  61.424747467041016 \n",
      "\n",
      "Iteration 7934, Loss: 36.053226470947266, L1: 9.884831428527832, L3: 26.16839599609375\n",
      "Current prediction:  61.51305389404297 \n",
      "\n",
      "Iteration 7935, Loss: 37.09288024902344, L1: 9.893442153930664, L3: 27.199440002441406\n",
      "Current prediction:  61.52861785888672 \n",
      "\n",
      "Iteration 7936, Loss: 35.582916259765625, L1: 9.878732681274414, L3: 25.70418357849121\n",
      "Current prediction:  61.36967086791992 \n",
      "\n",
      "Iteration 7937, Loss: 36.47180938720703, L1: 9.860366821289062, L3: 26.6114444732666\n",
      "Current prediction:  60.79316329956055 \n",
      "\n",
      "Iteration 7938, Loss: 36.144813537597656, L1: 9.913572311401367, L3: 26.23124122619629\n",
      "Current prediction:  60.616817474365234 \n",
      "\n",
      "Iteration 7939, Loss: 34.64919662475586, L1: 9.954737663269043, L3: 24.694459915161133\n",
      "Current prediction:  60.611839294433594 \n",
      "\n",
      "Iteration 7940, Loss: 36.56186294555664, L1: 9.954644203186035, L3: 26.607219696044922\n",
      "Current prediction:  60.62099838256836 \n",
      "\n",
      "Iteration 7941, Loss: 35.736053466796875, L1: 9.971029281616211, L3: 25.765024185180664\n",
      "Current prediction:  60.722286224365234 \n",
      "\n",
      "Iteration 7942, Loss: 35.53998565673828, L1: 9.897415161132812, L3: 25.642568588256836\n",
      "Current prediction:  60.9273796081543 \n",
      "\n",
      "Iteration 7943, Loss: 36.52192306518555, L1: 9.835541725158691, L3: 26.68638038635254\n",
      "Current prediction:  61.2291259765625 \n",
      "\n",
      "Iteration 7944, Loss: 36.08283615112305, L1: 9.784669876098633, L3: 26.298166275024414\n",
      "Current prediction:  61.49278259277344 \n",
      "\n",
      "Iteration 7945, Loss: 35.74219512939453, L1: 9.756001472473145, L3: 25.986194610595703\n",
      "Current prediction:  61.47731399536133 \n",
      "\n",
      "Iteration 7946, Loss: 36.44116973876953, L1: 9.800856590270996, L3: 26.64031219482422\n",
      "Current prediction:  61.297279357910156 \n",
      "\n",
      "Iteration 7947, Loss: 35.19215774536133, L1: 9.808815956115723, L3: 25.38334083557129\n",
      "Current prediction:  60.902496337890625 \n",
      "\n",
      "Iteration 7948, Loss: 35.462432861328125, L1: 9.841118812561035, L3: 25.621313095092773\n",
      "Current prediction:  60.93494415283203 \n",
      "\n",
      "Iteration 7949, Loss: 37.54346466064453, L1: 9.804974555969238, L3: 27.73849105834961\n",
      "Current prediction:  61.25053405761719 \n",
      "\n",
      "Iteration 7950, Loss: 36.22685241699219, L1: 9.757423400878906, L3: 26.469430923461914\n",
      "Current prediction:  61.51919937133789 \n",
      "\n",
      "Iteration 7951, Loss: 35.98451232910156, L1: 9.854963302612305, L3: 26.12955093383789\n",
      "Current prediction:  61.555049896240234 \n",
      "\n",
      "Iteration 7952, Loss: 36.74570083618164, L1: 9.811253547668457, L3: 26.934446334838867\n",
      "Current prediction:  61.36186599731445 \n",
      "\n",
      "Iteration 7953, Loss: 35.21962356567383, L1: 9.887024879455566, L3: 25.332597732543945\n",
      "Current prediction:  60.959712982177734 \n",
      "\n",
      "Iteration 7954, Loss: 36.11650848388672, L1: 9.828948974609375, L3: 26.287559509277344\n",
      "Current prediction:  60.801795959472656 \n",
      "\n",
      "Iteration 7955, Loss: 34.89574432373047, L1: 9.834696769714355, L3: 25.061046600341797\n",
      "Current prediction:  60.83015060424805 \n",
      "\n",
      "Iteration 7956, Loss: 36.07959747314453, L1: 9.867074012756348, L3: 26.212522506713867\n",
      "Current prediction:  60.84323501586914 \n",
      "\n",
      "Iteration 7957, Loss: 35.54736328125, L1: 9.840620994567871, L3: 25.706743240356445\n",
      "Current prediction:  61.087921142578125 \n",
      "\n",
      "Iteration 7958, Loss: 35.312255859375, L1: 9.886470794677734, L3: 25.425785064697266\n",
      "Current prediction:  61.339107513427734 \n",
      "\n",
      "Iteration 7959, Loss: 37.55067825317383, L1: 9.821951866149902, L3: 27.728727340698242\n",
      "Current prediction:  61.3758544921875 \n",
      "\n",
      "Iteration 7960, Loss: 36.904048919677734, L1: 9.878291130065918, L3: 27.0257568359375\n",
      "Current prediction:  61.08742904663086 \n",
      "\n",
      "Iteration 7961, Loss: 36.097164154052734, L1: 9.860727310180664, L3: 26.23643684387207\n",
      "Current prediction:  60.97379684448242 \n",
      "\n",
      "Iteration 7962, Loss: 36.038330078125, L1: 9.895703315734863, L3: 26.142627716064453\n",
      "Current prediction:  61.068939208984375 \n",
      "\n",
      "Iteration 7963, Loss: 35.6815185546875, L1: 9.858052253723145, L3: 25.82346534729004\n",
      "Current prediction:  61.07931137084961 \n",
      "\n",
      "Iteration 7964, Loss: 36.74050521850586, L1: 9.863080024719238, L3: 26.877424240112305\n",
      "Current prediction:  60.9969596862793 \n",
      "\n",
      "Iteration 7965, Loss: 36.32522964477539, L1: 9.8433198928833, L3: 26.481908798217773\n",
      "Current prediction:  60.965309143066406 \n",
      "\n",
      "Iteration 7966, Loss: 35.643218994140625, L1: 9.908736228942871, L3: 25.73448371887207\n",
      "Current prediction:  61.21295166015625 \n",
      "\n",
      "Iteration 7967, Loss: 35.73769760131836, L1: 9.833643913269043, L3: 25.904052734375\n",
      "Current prediction:  61.421119689941406 \n",
      "\n",
      "Iteration 7968, Loss: 36.269771575927734, L1: 9.839902877807617, L3: 26.429868698120117\n",
      "Current prediction:  61.46217727661133 \n",
      "\n",
      "Iteration 7969, Loss: 36.36579895019531, L1: 9.842809677124023, L3: 26.522991180419922\n",
      "Current prediction:  61.28495407104492 \n",
      "\n",
      "Iteration 7970, Loss: 35.40048599243164, L1: 9.804709434509277, L3: 25.59577751159668\n",
      "Current prediction:  60.90730285644531 \n",
      "\n",
      "Iteration 7971, Loss: 36.32839584350586, L1: 9.87053394317627, L3: 26.457862854003906\n",
      "Current prediction:  60.7789306640625 \n",
      "\n",
      "Iteration 7972, Loss: 36.025108337402344, L1: 9.837385177612305, L3: 26.187725067138672\n",
      "Current prediction:  60.94385528564453 \n",
      "\n",
      "Iteration 7973, Loss: 36.57483673095703, L1: 9.915055274963379, L3: 26.659780502319336\n",
      "Current prediction:  61.24688720703125 \n",
      "\n",
      "Iteration 7974, Loss: 35.59111022949219, L1: 9.845169067382812, L3: 25.745943069458008\n",
      "Current prediction:  61.54995346069336 \n",
      "\n",
      "Iteration 7975, Loss: 35.3756103515625, L1: 9.783127784729004, L3: 25.59248161315918\n",
      "Current prediction:  61.60547637939453 \n",
      "\n",
      "Iteration 7976, Loss: 36.953067779541016, L1: 9.838837623596191, L3: 27.11423110961914\n",
      "Current prediction:  61.607093811035156 \n",
      "\n",
      "Iteration 7977, Loss: 36.277618408203125, L1: 9.792998313903809, L3: 26.484619140625\n",
      "Current prediction:  61.574127197265625 \n",
      "\n",
      "Iteration 7978, Loss: 36.61725616455078, L1: 9.852705001831055, L3: 26.764549255371094\n",
      "Current prediction:  61.35456085205078 \n",
      "\n",
      "Iteration 7979, Loss: 36.08068084716797, L1: 9.808707237243652, L3: 26.27197265625\n",
      "Current prediction:  60.91718673706055 \n",
      "\n",
      "Iteration 7980, Loss: 35.50660705566406, L1: 9.793012619018555, L3: 25.713594436645508\n",
      "Current prediction:  60.87074279785156 \n",
      "\n",
      "Iteration 7981, Loss: 35.71296691894531, L1: 9.786405563354492, L3: 25.926559448242188\n",
      "Current prediction:  61.17545700073242 \n",
      "\n",
      "Iteration 7982, Loss: 36.29199981689453, L1: 9.812454223632812, L3: 26.479543685913086\n",
      "Current prediction:  61.48135757446289 \n",
      "\n",
      "Iteration 7983, Loss: 35.926090240478516, L1: 9.864218711853027, L3: 26.061872482299805\n",
      "Current prediction:  61.55735778808594 \n",
      "\n",
      "Iteration 7984, Loss: 35.88386535644531, L1: 9.889934539794922, L3: 25.99393081665039\n",
      "Current prediction:  61.54644012451172 \n",
      "\n",
      "Iteration 7985, Loss: 36.2924690246582, L1: 9.893424987792969, L3: 26.399044036865234\n",
      "Current prediction:  61.50107192993164 \n",
      "\n",
      "Iteration 7986, Loss: 36.03412628173828, L1: 9.848794937133789, L3: 26.185333251953125\n",
      "Current prediction:  61.20491409301758 \n",
      "\n",
      "Iteration 7987, Loss: 36.11701583862305, L1: 9.844511985778809, L3: 26.272502899169922\n",
      "Current prediction:  60.596126556396484 \n",
      "\n",
      "Iteration 7988, Loss: 36.20301818847656, L1: 9.920073509216309, L3: 26.282943725585938\n",
      "Current prediction:  60.546226501464844 \n",
      "\n",
      "Iteration 7989, Loss: 37.283573150634766, L1: 10.010565757751465, L3: 27.273006439208984\n",
      "Current prediction:  60.55143737792969 \n",
      "\n",
      "Iteration 7990, Loss: 36.485740661621094, L1: 9.98265552520752, L3: 26.503084182739258\n",
      "Current prediction:  60.56932830810547 \n",
      "\n",
      "Iteration 7991, Loss: 36.15129089355469, L1: 9.948352813720703, L3: 26.202938079833984\n",
      "Current prediction:  60.92344665527344 \n",
      "\n",
      "Iteration 7992, Loss: 36.59943771362305, L1: 9.841486930847168, L3: 26.757949829101562\n",
      "Current prediction:  61.54000473022461 \n",
      "\n",
      "Iteration 7993, Loss: 35.75931930541992, L1: 9.843449592590332, L3: 25.915870666503906\n",
      "Current prediction:  61.58414077758789 \n",
      "\n",
      "Iteration 7994, Loss: 35.77058410644531, L1: 9.878641128540039, L3: 25.891942977905273\n",
      "Current prediction:  61.60447692871094 \n",
      "\n",
      "Iteration 7995, Loss: 36.224884033203125, L1: 9.926837921142578, L3: 26.298046112060547\n",
      "Current prediction:  61.613712310791016 \n",
      "\n",
      "Iteration 7996, Loss: 36.825225830078125, L1: 9.919790267944336, L3: 26.905433654785156\n",
      "Current prediction:  61.611202239990234 \n",
      "\n",
      "Iteration 7997, Loss: 36.19712829589844, L1: 9.983744621276855, L3: 26.2133846282959\n",
      "Current prediction:  61.60600280761719 \n",
      "\n",
      "Iteration 7998, Loss: 35.858543395996094, L1: 9.975916862487793, L3: 25.882627487182617\n",
      "Current prediction:  61.589805603027344 \n",
      "\n",
      "â†³ LR reduced to 5.0e-04 at iteration 8000 \n",
      "\n",
      "Iteration 7999, Loss: 36.13572692871094, L1: 10.008552551269531, L3: 26.127174377441406\n",
      "Current prediction:  61.56509780883789 \n",
      "\n",
      "Iteration 8000, Loss: 37.02393341064453, L1: 9.968338966369629, L3: 27.055593490600586\n",
      "Current prediction:  61.537696838378906 \n",
      "\n",
      "Iteration 8001, Loss: 36.78932189941406, L1: 9.991471290588379, L3: 26.797849655151367\n",
      "Current prediction:  61.50651168823242 \n",
      "\n",
      "Iteration 8002, Loss: 35.68104553222656, L1: 9.931034088134766, L3: 25.750009536743164\n",
      "Current prediction:  61.433563232421875 \n",
      "\n",
      "Iteration 8003, Loss: 35.89873504638672, L1: 9.920248031616211, L3: 25.97848892211914\n",
      "Current prediction:  60.51462936401367 \n",
      "\n",
      "Iteration 8004, Loss: 36.377662658691406, L1: 10.019676208496094, L3: 26.35798454284668\n",
      "Current prediction:  60.47315216064453 \n",
      "\n",
      "Iteration 8005, Loss: 36.71907043457031, L1: 10.141942024230957, L3: 26.57712745666504\n",
      "Current prediction:  60.4716682434082 \n",
      "\n",
      "Iteration 8006, Loss: 36.284156799316406, L1: 10.096132278442383, L3: 26.18802261352539\n",
      "Current prediction:  60.480682373046875 \n",
      "\n",
      "Iteration 8007, Loss: 35.285980224609375, L1: 10.082427024841309, L3: 25.20355224609375\n",
      "Current prediction:  60.507423400878906 \n",
      "\n",
      "Iteration 8008, Loss: 36.102088928222656, L1: 10.024295806884766, L3: 26.07779312133789\n",
      "Current prediction:  60.74859619140625 \n",
      "\n",
      "Iteration 8009, Loss: 36.43894958496094, L1: 9.903157234191895, L3: 26.53579330444336\n",
      "Current prediction:  61.468406677246094 \n",
      "\n",
      "Iteration 8010, Loss: 35.86590576171875, L1: 9.907755851745605, L3: 25.95815086364746\n",
      "Current prediction:  61.531593322753906 \n",
      "\n",
      "Iteration 8011, Loss: 35.66716384887695, L1: 9.932866096496582, L3: 25.734296798706055\n",
      "Current prediction:  61.5629997253418 \n",
      "\n",
      "Iteration 8012, Loss: 35.069488525390625, L1: 9.95237922668457, L3: 25.117107391357422\n",
      "Current prediction:  61.591217041015625 \n",
      "\n",
      "Iteration 8013, Loss: 35.88954162597656, L1: 9.889771461486816, L3: 25.999771118164062\n",
      "Current prediction:  61.61669921875 \n",
      "\n",
      "Iteration 8014, Loss: 35.84811782836914, L1: 9.860297203063965, L3: 25.987821578979492\n",
      "Current prediction:  61.629173278808594 \n",
      "\n",
      "Iteration 8015, Loss: 36.382938385009766, L1: 9.832219123840332, L3: 26.550718307495117\n",
      "Current prediction:  61.63047790527344 \n",
      "\n",
      "Iteration 8016, Loss: 36.2048454284668, L1: 9.826398849487305, L3: 26.378446578979492\n",
      "Current prediction:  61.55385971069336 \n",
      "\n",
      "Iteration 8017, Loss: 36.290714263916016, L1: 9.781586647033691, L3: 26.50912857055664\n",
      "Current prediction:  61.118446350097656 \n",
      "\n",
      "Iteration 8018, Loss: 36.29318618774414, L1: 9.832307815551758, L3: 26.460878372192383\n",
      "Current prediction:  60.82079315185547 \n",
      "\n",
      "Iteration 8019, Loss: 35.831214904785156, L1: 9.850053787231445, L3: 25.981163024902344\n",
      "Current prediction:  60.74745559692383 \n",
      "\n",
      "Iteration 8020, Loss: 36.177825927734375, L1: 9.80402946472168, L3: 26.373794555664062\n",
      "Current prediction:  60.8187141418457 \n",
      "\n",
      "Iteration 8021, Loss: 36.791961669921875, L1: 9.806008338928223, L3: 26.985952377319336\n",
      "Current prediction:  61.184871673583984 \n",
      "\n",
      "Iteration 8022, Loss: 36.241737365722656, L1: 9.775358200073242, L3: 26.46637725830078\n",
      "Current prediction:  61.442970275878906 \n",
      "\n",
      "Iteration 8023, Loss: 36.083221435546875, L1: 9.790066719055176, L3: 26.293155670166016\n",
      "Current prediction:  61.41832733154297 \n",
      "\n",
      "Iteration 8024, Loss: 36.058876037597656, L1: 9.779289245605469, L3: 26.279586791992188\n",
      "Current prediction:  61.18149948120117 \n",
      "\n",
      "Iteration 8025, Loss: 37.034568786621094, L1: 9.836359024047852, L3: 27.198211669921875\n",
      "Current prediction:  60.84682083129883 \n",
      "\n",
      "Iteration 8026, Loss: 37.11656951904297, L1: 9.785892486572266, L3: 27.33067512512207\n",
      "Current prediction:  60.7404899597168 \n",
      "\n",
      "Iteration 8027, Loss: 36.57624816894531, L1: 9.902433395385742, L3: 26.673812866210938\n",
      "Current prediction:  60.92195129394531 \n",
      "\n",
      "Iteration 8028, Loss: 36.08659744262695, L1: 9.765446662902832, L3: 26.321149826049805\n",
      "Current prediction:  61.349029541015625 \n",
      "\n",
      "Iteration 8029, Loss: 35.98732376098633, L1: 9.843286514282227, L3: 26.1440372467041\n",
      "Current prediction:  61.34357452392578 \n",
      "\n",
      "Iteration 8030, Loss: 36.03348922729492, L1: 9.838225364685059, L3: 26.195262908935547\n",
      "Current prediction:  61.38051986694336 \n",
      "\n",
      "Iteration 8031, Loss: 36.21574401855469, L1: 9.785675048828125, L3: 26.430068969726562\n",
      "Current prediction:  61.24836730957031 \n",
      "\n",
      "Iteration 8032, Loss: 35.69478225708008, L1: 9.794707298278809, L3: 25.900075912475586\n",
      "Current prediction:  60.8295783996582 \n",
      "\n",
      "Iteration 8033, Loss: 35.36509704589844, L1: 9.804926872253418, L3: 25.560171127319336\n",
      "Current prediction:  60.765342712402344 \n",
      "\n",
      "Iteration 8034, Loss: 36.55936813354492, L1: 9.812605857849121, L3: 26.746763229370117\n",
      "Current prediction:  60.845008850097656 \n",
      "\n",
      "Iteration 8035, Loss: 36.421844482421875, L1: 9.80572509765625, L3: 26.616121292114258\n",
      "Current prediction:  61.11913299560547 \n",
      "\n",
      "Iteration 8036, Loss: 35.63499450683594, L1: 9.833938598632812, L3: 25.801057815551758\n",
      "Current prediction:  61.17012023925781 \n",
      "\n",
      "Iteration 8037, Loss: 36.198970794677734, L1: 9.76357650756836, L3: 26.435394287109375\n",
      "Current prediction:  61.111244201660156 \n",
      "\n",
      "Iteration 8038, Loss: 36.74062728881836, L1: 9.854862213134766, L3: 26.885765075683594\n",
      "Current prediction:  61.31719207763672 \n",
      "\n",
      "Iteration 8039, Loss: 35.70096969604492, L1: 9.812950134277344, L3: 25.888019561767578\n",
      "Current prediction:  61.34588623046875 \n",
      "\n",
      "Iteration 8040, Loss: 36.22105407714844, L1: 9.80405044555664, L3: 26.417003631591797\n",
      "Current prediction:  61.087039947509766 \n",
      "\n",
      "Iteration 8041, Loss: 36.418365478515625, L1: 9.79548454284668, L3: 26.622880935668945\n",
      "Current prediction:  61.33305740356445 \n",
      "\n",
      "Iteration 8042, Loss: 36.44073486328125, L1: 9.843873023986816, L3: 26.59686279296875\n",
      "Current prediction:  61.08479690551758 \n",
      "\n",
      "Iteration 8043, Loss: 35.49967956542969, L1: 9.783849716186523, L3: 25.715831756591797\n",
      "Current prediction:  60.75181198120117 \n",
      "\n",
      "Iteration 8044, Loss: 35.97425079345703, L1: 9.808464050292969, L3: 26.165786743164062\n",
      "Current prediction:  60.71743392944336 \n",
      "\n",
      "Iteration 8045, Loss: 35.25318145751953, L1: 9.8284330368042, L3: 25.42474937438965\n",
      "Current prediction:  60.886539459228516 \n",
      "\n",
      "Iteration 8046, Loss: 35.71632385253906, L1: 9.831883430480957, L3: 25.884441375732422\n",
      "Current prediction:  61.06120681762695 \n",
      "\n",
      "Iteration 8047, Loss: 36.80017852783203, L1: 9.79919719696045, L3: 27.000980377197266\n",
      "Current prediction:  61.0706787109375 \n",
      "\n",
      "Iteration 8048, Loss: 36.12249755859375, L1: 9.810853004455566, L3: 26.311643600463867\n",
      "Current prediction:  61.15608596801758 \n",
      "\n",
      "Iteration 8049, Loss: 35.29399108886719, L1: 9.744847297668457, L3: 25.549142837524414\n",
      "Current prediction:  61.1963005065918 \n",
      "\n",
      "Iteration 8050, Loss: 35.57405471801758, L1: 9.814432144165039, L3: 25.75962257385254\n",
      "Current prediction:  60.937747955322266 \n",
      "\n",
      "Iteration 8051, Loss: 36.464263916015625, L1: 9.776616096496582, L3: 26.687646865844727\n",
      "Current prediction:  60.91641616821289 \n",
      "\n",
      "Iteration 8052, Loss: 35.45732116699219, L1: 9.813556671142578, L3: 25.643762588500977\n",
      "Current prediction:  61.06993865966797 \n",
      "\n",
      "Iteration 8053, Loss: 35.7441520690918, L1: 9.774834632873535, L3: 25.969318389892578\n",
      "Current prediction:  61.4597053527832 \n",
      "\n",
      "Iteration 8054, Loss: 36.55847930908203, L1: 9.773578643798828, L3: 26.784902572631836\n",
      "Current prediction:  61.61051940917969 \n",
      "\n",
      "Iteration 8055, Loss: 36.202632904052734, L1: 9.779154777526855, L3: 26.423477172851562\n",
      "Current prediction:  61.6450309753418 \n",
      "\n",
      "Iteration 8056, Loss: 37.23529052734375, L1: 9.806465148925781, L3: 27.42882537841797\n",
      "Current prediction:  61.65736389160156 \n",
      "\n",
      "Iteration 8057, Loss: 36.27812957763672, L1: 9.785791397094727, L3: 26.492340087890625\n",
      "Current prediction:  61.65554428100586 \n",
      "\n",
      "Iteration 8058, Loss: 36.31023406982422, L1: 9.846763610839844, L3: 26.463472366333008\n",
      "Current prediction:  61.63528060913086 \n",
      "\n",
      "Iteration 8059, Loss: 36.30282211303711, L1: 9.826003074645996, L3: 26.476818084716797\n",
      "Current prediction:  61.448551177978516 \n",
      "\n",
      "Iteration 8060, Loss: 36.471317291259766, L1: 9.743653297424316, L3: 26.727664947509766\n",
      "Current prediction:  60.97560119628906 \n",
      "\n",
      "Iteration 8061, Loss: 35.892791748046875, L1: 9.762768745422363, L3: 26.130022048950195\n",
      "Current prediction:  60.679813385009766 \n",
      "\n",
      "Iteration 8062, Loss: 36.303184509277344, L1: 9.867265701293945, L3: 26.4359188079834\n",
      "Current prediction:  60.618019104003906 \n",
      "\n",
      "Iteration 8063, Loss: 36.108192443847656, L1: 9.901762008666992, L3: 26.20642852783203\n",
      "Current prediction:  60.6046257019043 \n",
      "\n",
      "Iteration 8064, Loss: 36.51761245727539, L1: 9.908981323242188, L3: 26.608631134033203\n",
      "Current prediction:  60.60511779785156 \n",
      "\n",
      "Iteration 8065, Loss: 35.875946044921875, L1: 9.870393753051758, L3: 26.005552291870117\n",
      "Current prediction:  60.721317291259766 \n",
      "\n",
      "Iteration 8066, Loss: 35.77232360839844, L1: 9.861991882324219, L3: 25.910329818725586\n",
      "Current prediction:  61.478614807128906 \n",
      "\n",
      "Iteration 8067, Loss: 35.912384033203125, L1: 9.876399993896484, L3: 26.03598403930664\n",
      "Current prediction:  61.54383850097656 \n",
      "\n",
      "Iteration 8068, Loss: 36.12329864501953, L1: 9.914743423461914, L3: 26.208553314208984\n",
      "Current prediction:  61.553836822509766 \n",
      "\n",
      "Iteration 8069, Loss: 36.64628601074219, L1: 9.88858413696289, L3: 26.757699966430664\n",
      "Current prediction:  61.55142593383789 \n",
      "\n",
      "Iteration 8070, Loss: 36.181243896484375, L1: 9.878528594970703, L3: 26.30271339416504\n",
      "Current prediction:  61.512596130371094 \n",
      "\n",
      "Iteration 8071, Loss: 35.31479263305664, L1: 9.808632850646973, L3: 25.506160736083984\n",
      "Current prediction:  60.837772369384766 \n",
      "\n",
      "Iteration 8072, Loss: 35.389930725097656, L1: 9.888663291931152, L3: 25.50126838684082\n",
      "Current prediction:  60.64594650268555 \n",
      "\n",
      "Iteration 8073, Loss: 35.479888916015625, L1: 9.890714645385742, L3: 25.589174270629883\n",
      "Current prediction:  60.604164123535156 \n",
      "\n",
      "Iteration 8074, Loss: 35.6794319152832, L1: 9.946287155151367, L3: 25.733144760131836\n",
      "Current prediction:  60.69137954711914 \n",
      "\n",
      "Iteration 8075, Loss: 35.83906555175781, L1: 9.886659622192383, L3: 25.95240592956543\n",
      "Current prediction:  60.838478088378906 \n",
      "\n",
      "Iteration 8076, Loss: 35.41513442993164, L1: 9.83287239074707, L3: 25.58226203918457\n",
      "Current prediction:  61.4602165222168 \n",
      "\n",
      "Iteration 8077, Loss: 35.777992248535156, L1: 9.77748966217041, L3: 26.000503540039062\n",
      "Current prediction:  61.478492736816406 \n",
      "\n",
      "Iteration 8078, Loss: 36.01508331298828, L1: 9.783953666687012, L3: 26.231130599975586\n",
      "Current prediction:  61.56605911254883 \n",
      "\n",
      "Iteration 8079, Loss: 34.766845703125, L1: 9.790658950805664, L3: 24.976186752319336\n",
      "Current prediction:  61.45121765136719 \n",
      "\n",
      "Iteration 8080, Loss: 35.313533782958984, L1: 9.783305168151855, L3: 25.530227661132812\n",
      "Current prediction:  61.110870361328125 \n",
      "\n",
      "Iteration 8081, Loss: 35.67474365234375, L1: 9.692743301391602, L3: 25.98200035095215\n",
      "Current prediction:  60.799625396728516 \n",
      "\n",
      "Iteration 8082, Loss: 36.71528244018555, L1: 9.767203330993652, L3: 26.948078155517578\n",
      "Current prediction:  60.78849411010742 \n",
      "\n",
      "Iteration 8083, Loss: 36.53379821777344, L1: 9.726375579833984, L3: 26.807424545288086\n",
      "Current prediction:  60.7800178527832 \n",
      "\n",
      "Iteration 8084, Loss: 36.14202117919922, L1: 9.78492546081543, L3: 26.357097625732422\n",
      "Current prediction:  60.77985763549805 \n",
      "\n",
      "Iteration 8085, Loss: 35.468833923339844, L1: 9.760464668273926, L3: 25.708370208740234\n",
      "Current prediction:  60.90297317504883 \n",
      "\n",
      "Iteration 8086, Loss: 36.2374267578125, L1: 9.76638412475586, L3: 26.471040725708008\n",
      "Current prediction:  60.975257873535156 \n",
      "\n",
      "Iteration 8087, Loss: 36.022613525390625, L1: 9.721023559570312, L3: 26.30158805847168\n",
      "Current prediction:  60.943572998046875 \n",
      "\n",
      "Iteration 8088, Loss: 36.800621032714844, L1: 9.723613739013672, L3: 27.077009201049805\n",
      "Current prediction:  60.932125091552734 \n",
      "\n",
      "Iteration 8089, Loss: 36.3747444152832, L1: 9.770817756652832, L3: 26.603927612304688\n",
      "Current prediction:  61.07575607299805 \n",
      "\n",
      "Iteration 8090, Loss: 36.005455017089844, L1: 9.732640266418457, L3: 26.272815704345703\n",
      "Current prediction:  61.53634262084961 \n",
      "\n",
      "Iteration 8091, Loss: 34.71509552001953, L1: 9.767350196838379, L3: 24.94774627685547\n",
      "Current prediction:  61.57201385498047 \n",
      "\n",
      "Iteration 8092, Loss: 37.41530990600586, L1: 9.815471649169922, L3: 27.599838256835938\n",
      "Current prediction:  61.42544937133789 \n",
      "\n",
      "Iteration 8093, Loss: 36.0997428894043, L1: 9.804434776306152, L3: 26.295307159423828\n",
      "Current prediction:  60.88119125366211 \n",
      "\n",
      "Iteration 8094, Loss: 35.047203063964844, L1: 9.822354316711426, L3: 25.224849700927734\n",
      "Current prediction:  60.6466064453125 \n",
      "\n",
      "Iteration 8095, Loss: 36.810028076171875, L1: 9.838376998901367, L3: 26.97165298461914\n",
      "Current prediction:  60.77883529663086 \n",
      "\n",
      "Iteration 8096, Loss: 35.577369689941406, L1: 9.851625442504883, L3: 25.72574234008789\n",
      "Current prediction:  61.39651107788086 \n",
      "\n",
      "Iteration 8097, Loss: 35.51116943359375, L1: 9.826979637145996, L3: 25.68419075012207\n",
      "Current prediction:  61.555931091308594 \n",
      "\n",
      "Iteration 8098, Loss: 35.81434631347656, L1: 9.84073543548584, L3: 25.97361183166504\n",
      "Current prediction:  61.56049728393555 \n",
      "\n",
      "Iteration 8099, Loss: 36.096012115478516, L1: 9.905351638793945, L3: 26.19066047668457\n",
      "Current prediction:  61.547664642333984 \n",
      "\n",
      "Iteration 8100, Loss: 37.05750274658203, L1: 9.964911460876465, L3: 27.09259033203125\n",
      "Current prediction:  61.53529357910156 \n",
      "\n",
      "Iteration 8101, Loss: 36.332733154296875, L1: 9.946533203125, L3: 26.386199951171875\n",
      "Current prediction:  61.523319244384766 \n",
      "\n",
      "Iteration 8102, Loss: 35.37176513671875, L1: 9.948795318603516, L3: 25.422971725463867\n",
      "Current prediction:  61.51183319091797 \n",
      "\n",
      "Iteration 8103, Loss: 36.09624481201172, L1: 9.913956642150879, L3: 26.182287216186523\n",
      "Current prediction:  61.47956848144531 \n",
      "\n",
      "Iteration 8104, Loss: 35.509307861328125, L1: 9.91069221496582, L3: 25.598615646362305\n",
      "Current prediction:  61.191043853759766 \n",
      "\n",
      "Iteration 8105, Loss: 35.9146728515625, L1: 9.84699821472168, L3: 26.067672729492188\n",
      "Current prediction:  60.640777587890625 \n",
      "\n",
      "Iteration 8106, Loss: 36.72023010253906, L1: 9.83968734741211, L3: 26.880544662475586\n",
      "Current prediction:  60.57987594604492 \n",
      "\n",
      "Iteration 8107, Loss: 35.23147964477539, L1: 9.925922393798828, L3: 25.305557250976562\n",
      "Current prediction:  60.597434997558594 \n",
      "\n",
      "Iteration 8108, Loss: 36.96040344238281, L1: 9.94282341003418, L3: 27.017578125\n",
      "Current prediction:  61.09670639038086 \n",
      "\n",
      "Iteration 8109, Loss: 35.26835632324219, L1: 9.835834503173828, L3: 25.43252182006836\n",
      "Current prediction:  61.49404525756836 \n",
      "\n",
      "Iteration 8110, Loss: 34.88738250732422, L1: 9.804880142211914, L3: 25.082502365112305\n",
      "Current prediction:  61.597023010253906 \n",
      "\n",
      "Iteration 8111, Loss: 35.4429817199707, L1: 9.822406768798828, L3: 25.620574951171875\n",
      "Current prediction:  61.61732864379883 \n",
      "\n",
      "Iteration 8112, Loss: 36.22217559814453, L1: 9.845236778259277, L3: 26.376937866210938\n",
      "Current prediction:  61.606666564941406 \n",
      "\n",
      "Iteration 8113, Loss: 35.75154113769531, L1: 9.844145774841309, L3: 25.907394409179688\n",
      "Current prediction:  61.29716873168945 \n",
      "\n",
      "Iteration 8114, Loss: 34.52622985839844, L1: 9.973362922668457, L3: 24.552865982055664\n",
      "Current prediction:  60.742897033691406 \n",
      "\n",
      "Iteration 8115, Loss: 36.274681091308594, L1: 9.9061918258667, L3: 26.36849021911621\n",
      "Current prediction:  60.67420196533203 \n",
      "\n",
      "Iteration 8116, Loss: 37.03211975097656, L1: 9.970551490783691, L3: 27.061569213867188\n",
      "Current prediction:  60.72288513183594 \n",
      "\n",
      "Iteration 8117, Loss: 36.30664825439453, L1: 9.959110260009766, L3: 26.3475399017334\n",
      "Current prediction:  60.96856689453125 \n",
      "\n",
      "Iteration 8118, Loss: 37.10554885864258, L1: 9.945314407348633, L3: 27.160234451293945\n",
      "Current prediction:  61.5018196105957 \n",
      "\n",
      "Iteration 8119, Loss: 36.50341796875, L1: 9.976242065429688, L3: 26.527175903320312\n",
      "Current prediction:  61.55843734741211 \n",
      "\n",
      "Iteration 8120, Loss: 36.146148681640625, L1: 9.970342636108398, L3: 26.175804138183594\n",
      "Current prediction:  61.52613830566406 \n",
      "\n",
      "Iteration 8121, Loss: 36.618133544921875, L1: 9.969953536987305, L3: 26.648181915283203\n",
      "Current prediction:  61.758296966552734 \n",
      "\n",
      "Iteration 8122, Loss: 36.533721923828125, L1: 9.905034065246582, L3: 26.628686904907227\n",
      "Current prediction:  61.72545623779297 \n",
      "\n",
      "Iteration 8123, Loss: 35.861900329589844, L1: 9.808172225952148, L3: 26.053728103637695\n",
      "Current prediction:  61.01660919189453 \n",
      "\n",
      "Iteration 8124, Loss: 35.359375, L1: 9.761772155761719, L3: 25.59760284423828\n",
      "Current prediction:  60.944725036621094 \n",
      "\n",
      "Iteration 8125, Loss: 35.85240173339844, L1: 9.84644889831543, L3: 26.005950927734375\n",
      "Current prediction:  60.97285079956055 \n",
      "\n",
      "Iteration 8126, Loss: 35.476585388183594, L1: 9.865460395812988, L3: 25.611125946044922\n",
      "Current prediction:  60.92131805419922 \n",
      "\n",
      "Iteration 8127, Loss: 36.65566635131836, L1: 9.996234893798828, L3: 26.65943145751953\n",
      "Current prediction:  60.374969482421875 \n",
      "\n",
      "Iteration 8128, Loss: 36.66405487060547, L1: 9.927251815795898, L3: 26.73680305480957\n",
      "Current prediction:  60.40274429321289 \n",
      "\n",
      "Iteration 8129, Loss: 35.774932861328125, L1: 9.974991798400879, L3: 25.799942016601562\n",
      "Current prediction:  60.472721099853516 \n",
      "\n",
      "Iteration 8130, Loss: 36.56769561767578, L1: 9.861896514892578, L3: 26.70579719543457\n",
      "Current prediction:  61.29151153564453 \n",
      "\n",
      "Iteration 8131, Loss: 36.02609634399414, L1: 9.843619346618652, L3: 26.182477951049805\n",
      "Current prediction:  61.46300506591797 \n",
      "\n",
      "Iteration 8132, Loss: 35.97553253173828, L1: 9.790560722351074, L3: 26.184972763061523\n",
      "Current prediction:  61.48280334472656 \n",
      "\n",
      "Iteration 8133, Loss: 35.88090896606445, L1: 9.761930465698242, L3: 26.11897850036621\n",
      "Current prediction:  62.0162467956543 \n",
      "\n",
      "Iteration 8134, Loss: 35.94257736206055, L1: 9.722052574157715, L3: 26.22052574157715\n",
      "Current prediction:  62.11336135864258 \n",
      "\n",
      "Iteration 8135, Loss: 36.58366775512695, L1: 9.681848526000977, L3: 26.901819229125977\n",
      "Current prediction:  62.0626220703125 \n",
      "\n",
      "Iteration 8136, Loss: 35.60783386230469, L1: 9.622051239013672, L3: 25.98578453063965\n",
      "Current prediction:  61.88059997558594 \n",
      "\n",
      "Iteration 8137, Loss: 36.7809944152832, L1: 9.593069076538086, L3: 27.187925338745117\n",
      "Current prediction:  61.12031936645508 \n",
      "\n",
      "Iteration 8138, Loss: 36.03936004638672, L1: 9.561234474182129, L3: 26.478124618530273\n",
      "Current prediction:  60.954856872558594 \n",
      "\n",
      "Iteration 8139, Loss: 35.498531341552734, L1: 9.786059379577637, L3: 25.712472915649414\n",
      "Current prediction:  60.93801498413086 \n",
      "\n",
      "Iteration 8140, Loss: 36.38011932373047, L1: 9.7489595413208, L3: 26.63115882873535\n",
      "Current prediction:  61.11979293823242 \n",
      "\n",
      "Iteration 8141, Loss: 36.457969665527344, L1: 9.781304359436035, L3: 26.676666259765625\n",
      "Current prediction:  61.4956169128418 \n",
      "\n",
      "Iteration 8142, Loss: 35.6358642578125, L1: 9.866105079650879, L3: 25.769760131835938\n",
      "Current prediction:  61.55295181274414 \n",
      "\n",
      "Iteration 8143, Loss: 35.49485778808594, L1: 9.822004318237305, L3: 25.672855377197266\n",
      "Current prediction:  61.537620544433594 \n",
      "\n",
      "Iteration 8144, Loss: 35.347599029541016, L1: 9.85684585571289, L3: 25.490753173828125\n",
      "Current prediction:  61.50191879272461 \n",
      "\n",
      "Iteration 8145, Loss: 36.930992126464844, L1: 9.849225997924805, L3: 27.081768035888672\n",
      "Current prediction:  61.43893814086914 \n",
      "\n",
      "Iteration 8146, Loss: 34.79668426513672, L1: 9.826821327209473, L3: 24.96986198425293\n",
      "Current prediction:  61.39683151245117 \n",
      "\n",
      "Iteration 8147, Loss: 36.25783157348633, L1: 9.942394256591797, L3: 26.31543731689453\n",
      "Current prediction:  61.31943130493164 \n",
      "\n",
      "Iteration 8148, Loss: 35.74248504638672, L1: 9.859890937805176, L3: 25.882593154907227\n",
      "Current prediction:  60.93252182006836 \n",
      "\n",
      "Iteration 8149, Loss: 35.66977310180664, L1: 9.93935775756836, L3: 25.73041534423828\n",
      "Current prediction:  60.48818588256836 \n",
      "\n",
      "Iteration 8150, Loss: 36.058502197265625, L1: 9.989028930664062, L3: 26.069475173950195\n",
      "Current prediction:  60.426815032958984 \n",
      "\n",
      "Iteration 8151, Loss: 36.49579620361328, L1: 10.031728744506836, L3: 26.464069366455078\n",
      "Current prediction:  60.469581604003906 \n",
      "\n",
      "Iteration 8152, Loss: 36.316986083984375, L1: 9.982730865478516, L3: 26.334253311157227\n",
      "Current prediction:  60.848209381103516 \n",
      "\n",
      "Iteration 8153, Loss: 36.622318267822266, L1: 9.844862937927246, L3: 26.777456283569336\n",
      "Current prediction:  61.3858642578125 \n",
      "\n",
      "Iteration 8154, Loss: 37.39808654785156, L1: 9.869438171386719, L3: 27.52864646911621\n",
      "Current prediction:  61.510562896728516 \n",
      "\n",
      "Iteration 8155, Loss: 36.59412384033203, L1: 9.812066078186035, L3: 26.78205680847168\n",
      "Current prediction:  61.54562759399414 \n",
      "\n",
      "Iteration 8156, Loss: 35.827545166015625, L1: 9.777252197265625, L3: 26.050294876098633\n",
      "Current prediction:  61.53562927246094 \n",
      "\n",
      "Iteration 8157, Loss: 36.14239501953125, L1: 9.779848098754883, L3: 26.362546920776367\n",
      "Current prediction:  61.2275505065918 \n",
      "\n",
      "Iteration 8158, Loss: 36.64746856689453, L1: 9.754514694213867, L3: 26.892953872680664\n",
      "Current prediction:  60.77724075317383 \n",
      "\n",
      "Iteration 8159, Loss: 35.78837585449219, L1: 9.780742645263672, L3: 26.007631301879883\n",
      "Current prediction:  60.69740676879883 \n",
      "\n",
      "Iteration 8160, Loss: 35.73078918457031, L1: 9.768790245056152, L3: 25.961999893188477\n",
      "Current prediction:  60.70502471923828 \n",
      "\n",
      "Iteration 8161, Loss: 37.17578125, L1: 9.794366836547852, L3: 27.381412506103516\n",
      "Current prediction:  60.79253387451172 \n",
      "\n",
      "Iteration 8162, Loss: 36.32568359375, L1: 9.773165702819824, L3: 26.552518844604492\n",
      "Current prediction:  61.36247253417969 \n",
      "\n",
      "Iteration 8163, Loss: 36.535377502441406, L1: 9.700178146362305, L3: 26.8351993560791\n",
      "Current prediction:  61.499446868896484 \n",
      "\n",
      "Iteration 8164, Loss: 36.06315612792969, L1: 9.690938949584961, L3: 26.372215270996094\n",
      "Current prediction:  61.60967254638672 \n",
      "\n",
      "Iteration 8165, Loss: 36.37153625488281, L1: 9.729954719543457, L3: 26.64158058166504\n",
      "Current prediction:  61.51435852050781 \n",
      "\n",
      "Iteration 8166, Loss: 36.26211166381836, L1: 9.729734420776367, L3: 26.532377243041992\n",
      "Current prediction:  61.253963470458984 \n",
      "\n",
      "Iteration 8167, Loss: 35.868038177490234, L1: 9.75455379486084, L3: 26.11348533630371\n",
      "Current prediction:  60.89086151123047 \n",
      "\n",
      "Iteration 8168, Loss: 35.88903045654297, L1: 9.7698335647583, L3: 26.11919593811035\n",
      "Current prediction:  60.67194366455078 \n",
      "\n",
      "Iteration 8169, Loss: 35.980899810791016, L1: 9.765331268310547, L3: 26.21556854248047\n",
      "Current prediction:  60.64530563354492 \n",
      "\n",
      "Iteration 8170, Loss: 36.39680862426758, L1: 9.82174015045166, L3: 26.575069427490234\n",
      "Current prediction:  60.83125686645508 \n",
      "\n",
      "Iteration 8171, Loss: 35.9235725402832, L1: 9.818129539489746, L3: 26.105443954467773\n",
      "Current prediction:  61.28704071044922 \n",
      "\n",
      "Iteration 8172, Loss: 36.6173095703125, L1: 9.815133094787598, L3: 26.80217742919922\n",
      "Current prediction:  61.365379333496094 \n",
      "\n",
      "Iteration 8173, Loss: 35.81929397583008, L1: 9.81836986541748, L3: 26.00092315673828\n",
      "Current prediction:  61.53668975830078 \n",
      "\n",
      "Iteration 8174, Loss: 36.14570617675781, L1: 9.846521377563477, L3: 26.29918670654297\n",
      "Current prediction:  61.54063034057617 \n",
      "\n",
      "Iteration 8175, Loss: 35.72144317626953, L1: 9.828389167785645, L3: 25.893054962158203\n",
      "Current prediction:  61.53479766845703 \n",
      "\n",
      "Iteration 8176, Loss: 35.569358825683594, L1: 9.866987228393555, L3: 25.702369689941406\n",
      "Current prediction:  61.40916061401367 \n",
      "\n",
      "Iteration 8177, Loss: 36.82966232299805, L1: 9.793509483337402, L3: 27.03615379333496\n",
      "Current prediction:  60.784420013427734 \n",
      "\n",
      "Iteration 8178, Loss: 36.667381286621094, L1: 9.828815460205078, L3: 26.838565826416016\n",
      "Current prediction:  60.58088302612305 \n",
      "\n",
      "Iteration 8179, Loss: 36.63129806518555, L1: 9.927248001098633, L3: 26.704050064086914\n",
      "Current prediction:  60.5656623840332 \n",
      "\n",
      "Iteration 8180, Loss: 35.60242462158203, L1: 9.921053886413574, L3: 25.681371688842773\n",
      "Current prediction:  60.57974624633789 \n",
      "\n",
      "Iteration 8181, Loss: 35.95481491088867, L1: 9.8760347366333, L3: 26.078779220581055\n",
      "Current prediction:  60.61357116699219 \n",
      "\n",
      "Iteration 8182, Loss: 36.7028923034668, L1: 9.822958946228027, L3: 26.879934310913086\n",
      "Current prediction:  60.86358642578125 \n",
      "\n",
      "Iteration 8183, Loss: 34.892250061035156, L1: 9.785990715026855, L3: 25.106260299682617\n",
      "Current prediction:  61.4947509765625 \n",
      "\n",
      "Iteration 8184, Loss: 35.74916458129883, L1: 9.728829383850098, L3: 26.020334243774414\n",
      "Current prediction:  61.59005355834961 \n",
      "\n",
      "Iteration 8185, Loss: 35.11888885498047, L1: 9.752758026123047, L3: 25.366132736206055\n",
      "Current prediction:  61.558650970458984 \n",
      "\n",
      "Iteration 8186, Loss: 36.417232513427734, L1: 9.75725269317627, L3: 26.65997886657715\n",
      "Current prediction:  61.41502380371094 \n",
      "\n",
      "Iteration 8187, Loss: 35.817691802978516, L1: 9.702978134155273, L3: 26.114713668823242\n",
      "Current prediction:  60.7626953125 \n",
      "\n",
      "Iteration 8188, Loss: 36.147972106933594, L1: 9.789942741394043, L3: 26.358028411865234\n",
      "Current prediction:  60.78324890136719 \n",
      "\n",
      "Iteration 8189, Loss: 35.08180236816406, L1: 9.791912078857422, L3: 25.289888381958008\n",
      "Current prediction:  61.487762451171875 \n",
      "\n",
      "Iteration 8190, Loss: 36.54620361328125, L1: 9.720856666564941, L3: 26.825345993041992\n",
      "Current prediction:  61.6296272277832 \n",
      "\n",
      "Iteration 8191, Loss: 35.99376678466797, L1: 9.79271125793457, L3: 26.201053619384766\n",
      "Current prediction:  61.63722229003906 \n",
      "\n",
      "Iteration 8192, Loss: 36.58302688598633, L1: 9.81727409362793, L3: 26.7657527923584\n",
      "Current prediction:  61.6270866394043 \n",
      "\n",
      "Iteration 8193, Loss: 37.3702507019043, L1: 9.78686237335205, L3: 27.58338737487793\n",
      "Current prediction:  61.610260009765625 \n",
      "\n",
      "Iteration 8194, Loss: 36.036163330078125, L1: 9.866325378417969, L3: 26.169836044311523\n",
      "Current prediction:  61.590660095214844 \n",
      "\n",
      "Iteration 8195, Loss: 35.64407730102539, L1: 9.861872673034668, L3: 25.782203674316406\n",
      "Current prediction:  61.55752944946289 \n",
      "\n",
      "Iteration 8196, Loss: 36.46818923950195, L1: 9.790852546691895, L3: 26.677337646484375\n",
      "Current prediction:  61.260894775390625 \n",
      "\n",
      "Iteration 8197, Loss: 36.029937744140625, L1: 9.819979667663574, L3: 26.209957122802734\n",
      "Current prediction:  60.603843688964844 \n",
      "\n",
      "Iteration 8198, Loss: 35.45823669433594, L1: 9.869401931762695, L3: 25.58883285522461\n",
      "Current prediction:  60.550628662109375 \n",
      "\n",
      "Iteration 8199, Loss: 36.8484001159668, L1: 9.929974555969238, L3: 26.918424606323242\n",
      "Current prediction:  60.55000305175781 \n",
      "\n",
      "Iteration 8200, Loss: 35.60816955566406, L1: 9.96491527557373, L3: 25.643253326416016\n",
      "Current prediction:  60.56071090698242 \n",
      "\n",
      "Iteration 8201, Loss: 36.848289489746094, L1: 10.037593841552734, L3: 26.810697555541992\n",
      "Current prediction:  60.5742073059082 \n",
      "\n",
      "Iteration 8202, Loss: 37.04111099243164, L1: 9.900677680969238, L3: 27.14043426513672\n",
      "Current prediction:  60.680931091308594 \n",
      "\n",
      "Iteration 8203, Loss: 36.156761169433594, L1: 9.833226203918457, L3: 26.32353401184082\n",
      "Current prediction:  61.53464126586914 \n",
      "\n",
      "Iteration 8204, Loss: 36.58372497558594, L1: 9.814220428466797, L3: 26.769506454467773\n",
      "Current prediction:  61.61640167236328 \n",
      "\n",
      "Iteration 8205, Loss: 36.4421501159668, L1: 9.839814186096191, L3: 26.602336883544922\n",
      "Current prediction:  61.619380950927734 \n",
      "\n",
      "Iteration 8206, Loss: 35.48515701293945, L1: 9.861425399780273, L3: 25.62373161315918\n",
      "Current prediction:  61.61077117919922 \n",
      "\n",
      "Iteration 8207, Loss: 36.018951416015625, L1: 9.864174842834473, L3: 26.154775619506836\n",
      "Current prediction:  61.598541259765625 \n",
      "\n",
      "Iteration 8208, Loss: 36.45793533325195, L1: 9.871572494506836, L3: 26.586362838745117\n",
      "Current prediction:  61.58092498779297 \n",
      "\n",
      "Iteration 8209, Loss: 34.880035400390625, L1: 9.823762893676758, L3: 25.056272506713867\n",
      "Current prediction:  61.5335693359375 \n",
      "\n",
      "Iteration 8210, Loss: 37.21951675415039, L1: 9.810720443725586, L3: 27.408796310424805\n",
      "Current prediction:  60.8456916809082 \n",
      "\n",
      "Iteration 8211, Loss: 36.600990295410156, L1: 9.762056350708008, L3: 26.838932037353516\n",
      "Current prediction:  60.546810150146484 \n",
      "\n",
      "Iteration 8212, Loss: 36.41070556640625, L1: 9.931697845458984, L3: 26.479005813598633\n",
      "Current prediction:  60.54142379760742 \n",
      "\n",
      "Iteration 8213, Loss: 36.606201171875, L1: 10.007011413574219, L3: 26.59918975830078\n",
      "Current prediction:  60.55516815185547 \n",
      "\n",
      "Iteration 8214, Loss: 36.56431198120117, L1: 9.901537895202637, L3: 26.66277503967285\n",
      "Current prediction:  60.617156982421875 \n",
      "\n",
      "Iteration 8215, Loss: 36.308719635009766, L1: 9.834498405456543, L3: 26.47422218322754\n",
      "Current prediction:  61.08210372924805 \n",
      "\n",
      "Iteration 8216, Loss: 36.16576385498047, L1: 9.750901222229004, L3: 26.41486167907715\n",
      "Current prediction:  61.442710876464844 \n",
      "\n",
      "Iteration 8217, Loss: 36.03297805786133, L1: 9.776721954345703, L3: 26.256256103515625\n",
      "Current prediction:  61.52867126464844 \n",
      "\n",
      "Iteration 8218, Loss: 36.61612319946289, L1: 9.729641914367676, L3: 26.8864803314209\n",
      "Current prediction:  61.551170349121094 \n",
      "\n",
      "Iteration 8219, Loss: 35.32815933227539, L1: 9.738045692443848, L3: 25.590112686157227\n",
      "Current prediction:  61.52225875854492 \n",
      "\n",
      "Iteration 8220, Loss: 35.89951705932617, L1: 9.74028205871582, L3: 26.15923500061035\n",
      "Current prediction:  61.21654510498047 \n",
      "\n",
      "Iteration 8221, Loss: 36.553863525390625, L1: 9.658488273620605, L3: 26.895374298095703\n",
      "Current prediction:  60.76434326171875 \n",
      "\n",
      "Iteration 8222, Loss: 35.849491119384766, L1: 9.772553443908691, L3: 26.076936721801758\n",
      "Current prediction:  60.70673370361328 \n",
      "\n",
      "Iteration 8223, Loss: 36.364295959472656, L1: 9.836776733398438, L3: 26.52752113342285\n",
      "Current prediction:  60.68800354003906 \n",
      "\n",
      "Iteration 8224, Loss: 36.34853744506836, L1: 9.811793327331543, L3: 26.5367431640625\n",
      "Current prediction:  60.78207778930664 \n",
      "\n",
      "Iteration 8225, Loss: 36.2490234375, L1: 9.785253524780273, L3: 26.46377182006836\n",
      "Current prediction:  61.357662200927734 \n",
      "\n",
      "Iteration 8226, Loss: 35.483177185058594, L1: 9.709640502929688, L3: 25.773534774780273\n",
      "Current prediction:  61.6451530456543 \n",
      "\n",
      "Iteration 8227, Loss: 36.66391372680664, L1: 9.747237205505371, L3: 26.916677474975586\n",
      "Current prediction:  61.65910339355469 \n",
      "\n",
      "Iteration 8228, Loss: 37.583614349365234, L1: 9.7631196975708, L3: 27.820493698120117\n",
      "Current prediction:  61.64906311035156 \n",
      "\n",
      "Iteration 8229, Loss: 35.890625, L1: 9.760207176208496, L3: 26.130416870117188\n",
      "Current prediction:  61.60773849487305 \n",
      "\n",
      "Iteration 8230, Loss: 36.80631637573242, L1: 9.804564476013184, L3: 27.001750946044922\n",
      "Current prediction:  61.15996551513672 \n",
      "\n",
      "Iteration 8231, Loss: 35.78871536254883, L1: 9.733846664428711, L3: 26.054868698120117\n",
      "Current prediction:  60.64064025878906 \n",
      "\n",
      "Iteration 8232, Loss: 35.28694152832031, L1: 9.819899559020996, L3: 25.467042922973633\n",
      "Current prediction:  60.61887741088867 \n",
      "\n",
      "Iteration 8233, Loss: 36.28004455566406, L1: 9.861180305480957, L3: 26.418865203857422\n",
      "Current prediction:  60.63188171386719 \n",
      "\n",
      "Iteration 8234, Loss: 36.838504791259766, L1: 9.812047004699707, L3: 27.026456832885742\n",
      "Current prediction:  61.05741500854492 \n",
      "\n",
      "Iteration 8235, Loss: 35.679412841796875, L1: 9.744563102722168, L3: 25.934850692749023\n",
      "Current prediction:  61.56974411010742 \n",
      "\n",
      "Iteration 8236, Loss: 35.97640609741211, L1: 9.756918907165527, L3: 26.2194881439209\n",
      "Current prediction:  61.59901428222656 \n",
      "\n",
      "Iteration 8237, Loss: 35.88513946533203, L1: 9.81367301940918, L3: 26.07146644592285\n",
      "Current prediction:  61.60350799560547 \n",
      "\n",
      "Iteration 8238, Loss: 35.33648681640625, L1: 9.754517555236816, L3: 25.581968307495117\n",
      "Current prediction:  61.588443756103516 \n",
      "\n",
      "Iteration 8239, Loss: 36.94226837158203, L1: 9.841059684753418, L3: 27.10120964050293\n",
      "Current prediction:  61.337188720703125 \n",
      "\n",
      "Iteration 8240, Loss: 35.93694305419922, L1: 9.754995346069336, L3: 26.181947708129883\n",
      "Current prediction:  60.6189079284668 \n",
      "\n",
      "Iteration 8241, Loss: 36.39543914794922, L1: 9.828768730163574, L3: 26.566669464111328\n",
      "Current prediction:  60.566993713378906 \n",
      "\n",
      "Iteration 8242, Loss: 36.477291107177734, L1: 9.918825149536133, L3: 26.5584659576416\n",
      "Current prediction:  60.56733703613281 \n",
      "\n",
      "Iteration 8243, Loss: 35.81707000732422, L1: 9.983470916748047, L3: 25.833599090576172\n",
      "Current prediction:  60.575626373291016 \n",
      "\n",
      "Iteration 8244, Loss: 36.71040344238281, L1: 9.9501953125, L3: 26.760210037231445\n",
      "Current prediction:  60.586814880371094 \n",
      "\n",
      "Iteration 8245, Loss: 36.773990631103516, L1: 9.978724479675293, L3: 26.795265197753906\n",
      "Current prediction:  60.60742950439453 \n",
      "\n",
      "Iteration 8246, Loss: 36.66535949707031, L1: 9.832578659057617, L3: 26.832778930664062\n",
      "Current prediction:  60.78255844116211 \n",
      "\n",
      "Iteration 8247, Loss: 36.205013275146484, L1: 9.72217082977295, L3: 26.48284149169922\n",
      "Current prediction:  61.591033935546875 \n",
      "\n",
      "Iteration 8248, Loss: 36.419063568115234, L1: 9.75122356414795, L3: 26.6678409576416\n",
      "Current prediction:  61.63463592529297 \n",
      "\n",
      "Iteration 8249, Loss: 36.185306549072266, L1: 9.769701957702637, L3: 26.415603637695312\n",
      "Current prediction:  61.63587951660156 \n",
      "\n",
      "Iteration 8250, Loss: 35.8353157043457, L1: 9.849836349487305, L3: 25.9854793548584\n",
      "Current prediction:  61.63278579711914 \n",
      "\n",
      "Iteration 8251, Loss: 35.72992706298828, L1: 9.803984642028809, L3: 25.92594337463379\n",
      "Current prediction:  61.624778747558594 \n",
      "\n",
      "Iteration 8252, Loss: 36.15410614013672, L1: 9.817170143127441, L3: 26.33693504333496\n",
      "Current prediction:  61.616600036621094 \n",
      "\n",
      "Iteration 8253, Loss: 36.13591766357422, L1: 9.857138633728027, L3: 26.278778076171875\n",
      "Current prediction:  61.60310745239258 \n",
      "\n",
      "Iteration 8254, Loss: 35.479270935058594, L1: 9.795488357543945, L3: 25.68378257751465\n",
      "Current prediction:  61.44074249267578 \n",
      "\n",
      "Iteration 8255, Loss: 36.57757568359375, L1: 9.805757522583008, L3: 26.771818161010742\n",
      "Current prediction:  60.64040756225586 \n",
      "\n",
      "Iteration 8256, Loss: 36.2479133605957, L1: 9.826504707336426, L3: 26.421409606933594\n",
      "Current prediction:  60.58106994628906 \n",
      "\n",
      "Iteration 8257, Loss: 36.23723220825195, L1: 9.857223510742188, L3: 26.380008697509766\n",
      "Current prediction:  60.59009552001953 \n",
      "\n",
      "Iteration 8258, Loss: 35.82526779174805, L1: 9.858633995056152, L3: 25.966632843017578\n",
      "Current prediction:  60.6434440612793 \n",
      "\n",
      "Iteration 8259, Loss: 35.653297424316406, L1: 9.834213256835938, L3: 25.819082260131836\n",
      "Current prediction:  60.8182373046875 \n",
      "\n",
      "Iteration 8260, Loss: 35.832763671875, L1: 9.749731063842773, L3: 26.083030700683594\n",
      "Current prediction:  61.199058532714844 \n",
      "\n",
      "Iteration 8261, Loss: 35.55518341064453, L1: 9.748772621154785, L3: 25.80640983581543\n",
      "Current prediction:  61.54477310180664 \n",
      "\n",
      "Iteration 8262, Loss: 35.74481201171875, L1: 9.789754867553711, L3: 25.95505714416504\n",
      "Current prediction:  61.584991455078125 \n",
      "\n",
      "Iteration 8263, Loss: 35.53872299194336, L1: 9.758252143859863, L3: 25.780471801757812\n",
      "Current prediction:  61.53294372558594 \n",
      "\n",
      "Iteration 8264, Loss: 34.971458435058594, L1: 9.75744342803955, L3: 25.21401596069336\n",
      "Current prediction:  61.27545928955078 \n",
      "\n",
      "Iteration 8265, Loss: 35.97074890136719, L1: 9.727176666259766, L3: 26.243574142456055\n",
      "Current prediction:  60.79866027832031 \n",
      "\n",
      "Iteration 8266, Loss: 35.719482421875, L1: 9.710081100463867, L3: 26.009403228759766\n",
      "Current prediction:  60.71036148071289 \n",
      "\n",
      "Iteration 8267, Loss: 36.103858947753906, L1: 9.73952579498291, L3: 26.364334106445312\n",
      "Current prediction:  60.769779205322266 \n",
      "\n",
      "Iteration 8268, Loss: 35.4002799987793, L1: 9.757246971130371, L3: 25.643033981323242\n",
      "Current prediction:  60.916534423828125 \n",
      "\n",
      "Iteration 8269, Loss: 37.09175491333008, L1: 9.741718292236328, L3: 27.35003662109375\n",
      "Current prediction:  61.28541564941406 \n",
      "\n",
      "Iteration 8270, Loss: 36.03595733642578, L1: 9.752007484436035, L3: 26.283950805664062\n",
      "Current prediction:  61.43243408203125 \n",
      "\n",
      "Iteration 8271, Loss: 36.148681640625, L1: 9.758557319641113, L3: 26.390125274658203\n",
      "Current prediction:  61.232723236083984 \n",
      "\n",
      "Iteration 8272, Loss: 36.0362548828125, L1: 9.755743980407715, L3: 26.2805118560791\n",
      "Current prediction:  61.202396392822266 \n",
      "\n",
      "Iteration 8273, Loss: 35.928497314453125, L1: 9.720450401306152, L3: 26.208045959472656\n",
      "Current prediction:  61.10486602783203 \n",
      "\n",
      "Iteration 8274, Loss: 35.87882614135742, L1: 9.79085636138916, L3: 26.087968826293945\n",
      "Current prediction:  60.950130462646484 \n",
      "\n",
      "Iteration 8275, Loss: 36.022525787353516, L1: 9.783385276794434, L3: 26.2391414642334\n",
      "Current prediction:  60.95978546142578 \n",
      "\n",
      "Iteration 8276, Loss: 35.64011001586914, L1: 9.743617057800293, L3: 25.896493911743164\n",
      "Current prediction:  60.7672119140625 \n",
      "\n",
      "Iteration 8277, Loss: 36.45271682739258, L1: 9.802498817443848, L3: 26.650218963623047\n",
      "Current prediction:  60.656707763671875 \n",
      "\n",
      "Iteration 8278, Loss: 35.469661712646484, L1: 9.825640678405762, L3: 25.64402198791504\n",
      "Current prediction:  60.69482421875 \n",
      "\n",
      "Iteration 8279, Loss: 35.96854019165039, L1: 9.838638305664062, L3: 26.129901885986328\n",
      "Current prediction:  60.75069046020508 \n",
      "\n",
      "Iteration 8280, Loss: 34.85613250732422, L1: 9.792000770568848, L3: 25.064132690429688\n",
      "Current prediction:  61.05231857299805 \n",
      "\n",
      "Iteration 8281, Loss: 36.106998443603516, L1: 9.768656730651855, L3: 26.338340759277344\n",
      "Current prediction:  61.48133850097656 \n",
      "\n",
      "Iteration 8282, Loss: 35.825225830078125, L1: 9.760957717895508, L3: 26.064266204833984\n",
      "Current prediction:  61.50710678100586 \n",
      "\n",
      "Iteration 8283, Loss: 36.524314880371094, L1: 9.66770076751709, L3: 26.856613159179688\n",
      "Current prediction:  60.989776611328125 \n",
      "\n",
      "Iteration 8284, Loss: 36.02239227294922, L1: 9.703359603881836, L3: 26.31903076171875\n",
      "Current prediction:  60.734188079833984 \n",
      "\n",
      "Iteration 8285, Loss: 36.275901794433594, L1: 9.730764389038086, L3: 26.545137405395508\n",
      "Current prediction:  60.72121047973633 \n",
      "\n",
      "Iteration 8286, Loss: 36.016014099121094, L1: 9.688364028930664, L3: 26.32765007019043\n",
      "Current prediction:  60.70083999633789 \n",
      "\n",
      "Iteration 8287, Loss: 35.69861602783203, L1: 9.763142585754395, L3: 25.935474395751953\n",
      "Current prediction:  60.91065216064453 \n",
      "\n",
      "Iteration 8288, Loss: 35.331459045410156, L1: 9.650175094604492, L3: 25.681283950805664\n",
      "Current prediction:  61.53806686401367 \n",
      "\n",
      "Iteration 8289, Loss: 37.47869873046875, L1: 9.640565872192383, L3: 27.838130950927734\n",
      "Current prediction:  61.62012481689453 \n",
      "\n",
      "Iteration 8290, Loss: 35.758384704589844, L1: 9.723352432250977, L3: 26.035030364990234\n",
      "Current prediction:  61.53178787231445 \n",
      "\n",
      "Iteration 8291, Loss: 36.53154754638672, L1: 9.630743026733398, L3: 26.900806427001953\n",
      "Current prediction:  61.47493362426758 \n",
      "\n",
      "Iteration 8292, Loss: 35.93665313720703, L1: 9.717572212219238, L3: 26.21908187866211\n",
      "Current prediction:  61.01335906982422 \n",
      "\n",
      "Iteration 8293, Loss: 36.362579345703125, L1: 9.680990219116211, L3: 26.681591033935547\n",
      "Current prediction:  60.636898040771484 \n",
      "\n",
      "Iteration 8294, Loss: 36.247947692871094, L1: 9.760442733764648, L3: 26.487503051757812\n",
      "Current prediction:  60.60691833496094 \n",
      "\n",
      "Iteration 8295, Loss: 36.178768157958984, L1: 9.837011337280273, L3: 26.34175682067871\n",
      "Current prediction:  60.6325798034668 \n",
      "\n",
      "Iteration 8296, Loss: 36.56807327270508, L1: 9.77774715423584, L3: 26.790325164794922\n",
      "Current prediction:  61.11341857910156 \n",
      "\n",
      "Iteration 8297, Loss: 36.0071907043457, L1: 9.73576831817627, L3: 26.271421432495117\n",
      "Current prediction:  61.55849838256836 \n",
      "\n",
      "Iteration 8298, Loss: 36.087158203125, L1: 9.809917449951172, L3: 26.277240753173828\n",
      "Current prediction:  61.57939910888672 \n",
      "\n",
      "Iteration 8299, Loss: 36.41063690185547, L1: 9.887720108032227, L3: 26.52291488647461\n",
      "Current prediction:  61.566104888916016 \n",
      "\n",
      "Iteration 8300, Loss: 36.521854400634766, L1: 9.848149299621582, L3: 26.6737060546875\n",
      "Current prediction:  61.552337646484375 \n",
      "\n",
      "Iteration 8301, Loss: 35.923545837402344, L1: 9.82400131225586, L3: 26.099544525146484\n",
      "Current prediction:  61.41388702392578 \n",
      "\n",
      "Iteration 8302, Loss: 35.621646881103516, L1: 9.801579475402832, L3: 25.820068359375\n",
      "Current prediction:  60.732078552246094 \n",
      "\n",
      "Iteration 8303, Loss: 35.537845611572266, L1: 9.75709342956543, L3: 25.780752182006836\n",
      "Current prediction:  60.557960510253906 \n",
      "\n",
      "Iteration 8304, Loss: 36.00259780883789, L1: 10.034811973571777, L3: 25.967784881591797\n",
      "Current prediction:  60.55274200439453 \n",
      "\n",
      "Iteration 8305, Loss: 35.88768005371094, L1: 9.949467658996582, L3: 25.938213348388672\n",
      "Current prediction:  60.56645965576172 \n",
      "\n",
      "Iteration 8306, Loss: 36.17152786254883, L1: 9.893012046813965, L3: 26.278514862060547\n",
      "Current prediction:  60.610435485839844 \n",
      "\n",
      "Iteration 8307, Loss: 36.03144073486328, L1: 9.824014663696289, L3: 26.20742416381836\n",
      "Current prediction:  61.30601501464844 \n",
      "\n",
      "Iteration 8308, Loss: 35.966976165771484, L1: 9.683337211608887, L3: 26.28363800048828\n",
      "Current prediction:  61.617698669433594 \n",
      "\n",
      "Iteration 8309, Loss: 35.9320182800293, L1: 9.74252986907959, L3: 26.18948745727539\n",
      "Current prediction:  61.63309097290039 \n",
      "\n",
      "Iteration 8310, Loss: 35.15713882446289, L1: 9.84514331817627, L3: 25.311994552612305\n",
      "Current prediction:  61.63533401489258 \n",
      "\n",
      "Iteration 8311, Loss: 35.98160934448242, L1: 9.866063117980957, L3: 26.11554718017578\n",
      "Current prediction:  61.63223648071289 \n",
      "\n",
      "Iteration 8312, Loss: 35.83799743652344, L1: 9.83637809753418, L3: 26.001617431640625\n",
      "Current prediction:  61.6268310546875 \n",
      "\n",
      "Iteration 8313, Loss: 35.75212097167969, L1: 9.751077651977539, L3: 26.00104522705078\n",
      "Current prediction:  61.59935760498047 \n",
      "\n",
      "Iteration 8314, Loss: 36.5050048828125, L1: 9.813072204589844, L3: 26.69193458557129\n",
      "Current prediction:  61.35729217529297 \n",
      "\n",
      "Iteration 8315, Loss: 36.05813980102539, L1: 9.7830228805542, L3: 26.275117874145508\n",
      "Current prediction:  60.72201919555664 \n",
      "\n",
      "Iteration 8316, Loss: 35.88702392578125, L1: 9.820066452026367, L3: 26.066959381103516\n",
      "Current prediction:  60.614471435546875 \n",
      "\n",
      "Iteration 8317, Loss: 35.521392822265625, L1: 9.854986190795898, L3: 25.666406631469727\n",
      "Current prediction:  60.6085090637207 \n",
      "\n",
      "Iteration 8318, Loss: 36.22705841064453, L1: 9.831100463867188, L3: 26.39595603942871\n",
      "Current prediction:  60.8973388671875 \n",
      "\n",
      "Iteration 8319, Loss: 36.500877380371094, L1: 9.796038627624512, L3: 26.7048397064209\n",
      "Current prediction:  61.28797912597656 \n",
      "\n",
      "Iteration 8320, Loss: 35.75360107421875, L1: 9.77415943145752, L3: 25.979440689086914\n",
      "Current prediction:  61.404903411865234 \n",
      "\n",
      "Iteration 8321, Loss: 36.128170013427734, L1: 9.769009590148926, L3: 26.359159469604492\n",
      "Current prediction:  61.37516784667969 \n",
      "\n",
      "Iteration 8322, Loss: 36.18099594116211, L1: 9.823310852050781, L3: 26.357685089111328\n",
      "Current prediction:  61.2357292175293 \n",
      "\n",
      "Iteration 8323, Loss: 36.22673034667969, L1: 9.747690200805664, L3: 26.47903823852539\n",
      "Current prediction:  61.02070236206055 \n",
      "\n",
      "Iteration 8324, Loss: 35.29708480834961, L1: 9.751953125, L3: 25.54513168334961\n",
      "Current prediction:  60.7750129699707 \n",
      "\n",
      "Iteration 8325, Loss: 35.9507942199707, L1: 9.750231742858887, L3: 26.2005615234375\n",
      "Current prediction:  60.80009460449219 \n",
      "\n",
      "Iteration 8326, Loss: 36.03416442871094, L1: 9.795600891113281, L3: 26.23856544494629\n",
      "Current prediction:  60.91873550415039 \n",
      "\n",
      "Iteration 8327, Loss: 35.7187614440918, L1: 9.735501289367676, L3: 25.983259201049805\n",
      "Current prediction:  61.13442611694336 \n",
      "\n",
      "Iteration 8328, Loss: 37.20812225341797, L1: 9.770090103149414, L3: 27.438032150268555\n",
      "Current prediction:  61.494140625 \n",
      "\n",
      "Iteration 8329, Loss: 35.606239318847656, L1: 9.766265869140625, L3: 25.839975357055664\n",
      "Current prediction:  61.53548812866211 \n",
      "\n",
      "Iteration 8330, Loss: 36.610801696777344, L1: 9.711017608642578, L3: 26.899782180786133\n",
      "Current prediction:  61.46980285644531 \n",
      "\n",
      "Iteration 8331, Loss: 35.541908264160156, L1: 9.697342872619629, L3: 25.844566345214844\n",
      "Current prediction:  61.41337585449219 \n",
      "\n",
      "Iteration 8332, Loss: 35.463348388671875, L1: 9.725154876708984, L3: 25.738191604614258\n",
      "Current prediction:  61.05318069458008 \n",
      "\n",
      "Iteration 8333, Loss: 34.99170684814453, L1: 9.750572204589844, L3: 25.241134643554688\n",
      "Current prediction:  60.794429779052734 \n",
      "\n",
      "Iteration 8334, Loss: 36.352638244628906, L1: 9.745832443237305, L3: 26.6068058013916\n",
      "Current prediction:  60.68731689453125 \n",
      "\n",
      "Iteration 8335, Loss: 36.22984313964844, L1: 9.794180870056152, L3: 26.4356632232666\n",
      "Current prediction:  60.6819953918457 \n",
      "\n",
      "Iteration 8336, Loss: 36.8267707824707, L1: 9.737515449523926, L3: 27.089256286621094\n",
      "Current prediction:  60.7447509765625 \n",
      "\n",
      "Iteration 8337, Loss: 35.42082977294922, L1: 9.755805015563965, L3: 25.66502571105957\n",
      "Current prediction:  61.38602066040039 \n",
      "\n",
      "Iteration 8338, Loss: 36.59290313720703, L1: 9.670099258422852, L3: 26.92280387878418\n",
      "Current prediction:  61.61741638183594 \n",
      "\n",
      "Iteration 8339, Loss: 35.71641540527344, L1: 9.75049114227295, L3: 25.965925216674805\n",
      "Current prediction:  61.6495361328125 \n",
      "\n",
      "Iteration 8340, Loss: 36.08155822753906, L1: 9.711023330688477, L3: 26.370534896850586\n",
      "Current prediction:  61.630489349365234 \n",
      "\n",
      "Iteration 8341, Loss: 36.49441909790039, L1: 9.691410064697266, L3: 26.803009033203125\n",
      "Current prediction:  61.14752960205078 \n",
      "\n",
      "Iteration 8342, Loss: 36.107261657714844, L1: 9.706884384155273, L3: 26.40037727355957\n",
      "Current prediction:  60.69545364379883 \n",
      "\n",
      "Iteration 8343, Loss: 37.47380828857422, L1: 9.726882934570312, L3: 27.74692726135254\n",
      "Current prediction:  60.68167495727539 \n",
      "\n",
      "Iteration 8344, Loss: 36.337013244628906, L1: 9.699539184570312, L3: 26.63747215270996\n",
      "Current prediction:  61.14259338378906 \n",
      "\n",
      "Iteration 8345, Loss: 36.26131057739258, L1: 9.728034019470215, L3: 26.53327751159668\n",
      "Current prediction:  61.570648193359375 \n",
      "\n",
      "Iteration 8346, Loss: 36.71670913696289, L1: 9.75500774383545, L3: 26.961700439453125\n",
      "Current prediction:  61.59572982788086 \n",
      "\n",
      "Iteration 8347, Loss: 36.797821044921875, L1: 9.818827629089355, L3: 26.978994369506836\n",
      "Current prediction:  61.590919494628906 \n",
      "\n",
      "Iteration 8348, Loss: 36.243507385253906, L1: 9.731133460998535, L3: 26.512374877929688\n",
      "Current prediction:  61.581295013427734 \n",
      "\n",
      "Iteration 8349, Loss: 35.41584014892578, L1: 9.8614501953125, L3: 25.55438804626465\n",
      "Current prediction:  61.57638931274414 \n",
      "\n",
      "Iteration 8350, Loss: 35.97513198852539, L1: 9.882322311401367, L3: 26.092809677124023\n",
      "Current prediction:  61.56448745727539 \n",
      "\n",
      "Iteration 8351, Loss: 35.37815475463867, L1: 9.871795654296875, L3: 25.506359100341797\n",
      "Current prediction:  61.54088592529297 \n",
      "\n",
      "Iteration 8352, Loss: 35.679039001464844, L1: 9.810324668884277, L3: 25.868715286254883\n",
      "Current prediction:  61.255592346191406 \n",
      "\n",
      "Iteration 8353, Loss: 35.83820724487305, L1: 9.770562171936035, L3: 26.067646026611328\n",
      "Current prediction:  60.560848236083984 \n",
      "\n",
      "Iteration 8354, Loss: 36.58319854736328, L1: 9.862245559692383, L3: 26.7209529876709\n",
      "Current prediction:  60.52439498901367 \n",
      "\n",
      "Iteration 8355, Loss: 36.61954879760742, L1: 9.89509105682373, L3: 26.724456787109375\n",
      "Current prediction:  60.535133361816406 \n",
      "\n",
      "Iteration 8356, Loss: 35.79790115356445, L1: 9.863551139831543, L3: 25.934349060058594\n",
      "Current prediction:  60.55412292480469 \n",
      "\n",
      "Iteration 8357, Loss: 35.74279022216797, L1: 9.872478485107422, L3: 25.870309829711914\n",
      "Current prediction:  60.59185791015625 \n",
      "\n",
      "Iteration 8358, Loss: 36.459388732910156, L1: 9.849645614624023, L3: 26.6097412109375\n",
      "Current prediction:  60.85490036010742 \n",
      "\n",
      "Iteration 8359, Loss: 36.328975677490234, L1: 9.760355949401855, L3: 26.568618774414062\n",
      "Current prediction:  61.569278717041016 \n",
      "\n",
      "Iteration 8360, Loss: 35.718894958496094, L1: 9.715124130249023, L3: 26.00377082824707\n",
      "Current prediction:  61.64689254760742 \n",
      "\n",
      "Iteration 8361, Loss: 37.818695068359375, L1: 9.683572769165039, L3: 28.13512420654297\n",
      "Current prediction:  61.665138244628906 \n",
      "\n",
      "Iteration 8362, Loss: 36.334938049316406, L1: 9.729837417602539, L3: 26.605100631713867\n",
      "Current prediction:  61.66440963745117 \n",
      "\n",
      "Iteration 8363, Loss: 35.51641082763672, L1: 9.72266674041748, L3: 25.793745040893555\n",
      "Current prediction:  61.6339111328125 \n",
      "\n",
      "Iteration 8364, Loss: 36.13936233520508, L1: 9.75731372833252, L3: 26.382049560546875\n",
      "Current prediction:  61.439353942871094 \n",
      "\n",
      "Iteration 8365, Loss: 35.748268127441406, L1: 9.679633140563965, L3: 26.068634033203125\n",
      "Current prediction:  60.93090057373047 \n",
      "\n",
      "Iteration 8366, Loss: 36.0765495300293, L1: 9.695582389831543, L3: 26.380966186523438\n",
      "Current prediction:  60.754878997802734 \n",
      "\n",
      "Iteration 8367, Loss: 36.43726348876953, L1: 9.661209106445312, L3: 26.77605628967285\n",
      "Current prediction:  60.789161682128906 \n",
      "\n",
      "Iteration 8368, Loss: 35.82188034057617, L1: 9.707386016845703, L3: 26.11449432373047\n",
      "Current prediction:  60.894412994384766 \n",
      "\n",
      "Iteration 8369, Loss: 35.52106475830078, L1: 9.768440246582031, L3: 25.752626419067383\n",
      "Current prediction:  61.390716552734375 \n",
      "\n",
      "Iteration 8370, Loss: 37.11814498901367, L1: 9.72019100189209, L3: 27.397953033447266\n",
      "Current prediction:  61.368587493896484 \n",
      "\n",
      "Iteration 8371, Loss: 35.871299743652344, L1: 9.721367835998535, L3: 26.149932861328125\n",
      "Current prediction:  61.2681999206543 \n",
      "\n",
      "Iteration 8372, Loss: 36.429588317871094, L1: 9.74259090423584, L3: 26.686996459960938\n",
      "Current prediction:  61.11866760253906 \n",
      "\n",
      "Iteration 8373, Loss: 35.10931396484375, L1: 9.756446838378906, L3: 25.35286521911621\n",
      "Current prediction:  61.31737518310547 \n",
      "\n",
      "Iteration 8374, Loss: 36.69355773925781, L1: 9.699065208435059, L3: 26.99449348449707\n",
      "Current prediction:  61.476593017578125 \n",
      "\n",
      "Iteration 8375, Loss: 35.78159713745117, L1: 9.798030853271484, L3: 25.983566284179688\n",
      "Current prediction:  61.43008804321289 \n",
      "\n",
      "Iteration 8376, Loss: 36.584800720214844, L1: 9.757085800170898, L3: 26.827714920043945\n",
      "Current prediction:  61.07481002807617 \n",
      "\n",
      "Iteration 8377, Loss: 35.296173095703125, L1: 9.72422981262207, L3: 25.571945190429688\n",
      "Current prediction:  60.86002731323242 \n",
      "\n",
      "Iteration 8378, Loss: 36.8447151184082, L1: 9.767525672912598, L3: 27.077190399169922\n",
      "Current prediction:  60.97674560546875 \n",
      "\n",
      "Iteration 8379, Loss: 35.58576965332031, L1: 9.775362968444824, L3: 25.810405731201172\n",
      "Current prediction:  61.17079544067383 \n",
      "\n",
      "Iteration 8380, Loss: 35.34746170043945, L1: 9.746682167053223, L3: 25.600780487060547\n",
      "Current prediction:  61.291595458984375 \n",
      "\n",
      "Iteration 8381, Loss: 35.94672775268555, L1: 9.732077598571777, L3: 26.214649200439453\n",
      "Current prediction:  61.13359069824219 \n",
      "\n",
      "Iteration 8382, Loss: 35.40256118774414, L1: 9.720510482788086, L3: 25.682050704956055\n",
      "Current prediction:  61.21561050415039 \n",
      "\n",
      "Iteration 8383, Loss: 36.38899230957031, L1: 9.718116760253906, L3: 26.670875549316406\n",
      "Current prediction:  61.035682678222656 \n",
      "\n",
      "Iteration 8384, Loss: 35.29486846923828, L1: 9.671360969543457, L3: 25.62350845336914\n",
      "Current prediction:  60.746158599853516 \n",
      "\n",
      "Iteration 8385, Loss: 35.955902099609375, L1: 9.713565826416016, L3: 26.242334365844727\n",
      "Current prediction:  60.72052001953125 \n",
      "\n",
      "Iteration 8386, Loss: 35.85785675048828, L1: 9.702116012573242, L3: 26.15574073791504\n",
      "Current prediction:  60.79929733276367 \n",
      "\n",
      "Iteration 8387, Loss: 36.18476104736328, L1: 9.72380256652832, L3: 26.46095848083496\n",
      "Current prediction:  61.159732818603516 \n",
      "\n",
      "Iteration 8388, Loss: 36.590641021728516, L1: 9.644627571105957, L3: 26.946012496948242\n",
      "Current prediction:  61.57218551635742 \n",
      "\n",
      "Iteration 8389, Loss: 35.73471450805664, L1: 9.670331001281738, L3: 26.06438446044922\n",
      "Current prediction:  61.61660385131836 \n",
      "\n",
      "Iteration 8390, Loss: 36.510921478271484, L1: 9.723063468933105, L3: 26.787857055664062\n",
      "Current prediction:  61.590030670166016 \n",
      "\n",
      "Iteration 8391, Loss: 36.556243896484375, L1: 9.681145668029785, L3: 26.875097274780273\n",
      "Current prediction:  61.465538024902344 \n",
      "\n",
      "Iteration 8392, Loss: 37.2142448425293, L1: 9.670698165893555, L3: 27.543546676635742\n",
      "Current prediction:  61.25336837768555 \n",
      "\n",
      "Iteration 8393, Loss: 35.52078628540039, L1: 9.683258056640625, L3: 25.837528228759766\n",
      "Current prediction:  61.23006820678711 \n",
      "\n",
      "Iteration 8394, Loss: 36.255496978759766, L1: 9.707282066345215, L3: 26.548215866088867\n",
      "Current prediction:  61.29023742675781 \n",
      "\n",
      "Iteration 8395, Loss: 36.88276672363281, L1: 9.714924812316895, L3: 27.167842864990234\n",
      "Current prediction:  61.159854888916016 \n",
      "\n",
      "Iteration 8396, Loss: 36.646080017089844, L1: 9.731684684753418, L3: 26.91439437866211\n",
      "Current prediction:  60.938201904296875 \n",
      "\n",
      "Iteration 8397, Loss: 36.931053161621094, L1: 9.7699613571167, L3: 27.161090850830078\n",
      "Current prediction:  60.840843200683594 \n",
      "\n",
      "Iteration 8398, Loss: 35.18080139160156, L1: 9.800359725952148, L3: 25.380443572998047\n",
      "Current prediction:  60.722557067871094 \n",
      "\n",
      "Iteration 8399, Loss: 36.22815704345703, L1: 9.808149337768555, L3: 26.42000961303711\n",
      "Current prediction:  60.82960891723633 \n",
      "\n",
      "Iteration 8400, Loss: 35.747596740722656, L1: 9.821907997131348, L3: 25.925689697265625\n",
      "Current prediction:  61.012489318847656 \n",
      "\n",
      "Iteration 8401, Loss: 35.68852996826172, L1: 9.801297187805176, L3: 25.88723373413086\n",
      "Current prediction:  61.3622932434082 \n",
      "\n",
      "Iteration 8402, Loss: 35.98918914794922, L1: 9.72696304321289, L3: 26.262224197387695\n",
      "Current prediction:  61.43351364135742 \n",
      "\n",
      "Iteration 8403, Loss: 35.923744201660156, L1: 9.769248008728027, L3: 26.154497146606445\n",
      "Current prediction:  61.294246673583984 \n",
      "\n",
      "Iteration 8404, Loss: 36.137855529785156, L1: 9.775677680969238, L3: 26.362178802490234\n",
      "Current prediction:  60.908103942871094 \n",
      "\n",
      "Iteration 8405, Loss: 35.81201934814453, L1: 9.724781036376953, L3: 26.087238311767578\n",
      "Current prediction:  60.73834228515625 \n",
      "\n",
      "Iteration 8406, Loss: 36.221988677978516, L1: 9.733023643493652, L3: 26.48896598815918\n",
      "Current prediction:  60.817989349365234 \n",
      "\n",
      "Iteration 8407, Loss: 36.59628677368164, L1: 9.726685523986816, L3: 26.869600296020508\n",
      "Current prediction:  61.18669509887695 \n",
      "\n",
      "Iteration 8408, Loss: 35.818016052246094, L1: 9.693562507629395, L3: 26.124452590942383\n",
      "Current prediction:  61.49885559082031 \n",
      "\n",
      "Iteration 8409, Loss: 36.13055419921875, L1: 9.641937255859375, L3: 26.488618850708008\n",
      "Current prediction:  61.62982177734375 \n",
      "\n",
      "Iteration 8410, Loss: 36.064659118652344, L1: 9.736470222473145, L3: 26.328187942504883\n",
      "Current prediction:  61.64078903198242 \n",
      "\n",
      "Iteration 8411, Loss: 35.216758728027344, L1: 9.681953430175781, L3: 25.534807205200195\n",
      "Current prediction:  61.354244232177734 \n",
      "\n",
      "Iteration 8412, Loss: 36.254356384277344, L1: 9.641823768615723, L3: 26.612531661987305\n",
      "Current prediction:  60.6795654296875 \n",
      "\n",
      "Iteration 8413, Loss: 36.692649841308594, L1: 9.767485618591309, L3: 26.9251651763916\n",
      "Current prediction:  60.61807632446289 \n",
      "\n",
      "Iteration 8414, Loss: 35.91129684448242, L1: 9.811380386352539, L3: 26.099916458129883\n",
      "Current prediction:  60.61947250366211 \n",
      "\n",
      "Iteration 8415, Loss: 36.11311340332031, L1: 9.815899848937988, L3: 26.297212600708008\n",
      "Current prediction:  60.641693115234375 \n",
      "\n",
      "Iteration 8416, Loss: 35.57244110107422, L1: 9.749114990234375, L3: 25.823328018188477\n",
      "Current prediction:  60.82427978515625 \n",
      "\n",
      "Iteration 8417, Loss: 36.835182189941406, L1: 9.651159286499023, L3: 27.184024810791016\n",
      "Current prediction:  61.439144134521484 \n",
      "\n",
      "Iteration 8418, Loss: 35.61656188964844, L1: 9.670351028442383, L3: 25.946212768554688\n",
      "Current prediction:  61.57758712768555 \n",
      "\n",
      "Iteration 8419, Loss: 35.61548614501953, L1: 9.694801330566406, L3: 25.920684814453125\n",
      "Current prediction:  61.52974319458008 \n",
      "\n",
      "Iteration 8420, Loss: 35.94236373901367, L1: 9.661093711853027, L3: 26.28127098083496\n",
      "Current prediction:  61.276546478271484 \n",
      "\n",
      "Iteration 8421, Loss: 36.43331527709961, L1: 9.638579368591309, L3: 26.794736862182617\n",
      "Current prediction:  60.7967414855957 \n",
      "\n",
      "Iteration 8422, Loss: 36.28208923339844, L1: 9.707050323486328, L3: 26.575037002563477\n",
      "Current prediction:  60.647762298583984 \n",
      "\n",
      "Iteration 8423, Loss: 35.32510757446289, L1: 9.723479270935059, L3: 25.601627349853516\n",
      "Current prediction:  60.67404556274414 \n",
      "\n",
      "Iteration 8424, Loss: 35.533226013183594, L1: 9.687779426574707, L3: 25.84544563293457\n",
      "Current prediction:  60.8979377746582 \n",
      "\n",
      "Iteration 8425, Loss: 35.18867874145508, L1: 9.704874992370605, L3: 25.48380470275879\n",
      "Current prediction:  61.06022262573242 \n",
      "\n",
      "Iteration 8426, Loss: 35.78515625, L1: 9.627028465270996, L3: 26.158126831054688\n",
      "Current prediction:  61.29928970336914 \n",
      "\n",
      "Iteration 8427, Loss: 36.10936737060547, L1: 9.664715766906738, L3: 26.444652557373047\n",
      "Current prediction:  61.49372863769531 \n",
      "\n",
      "Iteration 8428, Loss: 35.69712448120117, L1: 9.655121803283691, L3: 26.042003631591797\n",
      "Current prediction:  61.2518424987793 \n",
      "\n",
      "Iteration 8429, Loss: 35.68507385253906, L1: 9.65956974029541, L3: 26.025503158569336\n",
      "Current prediction:  60.75735092163086 \n",
      "\n",
      "Iteration 8430, Loss: 35.977142333984375, L1: 9.736005783081055, L3: 26.24113655090332\n",
      "Current prediction:  60.721378326416016 \n",
      "\n",
      "Iteration 8431, Loss: 35.243690490722656, L1: 9.735095977783203, L3: 25.508594512939453\n",
      "Current prediction:  60.8150634765625 \n",
      "\n",
      "Iteration 8432, Loss: 36.56364440917969, L1: 9.739509582519531, L3: 26.82413673400879\n",
      "Current prediction:  61.1955680847168 \n",
      "\n",
      "Iteration 8433, Loss: 36.0802001953125, L1: 9.693302154541016, L3: 26.38689613342285\n",
      "Current prediction:  61.496055603027344 \n",
      "\n",
      "Iteration 8434, Loss: 35.592918395996094, L1: 9.6612548828125, L3: 25.93166160583496\n",
      "Current prediction:  61.58563232421875 \n",
      "\n",
      "Iteration 8435, Loss: 35.11839294433594, L1: 9.678333282470703, L3: 25.440059661865234\n",
      "Current prediction:  61.55971908569336 \n",
      "\n",
      "Iteration 8436, Loss: 36.10042190551758, L1: 9.691317558288574, L3: 26.40910530090332\n",
      "Current prediction:  60.99641418457031 \n",
      "\n",
      "Iteration 8437, Loss: 36.50681686401367, L1: 9.68453598022461, L3: 26.822280883789062\n",
      "Current prediction:  60.66507339477539 \n",
      "\n",
      "Iteration 8438, Loss: 36.8220329284668, L1: 9.781951904296875, L3: 27.040081024169922\n",
      "Current prediction:  60.682533264160156 \n",
      "\n",
      "Iteration 8439, Loss: 37.047183990478516, L1: 9.746211051940918, L3: 27.30097198486328\n",
      "Current prediction:  61.3002815246582 \n",
      "\n",
      "Iteration 8440, Loss: 36.913936614990234, L1: 9.712515830993652, L3: 27.2014217376709\n",
      "Current prediction:  61.545372009277344 \n",
      "\n",
      "Iteration 8441, Loss: 35.691165924072266, L1: 9.734248161315918, L3: 25.95691680908203\n",
      "Current prediction:  61.56692886352539 \n",
      "\n",
      "Iteration 8442, Loss: 35.45461654663086, L1: 9.73845386505127, L3: 25.716161727905273\n",
      "Current prediction:  61.53028106689453 \n",
      "\n",
      "Iteration 8443, Loss: 36.72784423828125, L1: 9.802621841430664, L3: 26.925222396850586\n",
      "Current prediction:  61.395729064941406 \n",
      "\n",
      "Iteration 8444, Loss: 36.806087493896484, L1: 9.745247840881348, L3: 27.060840606689453\n",
      "Current prediction:  60.85043716430664 \n",
      "\n",
      "Iteration 8445, Loss: 35.77073669433594, L1: 9.777177810668945, L3: 25.99355697631836\n",
      "Current prediction:  60.6751594543457 \n",
      "\n",
      "Iteration 8446, Loss: 35.769474029541016, L1: 9.750279426574707, L3: 26.019193649291992\n",
      "Current prediction:  60.702484130859375 \n",
      "\n",
      "Iteration 8447, Loss: 35.62358856201172, L1: 9.805213928222656, L3: 25.81837272644043\n",
      "Current prediction:  60.69210433959961 \n",
      "\n",
      "Iteration 8448, Loss: 36.19800567626953, L1: 9.729382514953613, L3: 26.468624114990234\n",
      "Current prediction:  61.03485870361328 \n",
      "\n",
      "Iteration 8449, Loss: 36.07368469238281, L1: 9.78994369506836, L3: 26.283742904663086\n",
      "Current prediction:  61.335365295410156 \n",
      "\n",
      "Iteration 8450, Loss: 36.50001525878906, L1: 9.695088386535645, L3: 26.8049259185791\n",
      "Current prediction:  61.098079681396484 \n",
      "\n",
      "Iteration 8451, Loss: 35.800254821777344, L1: 9.643237113952637, L3: 26.15701675415039\n",
      "Current prediction:  60.9824104309082 \n",
      "\n",
      "Iteration 8452, Loss: 36.56867218017578, L1: 9.63424301147461, L3: 26.934431076049805\n",
      "Current prediction:  60.91853713989258 \n",
      "\n",
      "Iteration 8453, Loss: 36.3074951171875, L1: 9.6868896484375, L3: 26.62060546875\n",
      "Current prediction:  60.77203369140625 \n",
      "\n",
      "Iteration 8454, Loss: 36.2372932434082, L1: 9.67968463897705, L3: 26.55760955810547\n",
      "Current prediction:  60.71049499511719 \n",
      "\n",
      "Iteration 8455, Loss: 36.29935073852539, L1: 9.66705322265625, L3: 26.63229751586914\n",
      "Current prediction:  60.69172286987305 \n",
      "\n",
      "Iteration 8456, Loss: 35.094627380371094, L1: 9.697635650634766, L3: 25.39699363708496\n",
      "Current prediction:  60.801658630371094 \n",
      "\n",
      "Iteration 8457, Loss: 36.322364807128906, L1: 9.654867172241211, L3: 26.667497634887695\n",
      "Current prediction:  61.542049407958984 \n",
      "\n",
      "Iteration 8458, Loss: 37.436485290527344, L1: 9.63443660736084, L3: 27.80204963684082\n",
      "Current prediction:  61.67990493774414 \n",
      "\n",
      "Iteration 8459, Loss: 36.54540252685547, L1: 9.68226432800293, L3: 26.86313819885254\n",
      "Current prediction:  61.669490814208984 \n",
      "\n",
      "Iteration 8460, Loss: 34.42686462402344, L1: 9.714201927185059, L3: 24.712661743164062\n",
      "Current prediction:  61.65349197387695 \n",
      "\n",
      "Iteration 8461, Loss: 36.44257354736328, L1: 9.778875350952148, L3: 26.6636962890625\n",
      "Current prediction:  61.615116119384766 \n",
      "\n",
      "Iteration 8462, Loss: 36.104393005371094, L1: 9.701634407043457, L3: 26.402759552001953\n",
      "Current prediction:  61.247894287109375 \n",
      "\n",
      "Iteration 8463, Loss: 35.7259635925293, L1: 9.675192832946777, L3: 26.050771713256836\n",
      "Current prediction:  60.66626739501953 \n",
      "\n",
      "Iteration 8464, Loss: 36.483123779296875, L1: 9.752102851867676, L3: 26.731021881103516\n",
      "Current prediction:  60.60025405883789 \n",
      "\n",
      "Iteration 8465, Loss: 35.80895233154297, L1: 9.763237953186035, L3: 26.045713424682617\n",
      "Current prediction:  60.56661605834961 \n",
      "\n",
      "Iteration 8466, Loss: 35.89976501464844, L1: 9.835526466369629, L3: 26.064239501953125\n",
      "Current prediction:  60.55811309814453 \n",
      "\n",
      "Iteration 8467, Loss: 35.281654357910156, L1: 9.765253067016602, L3: 25.516399383544922\n",
      "Current prediction:  60.802818298339844 \n",
      "\n",
      "Iteration 8468, Loss: 36.27848815917969, L1: 9.720748901367188, L3: 26.557741165161133\n",
      "Current prediction:  61.40234375 \n",
      "\n",
      "Iteration 8469, Loss: 35.86067199707031, L1: 9.676115036010742, L3: 26.184558868408203\n",
      "Current prediction:  61.54844284057617 \n",
      "\n",
      "Iteration 8470, Loss: 36.37374496459961, L1: 9.740771293640137, L3: 26.632972717285156\n",
      "Current prediction:  61.57698059082031 \n",
      "\n",
      "Iteration 8471, Loss: 36.47815704345703, L1: 9.705132484436035, L3: 26.773025512695312\n",
      "Current prediction:  61.57527160644531 \n",
      "\n",
      "Iteration 8472, Loss: 35.91615295410156, L1: 9.728006362915039, L3: 26.18814468383789\n",
      "Current prediction:  61.47565841674805 \n",
      "\n",
      "Iteration 8473, Loss: 35.667022705078125, L1: 9.713963508605957, L3: 25.953060150146484\n",
      "Current prediction:  60.78138732910156 \n",
      "\n",
      "Iteration 8474, Loss: 36.077659606933594, L1: 9.671147346496582, L3: 26.406513214111328\n",
      "Current prediction:  60.59671401977539 \n",
      "\n",
      "Iteration 8475, Loss: 36.53299331665039, L1: 9.769291877746582, L3: 26.763700485229492\n",
      "Current prediction:  60.59461975097656 \n",
      "\n",
      "Iteration 8476, Loss: 35.80204772949219, L1: 9.791881561279297, L3: 26.010168075561523\n",
      "Current prediction:  60.97795867919922 \n",
      "\n",
      "Iteration 8477, Loss: 36.18041229248047, L1: 9.68803882598877, L3: 26.492374420166016\n",
      "Current prediction:  61.578182220458984 \n",
      "\n",
      "Iteration 8478, Loss: 36.44045639038086, L1: 9.641338348388672, L3: 26.799118041992188\n",
      "Current prediction:  61.6142578125 \n",
      "\n",
      "Iteration 8479, Loss: 36.397499084472656, L1: 9.726350784301758, L3: 26.67115020751953\n",
      "Current prediction:  61.6170539855957 \n",
      "\n",
      "Iteration 8480, Loss: 35.507843017578125, L1: 9.752983093261719, L3: 25.75486183166504\n",
      "Current prediction:  61.61487579345703 \n",
      "\n",
      "Iteration 8481, Loss: 36.94343566894531, L1: 9.737135887145996, L3: 27.206300735473633\n",
      "Current prediction:  61.59263610839844 \n",
      "\n",
      "Iteration 8482, Loss: 36.48773193359375, L1: 9.737686157226562, L3: 26.750043869018555\n",
      "Current prediction:  61.267967224121094 \n",
      "\n",
      "Iteration 8483, Loss: 35.907073974609375, L1: 9.683385848999023, L3: 26.22368812561035\n",
      "Current prediction:  60.68471145629883 \n",
      "\n",
      "Iteration 8484, Loss: 35.458824157714844, L1: 9.724342346191406, L3: 25.73448371887207\n",
      "Current prediction:  60.56831741333008 \n",
      "\n",
      "Iteration 8485, Loss: 35.95347595214844, L1: 9.727453231811523, L3: 26.226024627685547\n",
      "Current prediction:  60.675968170166016 \n",
      "\n",
      "Iteration 8486, Loss: 37.793174743652344, L1: 9.76301097869873, L3: 28.03016471862793\n",
      "Current prediction:  61.26799774169922 \n",
      "\n",
      "Iteration 8487, Loss: 35.94289779663086, L1: 9.76897144317627, L3: 26.173927307128906\n",
      "Current prediction:  61.51940155029297 \n",
      "\n",
      "Iteration 8488, Loss: 37.10076141357422, L1: 9.774774551391602, L3: 27.325986862182617\n",
      "Current prediction:  61.54295349121094 \n",
      "\n",
      "Iteration 8489, Loss: 36.2801628112793, L1: 9.730570793151855, L3: 26.549591064453125\n",
      "Current prediction:  61.404727935791016 \n",
      "\n",
      "Iteration 8490, Loss: 36.62367248535156, L1: 9.68722915649414, L3: 26.936443328857422\n",
      "Current prediction:  60.65792465209961 \n",
      "\n",
      "Iteration 8491, Loss: 35.29119873046875, L1: 9.71921443939209, L3: 25.571985244750977\n",
      "Current prediction:  60.55720520019531 \n",
      "\n",
      "Iteration 8492, Loss: 36.76880645751953, L1: 9.798723220825195, L3: 26.970081329345703\n",
      "Current prediction:  60.588077545166016 \n",
      "\n",
      "Iteration 8493, Loss: 34.78255844116211, L1: 9.785162925720215, L3: 24.99739646911621\n",
      "Current prediction:  60.95455551147461 \n",
      "\n",
      "Iteration 8494, Loss: 35.99144744873047, L1: 9.712169647216797, L3: 26.279277801513672\n",
      "Current prediction:  61.087730407714844 \n",
      "\n",
      "Iteration 8495, Loss: 35.64515686035156, L1: 9.707551956176758, L3: 25.937602996826172\n",
      "Current prediction:  61.20280075073242 \n",
      "\n",
      "Iteration 8496, Loss: 35.83680725097656, L1: 9.765114784240723, L3: 26.071691513061523\n",
      "Current prediction:  60.99216079711914 \n",
      "\n",
      "Iteration 8497, Loss: 36.698097229003906, L1: 9.6824951171875, L3: 27.01560401916504\n",
      "Current prediction:  61.12187576293945 \n",
      "\n",
      "Iteration 8498, Loss: 36.03554153442383, L1: 9.7061185836792, L3: 26.329421997070312\n",
      "Current prediction:  61.273284912109375 \n",
      "\n",
      "Iteration 8499, Loss: 36.43719482421875, L1: 9.678548812866211, L3: 26.758644104003906\n",
      "Current prediction:  61.0660400390625 \n",
      "\n",
      "Iteration 8500, Loss: 36.38551330566406, L1: 9.69530200958252, L3: 26.69021224975586\n",
      "Current prediction:  61.08845138549805 \n",
      "\n",
      "Iteration 8501, Loss: 36.16618728637695, L1: 9.746472358703613, L3: 26.419715881347656\n",
      "Current prediction:  61.38213348388672 \n",
      "\n",
      "Iteration 8502, Loss: 35.43041229248047, L1: 9.71507453918457, L3: 25.715335845947266\n",
      "Current prediction:  61.33336639404297 \n",
      "\n",
      "Iteration 8503, Loss: 36.38886642456055, L1: 9.710978507995605, L3: 26.677886962890625\n",
      "Current prediction:  61.13786315917969 \n",
      "\n",
      "Iteration 8504, Loss: 35.29658508300781, L1: 9.735000610351562, L3: 25.56158447265625\n",
      "Current prediction:  60.93372344970703 \n",
      "\n",
      "Iteration 8505, Loss: 35.314857482910156, L1: 9.654054641723633, L3: 25.66080093383789\n",
      "Current prediction:  61.1395149230957 \n",
      "\n",
      "Iteration 8506, Loss: 36.29612731933594, L1: 9.70815658569336, L3: 26.58797264099121\n",
      "Current prediction:  61.42067337036133 \n",
      "\n",
      "Iteration 8507, Loss: 36.15199661254883, L1: 9.68382453918457, L3: 26.468172073364258\n",
      "Current prediction:  61.55622863769531 \n",
      "\n",
      "Iteration 8508, Loss: 35.98699951171875, L1: 9.635786056518555, L3: 26.351211547851562\n",
      "Current prediction:  61.57899856567383 \n",
      "\n",
      "Iteration 8509, Loss: 36.07359313964844, L1: 9.61262321472168, L3: 26.46097183227539\n",
      "Current prediction:  61.47386932373047 \n",
      "\n",
      "Iteration 8510, Loss: 35.55823516845703, L1: 9.594049453735352, L3: 25.964187622070312\n",
      "Current prediction:  61.23395919799805 \n",
      "\n",
      "Iteration 8511, Loss: 37.10411834716797, L1: 9.60604190826416, L3: 27.498077392578125\n",
      "Current prediction:  60.87462615966797 \n",
      "\n",
      "Iteration 8512, Loss: 35.755027770996094, L1: 9.659536361694336, L3: 26.09549331665039\n",
      "Current prediction:  60.836265563964844 \n",
      "\n",
      "Iteration 8513, Loss: 36.688594818115234, L1: 9.62830638885498, L3: 27.060287475585938\n",
      "Current prediction:  61.2927360534668 \n",
      "\n",
      "Iteration 8514, Loss: 35.650211334228516, L1: 9.60502815246582, L3: 26.045183181762695\n",
      "Current prediction:  61.484500885009766 \n",
      "\n",
      "Iteration 8515, Loss: 35.55179214477539, L1: 9.61954402923584, L3: 25.932247161865234\n",
      "Current prediction:  61.501155853271484 \n",
      "\n",
      "Iteration 8516, Loss: 35.701080322265625, L1: 9.6133394241333, L3: 26.08774185180664\n",
      "Current prediction:  61.12480163574219 \n",
      "\n",
      "Iteration 8517, Loss: 36.09553909301758, L1: 9.624096870422363, L3: 26.47144317626953\n",
      "Current prediction:  60.71736526489258 \n",
      "\n",
      "Iteration 8518, Loss: 35.55791473388672, L1: 9.761850357055664, L3: 25.796064376831055\n",
      "Current prediction:  60.59336471557617 \n",
      "\n",
      "Iteration 8519, Loss: 36.39530944824219, L1: 9.7225341796875, L3: 26.67277717590332\n",
      "Current prediction:  60.72687911987305 \n",
      "\n",
      "Iteration 8520, Loss: 37.53556823730469, L1: 9.711487770080566, L3: 27.824081420898438\n",
      "Current prediction:  61.49599838256836 \n",
      "\n",
      "Iteration 8521, Loss: 35.50156021118164, L1: 9.731006622314453, L3: 25.770553588867188\n",
      "Current prediction:  61.56763458251953 \n",
      "\n",
      "Iteration 8522, Loss: 35.89616394042969, L1: 9.757865905761719, L3: 26.13829803466797\n",
      "Current prediction:  61.56889724731445 \n",
      "\n",
      "Iteration 8523, Loss: 36.71023941040039, L1: 9.788957595825195, L3: 26.921281814575195\n",
      "Current prediction:  61.56599044799805 \n",
      "\n",
      "Iteration 8524, Loss: 36.5992546081543, L1: 9.821195602416992, L3: 26.778059005737305\n",
      "Current prediction:  61.547969818115234 \n",
      "\n",
      "Iteration 8525, Loss: 35.295684814453125, L1: 9.798975944519043, L3: 25.4967098236084\n",
      "Current prediction:  61.35154724121094 \n",
      "\n",
      "Iteration 8526, Loss: 36.00897216796875, L1: 9.712952613830566, L3: 26.296018600463867\n",
      "Current prediction:  60.61259460449219 \n",
      "\n",
      "Iteration 8527, Loss: 35.995018005371094, L1: 9.727031707763672, L3: 26.267988204956055\n",
      "Current prediction:  60.50709533691406 \n",
      "\n",
      "Iteration 8528, Loss: 36.176612854003906, L1: 9.877582550048828, L3: 26.299028396606445\n",
      "Current prediction:  60.51144027709961 \n",
      "\n",
      "Iteration 8529, Loss: 36.88129425048828, L1: 9.834081649780273, L3: 27.047212600708008\n",
      "Current prediction:  60.51775360107422 \n",
      "\n",
      "Iteration 8530, Loss: 36.323673248291016, L1: 9.801104545593262, L3: 26.52256965637207\n",
      "Current prediction:  60.5679817199707 \n",
      "\n",
      "Iteration 8531, Loss: 35.46943664550781, L1: 9.761271476745605, L3: 25.708166122436523\n",
      "Current prediction:  61.317562103271484 \n",
      "\n",
      "Iteration 8532, Loss: 35.76832580566406, L1: 9.655437469482422, L3: 26.112890243530273\n",
      "Current prediction:  61.610633850097656 \n",
      "\n",
      "Iteration 8533, Loss: 36.186100006103516, L1: 9.702733039855957, L3: 26.483366012573242\n",
      "Current prediction:  61.62430191040039 \n",
      "\n",
      "Iteration 8534, Loss: 36.6103401184082, L1: 9.778504371643066, L3: 26.83183479309082\n",
      "Current prediction:  61.61993408203125 \n",
      "\n",
      "Iteration 8535, Loss: 36.19703674316406, L1: 9.845815658569336, L3: 26.351221084594727\n",
      "Current prediction:  61.61591720581055 \n",
      "\n",
      "Iteration 8536, Loss: 35.42958068847656, L1: 9.835150718688965, L3: 25.59442901611328\n",
      "Current prediction:  61.6068229675293 \n",
      "\n",
      "Iteration 8537, Loss: 36.47674560546875, L1: 9.718798637390137, L3: 26.757946014404297\n",
      "Current prediction:  61.433372497558594 \n",
      "\n",
      "Iteration 8538, Loss: 36.30113983154297, L1: 9.70104694366455, L3: 26.600093841552734\n",
      "Current prediction:  60.60902404785156 \n",
      "\n",
      "Iteration 8539, Loss: 35.702396392822266, L1: 9.71140193939209, L3: 25.99099349975586\n",
      "Current prediction:  60.52809524536133 \n",
      "\n",
      "Iteration 8540, Loss: 36.386009216308594, L1: 9.826757431030273, L3: 26.559249877929688\n",
      "Current prediction:  60.52045822143555 \n",
      "\n",
      "Iteration 8541, Loss: 37.01898193359375, L1: 9.816732406616211, L3: 27.20224952697754\n",
      "Current prediction:  60.51911163330078 \n",
      "\n",
      "Iteration 8542, Loss: 36.28624725341797, L1: 9.806098937988281, L3: 26.480146408081055\n",
      "Current prediction:  60.549217224121094 \n",
      "\n",
      "Iteration 8543, Loss: 34.93037033081055, L1: 9.719340324401855, L3: 25.211030960083008\n",
      "Current prediction:  61.33647918701172 \n",
      "\n",
      "Iteration 8544, Loss: 35.69043731689453, L1: 9.720527648925781, L3: 25.969911575317383\n",
      "Current prediction:  61.639686584472656 \n",
      "\n",
      "Iteration 8545, Loss: 36.05406951904297, L1: 9.776595115661621, L3: 26.277475357055664\n",
      "Current prediction:  61.655067443847656 \n",
      "\n",
      "Iteration 8546, Loss: 36.093997955322266, L1: 9.795442581176758, L3: 26.298555374145508\n",
      "Current prediction:  61.649356842041016 \n",
      "\n",
      "Iteration 8547, Loss: 35.9815559387207, L1: 9.794259071350098, L3: 26.187297821044922\n",
      "Current prediction:  61.631752014160156 \n",
      "\n",
      "Iteration 8548, Loss: 35.660953521728516, L1: 9.875967025756836, L3: 25.78498649597168\n",
      "Current prediction:  61.608055114746094 \n",
      "\n",
      "Iteration 8549, Loss: 36.221031188964844, L1: 9.858509063720703, L3: 26.36252212524414\n",
      "Current prediction:  61.58095932006836 \n",
      "\n",
      "Iteration 8550, Loss: 37.28303527832031, L1: 9.874931335449219, L3: 27.408103942871094\n",
      "Current prediction:  61.552127838134766 \n",
      "\n",
      "Iteration 8551, Loss: 36.255218505859375, L1: 9.853809356689453, L3: 26.401409149169922\n",
      "Current prediction:  61.53015899658203 \n",
      "\n",
      "Iteration 8552, Loss: 36.64391326904297, L1: 9.865381240844727, L3: 26.778533935546875\n",
      "Current prediction:  61.516563415527344 \n",
      "\n",
      "Iteration 8553, Loss: 35.505455017089844, L1: 9.813521385192871, L3: 25.691932678222656\n",
      "Current prediction:  61.48299789428711 \n",
      "\n",
      "Iteration 8554, Loss: 34.74845886230469, L1: 9.791484832763672, L3: 24.95697593688965\n",
      "Current prediction:  61.18360900878906 \n",
      "\n",
      "Iteration 8555, Loss: 36.02861022949219, L1: 9.74339485168457, L3: 26.285215377807617\n",
      "Current prediction:  60.56392288208008 \n",
      "\n",
      "Iteration 8556, Loss: 35.87622833251953, L1: 9.840742111206055, L3: 26.035484313964844\n",
      "Current prediction:  60.49441146850586 \n",
      "\n",
      "Iteration 8557, Loss: 36.38752746582031, L1: 9.886246681213379, L3: 26.50128173828125\n",
      "Current prediction:  60.52130126953125 \n",
      "\n",
      "Iteration 8558, Loss: 35.72843551635742, L1: 9.83194351196289, L3: 25.89649200439453\n",
      "Current prediction:  60.561363220214844 \n",
      "\n",
      "Iteration 8559, Loss: 36.94322967529297, L1: 9.770017623901367, L3: 27.17321014404297\n",
      "Current prediction:  60.66424560546875 \n",
      "\n",
      "Iteration 8560, Loss: 36.725318908691406, L1: 9.707664489746094, L3: 27.01765251159668\n",
      "Current prediction:  61.435184478759766 \n",
      "\n",
      "Iteration 8561, Loss: 34.94456481933594, L1: 9.630697250366211, L3: 25.31386947631836\n",
      "Current prediction:  61.6683235168457 \n",
      "\n",
      "Iteration 8562, Loss: 35.71916580200195, L1: 9.65027904510498, L3: 26.068885803222656\n",
      "Current prediction:  61.68611145019531 \n",
      "\n",
      "Iteration 8563, Loss: 36.303489685058594, L1: 9.664972305297852, L3: 26.63851547241211\n",
      "Current prediction:  61.68242645263672 \n",
      "\n",
      "Iteration 8564, Loss: 35.974876403808594, L1: 9.759862899780273, L3: 26.215015411376953\n",
      "Current prediction:  61.38383102416992 \n",
      "\n",
      "Iteration 8565, Loss: 35.53357696533203, L1: 9.592065811157227, L3: 25.941509246826172\n",
      "Current prediction:  60.660499572753906 \n",
      "\n",
      "Iteration 8566, Loss: 35.72801208496094, L1: 9.710495948791504, L3: 26.01751708984375\n",
      "Current prediction:  60.60013961791992 \n",
      "\n",
      "Iteration 8567, Loss: 36.05107879638672, L1: 9.787405014038086, L3: 26.263671875\n",
      "Current prediction:  60.591556549072266 \n",
      "\n",
      "Iteration 8568, Loss: 36.169166564941406, L1: 9.839948654174805, L3: 26.3292179107666\n",
      "Current prediction:  60.59103012084961 \n",
      "\n",
      "Iteration 8569, Loss: 36.76148223876953, L1: 9.807577133178711, L3: 26.953903198242188\n",
      "Current prediction:  60.61029815673828 \n",
      "\n",
      "Iteration 8570, Loss: 36.042938232421875, L1: 9.682487487792969, L3: 26.36045265197754\n",
      "Current prediction:  61.265296936035156 \n",
      "\n",
      "Iteration 8571, Loss: 36.65678405761719, L1: 9.616616249084473, L3: 27.04016876220703\n",
      "Current prediction:  61.6312370300293 \n",
      "\n",
      "Iteration 8572, Loss: 36.30694580078125, L1: 9.745831489562988, L3: 26.561113357543945\n",
      "Current prediction:  61.63636016845703 \n",
      "\n",
      "Iteration 8573, Loss: 37.12124252319336, L1: 9.786480903625488, L3: 27.334762573242188\n",
      "Current prediction:  61.617305755615234 \n",
      "\n",
      "Iteration 8574, Loss: 36.618309020996094, L1: 9.808892250061035, L3: 26.809415817260742\n",
      "Current prediction:  61.58131408691406 \n",
      "\n",
      "Iteration 8575, Loss: 35.912940979003906, L1: 9.83822250366211, L3: 26.074716567993164\n",
      "Current prediction:  61.437374114990234 \n",
      "\n",
      "Iteration 8576, Loss: 36.64815902709961, L1: 9.83707332611084, L3: 26.811084747314453\n",
      "Current prediction:  60.82822036743164 \n",
      "\n",
      "Iteration 8577, Loss: 36.63311004638672, L1: 9.745244979858398, L3: 26.88786506652832\n",
      "Current prediction:  60.74958801269531 \n",
      "\n",
      "Iteration 8578, Loss: 36.7255973815918, L1: 9.825261116027832, L3: 26.90033721923828\n",
      "Current prediction:  60.707088470458984 \n",
      "\n",
      "Iteration 8579, Loss: 36.72489929199219, L1: 9.84109878540039, L3: 26.88380241394043\n",
      "Current prediction:  60.68407440185547 \n",
      "\n",
      "Iteration 8580, Loss: 35.765625, L1: 9.9043550491333, L3: 25.861268997192383\n",
      "Current prediction:  60.9061393737793 \n",
      "\n",
      "Iteration 8581, Loss: 35.842037200927734, L1: 9.914877891540527, L3: 25.927160263061523\n",
      "Current prediction:  61.15679931640625 \n",
      "\n",
      "Iteration 8582, Loss: 34.93967819213867, L1: 9.973819732666016, L3: 24.965858459472656\n",
      "Current prediction:  61.217529296875 \n",
      "\n",
      "Iteration 8583, Loss: 35.91733932495117, L1: 10.036722183227539, L3: 25.880617141723633\n",
      "Current prediction:  61.27214813232422 \n",
      "\n",
      "Iteration 8584, Loss: 35.98750686645508, L1: 9.982797622680664, L3: 26.004709243774414\n",
      "Current prediction:  61.32071304321289 \n",
      "\n",
      "Iteration 8585, Loss: 36.28955078125, L1: 9.946352005004883, L3: 26.343196868896484\n",
      "Current prediction:  61.230194091796875 \n",
      "\n",
      "Iteration 8586, Loss: 36.1041374206543, L1: 9.863224029541016, L3: 26.24091339111328\n",
      "Current prediction:  60.90188217163086 \n",
      "\n",
      "Iteration 8587, Loss: 36.401947021484375, L1: 9.854476928710938, L3: 26.547468185424805\n",
      "Current prediction:  60.63780975341797 \n",
      "\n",
      "Iteration 8588, Loss: 36.118080139160156, L1: 9.737659454345703, L3: 26.380422592163086\n",
      "Current prediction:  60.66493225097656 \n",
      "\n",
      "Iteration 8589, Loss: 36.23861312866211, L1: 9.580021858215332, L3: 26.65859031677246\n",
      "Current prediction:  60.8946647644043 \n",
      "\n",
      "Iteration 8590, Loss: 37.311527252197266, L1: 9.570568084716797, L3: 27.74095916748047\n",
      "Current prediction:  61.30912399291992 \n",
      "\n",
      "Iteration 8591, Loss: 37.26180648803711, L1: 9.498169898986816, L3: 27.76363754272461\n",
      "Current prediction:  61.565486907958984 \n",
      "\n",
      "Iteration 8592, Loss: 36.099281311035156, L1: 9.394864082336426, L3: 26.704416275024414\n",
      "Current prediction:  61.378257751464844 \n",
      "\n",
      "Iteration 8593, Loss: 36.297569274902344, L1: 9.338467597961426, L3: 26.9591007232666\n",
      "Current prediction:  61.18389129638672 \n",
      "\n",
      "Iteration 8594, Loss: 36.72904968261719, L1: 9.426491737365723, L3: 27.30255699157715\n",
      "Current prediction:  61.10749816894531 \n",
      "\n",
      "Iteration 8595, Loss: 37.27000045776367, L1: 9.455330848693848, L3: 27.814668655395508\n",
      "Current prediction:  61.05330276489258 \n",
      "\n",
      "Iteration 8596, Loss: 36.52720642089844, L1: 9.503290176391602, L3: 27.023916244506836\n",
      "Current prediction:  61.04954528808594 \n",
      "\n",
      "Iteration 8597, Loss: 35.5822868347168, L1: 9.482983589172363, L3: 26.09930419921875\n",
      "Current prediction:  61.23192596435547 \n",
      "\n",
      "Iteration 8598, Loss: 36.782920837402344, L1: 9.562453269958496, L3: 27.220468521118164\n",
      "Current prediction:  61.4925537109375 \n",
      "\n",
      "Iteration 8599, Loss: 35.52626037597656, L1: 9.540818214416504, L3: 25.985441207885742\n",
      "Current prediction:  61.54303741455078 \n",
      "\n",
      "Iteration 8600, Loss: 35.844913482666016, L1: 9.692461967468262, L3: 26.15245246887207\n",
      "Current prediction:  61.46126937866211 \n",
      "\n",
      "Iteration 8601, Loss: 36.11262512207031, L1: 9.660465240478516, L3: 26.45216178894043\n",
      "Current prediction:  61.07072830200195 \n",
      "\n",
      "Iteration 8602, Loss: 35.77662658691406, L1: 9.716254234313965, L3: 26.06037139892578\n",
      "Current prediction:  60.76462936401367 \n",
      "\n",
      "Iteration 8603, Loss: 36.44654846191406, L1: 9.735647201538086, L3: 26.71090316772461\n",
      "Current prediction:  60.784568786621094 \n",
      "\n",
      "Iteration 8604, Loss: 35.332000732421875, L1: 9.79642391204834, L3: 25.53557586669922\n",
      "Current prediction:  61.178680419921875 \n",
      "\n",
      "Iteration 8605, Loss: 35.407047271728516, L1: 9.730433464050293, L3: 25.676612854003906\n",
      "Current prediction:  61.428462982177734 \n",
      "\n",
      "Iteration 8606, Loss: 36.06914138793945, L1: 9.737711906433105, L3: 26.331430435180664\n",
      "Current prediction:  61.50053787231445 \n",
      "\n",
      "Iteration 8607, Loss: 37.076416015625, L1: 9.801665306091309, L3: 27.274749755859375\n",
      "Current prediction:  61.495704650878906 \n",
      "\n",
      "Iteration 8608, Loss: 35.51423263549805, L1: 9.781543731689453, L3: 25.732688903808594\n",
      "Current prediction:  61.30231475830078 \n",
      "\n",
      "Iteration 8609, Loss: 36.50346374511719, L1: 9.81423568725586, L3: 26.689228057861328\n",
      "Current prediction:  61.06822967529297 \n",
      "\n",
      "Iteration 8610, Loss: 36.01891326904297, L1: 9.743448257446289, L3: 26.275463104248047\n",
      "Current prediction:  60.76314163208008 \n",
      "\n",
      "Iteration 8611, Loss: 35.46514892578125, L1: 9.698749542236328, L3: 25.766399383544922\n",
      "Current prediction:  60.76783752441406 \n",
      "\n",
      "Iteration 8612, Loss: 36.715782165527344, L1: 9.773567199707031, L3: 26.94221305847168\n",
      "Current prediction:  60.894649505615234 \n",
      "\n",
      "Iteration 8613, Loss: 36.314666748046875, L1: 9.663649559020996, L3: 26.651016235351562\n",
      "Current prediction:  61.1689338684082 \n",
      "\n",
      "Iteration 8614, Loss: 35.643978118896484, L1: 9.649277687072754, L3: 25.994699478149414\n",
      "Current prediction:  61.69524383544922 \n",
      "\n",
      "Iteration 8615, Loss: 36.453739166259766, L1: 9.600685119628906, L3: 26.85305404663086\n",
      "Current prediction:  61.876976013183594 \n",
      "\n",
      "Iteration 8616, Loss: 34.56731414794922, L1: 9.602166175842285, L3: 24.96514892578125\n",
      "Current prediction:  61.91737747192383 \n",
      "\n",
      "Iteration 8617, Loss: 35.5372428894043, L1: 9.606917381286621, L3: 25.930326461791992\n",
      "Current prediction:  61.899757385253906 \n",
      "\n",
      "Iteration 8618, Loss: 36.08222961425781, L1: 9.54515266418457, L3: 26.53707504272461\n",
      "Current prediction:  61.73686218261719 \n",
      "\n",
      "Iteration 8619, Loss: 36.43878173828125, L1: 9.550496101379395, L3: 26.88828468322754\n",
      "Current prediction:  60.905513763427734 \n",
      "\n",
      "Iteration 8620, Loss: 35.26734924316406, L1: 9.514148712158203, L3: 25.75320053100586\n",
      "Current prediction:  60.564998626708984 \n",
      "\n",
      "Iteration 8621, Loss: 36.80620574951172, L1: 9.618606567382812, L3: 27.187599182128906\n",
      "Current prediction:  60.4793701171875 \n",
      "\n",
      "Iteration 8622, Loss: 36.592994689941406, L1: 9.661770820617676, L3: 26.931224822998047\n",
      "Current prediction:  60.51392364501953 \n",
      "\n",
      "Iteration 8623, Loss: 35.08816909790039, L1: 9.60389232635498, L3: 25.484275817871094\n",
      "Current prediction:  60.632301330566406 \n",
      "\n",
      "Iteration 8624, Loss: 35.405426025390625, L1: 9.613863945007324, L3: 25.791563034057617\n",
      "Current prediction:  61.133567810058594 \n",
      "\n",
      "Iteration 8625, Loss: 35.52104187011719, L1: 9.536086082458496, L3: 25.984956741333008\n",
      "Current prediction:  61.38595199584961 \n",
      "\n",
      "Iteration 8626, Loss: 35.83302307128906, L1: 9.559635162353516, L3: 26.27338981628418\n",
      "Current prediction:  61.42670440673828 \n",
      "\n",
      "Iteration 8627, Loss: 35.538665771484375, L1: 9.548941612243652, L3: 25.989723205566406\n",
      "Current prediction:  61.26063537597656 \n",
      "\n",
      "Iteration 8628, Loss: 37.1878776550293, L1: 9.59772777557373, L3: 27.590150833129883\n",
      "Current prediction:  61.003902435302734 \n",
      "\n",
      "Iteration 8629, Loss: 36.35036849975586, L1: 9.6449613571167, L3: 26.705406188964844\n",
      "Current prediction:  60.64699172973633 \n",
      "\n",
      "Iteration 8630, Loss: 35.51753234863281, L1: 9.66611099243164, L3: 25.85141944885254\n",
      "Current prediction:  60.464332580566406 \n",
      "\n",
      "Iteration 8631, Loss: 36.43873596191406, L1: 9.695948600769043, L3: 26.742788314819336\n",
      "Current prediction:  60.39979934692383 \n",
      "\n",
      "Iteration 8632, Loss: 36.69855880737305, L1: 9.70078182220459, L3: 26.997777938842773\n",
      "Current prediction:  60.5071907043457 \n",
      "\n",
      "Iteration 8633, Loss: 35.822792053222656, L1: 9.682538986206055, L3: 26.140254974365234\n",
      "Current prediction:  61.29736328125 \n",
      "\n",
      "Iteration 8634, Loss: 36.02198791503906, L1: 9.688204765319824, L3: 26.333782196044922\n",
      "Current prediction:  61.40379333496094 \n",
      "\n",
      "Iteration 8635, Loss: 36.073219299316406, L1: 9.774827003479004, L3: 26.298391342163086\n",
      "Current prediction:  61.360443115234375 \n",
      "\n",
      "Iteration 8636, Loss: 36.05620574951172, L1: 9.767476081848145, L3: 26.28873062133789\n",
      "Current prediction:  61.3820686340332 \n",
      "\n",
      "Iteration 8637, Loss: 36.67799758911133, L1: 9.830682754516602, L3: 26.847314834594727\n",
      "Current prediction:  61.46430969238281 \n",
      "\n",
      "Iteration 8638, Loss: 36.06666564941406, L1: 9.709199905395508, L3: 26.357465744018555\n",
      "Current prediction:  61.376277923583984 \n",
      "\n",
      "Iteration 8639, Loss: 36.1226692199707, L1: 9.63012981414795, L3: 26.49254035949707\n",
      "Current prediction:  60.74382781982422 \n",
      "\n",
      "Iteration 8640, Loss: 36.19872283935547, L1: 9.785202026367188, L3: 26.413522720336914\n",
      "Current prediction:  60.67392349243164 \n",
      "\n",
      "Iteration 8641, Loss: 36.45134353637695, L1: 9.774720191955566, L3: 26.67662239074707\n",
      "Current prediction:  60.7849235534668 \n",
      "\n",
      "Iteration 8642, Loss: 35.221561431884766, L1: 9.689155578613281, L3: 25.532405853271484\n",
      "Current prediction:  61.14229965209961 \n",
      "\n",
      "Iteration 8643, Loss: 36.96681213378906, L1: 9.609800338745117, L3: 27.357011795043945\n",
      "Current prediction:  61.67218780517578 \n",
      "\n",
      "Iteration 8644, Loss: 36.49333953857422, L1: 9.598916053771973, L3: 26.894424438476562\n",
      "Current prediction:  61.76571273803711 \n",
      "\n",
      "Iteration 8645, Loss: 35.93867111206055, L1: 9.628003120422363, L3: 26.3106689453125\n",
      "Current prediction:  61.79121017456055 \n",
      "\n",
      "Iteration 8646, Loss: 36.295692443847656, L1: 9.673887252807617, L3: 26.621807098388672\n",
      "Current prediction:  61.79332733154297 \n",
      "\n",
      "Iteration 8647, Loss: 37.79372787475586, L1: 9.624279975891113, L3: 28.16944694519043\n",
      "Current prediction:  61.77724838256836 \n",
      "\n",
      "Iteration 8648, Loss: 35.79100036621094, L1: 9.650634765625, L3: 26.140365600585938\n",
      "Current prediction:  61.775943756103516 \n",
      "\n",
      "Iteration 8649, Loss: 36.529052734375, L1: 9.571521759033203, L3: 26.957530975341797\n",
      "Current prediction:  61.67961120605469 \n",
      "\n",
      "Iteration 8650, Loss: 35.75702667236328, L1: 9.622828483581543, L3: 26.134197235107422\n",
      "Current prediction:  60.92543029785156 \n",
      "\n",
      "Iteration 8651, Loss: 35.627750396728516, L1: 9.588883399963379, L3: 26.038867950439453\n",
      "Current prediction:  60.77523422241211 \n",
      "\n",
      "Iteration 8652, Loss: 35.8075065612793, L1: 9.591294288635254, L3: 26.21621322631836\n",
      "Current prediction:  60.71997833251953 \n",
      "\n",
      "Iteration 8653, Loss: 36.07666015625, L1: 9.60474681854248, L3: 26.471912384033203\n",
      "Current prediction:  60.92427444458008 \n",
      "\n",
      "Iteration 8654, Loss: 36.20206069946289, L1: 9.528122901916504, L3: 26.67393684387207\n",
      "Current prediction:  61.47850036621094 \n",
      "\n",
      "Iteration 8655, Loss: 35.71242141723633, L1: 9.730142593383789, L3: 25.98227882385254\n",
      "Current prediction:  61.393733978271484 \n",
      "\n",
      "Iteration 8656, Loss: 34.7849235534668, L1: 9.545276641845703, L3: 25.239646911621094\n",
      "Current prediction:  61.13705062866211 \n",
      "\n",
      "Iteration 8657, Loss: 35.9716682434082, L1: 9.58130931854248, L3: 26.39035987854004\n",
      "Current prediction:  60.963462829589844 \n",
      "\n",
      "Iteration 8658, Loss: 36.132774353027344, L1: 9.639960289001465, L3: 26.492813110351562\n",
      "Current prediction:  60.818336486816406 \n",
      "\n",
      "Iteration 8659, Loss: 36.165870666503906, L1: 9.668410301208496, L3: 26.497459411621094\n",
      "Current prediction:  60.71493911743164 \n",
      "\n",
      "Iteration 8660, Loss: 35.51829147338867, L1: 9.730072975158691, L3: 25.788219451904297\n",
      "Current prediction:  60.7221565246582 \n",
      "\n",
      "Iteration 8661, Loss: 35.98163604736328, L1: 9.707056999206543, L3: 26.274578094482422\n",
      "Current prediction:  61.27958679199219 \n",
      "\n",
      "Iteration 8662, Loss: 35.722103118896484, L1: 9.718424797058105, L3: 26.003677368164062\n",
      "Current prediction:  61.49665451049805 \n",
      "\n",
      "Iteration 8663, Loss: 36.09812545776367, L1: 9.78259563446045, L3: 26.31553077697754\n",
      "Current prediction:  61.52724838256836 \n",
      "\n",
      "Iteration 8664, Loss: 35.760887145996094, L1: 9.713750839233398, L3: 26.047138214111328\n",
      "Current prediction:  61.52188491821289 \n",
      "\n",
      "Iteration 8665, Loss: 35.9171142578125, L1: 9.667070388793945, L3: 26.250043869018555\n",
      "Current prediction:  61.36500930786133 \n",
      "\n",
      "Iteration 8666, Loss: 36.069068908691406, L1: 9.617773056030273, L3: 26.451297760009766\n",
      "Current prediction:  60.73978805541992 \n",
      "\n",
      "Iteration 8667, Loss: 35.46442794799805, L1: 9.721951484680176, L3: 25.742477416992188\n",
      "Current prediction:  60.54694747924805 \n",
      "\n",
      "Iteration 8668, Loss: 35.259765625, L1: 9.793234825134277, L3: 25.466529846191406\n",
      "Current prediction:  60.552913665771484 \n",
      "\n",
      "Iteration 8669, Loss: 35.57776641845703, L1: 9.747135162353516, L3: 25.830629348754883\n",
      "Current prediction:  60.70824432373047 \n",
      "\n",
      "Iteration 8670, Loss: 36.88618469238281, L1: 9.719877243041992, L3: 27.166305541992188\n",
      "Current prediction:  60.89448165893555 \n",
      "\n",
      "Iteration 8671, Loss: 35.91305160522461, L1: 9.604546546936035, L3: 26.30850601196289\n",
      "Current prediction:  61.261234283447266 \n",
      "\n",
      "Iteration 8672, Loss: 35.640377044677734, L1: 9.533465385437012, L3: 26.10691261291504\n",
      "Current prediction:  61.54656982421875 \n",
      "\n",
      "Iteration 8673, Loss: 36.480812072753906, L1: 9.518691062927246, L3: 26.962121963500977\n",
      "Current prediction:  61.65121841430664 \n",
      "\n",
      "Iteration 8674, Loss: 35.527626037597656, L1: 9.649477005004883, L3: 25.87814712524414\n",
      "Current prediction:  61.657814025878906 \n",
      "\n",
      "Iteration 8675, Loss: 36.303714752197266, L1: 9.540787696838379, L3: 26.762928009033203\n",
      "Current prediction:  61.60892105102539 \n",
      "\n",
      "Iteration 8676, Loss: 35.348548889160156, L1: 9.55543327331543, L3: 25.793115615844727\n",
      "Current prediction:  61.4117546081543 \n",
      "\n",
      "Iteration 8677, Loss: 36.10142517089844, L1: 9.499431610107422, L3: 26.601993560791016\n",
      "Current prediction:  60.78549575805664 \n",
      "\n",
      "Iteration 8678, Loss: 36.54551696777344, L1: 9.567132949829102, L3: 26.978382110595703\n",
      "Current prediction:  60.65800094604492 \n",
      "\n",
      "Iteration 8679, Loss: 34.8416862487793, L1: 9.67825984954834, L3: 25.163427352905273\n",
      "Current prediction:  60.67471694946289 \n",
      "\n",
      "Iteration 8680, Loss: 35.90754318237305, L1: 9.65339183807373, L3: 26.254150390625\n",
      "Current prediction:  60.98505783081055 \n",
      "\n",
      "Iteration 8681, Loss: 35.03831481933594, L1: 9.555959701538086, L3: 25.48235511779785\n",
      "Current prediction:  61.617942810058594 \n",
      "\n",
      "Iteration 8682, Loss: 36.987876892089844, L1: 9.595325469970703, L3: 27.392553329467773\n",
      "Current prediction:  61.6532096862793 \n",
      "\n",
      "Iteration 8683, Loss: 36.221981048583984, L1: 9.627545356750488, L3: 26.594436645507812\n",
      "Current prediction:  61.641666412353516 \n",
      "\n",
      "Iteration 8684, Loss: 36.53093719482422, L1: 9.667617797851562, L3: 26.86332130432129\n",
      "Current prediction:  61.53292465209961 \n",
      "\n",
      "Iteration 8685, Loss: 35.22062683105469, L1: 9.5929594039917, L3: 25.627666473388672\n",
      "Current prediction:  61.0966911315918 \n",
      "\n",
      "Iteration 8686, Loss: 36.01094055175781, L1: 9.715269088745117, L3: 26.295669555664062\n",
      "Current prediction:  60.716209411621094 \n",
      "\n",
      "Iteration 8687, Loss: 36.169708251953125, L1: 9.71588134765625, L3: 26.453828811645508\n",
      "Current prediction:  60.55311965942383 \n",
      "\n",
      "Iteration 8688, Loss: 35.57423400878906, L1: 9.776144027709961, L3: 25.7980899810791\n",
      "Current prediction:  60.87543487548828 \n",
      "\n",
      "Iteration 8689, Loss: 35.84504699707031, L1: 9.654104232788086, L3: 26.190940856933594\n",
      "Current prediction:  61.45746994018555 \n",
      "\n",
      "Iteration 8690, Loss: 36.34695053100586, L1: 9.716482162475586, L3: 26.630468368530273\n",
      "Current prediction:  61.5603141784668 \n",
      "\n",
      "Iteration 8691, Loss: 35.38414764404297, L1: 9.746126174926758, L3: 25.638023376464844\n",
      "Current prediction:  61.571590423583984 \n",
      "\n",
      "Iteration 8692, Loss: 36.4573860168457, L1: 9.686354637145996, L3: 26.77103042602539\n",
      "Current prediction:  61.434783935546875 \n",
      "\n",
      "Iteration 8693, Loss: 35.780879974365234, L1: 9.702262878417969, L3: 26.078617095947266\n",
      "Current prediction:  60.85094451904297 \n",
      "\n",
      "Iteration 8694, Loss: 35.659061431884766, L1: 9.692151069641113, L3: 25.966909408569336\n",
      "Current prediction:  60.84668731689453 \n",
      "\n",
      "Iteration 8695, Loss: 36.769317626953125, L1: 9.6123046875, L3: 27.157014846801758\n",
      "Current prediction:  61.22808837890625 \n",
      "\n",
      "Iteration 8696, Loss: 35.14152526855469, L1: 9.664878845214844, L3: 25.476648330688477\n",
      "Current prediction:  61.48367691040039 \n",
      "\n",
      "Iteration 8697, Loss: 35.73303985595703, L1: 9.666854858398438, L3: 26.066186904907227\n",
      "Current prediction:  61.57237243652344 \n",
      "\n",
      "Iteration 8698, Loss: 36.329078674316406, L1: 9.647546768188477, L3: 26.68153190612793\n",
      "Current prediction:  61.33439636230469 \n",
      "\n",
      "Iteration 8699, Loss: 36.30949020385742, L1: 9.63839340209961, L3: 26.671096801757812\n",
      "Current prediction:  60.684844970703125 \n",
      "\n",
      "Iteration 8700, Loss: 36.44097137451172, L1: 9.71911907196045, L3: 26.721853256225586\n",
      "Current prediction:  60.59940719604492 \n",
      "\n",
      "Iteration 8701, Loss: 35.91222381591797, L1: 9.706218719482422, L3: 26.206003189086914\n",
      "Current prediction:  60.64286422729492 \n",
      "\n",
      "Iteration 8702, Loss: 35.43912124633789, L1: 9.63815975189209, L3: 25.800962448120117\n",
      "Current prediction:  60.67683029174805 \n",
      "\n",
      "Iteration 8703, Loss: 35.55030822753906, L1: 9.629085540771484, L3: 25.921220779418945\n",
      "Current prediction:  60.957916259765625 \n",
      "\n",
      "Iteration 8704, Loss: 35.54328918457031, L1: 9.586098670959473, L3: 25.957191467285156\n",
      "Current prediction:  61.59048843383789 \n",
      "\n",
      "Iteration 8705, Loss: 35.79627227783203, L1: 9.656929969787598, L3: 26.139341354370117\n",
      "Current prediction:  61.652626037597656 \n",
      "\n",
      "Iteration 8706, Loss: 35.74794006347656, L1: 9.663171768188477, L3: 26.084766387939453\n",
      "Current prediction:  61.670753479003906 \n",
      "\n",
      "Iteration 8707, Loss: 35.32695770263672, L1: 9.67319107055664, L3: 25.65376853942871\n",
      "Current prediction:  61.67741012573242 \n",
      "\n",
      "Iteration 8708, Loss: 36.730228424072266, L1: 9.717110633850098, L3: 27.013118743896484\n",
      "Current prediction:  61.652320861816406 \n",
      "\n",
      "Iteration 8709, Loss: 36.012046813964844, L1: 9.607183456420898, L3: 26.404863357543945\n",
      "Current prediction:  61.50535583496094 \n",
      "\n",
      "Iteration 8710, Loss: 35.51238250732422, L1: 9.545008659362793, L3: 25.96737289428711\n",
      "Current prediction:  60.93577575683594 \n",
      "\n",
      "Iteration 8711, Loss: 36.37074661254883, L1: 9.571893692016602, L3: 26.798852920532227\n",
      "Current prediction:  60.669097900390625 \n",
      "\n",
      "Iteration 8712, Loss: 36.28182601928711, L1: 9.601572036743164, L3: 26.680253982543945\n",
      "Current prediction:  60.662776947021484 \n",
      "\n",
      "Iteration 8713, Loss: 36.36802673339844, L1: 9.634016036987305, L3: 26.7340087890625\n",
      "Current prediction:  60.74348068237305 \n",
      "\n",
      "Iteration 8714, Loss: 35.46770095825195, L1: 9.652216911315918, L3: 25.81548500061035\n",
      "Current prediction:  60.778743743896484 \n",
      "\n",
      "Iteration 8715, Loss: 35.60298156738281, L1: 9.636404037475586, L3: 25.966577529907227\n",
      "Current prediction:  60.82903289794922 \n",
      "\n",
      "Iteration 8716, Loss: 34.13270568847656, L1: 9.64676570892334, L3: 24.485939025878906\n",
      "Current prediction:  61.48056411743164 \n",
      "\n",
      "Iteration 8717, Loss: 35.50349807739258, L1: 9.542426109313965, L3: 25.961071014404297\n",
      "Current prediction:  61.645790100097656 \n",
      "\n",
      "Iteration 8718, Loss: 35.77168655395508, L1: 9.620504379272461, L3: 26.151182174682617\n",
      "Current prediction:  61.66611862182617 \n",
      "\n",
      "Iteration 8719, Loss: 36.12956619262695, L1: 9.624539375305176, L3: 26.50502586364746\n",
      "Current prediction:  61.663414001464844 \n",
      "\n",
      "Iteration 8720, Loss: 35.41923141479492, L1: 9.68645191192627, L3: 25.732778549194336\n",
      "Current prediction:  61.65370178222656 \n",
      "\n",
      "Iteration 8721, Loss: 35.5693359375, L1: 9.705280303955078, L3: 25.864055633544922\n",
      "Current prediction:  61.63945388793945 \n",
      "\n",
      "Iteration 8722, Loss: 35.873878479003906, L1: 9.677807807922363, L3: 26.19607162475586\n",
      "Current prediction:  61.61312484741211 \n",
      "\n",
      "Iteration 8723, Loss: 36.511985778808594, L1: 9.639558792114258, L3: 26.87242889404297\n",
      "Current prediction:  61.34911346435547 \n",
      "\n",
      "Iteration 8724, Loss: 36.74449157714844, L1: 9.610329627990723, L3: 27.13416290283203\n",
      "Current prediction:  60.56843948364258 \n",
      "\n",
      "Iteration 8725, Loss: 35.59901428222656, L1: 9.662111282348633, L3: 25.936901092529297\n",
      "Current prediction:  60.49559783935547 \n",
      "\n",
      "Iteration 8726, Loss: 35.64503479003906, L1: 9.787419319152832, L3: 25.857616424560547\n",
      "Current prediction:  60.5051383972168 \n",
      "\n",
      "Iteration 8727, Loss: 35.417457580566406, L1: 9.827198028564453, L3: 25.59025764465332\n",
      "Current prediction:  60.589027404785156 \n",
      "\n",
      "Iteration 8728, Loss: 36.502037048339844, L1: 9.684423446655273, L3: 26.817611694335938\n",
      "Current prediction:  61.25572204589844 \n",
      "\n",
      "Iteration 8729, Loss: 34.64496994018555, L1: 9.619345664978027, L3: 25.025625228881836\n",
      "Current prediction:  61.61836242675781 \n",
      "\n",
      "Iteration 8730, Loss: 36.124698638916016, L1: 9.649577140808105, L3: 26.475122451782227\n",
      "Current prediction:  61.662841796875 \n",
      "\n",
      "Iteration 8731, Loss: 35.79253387451172, L1: 9.594880104064941, L3: 26.197654724121094\n",
      "Current prediction:  61.67538070678711 \n",
      "\n",
      "Iteration 8732, Loss: 35.81575012207031, L1: 9.628165245056152, L3: 26.187583923339844\n",
      "Current prediction:  61.628662109375 \n",
      "\n",
      "Iteration 8733, Loss: 36.172828674316406, L1: 9.552267074584961, L3: 26.620561599731445\n",
      "Current prediction:  61.41161346435547 \n",
      "\n",
      "Iteration 8734, Loss: 35.65241241455078, L1: 9.56680679321289, L3: 26.08560562133789\n",
      "Current prediction:  60.8293342590332 \n",
      "\n",
      "Iteration 8735, Loss: 36.086944580078125, L1: 9.562955856323242, L3: 26.52398681640625\n",
      "Current prediction:  60.60663986206055 \n",
      "\n",
      "Iteration 8736, Loss: 35.571773529052734, L1: 9.74567699432373, L3: 25.826095581054688\n",
      "Current prediction:  60.58290100097656 \n",
      "\n",
      "Iteration 8737, Loss: 35.77870559692383, L1: 9.705011367797852, L3: 26.073694229125977\n",
      "Current prediction:  60.58848190307617 \n",
      "\n",
      "Iteration 8738, Loss: 36.52450942993164, L1: 9.737356185913086, L3: 26.787153244018555\n",
      "Current prediction:  60.6281623840332 \n",
      "\n",
      "Iteration 8739, Loss: 34.312557220458984, L1: 9.653800010681152, L3: 24.65875816345215\n",
      "Current prediction:  60.96058654785156 \n",
      "\n",
      "Iteration 8740, Loss: 36.13559341430664, L1: 9.580123901367188, L3: 26.555469512939453\n",
      "Current prediction:  61.62389373779297 \n",
      "\n",
      "Iteration 8741, Loss: 35.80552291870117, L1: 9.574429512023926, L3: 26.231094360351562\n",
      "Current prediction:  61.70494079589844 \n",
      "\n",
      "Iteration 8742, Loss: 35.546424865722656, L1: 9.587844848632812, L3: 25.958581924438477\n",
      "Current prediction:  61.696083068847656 \n",
      "\n",
      "Iteration 8743, Loss: 35.373069763183594, L1: 9.668818473815918, L3: 25.704252243041992\n",
      "Current prediction:  61.679805755615234 \n",
      "\n",
      "Iteration 8744, Loss: 36.166011810302734, L1: 9.660486221313477, L3: 26.505525588989258\n",
      "Current prediction:  61.65702438354492 \n",
      "\n",
      "Iteration 8745, Loss: 35.54814910888672, L1: 9.64084243774414, L3: 25.907304763793945\n",
      "Current prediction:  61.62654495239258 \n",
      "\n",
      "Iteration 8746, Loss: 35.63206481933594, L1: 9.68934440612793, L3: 25.94272232055664\n",
      "Current prediction:  61.445945739746094 \n",
      "\n",
      "Iteration 8747, Loss: 36.468387603759766, L1: 9.619351387023926, L3: 26.849037170410156\n",
      "Current prediction:  60.61029815673828 \n",
      "\n",
      "Iteration 8748, Loss: 35.59867477416992, L1: 9.675625801086426, L3: 25.923049926757812\n",
      "Current prediction:  60.457401275634766 \n",
      "\n",
      "Iteration 8749, Loss: 35.57678985595703, L1: 9.789458274841309, L3: 25.787330627441406\n",
      "Current prediction:  60.44184875488281 \n",
      "\n",
      "Iteration 8750, Loss: 37.860443115234375, L1: 9.79469108581543, L3: 28.065750122070312\n",
      "Current prediction:  60.474239349365234 \n",
      "\n",
      "Iteration 8751, Loss: 35.631385803222656, L1: 9.715827941894531, L3: 25.915555953979492\n",
      "Current prediction:  60.95137405395508 \n",
      "\n",
      "Iteration 8752, Loss: 35.47577667236328, L1: 9.70030403137207, L3: 25.77547264099121\n",
      "Current prediction:  61.45848083496094 \n",
      "\n",
      "Iteration 8753, Loss: 35.62615203857422, L1: 9.648294448852539, L3: 25.97785758972168\n",
      "Current prediction:  61.5006103515625 \n",
      "\n",
      "Iteration 8754, Loss: 36.93144607543945, L1: 9.750102996826172, L3: 27.18134307861328\n",
      "Current prediction:  61.352783203125 \n",
      "\n",
      "Iteration 8755, Loss: 35.65082550048828, L1: 9.651823043823242, L3: 25.999000549316406\n",
      "Current prediction:  60.58628463745117 \n",
      "\n",
      "Iteration 8756, Loss: 36.14408874511719, L1: 9.719114303588867, L3: 26.42497444152832\n",
      "Current prediction:  60.510135650634766 \n",
      "\n",
      "Iteration 8757, Loss: 35.277793884277344, L1: 9.766250610351562, L3: 25.51154136657715\n",
      "Current prediction:  60.543731689453125 \n",
      "\n",
      "Iteration 8758, Loss: 36.948001861572266, L1: 9.691040992736816, L3: 27.256959915161133\n",
      "Current prediction:  60.60630416870117 \n",
      "\n",
      "Iteration 8759, Loss: 35.43427276611328, L1: 9.614333152770996, L3: 25.8199405670166\n",
      "Current prediction:  61.368038177490234 \n",
      "\n",
      "Iteration 8760, Loss: 36.05290222167969, L1: 9.535021781921387, L3: 26.517881393432617\n",
      "Current prediction:  61.73927307128906 \n",
      "\n",
      "Iteration 8761, Loss: 35.2471809387207, L1: 9.502036094665527, L3: 25.745145797729492\n",
      "Current prediction:  61.771427154541016 \n",
      "\n",
      "Iteration 8762, Loss: 36.0661735534668, L1: 9.5668363571167, L3: 26.499338150024414\n",
      "Current prediction:  61.78102493286133 \n",
      "\n",
      "Iteration 8763, Loss: 35.450809478759766, L1: 9.606769561767578, L3: 25.844039916992188\n",
      "Current prediction:  61.777984619140625 \n",
      "\n",
      "Iteration 8764, Loss: 36.45478439331055, L1: 9.608046531677246, L3: 26.846736907958984\n",
      "Current prediction:  61.758174896240234 \n",
      "\n",
      "Iteration 8765, Loss: 37.10476303100586, L1: 9.594828605651855, L3: 27.509933471679688\n",
      "Current prediction:  61.71747589111328 \n",
      "\n",
      "Iteration 8766, Loss: 36.25208282470703, L1: 9.630847930908203, L3: 26.62123680114746\n",
      "Current prediction:  61.505706787109375 \n",
      "\n",
      "Iteration 8767, Loss: 35.5635986328125, L1: 9.532636642456055, L3: 26.030963897705078\n",
      "Current prediction:  60.58439254760742 \n",
      "\n",
      "Iteration 8768, Loss: 36.999935150146484, L1: 9.679791450500488, L3: 27.320144653320312\n",
      "Current prediction:  60.4850959777832 \n",
      "\n",
      "Iteration 8769, Loss: 36.66097640991211, L1: 9.777256965637207, L3: 26.883718490600586\n",
      "Current prediction:  60.44184875488281 \n",
      "\n",
      "Iteration 8770, Loss: 36.19734191894531, L1: 9.94754409790039, L3: 26.249799728393555\n",
      "Current prediction:  60.409996032714844 \n",
      "\n",
      "Iteration 8771, Loss: 36.4293327331543, L1: 9.941052436828613, L3: 26.48828125\n",
      "Current prediction:  60.393367767333984 \n",
      "\n",
      "Iteration 8772, Loss: 36.64421081542969, L1: 9.900341033935547, L3: 26.743867874145508\n",
      "Current prediction:  60.450096130371094 \n",
      "\n",
      "Iteration 8773, Loss: 35.593597412109375, L1: 9.796361923217773, L3: 25.797237396240234\n",
      "Current prediction:  61.40092468261719 \n",
      "\n",
      "Iteration 8774, Loss: 36.40997314453125, L1: 9.704030990600586, L3: 26.70594024658203\n",
      "Current prediction:  61.50044631958008 \n",
      "\n",
      "Iteration 8775, Loss: 35.88560485839844, L1: 9.80112075805664, L3: 26.084484100341797\n",
      "Current prediction:  61.514705657958984 \n",
      "\n",
      "Iteration 8776, Loss: 35.503143310546875, L1: 9.781312942504883, L3: 25.72182846069336\n",
      "Current prediction:  61.525020599365234 \n",
      "\n",
      "Iteration 8777, Loss: 36.78162384033203, L1: 9.788382530212402, L3: 26.993240356445312\n",
      "Current prediction:  61.51268768310547 \n",
      "\n",
      "Iteration 8778, Loss: 35.475189208984375, L1: 9.674113273620605, L3: 25.801074981689453\n",
      "Current prediction:  61.241641998291016 \n",
      "\n",
      "Iteration 8779, Loss: 35.685386657714844, L1: 9.701679229736328, L3: 25.98370933532715\n",
      "Current prediction:  60.5927734375 \n",
      "\n",
      "Iteration 8780, Loss: 35.81888198852539, L1: 9.649630546569824, L3: 26.169252395629883\n",
      "Current prediction:  60.55931854248047 \n",
      "\n",
      "Iteration 8781, Loss: 36.84428405761719, L1: 9.78201675415039, L3: 27.06226921081543\n",
      "Current prediction:  60.61484146118164 \n",
      "\n",
      "Iteration 8782, Loss: 35.55009460449219, L1: 9.760112762451172, L3: 25.78998374938965\n",
      "Current prediction:  60.70138168334961 \n",
      "\n",
      "Iteration 8783, Loss: 35.945762634277344, L1: 9.658655166625977, L3: 26.287109375\n",
      "Current prediction:  60.917274475097656 \n",
      "\n",
      "Iteration 8784, Loss: 35.8663330078125, L1: 9.604896545410156, L3: 26.26143455505371\n",
      "Current prediction:  60.872154235839844 \n",
      "\n",
      "Iteration 8785, Loss: 36.11880874633789, L1: 9.566049575805664, L3: 26.552759170532227\n",
      "Current prediction:  61.05266571044922 \n",
      "\n",
      "Iteration 8786, Loss: 35.67298889160156, L1: 9.544246673583984, L3: 26.128742218017578\n",
      "Current prediction:  61.31739807128906 \n",
      "\n",
      "Iteration 8787, Loss: 34.825721740722656, L1: 9.54104995727539, L3: 25.284671783447266\n",
      "Current prediction:  61.48032760620117 \n",
      "\n",
      "Iteration 8788, Loss: 35.87700271606445, L1: 9.542675018310547, L3: 26.334327697753906\n",
      "Current prediction:  61.39188766479492 \n",
      "\n",
      "Iteration 8789, Loss: 36.83378601074219, L1: 9.621783256530762, L3: 27.212003707885742\n",
      "Current prediction:  61.561790466308594 \n",
      "\n",
      "Iteration 8790, Loss: 36.21361541748047, L1: 9.6289644241333, L3: 26.58465003967285\n",
      "Current prediction:  61.60358428955078 \n",
      "\n",
      "Iteration 8791, Loss: 36.02691650390625, L1: 9.623054504394531, L3: 26.403860092163086\n",
      "Current prediction:  61.518882751464844 \n",
      "\n",
      "Iteration 8792, Loss: 35.74693298339844, L1: 9.609986305236816, L3: 26.136947631835938\n",
      "Current prediction:  61.02548599243164 \n",
      "\n",
      "Iteration 8793, Loss: 35.98023986816406, L1: 9.627256393432617, L3: 26.352981567382812\n",
      "Current prediction:  60.53793716430664 \n",
      "\n",
      "Iteration 8794, Loss: 36.10483169555664, L1: 9.71992301940918, L3: 26.38490867614746\n",
      "Current prediction:  60.482765197753906 \n",
      "\n",
      "Iteration 8795, Loss: 35.515541076660156, L1: 9.769521713256836, L3: 25.746021270751953\n",
      "Current prediction:  60.49011993408203 \n",
      "\n",
      "Iteration 8796, Loss: 35.30603790283203, L1: 9.84121322631836, L3: 25.464826583862305\n",
      "Current prediction:  60.611480712890625 \n",
      "\n",
      "Iteration 8797, Loss: 34.443931579589844, L1: 9.663690567016602, L3: 24.780241012573242\n",
      "Current prediction:  61.23133087158203 \n",
      "\n",
      "Iteration 8798, Loss: 35.25830078125, L1: 9.601797103881836, L3: 25.656505584716797\n",
      "Current prediction:  61.58283615112305 \n",
      "\n",
      "Iteration 8799, Loss: 35.07529067993164, L1: 9.650164604187012, L3: 25.425127029418945\n",
      "Current prediction:  61.624534606933594 \n",
      "\n",
      "Iteration 8800, Loss: 35.078155517578125, L1: 9.614645004272461, L3: 25.463512420654297\n",
      "Current prediction:  61.615726470947266 \n",
      "\n",
      "Iteration 8801, Loss: 35.98884963989258, L1: 9.641623497009277, L3: 26.347225189208984\n",
      "Current prediction:  61.34383010864258 \n",
      "\n",
      "Iteration 8802, Loss: 36.70109939575195, L1: 9.586042404174805, L3: 27.11505699157715\n",
      "Current prediction:  60.66412353515625 \n",
      "\n",
      "Iteration 8803, Loss: 35.72587203979492, L1: 9.60741138458252, L3: 26.118459701538086\n",
      "Current prediction:  60.57573699951172 \n",
      "\n",
      "Iteration 8804, Loss: 35.54513168334961, L1: 9.711783409118652, L3: 25.83334732055664\n",
      "Current prediction:  60.63349151611328 \n",
      "\n",
      "Iteration 8805, Loss: 37.03116226196289, L1: 9.64240837097168, L3: 27.38875389099121\n",
      "Current prediction:  60.921592712402344 \n",
      "\n",
      "Iteration 8806, Loss: 36.11992645263672, L1: 9.584968566894531, L3: 26.53495979309082\n",
      "Current prediction:  61.595619201660156 \n",
      "\n",
      "Iteration 8807, Loss: 36.09757995605469, L1: 9.54249095916748, L3: 26.55508804321289\n",
      "Current prediction:  61.63455581665039 \n",
      "\n",
      "Iteration 8808, Loss: 36.41470718383789, L1: 9.578179359436035, L3: 26.836528778076172\n",
      "Current prediction:  61.56903076171875 \n",
      "\n",
      "Iteration 8809, Loss: 36.480918884277344, L1: 9.561562538146973, L3: 26.919355392456055\n",
      "Current prediction:  61.48446273803711 \n",
      "\n",
      "Iteration 8810, Loss: 36.39612579345703, L1: 9.654406547546387, L3: 26.741718292236328\n",
      "Current prediction:  61.375404357910156 \n",
      "\n",
      "Iteration 8811, Loss: 35.501800537109375, L1: 9.597014427185059, L3: 25.90478515625\n",
      "Current prediction:  60.896541595458984 \n",
      "\n",
      "Iteration 8812, Loss: 36.32180404663086, L1: 9.63471508026123, L3: 26.687088012695312\n",
      "Current prediction:  60.67129135131836 \n",
      "\n",
      "Iteration 8813, Loss: 35.6340217590332, L1: 9.67911148071289, L3: 25.954910278320312\n",
      "Current prediction:  60.61209487915039 \n",
      "\n",
      "Iteration 8814, Loss: 35.65931701660156, L1: 9.635254859924316, L3: 26.024063110351562\n",
      "Current prediction:  60.66388702392578 \n",
      "\n",
      "Iteration 8815, Loss: 35.942543029785156, L1: 9.673240661621094, L3: 26.269302368164062\n",
      "Current prediction:  61.358402252197266 \n",
      "\n",
      "Iteration 8816, Loss: 35.792171478271484, L1: 9.62595272064209, L3: 26.166217803955078\n",
      "Current prediction:  61.573326110839844 \n",
      "\n",
      "Iteration 8817, Loss: 36.01581573486328, L1: 9.633855819702148, L3: 26.381959915161133\n",
      "Current prediction:  61.60525894165039 \n",
      "\n",
      "Iteration 8818, Loss: 34.395782470703125, L1: 9.66773796081543, L3: 24.728046417236328\n",
      "Current prediction:  61.60639953613281 \n",
      "\n",
      "Iteration 8819, Loss: 36.19610595703125, L1: 9.658785820007324, L3: 26.53731918334961\n",
      "Current prediction:  61.48304748535156 \n",
      "\n",
      "Iteration 8820, Loss: 36.28935241699219, L1: 9.630498886108398, L3: 26.65885353088379\n",
      "Current prediction:  60.836708068847656 \n",
      "\n",
      "Iteration 8821, Loss: 35.75059509277344, L1: 9.636014938354492, L3: 26.114582061767578\n",
      "Current prediction:  60.54738235473633 \n",
      "\n",
      "Iteration 8822, Loss: 35.33134078979492, L1: 9.689082145690918, L3: 25.64225959777832\n",
      "Current prediction:  60.56452941894531 \n",
      "\n",
      "Iteration 8823, Loss: 36.91973114013672, L1: 9.704864501953125, L3: 27.214868545532227\n",
      "Current prediction:  61.0877799987793 \n",
      "\n",
      "Iteration 8824, Loss: 36.646366119384766, L1: 9.579059600830078, L3: 27.067306518554688\n",
      "Current prediction:  61.59733200073242 \n",
      "\n",
      "Iteration 8825, Loss: 35.77912139892578, L1: 9.681103706359863, L3: 26.098018646240234\n",
      "Current prediction:  61.62167739868164 \n",
      "\n",
      "Iteration 8826, Loss: 35.265926361083984, L1: 9.65332317352295, L3: 25.61260223388672\n",
      "Current prediction:  61.619544982910156 \n",
      "\n",
      "Iteration 8827, Loss: 36.763648986816406, L1: 9.671483993530273, L3: 27.092164993286133\n",
      "Current prediction:  61.579246520996094 \n",
      "\n",
      "Iteration 8828, Loss: 35.302894592285156, L1: 9.574295043945312, L3: 25.728599548339844\n",
      "Current prediction:  61.0156364440918 \n",
      "\n",
      "Iteration 8829, Loss: 35.281742095947266, L1: 9.584785461425781, L3: 25.696956634521484\n",
      "Current prediction:  60.58357620239258 \n",
      "\n",
      "Iteration 8830, Loss: 35.43841552734375, L1: 9.65461540222168, L3: 25.783798217773438\n",
      "Current prediction:  60.56259536743164 \n",
      "\n",
      "Iteration 8831, Loss: 36.58007049560547, L1: 9.68138599395752, L3: 26.898683547973633\n",
      "Current prediction:  60.773590087890625 \n",
      "\n",
      "Iteration 8832, Loss: 36.19371795654297, L1: 9.638591766357422, L3: 26.55512809753418\n",
      "Current prediction:  61.550537109375 \n",
      "\n",
      "Iteration 8833, Loss: 35.59716033935547, L1: 9.63937759399414, L3: 25.957780838012695\n",
      "Current prediction:  61.62088394165039 \n",
      "\n",
      "Iteration 8834, Loss: 34.91628646850586, L1: 9.596650123596191, L3: 25.31963539123535\n",
      "Current prediction:  61.57994079589844 \n",
      "\n",
      "Iteration 8835, Loss: 35.63359832763672, L1: 9.607502937316895, L3: 26.026094436645508\n",
      "Current prediction:  60.79118347167969 \n",
      "\n",
      "Iteration 8836, Loss: 34.8282356262207, L1: 9.572111129760742, L3: 25.25612449645996\n",
      "Current prediction:  60.65856170654297 \n",
      "\n",
      "Iteration 8837, Loss: 36.6147575378418, L1: 9.47419261932373, L3: 27.140565872192383\n",
      "Current prediction:  60.727272033691406 \n",
      "\n",
      "Iteration 8838, Loss: 35.54674530029297, L1: 9.611515998840332, L3: 25.93522834777832\n",
      "Current prediction:  61.146663665771484 \n",
      "\n",
      "Iteration 8839, Loss: 35.639244079589844, L1: 9.552998542785645, L3: 26.086246490478516\n",
      "Current prediction:  61.64318084716797 \n",
      "\n",
      "Iteration 8840, Loss: 35.61279296875, L1: 9.571378707885742, L3: 26.041414260864258\n",
      "Current prediction:  61.691200256347656 \n",
      "\n",
      "Iteration 8841, Loss: 36.038673400878906, L1: 9.570466041564941, L3: 26.46820640563965\n",
      "Current prediction:  61.67969512939453 \n",
      "\n",
      "Iteration 8842, Loss: 35.54974365234375, L1: 9.611804008483887, L3: 25.937938690185547\n",
      "Current prediction:  61.65656661987305 \n",
      "\n",
      "Iteration 8843, Loss: 36.16836166381836, L1: 9.57320499420166, L3: 26.595155715942383\n",
      "Current prediction:  61.57750701904297 \n",
      "\n",
      "Iteration 8844, Loss: 35.42284393310547, L1: 9.616747856140137, L3: 25.80609703063965\n",
      "Current prediction:  61.251258850097656 \n",
      "\n",
      "Iteration 8845, Loss: 36.06367492675781, L1: 9.640819549560547, L3: 26.422855377197266\n",
      "Current prediction:  60.839759826660156 \n",
      "\n",
      "Iteration 8846, Loss: 37.389739990234375, L1: 9.6135835647583, L3: 27.776155471801758\n",
      "Current prediction:  60.544925689697266 \n",
      "\n",
      "Iteration 8847, Loss: 36.777061462402344, L1: 9.691882133483887, L3: 27.08517837524414\n",
      "Current prediction:  60.60640335083008 \n",
      "\n",
      "Iteration 8848, Loss: 36.23126983642578, L1: 9.665218353271484, L3: 26.566049575805664\n",
      "Current prediction:  60.89784240722656 \n",
      "\n",
      "Iteration 8849, Loss: 36.57855224609375, L1: 9.635320663452148, L3: 26.943233489990234\n",
      "Current prediction:  61.289207458496094 \n",
      "\n",
      "Iteration 8850, Loss: 35.96405792236328, L1: 9.636916160583496, L3: 26.32714080810547\n",
      "Current prediction:  61.420467376708984 \n",
      "\n",
      "Iteration 8851, Loss: 35.520973205566406, L1: 9.673355102539062, L3: 25.84761619567871\n",
      "Current prediction:  61.46856689453125 \n",
      "\n",
      "Iteration 8852, Loss: 36.928977966308594, L1: 9.653523445129395, L3: 27.275453567504883\n",
      "Current prediction:  61.35826110839844 \n",
      "\n",
      "Iteration 8853, Loss: 36.383663177490234, L1: 9.568418502807617, L3: 26.815244674682617\n",
      "Current prediction:  61.23533630371094 \n",
      "\n",
      "Iteration 8854, Loss: 36.19102478027344, L1: 9.605913162231445, L3: 26.585111618041992\n",
      "Current prediction:  61.12589645385742 \n",
      "\n",
      "Iteration 8855, Loss: 36.23143768310547, L1: 9.58600902557373, L3: 26.645427703857422\n",
      "Current prediction:  61.262325286865234 \n",
      "\n",
      "Iteration 8856, Loss: 35.78708267211914, L1: 9.607272148132324, L3: 26.1798095703125\n",
      "Current prediction:  61.03871154785156 \n",
      "\n",
      "Iteration 8857, Loss: 35.495750427246094, L1: 9.580248832702637, L3: 25.91550064086914\n",
      "Current prediction:  60.77943420410156 \n",
      "\n",
      "Iteration 8858, Loss: 35.844234466552734, L1: 9.610712051391602, L3: 26.233522415161133\n",
      "Current prediction:  60.74320602416992 \n",
      "\n",
      "Iteration 8859, Loss: 35.08149719238281, L1: 9.5872220993042, L3: 25.49427604675293\n",
      "Current prediction:  60.81283950805664 \n",
      "\n",
      "Iteration 8860, Loss: 36.452613830566406, L1: 9.617868423461914, L3: 26.83474349975586\n",
      "Current prediction:  60.875030517578125 \n",
      "\n",
      "Iteration 8861, Loss: 35.000858306884766, L1: 9.619235038757324, L3: 25.381624221801758\n",
      "Current prediction:  61.09529113769531 \n",
      "\n",
      "Iteration 8862, Loss: 36.08642578125, L1: 9.570438385009766, L3: 26.515987396240234\n",
      "Current prediction:  61.07110595703125 \n",
      "\n",
      "Iteration 8863, Loss: 36.167110443115234, L1: 9.628108024597168, L3: 26.539003372192383\n",
      "Current prediction:  61.07059860229492 \n",
      "\n",
      "Iteration 8864, Loss: 35.878883361816406, L1: 9.624283790588379, L3: 26.254600524902344\n",
      "Current prediction:  61.25978088378906 \n",
      "\n",
      "Iteration 8865, Loss: 36.20624542236328, L1: 9.534738540649414, L3: 26.671506881713867\n",
      "Current prediction:  61.384437561035156 \n",
      "\n",
      "Iteration 8866, Loss: 36.843109130859375, L1: 9.581222534179688, L3: 27.261886596679688\n",
      "Current prediction:  61.313232421875 \n",
      "\n",
      "Iteration 8867, Loss: 35.82398986816406, L1: 9.604615211486816, L3: 26.219375610351562\n",
      "Current prediction:  60.99336624145508 \n",
      "\n",
      "Iteration 8868, Loss: 36.08689498901367, L1: 9.570300102233887, L3: 26.51659393310547\n",
      "Current prediction:  61.069175720214844 \n",
      "\n",
      "Iteration 8869, Loss: 35.39679718017578, L1: 9.592506408691406, L3: 25.804288864135742\n",
      "Current prediction:  61.239356994628906 \n",
      "\n",
      "Iteration 8870, Loss: 34.74259948730469, L1: 9.507157325744629, L3: 25.235443115234375\n",
      "Current prediction:  61.43755340576172 \n",
      "\n",
      "Iteration 8871, Loss: 35.74201965332031, L1: 9.541802406311035, L3: 26.20021629333496\n",
      "Current prediction:  61.510623931884766 \n",
      "\n",
      "Iteration 8872, Loss: 34.78052520751953, L1: 9.595602035522461, L3: 25.184925079345703\n",
      "Current prediction:  61.56122970581055 \n",
      "\n",
      "Iteration 8873, Loss: 35.58655548095703, L1: 9.57318115234375, L3: 26.01337242126465\n",
      "Current prediction:  61.4967155456543 \n",
      "\n",
      "Iteration 8874, Loss: 36.329437255859375, L1: 9.596473693847656, L3: 26.732961654663086\n",
      "Current prediction:  61.13752365112305 \n",
      "\n",
      "Iteration 8875, Loss: 36.33261489868164, L1: 9.598725318908691, L3: 26.733890533447266\n",
      "Current prediction:  60.69508743286133 \n",
      "\n",
      "Iteration 8876, Loss: 35.387847900390625, L1: 9.598430633544922, L3: 25.789419174194336\n",
      "Current prediction:  60.62297439575195 \n",
      "\n",
      "Iteration 8877, Loss: 36.14208221435547, L1: 9.631888389587402, L3: 26.51019287109375\n",
      "Current prediction:  60.7022705078125 \n",
      "\n",
      "Iteration 8878, Loss: 36.08528518676758, L1: 9.606192588806152, L3: 26.479093551635742\n",
      "Current prediction:  60.87196350097656 \n",
      "\n",
      "Iteration 8879, Loss: 35.617828369140625, L1: 9.551968574523926, L3: 26.065860748291016\n",
      "Current prediction:  60.91096115112305 \n",
      "\n",
      "Iteration 8880, Loss: 36.13013458251953, L1: 9.614973068237305, L3: 26.51516342163086\n",
      "Current prediction:  61.13062286376953 \n",
      "\n",
      "Iteration 8881, Loss: 36.54374694824219, L1: 9.586238861083984, L3: 26.95750617980957\n",
      "Current prediction:  61.20132064819336 \n",
      "\n",
      "Iteration 8882, Loss: 36.13741683959961, L1: 9.60156536102295, L3: 26.535850524902344\n",
      "Current prediction:  61.34635543823242 \n",
      "\n",
      "Iteration 8883, Loss: 36.461299896240234, L1: 9.595505714416504, L3: 26.865793228149414\n",
      "Current prediction:  61.22291946411133 \n",
      "\n",
      "Iteration 8884, Loss: 35.522491455078125, L1: 9.631102561950684, L3: 25.891389846801758\n",
      "Current prediction:  61.30268859863281 \n",
      "\n",
      "Iteration 8885, Loss: 36.75045394897461, L1: 9.61060619354248, L3: 27.139846801757812\n",
      "Current prediction:  60.899147033691406 \n",
      "\n",
      "Iteration 8886, Loss: 35.5731086730957, L1: 9.615113258361816, L3: 25.95799446105957\n",
      "Current prediction:  60.568294525146484 \n",
      "\n",
      "Iteration 8887, Loss: 35.89827346801758, L1: 9.700845718383789, L3: 26.19742774963379\n",
      "Current prediction:  60.54698944091797 \n",
      "\n",
      "Iteration 8888, Loss: 35.170936584472656, L1: 9.63616943359375, L3: 25.534767150878906\n",
      "Current prediction:  60.96400451660156 \n",
      "\n",
      "Iteration 8889, Loss: 35.41694259643555, L1: 9.651494979858398, L3: 25.76544761657715\n",
      "Current prediction:  61.382423400878906 \n",
      "\n",
      "Iteration 8890, Loss: 36.77033996582031, L1: 9.628035545349121, L3: 27.142303466796875\n",
      "Current prediction:  61.40239715576172 \n",
      "\n",
      "Iteration 8891, Loss: 35.60584259033203, L1: 9.6596097946167, L3: 25.94623374938965\n",
      "Current prediction:  61.47021484375 \n",
      "\n",
      "Iteration 8892, Loss: 36.467613220214844, L1: 9.674430847167969, L3: 26.793180465698242\n",
      "Current prediction:  61.36273956298828 \n",
      "\n",
      "Iteration 8893, Loss: 36.1518669128418, L1: 9.567161560058594, L3: 26.584705352783203\n",
      "Current prediction:  61.02518844604492 \n",
      "\n",
      "Iteration 8894, Loss: 35.646484375, L1: 9.612064361572266, L3: 26.034421920776367\n",
      "Current prediction:  60.823177337646484 \n",
      "\n",
      "Iteration 8895, Loss: 35.92818069458008, L1: 9.541232109069824, L3: 26.386947631835938\n",
      "Current prediction:  60.76993179321289 \n",
      "\n",
      "Iteration 8896, Loss: 36.6506233215332, L1: 9.519103050231934, L3: 27.131521224975586\n",
      "Current prediction:  60.734588623046875 \n",
      "\n",
      "Iteration 8897, Loss: 35.09400939941406, L1: 9.59791374206543, L3: 25.496097564697266\n",
      "Current prediction:  61.00177001953125 \n",
      "\n",
      "Iteration 8898, Loss: 36.05360412597656, L1: 9.434738159179688, L3: 26.618867874145508\n",
      "Current prediction:  61.563209533691406 \n",
      "\n",
      "Iteration 8899, Loss: 35.600685119628906, L1: 9.523621559143066, L3: 26.077062606811523\n",
      "Current prediction:  61.721534729003906 \n",
      "\n",
      "Iteration 8900, Loss: 34.42667007446289, L1: 9.592909812927246, L3: 24.83376121520996\n",
      "Current prediction:  61.73958206176758 \n",
      "\n",
      "Iteration 8901, Loss: 36.36081314086914, L1: 9.579411506652832, L3: 26.781400680541992\n",
      "Current prediction:  61.73094940185547 \n",
      "\n",
      "Iteration 8902, Loss: 35.59222412109375, L1: 9.59020709991455, L3: 26.002017974853516\n",
      "Current prediction:  61.67543029785156 \n",
      "\n",
      "Iteration 8903, Loss: 35.67707824707031, L1: 9.49594497680664, L3: 26.18113136291504\n",
      "Current prediction:  60.81396484375 \n",
      "\n",
      "Iteration 8904, Loss: 35.61918258666992, L1: 9.554485321044922, L3: 26.064697265625\n",
      "Current prediction:  60.57122039794922 \n",
      "\n",
      "Iteration 8905, Loss: 36.57365417480469, L1: 9.718069076538086, L3: 26.8555850982666\n",
      "Current prediction:  60.54078674316406 \n",
      "\n",
      "Iteration 8906, Loss: 35.914588928222656, L1: 9.776607513427734, L3: 26.13797950744629\n",
      "Current prediction:  60.53382873535156 \n",
      "\n",
      "Iteration 8907, Loss: 36.237125396728516, L1: 9.709742546081543, L3: 26.52738380432129\n",
      "Current prediction:  60.562686920166016 \n",
      "\n",
      "Iteration 8908, Loss: 36.05767822265625, L1: 9.596651077270508, L3: 26.46102523803711\n",
      "Current prediction:  60.88124084472656 \n",
      "\n",
      "Iteration 8909, Loss: 35.69758605957031, L1: 9.591407775878906, L3: 26.10618019104004\n",
      "Current prediction:  61.57109451293945 \n",
      "\n",
      "Iteration 8910, Loss: 35.02826690673828, L1: 9.615721702575684, L3: 25.412546157836914\n",
      "Current prediction:  61.65068054199219 \n",
      "\n",
      "Iteration 8911, Loss: 35.58103942871094, L1: 9.629205703735352, L3: 25.951831817626953\n",
      "Current prediction:  61.639129638671875 \n",
      "\n",
      "Iteration 8912, Loss: 35.8756103515625, L1: 9.625, L3: 26.250612258911133\n",
      "Current prediction:  61.60731887817383 \n",
      "\n",
      "Iteration 8913, Loss: 35.65321731567383, L1: 9.599112510681152, L3: 26.05410385131836\n",
      "Current prediction:  61.37221908569336 \n",
      "\n",
      "Iteration 8914, Loss: 36.75010681152344, L1: 9.599559783935547, L3: 27.15054702758789\n",
      "Current prediction:  60.56387710571289 \n",
      "\n",
      "Iteration 8915, Loss: 35.7547721862793, L1: 9.655121803283691, L3: 26.099651336669922\n",
      "Current prediction:  60.485870361328125 \n",
      "\n",
      "Iteration 8916, Loss: 35.56052017211914, L1: 9.763104438781738, L3: 25.79741668701172\n",
      "Current prediction:  60.53471755981445 \n",
      "\n",
      "Iteration 8917, Loss: 37.04798889160156, L1: 9.679888725280762, L3: 27.368099212646484\n",
      "Current prediction:  61.3204345703125 \n",
      "\n",
      "Iteration 8918, Loss: 35.53999710083008, L1: 9.663411140441895, L3: 25.8765869140625\n",
      "Current prediction:  61.528018951416016 \n",
      "\n",
      "Iteration 8919, Loss: 35.262367248535156, L1: 9.657578468322754, L3: 25.60478973388672\n",
      "Current prediction:  61.54664611816406 \n",
      "\n",
      "Iteration 8920, Loss: 35.74924850463867, L1: 9.712573051452637, L3: 26.03667640686035\n",
      "Current prediction:  61.5498161315918 \n",
      "\n",
      "Iteration 8921, Loss: 35.089847564697266, L1: 9.710464477539062, L3: 25.379383087158203\n",
      "Current prediction:  61.55097961425781 \n",
      "\n",
      "Iteration 8922, Loss: 36.567752838134766, L1: 9.67706298828125, L3: 26.890689849853516\n",
      "Current prediction:  61.531978607177734 \n",
      "\n",
      "Iteration 8923, Loss: 36.15217208862305, L1: 9.655818939208984, L3: 26.496353149414062\n",
      "Current prediction:  61.19771957397461 \n",
      "\n",
      "Iteration 8924, Loss: 35.63107681274414, L1: 9.554991722106934, L3: 26.07608413696289\n",
      "Current prediction:  60.54936981201172 \n",
      "\n",
      "Iteration 8925, Loss: 36.109901428222656, L1: 9.658723831176758, L3: 26.4511775970459\n",
      "Current prediction:  60.45792770385742 \n",
      "\n",
      "Iteration 8926, Loss: 36.28472137451172, L1: 9.770708084106445, L3: 26.514013290405273\n",
      "Current prediction:  60.49213790893555 \n",
      "\n",
      "Iteration 8927, Loss: 35.02236557006836, L1: 9.743529319763184, L3: 25.278837203979492\n",
      "Current prediction:  60.72551727294922 \n",
      "\n",
      "Iteration 8928, Loss: 36.19244384765625, L1: 9.600255012512207, L3: 26.592187881469727\n",
      "Current prediction:  61.51997756958008 \n",
      "\n",
      "Iteration 8929, Loss: 36.11796569824219, L1: 9.50699234008789, L3: 26.61097526550293\n",
      "Current prediction:  61.627376556396484 \n",
      "\n",
      "Iteration 8930, Loss: 36.262779235839844, L1: 9.669662475585938, L3: 26.59311866760254\n",
      "Current prediction:  61.64253234863281 \n",
      "\n",
      "Iteration 8931, Loss: 36.013710021972656, L1: 9.631919860839844, L3: 26.38178825378418\n",
      "Current prediction:  61.64347839355469 \n",
      "\n",
      "Iteration 8932, Loss: 36.06055450439453, L1: 9.706550598144531, L3: 26.35400390625\n",
      "Current prediction:  61.64030838012695 \n",
      "\n",
      "Iteration 8933, Loss: 36.17859649658203, L1: 9.701641082763672, L3: 26.476953506469727\n",
      "Current prediction:  61.63146209716797 \n",
      "\n",
      "Iteration 8934, Loss: 37.843814849853516, L1: 9.633002281188965, L3: 28.210811614990234\n",
      "Current prediction:  61.58540344238281 \n",
      "\n",
      "Iteration 8935, Loss: 36.132259368896484, L1: 9.576593399047852, L3: 26.555665969848633\n",
      "Current prediction:  61.093902587890625 \n",
      "\n",
      "Iteration 8936, Loss: 36.236385345458984, L1: 9.582906723022461, L3: 26.653478622436523\n",
      "Current prediction:  60.53703308105469 \n",
      "\n",
      "Iteration 8937, Loss: 35.86958312988281, L1: 9.675779342651367, L3: 26.193803787231445\n",
      "Current prediction:  60.4736328125 \n",
      "\n",
      "Iteration 8938, Loss: 36.074745178222656, L1: 9.684808731079102, L3: 26.389934539794922\n",
      "Current prediction:  60.4693603515625 \n",
      "\n",
      "Iteration 8939, Loss: 35.90460205078125, L1: 9.732048988342285, L3: 26.17255210876465\n",
      "Current prediction:  60.65807342529297 \n",
      "\n",
      "Iteration 8940, Loss: 35.846092224121094, L1: 9.616473197937012, L3: 26.229618072509766\n",
      "Current prediction:  61.460140228271484 \n",
      "\n",
      "Iteration 8941, Loss: 35.44451904296875, L1: 9.606489181518555, L3: 25.838031768798828\n",
      "Current prediction:  61.59849548339844 \n",
      "\n",
      "Iteration 8942, Loss: 36.06674575805664, L1: 9.601841926574707, L3: 26.464902877807617\n",
      "Current prediction:  61.60390090942383 \n",
      "\n",
      "Iteration 8943, Loss: 36.91405487060547, L1: 9.680248260498047, L3: 27.233806610107422\n",
      "Current prediction:  61.60611343383789 \n",
      "\n",
      "Iteration 8944, Loss: 36.51028060913086, L1: 9.651399612426758, L3: 26.8588809967041\n",
      "Current prediction:  61.55449295043945 \n",
      "\n",
      "Iteration 8945, Loss: 36.13138961791992, L1: 9.571186065673828, L3: 26.560203552246094\n",
      "Current prediction:  60.90304946899414 \n",
      "\n",
      "Iteration 8946, Loss: 35.94822692871094, L1: 9.585148811340332, L3: 26.363079071044922\n",
      "Current prediction:  60.52810287475586 \n",
      "\n",
      "Iteration 8947, Loss: 35.33694839477539, L1: 9.629304885864258, L3: 25.707643508911133\n",
      "Current prediction:  60.51143264770508 \n",
      "\n",
      "Iteration 8948, Loss: 38.31068420410156, L1: 12.220134735107422, L3: 26.090547561645508\n",
      "Current prediction:  60.808292388916016 \n",
      "\n",
      "Iteration 8949, Loss: 35.71407699584961, L1: 9.60866641998291, L3: 26.105409622192383\n",
      "Current prediction:  61.41758346557617 \n",
      "\n",
      "Iteration 8950, Loss: 36.15141296386719, L1: 9.583895683288574, L3: 26.56751823425293\n",
      "Current prediction:  61.5485954284668 \n",
      "\n",
      "Iteration 8951, Loss: 36.090003967285156, L1: 9.56719970703125, L3: 26.52280616760254\n",
      "Current prediction:  61.47607421875 \n",
      "\n",
      "Iteration 8952, Loss: 34.76033401489258, L1: 9.573583602905273, L3: 25.186750411987305\n",
      "Current prediction:  61.107608795166016 \n",
      "\n",
      "Iteration 8953, Loss: 35.302093505859375, L1: 9.591331481933594, L3: 25.71076202392578\n",
      "Current prediction:  60.81018829345703 \n",
      "\n",
      "Iteration 8954, Loss: 35.30584716796875, L1: 9.54574203491211, L3: 25.760107040405273\n",
      "Current prediction:  61.034828186035156 \n",
      "\n",
      "Iteration 8955, Loss: 35.4962272644043, L1: 9.582098007202148, L3: 25.91412925720215\n",
      "Current prediction:  61.38802719116211 \n",
      "\n",
      "Iteration 8956, Loss: 36.49295425415039, L1: 9.564658164978027, L3: 26.92829704284668\n",
      "Current prediction:  61.3074836730957 \n",
      "\n",
      "Iteration 8957, Loss: 35.860084533691406, L1: 9.461288452148438, L3: 26.39879608154297\n",
      "Current prediction:  60.95286178588867 \n",
      "\n",
      "Iteration 8958, Loss: 35.85829162597656, L1: 9.530115127563477, L3: 26.32817840576172\n",
      "Current prediction:  61.24360275268555 \n",
      "\n",
      "Iteration 8959, Loss: 35.144691467285156, L1: 9.573406219482422, L3: 25.571285247802734\n",
      "Current prediction:  61.327274322509766 \n",
      "\n",
      "Iteration 8960, Loss: 35.93553924560547, L1: 9.530515670776367, L3: 26.405025482177734\n",
      "Current prediction:  61.28184127807617 \n",
      "\n",
      "Iteration 8961, Loss: 35.79731750488281, L1: 9.502130508422852, L3: 26.29518699645996\n",
      "Current prediction:  61.03144836425781 \n",
      "\n",
      "Iteration 8962, Loss: 35.669090270996094, L1: 9.520162582397461, L3: 26.14892578125\n",
      "Current prediction:  61.26400375366211 \n",
      "\n",
      "Iteration 8963, Loss: 35.30665588378906, L1: 9.469388961791992, L3: 25.837268829345703\n",
      "Current prediction:  61.44185256958008 \n",
      "\n",
      "Iteration 8964, Loss: 35.71320343017578, L1: 9.482240676879883, L3: 26.2309627532959\n",
      "Current prediction:  61.50095748901367 \n",
      "\n",
      "Iteration 8965, Loss: 35.97124099731445, L1: 9.498967170715332, L3: 26.472274780273438\n",
      "Current prediction:  61.10468292236328 \n",
      "\n",
      "Iteration 8966, Loss: 35.42829132080078, L1: 9.469381332397461, L3: 25.958911895751953\n",
      "Current prediction:  60.64455032348633 \n",
      "\n",
      "Iteration 8967, Loss: 35.436798095703125, L1: 9.600555419921875, L3: 25.83624267578125\n",
      "Current prediction:  60.96265411376953 \n",
      "\n",
      "Iteration 8968, Loss: 36.85086441040039, L1: 9.563897132873535, L3: 27.286968231201172\n",
      "Current prediction:  61.140892028808594 \n",
      "\n",
      "Iteration 8969, Loss: 35.815162658691406, L1: 9.56308364868164, L3: 26.252079010009766\n",
      "Current prediction:  61.44402313232422 \n",
      "\n",
      "Iteration 8970, Loss: 37.440711975097656, L1: 9.544951438903809, L3: 27.89575958251953\n",
      "Current prediction:  61.38064193725586 \n",
      "\n",
      "Iteration 8971, Loss: 35.64385223388672, L1: 9.558237075805664, L3: 26.085613250732422\n",
      "Current prediction:  60.856590270996094 \n",
      "\n",
      "Iteration 8972, Loss: 36.177391052246094, L1: 9.542390823364258, L3: 26.635000228881836\n",
      "Current prediction:  60.563011169433594 \n",
      "\n",
      "Iteration 8973, Loss: 35.41469955444336, L1: 9.665087699890137, L3: 25.74961280822754\n",
      "Current prediction:  60.47775650024414 \n",
      "\n",
      "Iteration 8974, Loss: 35.184261322021484, L1: 9.677932739257812, L3: 25.506328582763672\n",
      "Current prediction:  60.45033645629883 \n",
      "\n",
      "Iteration 8975, Loss: 35.71586608886719, L1: 9.708786010742188, L3: 26.007078170776367\n",
      "Current prediction:  60.669677734375 \n",
      "\n",
      "Iteration 8976, Loss: 34.882904052734375, L1: 9.628289222717285, L3: 25.254615783691406\n",
      "Current prediction:  61.53882598876953 \n",
      "\n",
      "Iteration 8977, Loss: 36.32763671875, L1: 9.570727348327637, L3: 26.75691032409668\n",
      "Current prediction:  61.61516571044922 \n",
      "\n",
      "Iteration 8978, Loss: 35.56277847290039, L1: 9.689562797546387, L3: 25.873214721679688\n",
      "Current prediction:  61.637962341308594 \n",
      "\n",
      "Iteration 8979, Loss: 36.47343826293945, L1: 9.68118953704834, L3: 26.79224967956543\n",
      "Current prediction:  61.64891052246094 \n",
      "\n",
      "Iteration 8980, Loss: 35.12858581542969, L1: 9.67011833190918, L3: 25.458465576171875\n",
      "Current prediction:  61.64981460571289 \n",
      "\n",
      "Iteration 8981, Loss: 36.21692657470703, L1: 9.63995361328125, L3: 26.57697105407715\n",
      "Current prediction:  61.63028335571289 \n",
      "\n",
      "Iteration 8982, Loss: 35.90947723388672, L1: 9.633796691894531, L3: 26.275678634643555\n",
      "Current prediction:  61.44306564331055 \n",
      "\n",
      "Iteration 8983, Loss: 36.0679931640625, L1: 9.521635055541992, L3: 26.546356201171875\n",
      "Current prediction:  60.83043670654297 \n",
      "\n",
      "Iteration 8984, Loss: 35.69059753417969, L1: 9.554802894592285, L3: 26.135793685913086\n",
      "Current prediction:  60.66291809082031 \n",
      "\n",
      "Iteration 8985, Loss: 35.78165054321289, L1: 9.559576988220215, L3: 26.222074508666992\n",
      "Current prediction:  60.65985870361328 \n",
      "\n",
      "Iteration 8986, Loss: 35.01512908935547, L1: 9.511588096618652, L3: 25.5035400390625\n",
      "Current prediction:  60.769493103027344 \n",
      "\n",
      "Iteration 8987, Loss: 36.013553619384766, L1: 9.504548072814941, L3: 26.50900650024414\n",
      "Current prediction:  61.14501190185547 \n",
      "\n",
      "Iteration 8988, Loss: 35.41962432861328, L1: 9.479835510253906, L3: 25.939790725708008\n",
      "Current prediction:  61.6378173828125 \n",
      "\n",
      "Iteration 8989, Loss: 36.14866638183594, L1: 9.575483322143555, L3: 26.57318115234375\n",
      "Current prediction:  61.702816009521484 \n",
      "\n",
      "Iteration 8990, Loss: 36.534732818603516, L1: 9.598359107971191, L3: 26.93637466430664\n",
      "Current prediction:  61.6999626159668 \n",
      "\n",
      "Iteration 8991, Loss: 36.21235656738281, L1: 9.66579532623291, L3: 26.54656219482422\n",
      "Current prediction:  61.67723083496094 \n",
      "\n",
      "Iteration 8992, Loss: 36.0850944519043, L1: 9.623651504516602, L3: 26.461442947387695\n",
      "Current prediction:  61.637027740478516 \n",
      "\n",
      "Iteration 8993, Loss: 36.090415954589844, L1: 9.658242225646973, L3: 26.432174682617188\n",
      "Current prediction:  61.568992614746094 \n",
      "\n",
      "Iteration 8994, Loss: 36.04523468017578, L1: 9.617483139038086, L3: 26.427753448486328\n",
      "Current prediction:  61.275970458984375 \n",
      "\n",
      "Iteration 8995, Loss: 35.811302185058594, L1: 9.602986335754395, L3: 26.208316802978516\n",
      "Current prediction:  60.62847137451172 \n",
      "\n",
      "Iteration 8996, Loss: 36.18939971923828, L1: 9.623584747314453, L3: 26.565813064575195\n",
      "Current prediction:  60.49238204956055 \n",
      "\n",
      "Iteration 8997, Loss: 35.988380432128906, L1: 9.682503700256348, L3: 26.305875778198242\n",
      "Current prediction:  60.573814392089844 \n",
      "\n",
      "Iteration 8998, Loss: 36.56624221801758, L1: 9.713760375976562, L3: 26.852481842041016\n",
      "Current prediction:  60.846832275390625 \n",
      "\n",
      "â†³ LR reduced to 5.0e-04 at iteration 9000 \n",
      "\n",
      "Iteration 8999, Loss: 35.8653564453125, L1: 9.677083969116211, L3: 26.188274383544922\n",
      "Current prediction:  61.109771728515625 \n",
      "\n",
      "Iteration 9000, Loss: 36.52212142944336, L1: 9.584142684936523, L3: 26.937978744506836\n",
      "Current prediction:  61.16093063354492 \n",
      "\n",
      "Iteration 9001, Loss: 34.92976760864258, L1: 9.579591751098633, L3: 25.350175857543945\n",
      "Current prediction:  61.08266067504883 \n",
      "\n",
      "Iteration 9002, Loss: 37.810997009277344, L1: 9.539898872375488, L3: 28.271099090576172\n",
      "Current prediction:  60.92131805419922 \n",
      "\n",
      "Iteration 9003, Loss: 35.65431213378906, L1: 9.578770637512207, L3: 26.07554054260254\n",
      "Current prediction:  60.89629364013672 \n",
      "\n",
      "Iteration 9004, Loss: 35.16536331176758, L1: 9.617751121520996, L3: 25.5476131439209\n",
      "Current prediction:  60.75054931640625 \n",
      "\n",
      "Iteration 9005, Loss: 35.09084701538086, L1: 9.510980606079102, L3: 25.579866409301758\n",
      "Current prediction:  61.05559539794922 \n",
      "\n",
      "Iteration 9006, Loss: 37.07973098754883, L1: 9.541924476623535, L3: 27.537805557250977\n",
      "Current prediction:  61.417442321777344 \n",
      "\n",
      "Iteration 9007, Loss: 36.20768737792969, L1: 9.536670684814453, L3: 26.6710147857666\n",
      "Current prediction:  61.579750061035156 \n",
      "\n",
      "Iteration 9008, Loss: 37.25225830078125, L1: 9.550442695617676, L3: 27.70181655883789\n",
      "Current prediction:  61.58475112915039 \n",
      "\n",
      "Iteration 9009, Loss: 35.402244567871094, L1: 9.496637344360352, L3: 25.90560531616211\n",
      "Current prediction:  61.387752532958984 \n",
      "\n",
      "Iteration 9010, Loss: 35.20976257324219, L1: 9.507111549377441, L3: 25.70265007019043\n",
      "Current prediction:  60.901695251464844 \n",
      "\n",
      "Iteration 9011, Loss: 37.08617401123047, L1: 9.427196502685547, L3: 27.65897560119629\n",
      "Current prediction:  60.609466552734375 \n",
      "\n",
      "Iteration 9012, Loss: 36.23247528076172, L1: 9.531095504760742, L3: 26.70138168334961\n",
      "Current prediction:  60.56974792480469 \n",
      "\n",
      "Iteration 9013, Loss: 35.84307861328125, L1: 9.657392501831055, L3: 26.185686111450195\n",
      "Current prediction:  60.72712326049805 \n",
      "\n",
      "Iteration 9014, Loss: 35.97181701660156, L1: 9.491405487060547, L3: 26.480409622192383\n",
      "Current prediction:  61.39802169799805 \n",
      "\n",
      "Iteration 9015, Loss: 35.14579772949219, L1: 9.512099266052246, L3: 25.633697509765625\n",
      "Current prediction:  61.597328186035156 \n",
      "\n",
      "Iteration 9016, Loss: 35.79507064819336, L1: 9.49744701385498, L3: 26.297622680664062\n",
      "Current prediction:  61.57563781738281 \n",
      "\n",
      "Iteration 9017, Loss: 36.720069885253906, L1: 9.515912055969238, L3: 27.20415687561035\n",
      "Current prediction:  61.537872314453125 \n",
      "\n",
      "Iteration 9018, Loss: 35.50004577636719, L1: 9.573594093322754, L3: 25.926450729370117\n",
      "Current prediction:  61.22065734863281 \n",
      "\n",
      "Iteration 9019, Loss: 35.74421310424805, L1: 9.515488624572754, L3: 26.22872543334961\n",
      "Current prediction:  60.58252716064453 \n",
      "\n",
      "Iteration 9020, Loss: 35.561485290527344, L1: 9.571184158325195, L3: 25.99030113220215\n",
      "Current prediction:  60.49642562866211 \n",
      "\n",
      "Iteration 9021, Loss: 35.220977783203125, L1: 9.625080108642578, L3: 25.59589958190918\n",
      "Current prediction:  60.57157897949219 \n",
      "\n",
      "Iteration 9022, Loss: 36.05138397216797, L1: 9.60873031616211, L3: 26.44265365600586\n",
      "Current prediction:  60.83621597290039 \n",
      "\n",
      "Iteration 9023, Loss: 36.63896560668945, L1: 9.510720252990723, L3: 27.128244400024414\n",
      "Current prediction:  61.5057258605957 \n",
      "\n",
      "Iteration 9024, Loss: 35.33811950683594, L1: 9.610336303710938, L3: 25.727785110473633\n",
      "Current prediction:  61.545196533203125 \n",
      "\n",
      "Iteration 9025, Loss: 35.35615921020508, L1: 9.619307518005371, L3: 25.73685073852539\n",
      "Current prediction:  61.43264389038086 \n",
      "\n",
      "Iteration 9026, Loss: 35.823421478271484, L1: 9.52941608428955, L3: 26.29400634765625\n",
      "Current prediction:  61.183040618896484 \n",
      "\n",
      "Iteration 9027, Loss: 35.473976135253906, L1: 9.551969528198242, L3: 25.92200469970703\n",
      "Current prediction:  60.77915954589844 \n",
      "\n",
      "Iteration 9028, Loss: 35.71643829345703, L1: 9.551775932312012, L3: 26.164661407470703\n",
      "Current prediction:  60.70564651489258 \n",
      "\n",
      "Iteration 9029, Loss: 34.858821868896484, L1: 9.480990409851074, L3: 25.377830505371094\n",
      "Current prediction:  60.726829528808594 \n",
      "\n",
      "Iteration 9030, Loss: 35.67104721069336, L1: 9.527976036071777, L3: 26.1430721282959\n",
      "Current prediction:  61.0794792175293 \n",
      "\n",
      "Iteration 9031, Loss: 35.6241569519043, L1: 9.491829872131348, L3: 26.132326126098633\n",
      "Current prediction:  61.674800872802734 \n",
      "\n",
      "Iteration 9032, Loss: 35.2541618347168, L1: 9.430150032043457, L3: 25.824010848999023\n",
      "Current prediction:  61.7532844543457 \n",
      "\n",
      "Iteration 9033, Loss: 36.36254119873047, L1: 9.536893844604492, L3: 26.825647354125977\n",
      "Current prediction:  61.7607536315918 \n",
      "\n",
      "Iteration 9034, Loss: 35.54341125488281, L1: 9.502196311950684, L3: 26.041213989257812\n",
      "Current prediction:  61.72137451171875 \n",
      "\n",
      "Iteration 9035, Loss: 35.31462097167969, L1: 9.46296215057373, L3: 25.85165786743164\n",
      "Current prediction:  61.294437408447266 \n",
      "\n",
      "Iteration 9036, Loss: 36.13761901855469, L1: 9.393203735351562, L3: 26.744415283203125\n",
      "Current prediction:  60.72548294067383 \n",
      "\n",
      "Iteration 9037, Loss: 36.59028625488281, L1: 9.526110649108887, L3: 27.06417465209961\n",
      "Current prediction:  60.60017776489258 \n",
      "\n",
      "Iteration 9038, Loss: 36.07948684692383, L1: 9.55187702178955, L3: 26.52760887145996\n",
      "Current prediction:  60.62693405151367 \n",
      "\n",
      "Iteration 9039, Loss: 35.503028869628906, L1: 9.573357582092285, L3: 25.929670333862305\n",
      "Current prediction:  61.121368408203125 \n",
      "\n",
      "Iteration 9040, Loss: 35.25748062133789, L1: 9.467462539672852, L3: 25.79001808166504\n",
      "Current prediction:  61.64913558959961 \n",
      "\n",
      "Iteration 9041, Loss: 36.43040466308594, L1: 9.606120109558105, L3: 26.824283599853516\n",
      "Current prediction:  61.66241455078125 \n",
      "\n",
      "Iteration 9042, Loss: 36.041378021240234, L1: 9.678311347961426, L3: 26.363065719604492\n",
      "Current prediction:  61.639244079589844 \n",
      "\n",
      "Iteration 9043, Loss: 36.913352966308594, L1: 9.627084732055664, L3: 27.28626823425293\n",
      "Current prediction:  61.597782135009766 \n",
      "\n",
      "Iteration 9044, Loss: 35.933563232421875, L1: 9.636096954345703, L3: 26.297468185424805\n",
      "Current prediction:  61.487247467041016 \n",
      "\n",
      "Iteration 9045, Loss: 34.97345733642578, L1: 9.615818977355957, L3: 25.35763931274414\n",
      "Current prediction:  60.810020446777344 \n",
      "\n",
      "Iteration 9046, Loss: 35.888450622558594, L1: 9.630265235900879, L3: 26.2581844329834\n",
      "Current prediction:  60.45649719238281 \n",
      "\n",
      "Iteration 9047, Loss: 36.432167053222656, L1: 9.682062149047852, L3: 26.750102996826172\n",
      "Current prediction:  60.461673736572266 \n",
      "\n",
      "Iteration 9048, Loss: 35.900028228759766, L1: 9.75069808959961, L3: 26.149330139160156\n",
      "Current prediction:  60.85869598388672 \n",
      "\n",
      "Iteration 9049, Loss: 35.68958282470703, L1: 9.607586860656738, L3: 26.081995010375977\n",
      "Current prediction:  61.508941650390625 \n",
      "\n",
      "Iteration 9050, Loss: 35.265830993652344, L1: 9.72365951538086, L3: 25.54216957092285\n",
      "Current prediction:  61.56501770019531 \n",
      "\n",
      "Iteration 9051, Loss: 36.047874450683594, L1: 9.663719177246094, L3: 26.384153366088867\n",
      "Current prediction:  61.59494400024414 \n",
      "\n",
      "Iteration 9052, Loss: 35.31260299682617, L1: 9.73083782196045, L3: 25.58176612854004\n",
      "Current prediction:  61.61576843261719 \n",
      "\n",
      "Iteration 9053, Loss: 34.43532943725586, L1: 9.705645561218262, L3: 24.72968292236328\n",
      "Current prediction:  61.632225036621094 \n",
      "\n",
      "Iteration 9054, Loss: 37.02245330810547, L1: 9.738898277282715, L3: 27.28355598449707\n",
      "Current prediction:  61.6398811340332 \n",
      "\n",
      "Iteration 9055, Loss: 36.66182327270508, L1: 9.727435111999512, L3: 26.93438720703125\n",
      "Current prediction:  61.62554931640625 \n",
      "\n",
      "Iteration 9056, Loss: 36.79083251953125, L1: 9.677927017211914, L3: 27.112905502319336\n",
      "Current prediction:  61.54475021362305 \n",
      "\n",
      "Iteration 9057, Loss: 36.07733917236328, L1: 9.589500427246094, L3: 26.48784065246582\n",
      "Current prediction:  60.628448486328125 \n",
      "\n",
      "Iteration 9058, Loss: 35.70262908935547, L1: 9.57735824584961, L3: 26.12527084350586\n",
      "Current prediction:  60.440673828125 \n",
      "\n",
      "Iteration 9059, Loss: 36.829959869384766, L1: 9.720291137695312, L3: 27.109668731689453\n",
      "Current prediction:  60.43012619018555 \n",
      "\n",
      "Iteration 9060, Loss: 36.0146484375, L1: 9.776191711425781, L3: 26.23845863342285\n",
      "Current prediction:  60.43940353393555 \n",
      "\n",
      "Iteration 9061, Loss: 35.083885192871094, L1: 9.750442504882812, L3: 25.333444595336914\n",
      "Current prediction:  60.45269012451172 \n",
      "\n",
      "Iteration 9062, Loss: 36.10962677001953, L1: 9.815738677978516, L3: 26.293888092041016\n",
      "Current prediction:  60.4906005859375 \n",
      "\n",
      "Iteration 9063, Loss: 35.785064697265625, L1: 9.609670639038086, L3: 26.17539405822754\n",
      "Current prediction:  60.85681915283203 \n",
      "\n",
      "Iteration 9064, Loss: 36.077091217041016, L1: 9.53085708618164, L3: 26.546234130859375\n",
      "Current prediction:  61.60902404785156 \n",
      "\n",
      "Iteration 9065, Loss: 35.89825439453125, L1: 9.496268272399902, L3: 26.401987075805664\n",
      "Current prediction:  61.66682815551758 \n",
      "\n",
      "Iteration 9066, Loss: 36.28594970703125, L1: 9.623332977294922, L3: 26.66261863708496\n",
      "Current prediction:  61.66515350341797 \n",
      "\n",
      "Iteration 9067, Loss: 36.3883171081543, L1: 9.575509071350098, L3: 26.812807083129883\n",
      "Current prediction:  61.655517578125 \n",
      "\n",
      "Iteration 9068, Loss: 35.665306091308594, L1: 9.629748344421387, L3: 26.035558700561523\n",
      "Current prediction:  61.63759994506836 \n",
      "\n",
      "Iteration 9069, Loss: 36.98488235473633, L1: 9.581755638122559, L3: 27.403127670288086\n",
      "Current prediction:  61.55925369262695 \n",
      "\n",
      "Iteration 9070, Loss: 36.796627044677734, L1: 9.581415176391602, L3: 27.215211868286133\n",
      "Current prediction:  61.18537521362305 \n",
      "\n",
      "Iteration 9071, Loss: 35.17929458618164, L1: 9.545201301574707, L3: 25.63409423828125\n",
      "Current prediction:  60.672119140625 \n",
      "\n",
      "Iteration 9072, Loss: 36.780147552490234, L1: 9.597367286682129, L3: 27.182781219482422\n",
      "Current prediction:  60.48598098754883 \n",
      "\n",
      "Iteration 9073, Loss: 36.52277374267578, L1: 9.629379272460938, L3: 26.893394470214844\n",
      "Current prediction:  60.49036407470703 \n",
      "\n",
      "Iteration 9074, Loss: 36.185176849365234, L1: 9.647208213806152, L3: 26.5379695892334\n",
      "Current prediction:  60.52477264404297 \n",
      "\n",
      "Iteration 9075, Loss: 36.357460021972656, L1: 9.609294891357422, L3: 26.748165130615234\n",
      "Current prediction:  60.73533630371094 \n",
      "\n",
      "Iteration 9076, Loss: 35.79777908325195, L1: 9.533390998840332, L3: 26.264387130737305\n",
      "Current prediction:  61.251407623291016 \n",
      "\n",
      "Iteration 9077, Loss: 36.197669982910156, L1: 9.462226867675781, L3: 26.735441207885742\n",
      "Current prediction:  61.23737335205078 \n",
      "\n",
      "Iteration 9078, Loss: 35.86946487426758, L1: 9.394789695739746, L3: 26.474674224853516\n",
      "Current prediction:  60.865543365478516 \n",
      "\n",
      "Iteration 9079, Loss: 35.710018157958984, L1: 9.445867538452148, L3: 26.264150619506836\n",
      "Current prediction:  60.916473388671875 \n",
      "\n",
      "Iteration 9080, Loss: 35.72500228881836, L1: 9.438192367553711, L3: 26.28680992126465\n",
      "Current prediction:  61.43111801147461 \n",
      "\n",
      "Iteration 9081, Loss: 35.75357437133789, L1: 9.434975624084473, L3: 26.3185977935791\n",
      "Current prediction:  61.34028244018555 \n",
      "\n",
      "Iteration 9082, Loss: 36.73440933227539, L1: 9.392176628112793, L3: 27.34223175048828\n",
      "Current prediction:  60.963321685791016 \n",
      "\n",
      "Iteration 9083, Loss: 35.42841720581055, L1: 9.530312538146973, L3: 25.898103713989258\n",
      "Current prediction:  60.692684173583984 \n",
      "\n",
      "Iteration 9084, Loss: 35.79815673828125, L1: 9.493868827819824, L3: 26.30428695678711\n",
      "Current prediction:  60.56949234008789 \n",
      "\n",
      "Iteration 9085, Loss: 35.211639404296875, L1: 9.620548248291016, L3: 25.591093063354492\n",
      "Current prediction:  60.57781982421875 \n",
      "\n",
      "Iteration 9086, Loss: 36.846466064453125, L1: 9.527833938598633, L3: 27.318634033203125\n",
      "Current prediction:  60.78020477294922 \n",
      "\n",
      "Iteration 9087, Loss: 37.30828094482422, L1: 9.492146492004395, L3: 27.816133499145508\n",
      "Current prediction:  61.335575103759766 \n",
      "\n",
      "Iteration 9088, Loss: 36.09868621826172, L1: 9.482385635375977, L3: 26.616300582885742\n",
      "Current prediction:  61.48117446899414 \n",
      "\n",
      "Iteration 9089, Loss: 36.34461212158203, L1: 9.575162887573242, L3: 26.76944923400879\n",
      "Current prediction:  61.42610168457031 \n",
      "\n",
      "Iteration 9090, Loss: 34.85993194580078, L1: 9.483039855957031, L3: 25.376890182495117\n",
      "Current prediction:  61.115684509277344 \n",
      "\n",
      "Iteration 9091, Loss: 36.36651611328125, L1: 9.49018669128418, L3: 26.876327514648438\n",
      "Current prediction:  60.854671478271484 \n",
      "\n",
      "Iteration 9092, Loss: 36.44464111328125, L1: 9.556324005126953, L3: 26.888315200805664\n",
      "Current prediction:  60.700199127197266 \n",
      "\n",
      "Iteration 9093, Loss: 35.305580139160156, L1: 9.577132225036621, L3: 25.72844886779785\n",
      "Current prediction:  60.74970626831055 \n",
      "\n",
      "Iteration 9094, Loss: 37.11907196044922, L1: 9.514198303222656, L3: 27.60487174987793\n",
      "Current prediction:  61.245948791503906 \n",
      "\n",
      "Iteration 9095, Loss: 35.848758697509766, L1: 9.530609130859375, L3: 26.31814956665039\n",
      "Current prediction:  61.50361251831055 \n",
      "\n",
      "Iteration 9096, Loss: 35.6923828125, L1: 9.546934127807617, L3: 26.145450592041016\n",
      "Current prediction:  61.54842758178711 \n",
      "\n",
      "Iteration 9097, Loss: 36.625343322753906, L1: 9.578668594360352, L3: 27.046672821044922\n",
      "Current prediction:  61.41034698486328 \n",
      "\n",
      "Iteration 9098, Loss: 35.66204833984375, L1: 9.590778350830078, L3: 26.071269989013672\n",
      "Current prediction:  60.82053756713867 \n",
      "\n",
      "Iteration 9099, Loss: 35.38811492919922, L1: 9.544787406921387, L3: 25.84332847595215\n",
      "Current prediction:  60.54705810546875 \n",
      "\n",
      "Iteration 9100, Loss: 35.55574035644531, L1: 9.58342456817627, L3: 25.972314834594727\n",
      "Current prediction:  60.514774322509766 \n",
      "\n",
      "Iteration 9101, Loss: 35.23439025878906, L1: 9.522229194641113, L3: 25.712160110473633\n",
      "Current prediction:  60.68075180053711 \n",
      "\n",
      "Iteration 9102, Loss: 35.71980285644531, L1: 9.462050437927246, L3: 26.257753372192383\n",
      "Current prediction:  61.141014099121094 \n",
      "\n",
      "Iteration 9103, Loss: 35.71690368652344, L1: 9.588190078735352, L3: 26.128713607788086\n",
      "Current prediction:  61.46494674682617 \n",
      "\n",
      "Iteration 9104, Loss: 36.61719512939453, L1: 9.404351234436035, L3: 27.21284294128418\n",
      "Current prediction:  61.483585357666016 \n",
      "\n",
      "Iteration 9105, Loss: 35.92481994628906, L1: 9.49679946899414, L3: 26.428022384643555\n",
      "Current prediction:  61.504478454589844 \n",
      "\n",
      "Iteration 9106, Loss: 35.11258316040039, L1: 9.476286888122559, L3: 25.636295318603516\n",
      "Current prediction:  61.34505081176758 \n",
      "\n",
      "Iteration 9107, Loss: 35.71063995361328, L1: 9.506922721862793, L3: 26.203718185424805\n",
      "Current prediction:  61.07213592529297 \n",
      "\n",
      "Iteration 9108, Loss: 34.77207946777344, L1: 9.554158210754395, L3: 25.21792221069336\n",
      "Current prediction:  61.15851974487305 \n",
      "\n",
      "Iteration 9109, Loss: 35.56852340698242, L1: 9.512707710266113, L3: 26.055816650390625\n",
      "Current prediction:  61.197330474853516 \n",
      "\n",
      "Iteration 9110, Loss: 36.2071533203125, L1: 9.564214706420898, L3: 26.642940521240234\n",
      "Current prediction:  61.22782897949219 \n",
      "\n",
      "Iteration 9111, Loss: 36.246788024902344, L1: 9.589405059814453, L3: 26.657381057739258\n",
      "Current prediction:  61.200706481933594 \n",
      "\n",
      "Iteration 9112, Loss: 36.007415771484375, L1: 9.52999210357666, L3: 26.47742462158203\n",
      "Current prediction:  61.267608642578125 \n",
      "\n",
      "Iteration 9113, Loss: 36.36652374267578, L1: 9.540275573730469, L3: 26.82624626159668\n",
      "Current prediction:  61.13219451904297 \n",
      "\n",
      "Iteration 9114, Loss: 35.78539276123047, L1: 9.525715827941895, L3: 26.25967788696289\n",
      "Current prediction:  60.95571517944336 \n",
      "\n",
      "Iteration 9115, Loss: 35.975643157958984, L1: 9.48381519317627, L3: 26.49182891845703\n",
      "Current prediction:  60.696598052978516 \n",
      "\n",
      "Iteration 9116, Loss: 34.574562072753906, L1: 9.545160293579102, L3: 25.029399871826172\n",
      "Current prediction:  60.53276824951172 \n",
      "\n",
      "Iteration 9117, Loss: 36.25443649291992, L1: 9.649384498596191, L3: 26.605052947998047\n",
      "Current prediction:  60.569828033447266 \n",
      "\n",
      "Iteration 9118, Loss: 35.75965881347656, L1: 9.560644149780273, L3: 26.199012756347656\n",
      "Current prediction:  60.63920593261719 \n",
      "\n",
      "Iteration 9119, Loss: 36.36223220825195, L1: 9.602299690246582, L3: 26.759931564331055\n",
      "Current prediction:  61.14484786987305 \n",
      "\n",
      "Iteration 9120, Loss: 35.830162048339844, L1: 9.487081527709961, L3: 26.343080520629883\n",
      "Current prediction:  61.44150161743164 \n",
      "\n",
      "Iteration 9121, Loss: 35.56262969970703, L1: 9.54326057434082, L3: 26.019367218017578\n",
      "Current prediction:  61.352088928222656 \n",
      "\n",
      "Iteration 9122, Loss: 36.29454040527344, L1: 9.54012680053711, L3: 26.75441551208496\n",
      "Current prediction:  60.943756103515625 \n",
      "\n",
      "Iteration 9123, Loss: 36.32038116455078, L1: 9.552268028259277, L3: 26.768112182617188\n",
      "Current prediction:  60.9298095703125 \n",
      "\n",
      "Iteration 9124, Loss: 36.09878921508789, L1: 9.510917663574219, L3: 26.587871551513672\n",
      "Current prediction:  60.9905891418457 \n",
      "\n",
      "Iteration 9125, Loss: 35.201141357421875, L1: 9.484070777893066, L3: 25.717069625854492\n",
      "Current prediction:  61.35835647583008 \n",
      "\n",
      "Iteration 9126, Loss: 36.56731033325195, L1: 9.45658016204834, L3: 27.11073112487793\n",
      "Current prediction:  61.20733642578125 \n",
      "\n",
      "Iteration 9127, Loss: 35.249298095703125, L1: 9.429248809814453, L3: 25.820049285888672\n",
      "Current prediction:  60.63051986694336 \n",
      "\n",
      "Iteration 9128, Loss: 35.41841125488281, L1: 9.647530555725098, L3: 25.7708797454834\n",
      "Current prediction:  60.559627532958984 \n",
      "\n",
      "Iteration 9129, Loss: 35.44605255126953, L1: 9.541879653930664, L3: 25.904170989990234\n",
      "Current prediction:  60.6306266784668 \n",
      "\n",
      "Iteration 9130, Loss: 35.95014190673828, L1: 9.552775382995605, L3: 26.39736557006836\n",
      "Current prediction:  61.23259353637695 \n",
      "\n",
      "Iteration 9131, Loss: 36.18821716308594, L1: 9.512335777282715, L3: 26.675880432128906\n",
      "Current prediction:  61.476104736328125 \n",
      "\n",
      "Iteration 9132, Loss: 35.479888916015625, L1: 9.527857780456543, L3: 25.9520320892334\n",
      "Current prediction:  61.526187896728516 \n",
      "\n",
      "Iteration 9133, Loss: 36.2081184387207, L1: 9.615999221801758, L3: 26.592119216918945\n",
      "Current prediction:  61.50777816772461 \n",
      "\n",
      "Iteration 9134, Loss: 35.709720611572266, L1: 9.569571495056152, L3: 26.140148162841797\n",
      "Current prediction:  61.25961685180664 \n",
      "\n",
      "Iteration 9135, Loss: 34.131629943847656, L1: 9.58645248413086, L3: 24.545177459716797\n",
      "Current prediction:  60.649505615234375 \n",
      "\n",
      "Iteration 9136, Loss: 35.44198226928711, L1: 9.629349708557129, L3: 25.812631607055664\n",
      "Current prediction:  60.94546127319336 \n",
      "\n",
      "Iteration 9137, Loss: 37.18619155883789, L1: 9.814923286437988, L3: 27.37126922607422\n",
      "Current prediction:  61.11129379272461 \n",
      "\n",
      "Iteration 9138, Loss: 35.57238006591797, L1: 9.724912643432617, L3: 25.84746742248535\n",
      "Current prediction:  61.1733512878418 \n",
      "\n",
      "Iteration 9139, Loss: 35.511924743652344, L1: 9.749686241149902, L3: 25.762239456176758\n",
      "Current prediction:  61.2955436706543 \n",
      "\n",
      "Iteration 9140, Loss: 35.47956848144531, L1: 9.687583923339844, L3: 25.7919864654541\n",
      "Current prediction:  61.16788101196289 \n",
      "\n",
      "Iteration 9141, Loss: 36.11626434326172, L1: 9.660717010498047, L3: 26.455547332763672\n",
      "Current prediction:  60.94444274902344 \n",
      "\n",
      "Iteration 9142, Loss: 36.53876876831055, L1: 9.614800453186035, L3: 26.923969268798828\n",
      "Current prediction:  60.627967834472656 \n",
      "\n",
      "Iteration 9143, Loss: 36.03385925292969, L1: 9.599308967590332, L3: 26.43454933166504\n",
      "Current prediction:  60.57404708862305 \n",
      "\n",
      "Iteration 9144, Loss: 36.64390563964844, L1: 9.490434646606445, L3: 27.153470993041992\n",
      "Current prediction:  60.571754455566406 \n",
      "\n",
      "Iteration 9145, Loss: 35.86646270751953, L1: 9.543182373046875, L3: 26.323280334472656\n",
      "Current prediction:  60.78937530517578 \n",
      "\n",
      "Iteration 9146, Loss: 35.407554626464844, L1: 9.36468505859375, L3: 26.042869567871094\n",
      "Current prediction:  61.44815444946289 \n",
      "\n",
      "Iteration 9147, Loss: 36.692665100097656, L1: 9.300592422485352, L3: 27.392074584960938\n",
      "Current prediction:  61.50218963623047 \n",
      "\n",
      "Iteration 9148, Loss: 36.16814041137695, L1: 9.325457572937012, L3: 26.842683792114258\n",
      "Current prediction:  61.766902923583984 \n",
      "\n",
      "Iteration 9149, Loss: 36.201358795166016, L1: 9.38038158416748, L3: 26.82097816467285\n",
      "Current prediction:  61.60176467895508 \n",
      "\n",
      "Iteration 9150, Loss: 35.615482330322266, L1: 9.392193794250488, L3: 26.22328758239746\n",
      "Current prediction:  61.7503662109375 \n",
      "\n",
      "Iteration 9151, Loss: 36.399818420410156, L1: 9.417781829833984, L3: 26.982036590576172\n",
      "Current prediction:  61.78419876098633 \n",
      "\n",
      "Iteration 9152, Loss: 36.124656677246094, L1: 9.523509979248047, L3: 26.601146697998047\n",
      "Current prediction:  61.68585205078125 \n",
      "\n",
      "Iteration 9153, Loss: 35.9012451171875, L1: 9.473302841186523, L3: 26.427940368652344\n",
      "Current prediction:  61.22815704345703 \n",
      "\n",
      "Iteration 9154, Loss: 35.934471130371094, L1: 9.495116233825684, L3: 26.439353942871094\n",
      "Current prediction:  60.72591781616211 \n",
      "\n",
      "Iteration 9155, Loss: 35.183712005615234, L1: 9.609452247619629, L3: 25.574260711669922\n",
      "Current prediction:  60.67772674560547 \n",
      "\n",
      "Iteration 9156, Loss: 35.36419677734375, L1: 9.63561725616455, L3: 25.728580474853516\n",
      "Current prediction:  61.146934509277344 \n",
      "\n",
      "Iteration 9157, Loss: 35.7527961730957, L1: 9.575777053833008, L3: 26.177019119262695\n",
      "Current prediction:  61.19614028930664 \n",
      "\n",
      "Iteration 9158, Loss: 36.06154251098633, L1: 9.605267524719238, L3: 26.456275939941406\n",
      "Current prediction:  60.92333984375 \n",
      "\n",
      "Iteration 9159, Loss: 35.81471252441406, L1: 9.557072639465332, L3: 26.257640838623047\n",
      "Current prediction:  60.921348571777344 \n",
      "\n",
      "Iteration 9160, Loss: 35.40869903564453, L1: 9.60760498046875, L3: 25.80109405517578\n",
      "Current prediction:  60.9619140625 \n",
      "\n",
      "Iteration 9161, Loss: 35.354705810546875, L1: 9.600484848022461, L3: 25.754220962524414\n",
      "Current prediction:  61.36628341674805 \n",
      "\n",
      "Iteration 9162, Loss: 36.07821273803711, L1: 9.514762878417969, L3: 26.56344985961914\n",
      "Current prediction:  61.38715744018555 \n",
      "\n",
      "Iteration 9163, Loss: 35.365699768066406, L1: 9.520934104919434, L3: 25.844764709472656\n",
      "Current prediction:  61.147613525390625 \n",
      "\n",
      "Iteration 9164, Loss: 36.06142807006836, L1: 9.536515235900879, L3: 26.524911880493164\n",
      "Current prediction:  61.02977752685547 \n",
      "\n",
      "Iteration 9165, Loss: 35.15312576293945, L1: 9.507177352905273, L3: 25.64594841003418\n",
      "Current prediction:  61.20942306518555 \n",
      "\n",
      "Iteration 9166, Loss: 36.127586364746094, L1: 9.455028533935547, L3: 26.672555923461914\n",
      "Current prediction:  61.4121208190918 \n",
      "\n",
      "Iteration 9167, Loss: 35.29914093017578, L1: 9.418293952941895, L3: 25.880847930908203\n",
      "Current prediction:  61.45029830932617 \n",
      "\n",
      "Iteration 9168, Loss: 35.05628967285156, L1: 9.432525634765625, L3: 25.623764038085938\n",
      "Current prediction:  61.32856369018555 \n",
      "\n",
      "Iteration 9169, Loss: 35.90209197998047, L1: 9.507396697998047, L3: 26.39469337463379\n",
      "Current prediction:  61.5003776550293 \n",
      "\n",
      "Iteration 9170, Loss: 35.210594177246094, L1: 9.455253601074219, L3: 25.755338668823242\n",
      "Current prediction:  61.7183723449707 \n",
      "\n",
      "Iteration 9171, Loss: 35.694828033447266, L1: 9.486800193786621, L3: 26.20802879333496\n",
      "Current prediction:  61.76659393310547 \n",
      "\n",
      "Iteration 9172, Loss: 35.3481330871582, L1: 9.461180686950684, L3: 25.886953353881836\n",
      "Current prediction:  61.74601745605469 \n",
      "\n",
      "Iteration 9173, Loss: 36.41625213623047, L1: 9.489264488220215, L3: 26.926986694335938\n",
      "Current prediction:  61.63399887084961 \n",
      "\n",
      "Iteration 9174, Loss: 36.275672912597656, L1: 9.471429824829102, L3: 26.804241180419922\n",
      "Current prediction:  61.35721206665039 \n",
      "\n",
      "Iteration 9175, Loss: 36.199432373046875, L1: 9.415184020996094, L3: 26.784250259399414\n",
      "Current prediction:  61.13008499145508 \n",
      "\n",
      "Iteration 9176, Loss: 36.2763557434082, L1: 9.528162956237793, L3: 26.748193740844727\n",
      "Current prediction:  61.151859283447266 \n",
      "\n",
      "Iteration 9177, Loss: 36.184043884277344, L1: 9.458148002624512, L3: 26.72589683532715\n",
      "Current prediction:  61.265289306640625 \n",
      "\n",
      "Iteration 9178, Loss: 35.441871643066406, L1: 9.451218605041504, L3: 25.99065399169922\n",
      "Current prediction:  61.2632942199707 \n",
      "\n",
      "Iteration 9179, Loss: 36.605010986328125, L1: 9.493488311767578, L3: 27.11152458190918\n",
      "Current prediction:  61.535675048828125 \n",
      "\n",
      "Iteration 9180, Loss: 36.30694580078125, L1: 9.442131042480469, L3: 26.86481475830078\n",
      "Current prediction:  61.6958122253418 \n",
      "\n",
      "Iteration 9181, Loss: 35.839256286621094, L1: 9.480500221252441, L3: 26.35875701904297\n",
      "Current prediction:  61.666255950927734 \n",
      "\n",
      "Iteration 9182, Loss: 35.400821685791016, L1: 9.491395950317383, L3: 25.909425735473633\n",
      "Current prediction:  61.53379821777344 \n",
      "\n",
      "Iteration 9183, Loss: 36.21415328979492, L1: 9.462021827697754, L3: 26.75213050842285\n",
      "Current prediction:  61.10066604614258 \n",
      "\n",
      "Iteration 9184, Loss: 37.1827392578125, L1: 9.45460033416748, L3: 27.728139877319336\n",
      "Current prediction:  60.82291030883789 \n",
      "\n",
      "Iteration 9185, Loss: 35.64624786376953, L1: 9.426958084106445, L3: 26.219289779663086\n",
      "Current prediction:  60.777103424072266 \n",
      "\n",
      "Iteration 9186, Loss: 35.25407028198242, L1: 9.397500038146973, L3: 25.856569290161133\n",
      "Current prediction:  60.95917510986328 \n",
      "\n",
      "Iteration 9187, Loss: 35.912872314453125, L1: 9.470345497131348, L3: 26.44252586364746\n",
      "Current prediction:  61.496212005615234 \n",
      "\n",
      "Iteration 9188, Loss: 36.31160354614258, L1: 9.416749000549316, L3: 26.894853591918945\n",
      "Current prediction:  61.770721435546875 \n",
      "\n",
      "Iteration 9189, Loss: 35.618690490722656, L1: 9.38663387298584, L3: 26.2320556640625\n",
      "Current prediction:  61.77287673950195 \n",
      "\n",
      "Iteration 9190, Loss: 36.69196701049805, L1: 9.45964527130127, L3: 27.232322692871094\n",
      "Current prediction:  61.704959869384766 \n",
      "\n",
      "Iteration 9191, Loss: 35.431846618652344, L1: 9.42387580871582, L3: 26.00796890258789\n",
      "Current prediction:  61.526695251464844 \n",
      "\n",
      "Iteration 9192, Loss: 36.20347595214844, L1: 9.416558265686035, L3: 26.78691864013672\n",
      "Current prediction:  61.17390823364258 \n",
      "\n",
      "Iteration 9193, Loss: 37.02097702026367, L1: 9.423678398132324, L3: 27.597299575805664\n",
      "Current prediction:  60.970062255859375 \n",
      "\n",
      "Iteration 9194, Loss: 35.833065032958984, L1: 9.459037780761719, L3: 26.374027252197266\n",
      "Current prediction:  60.85646438598633 \n",
      "\n",
      "Iteration 9195, Loss: 35.5213508605957, L1: 9.52873706817627, L3: 25.99261474609375\n",
      "Current prediction:  60.9038200378418 \n",
      "\n",
      "Iteration 9196, Loss: 35.82270050048828, L1: 9.494882583618164, L3: 26.327816009521484\n",
      "Current prediction:  60.74827575683594 \n",
      "\n",
      "Iteration 9197, Loss: 34.93028259277344, L1: 9.601295471191406, L3: 25.328989028930664\n",
      "Current prediction:  60.95364761352539 \n",
      "\n",
      "Iteration 9198, Loss: 35.7112922668457, L1: 9.527482986450195, L3: 26.183809280395508\n",
      "Current prediction:  61.40652847290039 \n",
      "\n",
      "Iteration 9199, Loss: 35.52537536621094, L1: 9.516439437866211, L3: 26.008934020996094\n",
      "Current prediction:  61.64945602416992 \n",
      "\n",
      "Iteration 9200, Loss: 35.64710998535156, L1: 9.612142562866211, L3: 26.03496742248535\n",
      "Current prediction:  61.64155578613281 \n",
      "\n",
      "Iteration 9201, Loss: 35.140594482421875, L1: 9.532095909118652, L3: 25.608497619628906\n",
      "Current prediction:  61.504180908203125 \n",
      "\n",
      "Iteration 9202, Loss: 35.650447845458984, L1: 9.595864295959473, L3: 26.054584503173828\n",
      "Current prediction:  61.12191390991211 \n",
      "\n",
      "Iteration 9203, Loss: 36.13859558105469, L1: 9.543695449829102, L3: 26.594900131225586\n",
      "Current prediction:  60.96432876586914 \n",
      "\n",
      "Iteration 9204, Loss: 36.42171859741211, L1: 9.542393684387207, L3: 26.87932586669922\n",
      "Current prediction:  60.884498596191406 \n",
      "\n",
      "Iteration 9205, Loss: 36.54121398925781, L1: 9.487003326416016, L3: 27.054208755493164\n",
      "Current prediction:  60.85651397705078 \n",
      "\n",
      "Iteration 9206, Loss: 34.731327056884766, L1: 9.4612455368042, L3: 25.27008056640625\n",
      "Current prediction:  61.149871826171875 \n",
      "\n",
      "Iteration 9207, Loss: 36.04651641845703, L1: 9.372686386108398, L3: 26.673831939697266\n",
      "Current prediction:  61.35287857055664 \n",
      "\n",
      "Iteration 9208, Loss: 36.461387634277344, L1: 9.307696342468262, L3: 27.153690338134766\n",
      "Current prediction:  61.46993637084961 \n",
      "\n",
      "Iteration 9209, Loss: 35.82826232910156, L1: 9.404973983764648, L3: 26.423288345336914\n",
      "Current prediction:  61.7700080871582 \n",
      "\n",
      "Iteration 9210, Loss: 35.07078170776367, L1: 9.245071411132812, L3: 25.82571029663086\n",
      "Current prediction:  61.926944732666016 \n",
      "\n",
      "Iteration 9211, Loss: 35.49816131591797, L1: 9.332878112792969, L3: 26.165285110473633\n",
      "Current prediction:  61.96615982055664 \n",
      "\n",
      "Iteration 9212, Loss: 35.06610107421875, L1: 9.402032852172852, L3: 25.664066314697266\n",
      "Current prediction:  61.84885025024414 \n",
      "\n",
      "Iteration 9213, Loss: 35.26402282714844, L1: 9.30916690826416, L3: 25.954856872558594\n",
      "Current prediction:  61.48469161987305 \n",
      "\n",
      "Iteration 9214, Loss: 36.786285400390625, L1: 9.337699890136719, L3: 27.44858741760254\n",
      "Current prediction:  60.97825622558594 \n",
      "\n",
      "Iteration 9215, Loss: 35.37125778198242, L1: 9.408919334411621, L3: 25.962337493896484\n",
      "Current prediction:  60.993064880371094 \n",
      "\n",
      "Iteration 9216, Loss: 35.257972717285156, L1: 9.390439987182617, L3: 25.867530822753906\n",
      "Current prediction:  60.92168045043945 \n",
      "\n",
      "Iteration 9217, Loss: 35.21548843383789, L1: 9.397854804992676, L3: 25.81763458251953\n",
      "Current prediction:  61.01183319091797 \n",
      "\n",
      "Iteration 9218, Loss: 35.83725357055664, L1: 9.406601905822754, L3: 26.430652618408203\n",
      "Current prediction:  61.25839614868164 \n",
      "\n",
      "Iteration 9219, Loss: 36.90484619140625, L1: 9.489383697509766, L3: 27.415462493896484\n",
      "Current prediction:  61.55039596557617 \n",
      "\n",
      "Iteration 9220, Loss: 35.959259033203125, L1: 9.525184631347656, L3: 26.434072494506836\n",
      "Current prediction:  61.15620803833008 \n",
      "\n",
      "Iteration 9221, Loss: 36.034332275390625, L1: 9.444866180419922, L3: 26.589468002319336\n",
      "Current prediction:  61.10776138305664 \n",
      "\n",
      "Iteration 9222, Loss: 35.62216567993164, L1: 9.555407524108887, L3: 26.066757202148438\n",
      "Current prediction:  61.194881439208984 \n",
      "\n",
      "Iteration 9223, Loss: 36.236690521240234, L1: 9.464674949645996, L3: 26.772016525268555\n",
      "Current prediction:  61.077945709228516 \n",
      "\n",
      "Iteration 9224, Loss: 36.31343078613281, L1: 9.485536575317383, L3: 26.827896118164062\n",
      "Current prediction:  60.92047882080078 \n",
      "\n",
      "Iteration 9225, Loss: 36.021793365478516, L1: 9.531493186950684, L3: 26.490299224853516\n",
      "Current prediction:  61.001686096191406 \n",
      "\n",
      "Iteration 9226, Loss: 34.92474365234375, L1: 9.482996940612793, L3: 25.441747665405273\n",
      "Current prediction:  61.09905242919922 \n",
      "\n",
      "Iteration 9227, Loss: 36.633853912353516, L1: 9.412984848022461, L3: 27.220869064331055\n",
      "Current prediction:  61.074607849121094 \n",
      "\n",
      "Iteration 9228, Loss: 35.22554016113281, L1: 9.408858299255371, L3: 25.816680908203125\n",
      "Current prediction:  60.70241165161133 \n",
      "\n",
      "Iteration 9229, Loss: 36.611812591552734, L1: 9.50841999053955, L3: 27.1033935546875\n",
      "Current prediction:  60.79112243652344 \n",
      "\n",
      "Iteration 9230, Loss: 35.0199089050293, L1: 9.420005798339844, L3: 25.599903106689453\n",
      "Current prediction:  61.353912353515625 \n",
      "\n",
      "Iteration 9231, Loss: 36.37928771972656, L1: 9.43991756439209, L3: 26.939369201660156\n",
      "Current prediction:  61.74340057373047 \n",
      "\n",
      "Iteration 9232, Loss: 35.90027618408203, L1: 9.418684005737305, L3: 26.481590270996094\n",
      "Current prediction:  61.784751892089844 \n",
      "\n",
      "Iteration 9233, Loss: 34.49855422973633, L1: 9.546950340270996, L3: 24.95160484313965\n",
      "Current prediction:  61.76160430908203 \n",
      "\n",
      "Iteration 9234, Loss: 35.85665512084961, L1: 9.439282417297363, L3: 26.41737174987793\n",
      "Current prediction:  61.36454772949219 \n",
      "\n",
      "Iteration 9235, Loss: 35.68553924560547, L1: 9.406939506530762, L3: 26.278600692749023\n",
      "Current prediction:  60.724552154541016 \n",
      "\n",
      "Iteration 9236, Loss: 35.87803649902344, L1: 9.51420783996582, L3: 26.36383056640625\n",
      "Current prediction:  60.61454391479492 \n",
      "\n",
      "Iteration 9237, Loss: 36.01604461669922, L1: 9.555583953857422, L3: 26.46046257019043\n",
      "Current prediction:  60.61075973510742 \n",
      "\n",
      "Iteration 9238, Loss: 35.01191711425781, L1: 9.63486385345459, L3: 25.377052307128906\n",
      "Current prediction:  60.64537048339844 \n",
      "\n",
      "Iteration 9239, Loss: 36.248016357421875, L1: 9.562034606933594, L3: 26.685983657836914\n",
      "Current prediction:  61.3221435546875 \n",
      "\n",
      "Iteration 9240, Loss: 35.30839538574219, L1: 9.485803604125977, L3: 25.822593688964844\n",
      "Current prediction:  61.80917739868164 \n",
      "\n",
      "Iteration 9241, Loss: 35.484466552734375, L1: 9.566266059875488, L3: 25.918201446533203\n",
      "Current prediction:  61.850791931152344 \n",
      "\n",
      "Iteration 9242, Loss: 35.892250061035156, L1: 9.621955871582031, L3: 26.270296096801758\n",
      "Current prediction:  61.8792724609375 \n",
      "\n",
      "Iteration 9243, Loss: 36.67683029174805, L1: 9.636435508728027, L3: 27.040393829345703\n",
      "Current prediction:  61.90449523925781 \n",
      "\n",
      "Iteration 9244, Loss: 35.70768737792969, L1: 9.629512786865234, L3: 26.078176498413086\n",
      "Current prediction:  61.908103942871094 \n",
      "\n",
      "Iteration 9245, Loss: 36.00142288208008, L1: 9.564784049987793, L3: 26.4366397857666\n",
      "Current prediction:  61.884464263916016 \n",
      "\n",
      "Iteration 9246, Loss: 35.24674987792969, L1: 9.493456840515137, L3: 25.753292083740234\n",
      "Current prediction:  61.42362976074219 \n",
      "\n",
      "Iteration 9247, Loss: 36.66123962402344, L1: 9.360228538513184, L3: 27.301010131835938\n",
      "Current prediction:  60.71083450317383 \n",
      "\n",
      "Iteration 9248, Loss: 35.04486846923828, L1: 9.464025497436523, L3: 25.580842971801758\n",
      "Current prediction:  60.69260025024414 \n",
      "\n",
      "Iteration 9249, Loss: 35.36375045776367, L1: 9.525394439697266, L3: 25.838356018066406\n",
      "Current prediction:  60.69276428222656 \n",
      "\n",
      "Iteration 9250, Loss: 35.75484848022461, L1: 9.56274700164795, L3: 26.192102432250977\n",
      "Current prediction:  61.07819366455078 \n",
      "\n",
      "Iteration 9251, Loss: 34.66736602783203, L1: 9.39430046081543, L3: 25.273067474365234\n",
      "Current prediction:  61.83536911010742 \n",
      "\n",
      "Iteration 9252, Loss: 35.98329544067383, L1: 9.45318603515625, L3: 26.530109405517578\n",
      "Current prediction:  61.8683967590332 \n",
      "\n",
      "Iteration 9253, Loss: 35.94010925292969, L1: 9.476996421813965, L3: 26.463111877441406\n",
      "Current prediction:  61.86477279663086 \n",
      "\n",
      "Iteration 9254, Loss: 36.27269744873047, L1: 9.534446716308594, L3: 26.738250732421875\n",
      "Current prediction:  61.851531982421875 \n",
      "\n",
      "Iteration 9255, Loss: 36.35509490966797, L1: 9.567955017089844, L3: 26.787137985229492\n",
      "Current prediction:  61.8195686340332 \n",
      "\n",
      "Iteration 9256, Loss: 36.01308822631836, L1: 9.490632057189941, L3: 26.522457122802734\n",
      "Current prediction:  61.743255615234375 \n",
      "\n",
      "Iteration 9257, Loss: 35.06672668457031, L1: 9.425312995910645, L3: 25.64141273498535\n",
      "Current prediction:  61.11189270019531 \n",
      "\n",
      "Iteration 9258, Loss: 35.48719024658203, L1: 9.397533416748047, L3: 26.089658737182617\n",
      "Current prediction:  60.58882522583008 \n",
      "\n",
      "Iteration 9259, Loss: 36.450714111328125, L1: 9.56988525390625, L3: 26.880828857421875\n",
      "Current prediction:  60.566383361816406 \n",
      "\n",
      "Iteration 9260, Loss: 35.788185119628906, L1: 9.697940826416016, L3: 26.090246200561523\n",
      "Current prediction:  60.57021713256836 \n",
      "\n",
      "Iteration 9261, Loss: 37.36470413208008, L1: 9.788208961486816, L3: 27.576496124267578\n",
      "Current prediction:  60.57389831542969 \n",
      "\n",
      "Iteration 9262, Loss: 36.34761047363281, L1: 9.754448890686035, L3: 26.593162536621094\n",
      "Current prediction:  60.5897102355957 \n",
      "\n",
      "Iteration 9263, Loss: 36.610565185546875, L1: 9.571791648864746, L3: 27.038774490356445\n",
      "Current prediction:  60.98688507080078 \n",
      "\n",
      "Iteration 9264, Loss: 36.92611312866211, L1: 9.418778419494629, L3: 27.507333755493164\n",
      "Current prediction:  61.78200149536133 \n",
      "\n",
      "Iteration 9265, Loss: 35.93618392944336, L1: 9.533534049987793, L3: 26.402650833129883\n",
      "Current prediction:  61.78202819824219 \n",
      "\n",
      "Iteration 9266, Loss: 37.0789680480957, L1: 9.612605094909668, L3: 27.46636199951172\n",
      "Current prediction:  61.75362014770508 \n",
      "\n",
      "Iteration 9267, Loss: 36.35177993774414, L1: 9.69856071472168, L3: 26.65321922302246\n",
      "Current prediction:  61.714054107666016 \n",
      "\n",
      "Iteration 9268, Loss: 36.64882278442383, L1: 9.742293357849121, L3: 26.90652847290039\n",
      "Current prediction:  61.66940689086914 \n",
      "\n",
      "Iteration 9269, Loss: 35.84960174560547, L1: 9.810691833496094, L3: 26.038911819458008\n",
      "Current prediction:  61.63286590576172 \n",
      "\n",
      "Iteration 9270, Loss: 35.56950378417969, L1: 9.828961372375488, L3: 25.740541458129883\n",
      "Current prediction:  61.604427337646484 \n",
      "\n",
      "Iteration 9271, Loss: 36.84107208251953, L1: 9.856945037841797, L3: 26.984128952026367\n",
      "Current prediction:  61.58620071411133 \n",
      "\n",
      "Iteration 9272, Loss: 35.3339958190918, L1: 9.772215843200684, L3: 25.561779022216797\n",
      "Current prediction:  61.55438232421875 \n",
      "\n",
      "Iteration 9273, Loss: 36.1425666809082, L1: 9.636672973632812, L3: 26.50589370727539\n",
      "Current prediction:  60.6251335144043 \n",
      "\n",
      "Iteration 9274, Loss: 35.383445739746094, L1: 9.490741729736328, L3: 25.892704010009766\n",
      "Current prediction:  60.431331634521484 \n",
      "\n",
      "Iteration 9275, Loss: 35.315513610839844, L1: 9.696619987487793, L3: 25.618892669677734\n",
      "Current prediction:  60.46046829223633 \n",
      "\n",
      "Iteration 9276, Loss: 36.69086456298828, L1: 9.73835563659668, L3: 26.95250701904297\n",
      "Current prediction:  60.5056266784668 \n",
      "\n",
      "Iteration 9277, Loss: 36.25135040283203, L1: 9.750396728515625, L3: 26.50095558166504\n",
      "Current prediction:  60.57151412963867 \n",
      "\n",
      "Iteration 9278, Loss: 36.5211067199707, L1: 9.568642616271973, L3: 26.952465057373047\n",
      "Current prediction:  60.88434600830078 \n",
      "\n",
      "Iteration 9279, Loss: 35.41435623168945, L1: 9.425187110900879, L3: 25.989168167114258\n",
      "Current prediction:  61.80384826660156 \n",
      "\n",
      "Iteration 9280, Loss: 36.0894775390625, L1: 9.398309707641602, L3: 26.6911678314209\n",
      "Current prediction:  61.846717834472656 \n",
      "\n",
      "Iteration 9281, Loss: 35.72726821899414, L1: 9.597010612487793, L3: 26.130258560180664\n",
      "Current prediction:  61.85087585449219 \n",
      "\n",
      "Iteration 9282, Loss: 35.86785125732422, L1: 9.582868576049805, L3: 26.28498077392578\n",
      "Current prediction:  61.83387756347656 \n",
      "\n",
      "Iteration 9283, Loss: 36.796913146972656, L1: 9.504180908203125, L3: 27.292734146118164\n",
      "Current prediction:  61.793357849121094 \n",
      "\n",
      "Iteration 9284, Loss: 34.796260833740234, L1: 9.540164947509766, L3: 25.25609588623047\n",
      "Current prediction:  61.72172164916992 \n",
      "\n",
      "Iteration 9285, Loss: 35.0976448059082, L1: 9.471026420593262, L3: 25.626619338989258\n",
      "Current prediction:  60.63096237182617 \n",
      "\n",
      "Iteration 9286, Loss: 35.19839859008789, L1: 9.482049942016602, L3: 25.71634864807129\n",
      "Current prediction:  60.486793518066406 \n",
      "\n",
      "Iteration 9287, Loss: 36.445556640625, L1: 9.787995338439941, L3: 26.657562255859375\n",
      "Current prediction:  60.460697174072266 \n",
      "\n",
      "Iteration 9288, Loss: 35.573429107666016, L1: 9.943758010864258, L3: 25.629671096801758\n",
      "Current prediction:  60.444515228271484 \n",
      "\n",
      "Iteration 9289, Loss: 36.11647033691406, L1: 10.035611152648926, L3: 26.08085823059082\n",
      "Current prediction:  60.46044158935547 \n",
      "\n",
      "Iteration 9290, Loss: 35.03790283203125, L1: 9.915019035339355, L3: 25.12288475036621\n",
      "Current prediction:  60.49039840698242 \n",
      "\n",
      "Iteration 9291, Loss: 37.14014434814453, L1: 9.850706100463867, L3: 27.289438247680664\n",
      "Current prediction:  60.54495620727539 \n",
      "\n",
      "Iteration 9292, Loss: 35.76321029663086, L1: 9.639692306518555, L3: 26.123517990112305\n",
      "Current prediction:  61.27372360229492 \n",
      "\n",
      "Iteration 9293, Loss: 35.495391845703125, L1: 9.487009048461914, L3: 26.008384704589844\n",
      "Current prediction:  61.39668273925781 \n",
      "\n",
      "Iteration 9294, Loss: 36.665496826171875, L1: 9.661178588867188, L3: 27.004318237304688\n",
      "Current prediction:  61.44111633300781 \n",
      "\n",
      "Iteration 9295, Loss: 37.30754470825195, L1: 9.633550643920898, L3: 27.673994064331055\n",
      "Current prediction:  61.470558166503906 \n",
      "\n",
      "Iteration 9296, Loss: 36.43399429321289, L1: 9.723309516906738, L3: 26.710683822631836\n",
      "Current prediction:  61.49745178222656 \n",
      "\n",
      "Iteration 9297, Loss: 36.35733413696289, L1: 9.687643051147461, L3: 26.66969108581543\n",
      "Current prediction:  61.511627197265625 \n",
      "\n",
      "Iteration 9298, Loss: 35.39881134033203, L1: 9.718711853027344, L3: 25.680097579956055\n",
      "Current prediction:  61.51652526855469 \n",
      "\n",
      "Iteration 9299, Loss: 36.677490234375, L1: 9.573566436767578, L3: 27.10392189025879\n",
      "Current prediction:  60.92995834350586 \n",
      "\n",
      "Iteration 9300, Loss: 36.22313690185547, L1: 9.456860542297363, L3: 26.766277313232422\n",
      "Current prediction:  60.3499755859375 \n",
      "\n",
      "Iteration 9301, Loss: 35.32372283935547, L1: 9.606658935546875, L3: 25.717063903808594\n",
      "Current prediction:  60.33869171142578 \n",
      "\n",
      "Iteration 9302, Loss: 36.391605377197266, L1: 9.741728782653809, L3: 26.649877548217773\n",
      "Current prediction:  60.34122085571289 \n",
      "\n",
      "Iteration 9303, Loss: 36.150779724121094, L1: 9.801496505737305, L3: 26.349281311035156\n",
      "Current prediction:  60.34965515136719 \n",
      "\n",
      "Iteration 9304, Loss: 36.69948196411133, L1: 9.699912071228027, L3: 26.999568939208984\n",
      "Current prediction:  60.371925354003906 \n",
      "\n",
      "Iteration 9305, Loss: 34.88125228881836, L1: 9.5186767578125, L3: 25.36257553100586\n",
      "Current prediction:  60.98421859741211 \n",
      "\n",
      "Iteration 9306, Loss: 35.252227783203125, L1: 9.345766067504883, L3: 25.90645980834961\n",
      "Current prediction:  61.559139251708984 \n",
      "\n",
      "Iteration 9307, Loss: 36.2811393737793, L1: 9.375706672668457, L3: 26.905433654785156\n",
      "Current prediction:  61.567195892333984 \n",
      "\n",
      "Iteration 9308, Loss: 36.346622467041016, L1: 9.497593879699707, L3: 26.849027633666992\n",
      "Current prediction:  61.55012512207031 \n",
      "\n",
      "Iteration 9309, Loss: 34.65544891357422, L1: 9.523601531982422, L3: 25.131847381591797\n",
      "Current prediction:  61.60008239746094 \n",
      "\n",
      "Iteration 9310, Loss: 35.76216506958008, L1: 9.515459060668945, L3: 26.246706008911133\n",
      "Current prediction:  61.56254959106445 \n",
      "\n",
      "Iteration 9311, Loss: 35.59941101074219, L1: 9.435308456420898, L3: 26.16410255432129\n",
      "Current prediction:  60.66167449951172 \n",
      "\n",
      "Iteration 9312, Loss: 35.40610885620117, L1: 9.514485359191895, L3: 25.89162254333496\n",
      "Current prediction:  60.62113952636719 \n",
      "\n",
      "Iteration 9313, Loss: 36.23276901245117, L1: 9.585589408874512, L3: 26.647178649902344\n",
      "Current prediction:  60.597869873046875 \n",
      "\n",
      "Iteration 9314, Loss: 35.704307556152344, L1: 9.675507545471191, L3: 26.028799057006836\n",
      "Current prediction:  60.59209060668945 \n",
      "\n",
      "Iteration 9315, Loss: 35.25507736206055, L1: 9.580976486206055, L3: 25.674100875854492\n",
      "Current prediction:  60.628761291503906 \n",
      "\n",
      "Iteration 9316, Loss: 36.59788513183594, L1: 9.508862495422363, L3: 27.089021682739258\n",
      "Current prediction:  60.8001708984375 \n",
      "\n",
      "Iteration 9317, Loss: 36.29031753540039, L1: 9.461410522460938, L3: 26.828907012939453\n",
      "Current prediction:  61.55032730102539 \n",
      "\n",
      "Iteration 9318, Loss: 35.29403305053711, L1: 9.402953147888184, L3: 25.891080856323242\n",
      "Current prediction:  61.76728439331055 \n",
      "\n",
      "Iteration 9319, Loss: 35.37614440917969, L1: 9.511129379272461, L3: 25.865013122558594\n",
      "Current prediction:  61.796409606933594 \n",
      "\n",
      "Iteration 9320, Loss: 34.800437927246094, L1: 9.508881568908691, L3: 25.291555404663086\n",
      "Current prediction:  61.82143783569336 \n",
      "\n",
      "Iteration 9321, Loss: 35.70466613769531, L1: 9.455057144165039, L3: 26.24960708618164\n",
      "Current prediction:  61.81696701049805 \n",
      "\n",
      "Iteration 9322, Loss: 35.523494720458984, L1: 9.465639114379883, L3: 26.0578556060791\n",
      "Current prediction:  61.782493591308594 \n",
      "\n",
      "Iteration 9323, Loss: 35.72698974609375, L1: 9.414901733398438, L3: 26.312088012695312\n",
      "Current prediction:  61.13783645629883 \n",
      "\n",
      "Iteration 9324, Loss: 35.46577453613281, L1: 9.348068237304688, L3: 26.117704391479492\n",
      "Current prediction:  60.634559631347656 \n",
      "\n",
      "Iteration 9325, Loss: 36.286170959472656, L1: 9.605167388916016, L3: 26.68100357055664\n",
      "Current prediction:  60.609867095947266 \n",
      "\n",
      "Iteration 9326, Loss: 36.0468864440918, L1: 9.630779266357422, L3: 26.416107177734375\n",
      "Current prediction:  60.60969543457031 \n",
      "\n",
      "Iteration 9327, Loss: 36.496341705322266, L1: 9.593579292297363, L3: 26.90276336669922\n",
      "Current prediction:  60.70660400390625 \n",
      "\n",
      "Iteration 9328, Loss: 34.91523361206055, L1: 9.40257453918457, L3: 25.512659072875977\n",
      "Current prediction:  61.40829849243164 \n",
      "\n",
      "Iteration 9329, Loss: 35.453495025634766, L1: 9.462281227111816, L3: 25.991214752197266\n",
      "Current prediction:  61.73822784423828 \n",
      "\n",
      "Iteration 9330, Loss: 36.27885055541992, L1: 9.534412384033203, L3: 26.74443817138672\n",
      "Current prediction:  61.75958251953125 \n",
      "\n",
      "Iteration 9331, Loss: 35.32252502441406, L1: 9.593229293823242, L3: 25.72929573059082\n",
      "Current prediction:  61.75908660888672 \n",
      "\n",
      "Iteration 9332, Loss: 37.22370147705078, L1: 9.598553657531738, L3: 27.625146865844727\n",
      "Current prediction:  61.76149368286133 \n",
      "\n",
      "Iteration 9333, Loss: 37.576316833496094, L1: 9.567584037780762, L3: 28.00873374938965\n",
      "Current prediction:  61.736385345458984 \n",
      "\n",
      "Iteration 9334, Loss: 35.99934387207031, L1: 9.49691104888916, L3: 26.50243377685547\n",
      "Current prediction:  61.54951477050781 \n",
      "\n",
      "Iteration 9335, Loss: 36.10101318359375, L1: 9.482803344726562, L3: 26.61821174621582\n",
      "Current prediction:  60.76498031616211 \n",
      "\n",
      "Iteration 9336, Loss: 35.38507080078125, L1: 9.481199264526367, L3: 25.90386962890625\n",
      "Current prediction:  60.5240478515625 \n",
      "\n",
      "Iteration 9337, Loss: 35.714073181152344, L1: 9.602855682373047, L3: 26.111217498779297\n",
      "Current prediction:  60.44468307495117 \n",
      "\n",
      "Iteration 9338, Loss: 35.03523635864258, L1: 9.619601249694824, L3: 25.415634155273438\n",
      "Current prediction:  60.538421630859375 \n",
      "\n",
      "Iteration 9339, Loss: 35.23028564453125, L1: 9.4107666015625, L3: 25.81951904296875\n",
      "Current prediction:  61.2313346862793 \n",
      "\n",
      "Iteration 9340, Loss: 35.701297760009766, L1: 9.446281433105469, L3: 26.255016326904297\n",
      "Current prediction:  61.58075714111328 \n",
      "\n",
      "Iteration 9341, Loss: 35.36555480957031, L1: 9.30557632446289, L3: 26.059978485107422\n",
      "Current prediction:  61.67985916137695 \n",
      "\n",
      "Iteration 9342, Loss: 36.33641052246094, L1: 9.428309440612793, L3: 26.908100128173828\n",
      "Current prediction:  61.38874816894531 \n",
      "\n",
      "Iteration 9343, Loss: 35.76158905029297, L1: 9.272774696350098, L3: 26.488813400268555\n",
      "Current prediction:  60.77482986450195 \n",
      "\n",
      "Iteration 9344, Loss: 36.17573547363281, L1: 9.433987617492676, L3: 26.74174690246582\n",
      "Current prediction:  60.729976654052734 \n",
      "\n",
      "Iteration 9345, Loss: 35.77311325073242, L1: 9.499031066894531, L3: 26.27408218383789\n",
      "Current prediction:  60.743656158447266 \n",
      "\n",
      "Iteration 9346, Loss: 35.86775207519531, L1: 9.40914249420166, L3: 26.458608627319336\n",
      "Current prediction:  61.14812469482422 \n",
      "\n",
      "Iteration 9347, Loss: 35.84943389892578, L1: 9.34292221069336, L3: 26.506511688232422\n",
      "Current prediction:  61.80992126464844 \n",
      "\n",
      "Iteration 9348, Loss: 35.823692321777344, L1: 9.314130783081055, L3: 26.509563446044922\n",
      "Current prediction:  61.77037811279297 \n",
      "\n",
      "Iteration 9349, Loss: 35.13705062866211, L1: 9.33158016204834, L3: 25.805469512939453\n",
      "Current prediction:  61.55344009399414 \n",
      "\n",
      "Iteration 9350, Loss: 36.28823471069336, L1: 9.33383846282959, L3: 26.954397201538086\n",
      "Current prediction:  61.14524459838867 \n",
      "\n",
      "Iteration 9351, Loss: 35.58543395996094, L1: 9.381641387939453, L3: 26.20379066467285\n",
      "Current prediction:  60.86959457397461 \n",
      "\n",
      "Iteration 9352, Loss: 35.614234924316406, L1: 9.35653018951416, L3: 26.25770378112793\n",
      "Current prediction:  60.83938980102539 \n",
      "\n",
      "Iteration 9353, Loss: 36.516170501708984, L1: 9.422755241394043, L3: 27.093416213989258\n",
      "Current prediction:  61.16887664794922 \n",
      "\n",
      "Iteration 9354, Loss: 36.21454620361328, L1: 9.406440734863281, L3: 26.80810546875\n",
      "Current prediction:  61.294219970703125 \n",
      "\n",
      "Iteration 9355, Loss: 35.44377899169922, L1: 9.483470916748047, L3: 25.96030616760254\n",
      "Current prediction:  61.458866119384766 \n",
      "\n",
      "Iteration 9356, Loss: 35.519744873046875, L1: 9.462080955505371, L3: 26.057662963867188\n",
      "Current prediction:  61.4920539855957 \n",
      "\n",
      "Iteration 9357, Loss: 35.8104362487793, L1: 9.490843772888184, L3: 26.31959342956543\n",
      "Current prediction:  61.221923828125 \n",
      "\n",
      "Iteration 9358, Loss: 36.21043395996094, L1: 9.482328414916992, L3: 26.728103637695312\n",
      "Current prediction:  60.85129928588867 \n",
      "\n",
      "Iteration 9359, Loss: 36.6170539855957, L1: 9.497217178344727, L3: 27.119836807250977\n",
      "Current prediction:  60.68484878540039 \n",
      "\n",
      "Iteration 9360, Loss: 35.78877258300781, L1: 9.510503768920898, L3: 26.278268814086914\n",
      "Current prediction:  60.833683013916016 \n",
      "\n",
      "Iteration 9361, Loss: 36.0634765625, L1: 9.468375205993652, L3: 26.595102310180664\n",
      "Current prediction:  61.51188659667969 \n",
      "\n",
      "Iteration 9362, Loss: 35.870811462402344, L1: 9.478571891784668, L3: 26.392240524291992\n",
      "Current prediction:  61.710819244384766 \n",
      "\n",
      "Iteration 9363, Loss: 35.483787536621094, L1: 9.460105895996094, L3: 26.023683547973633\n",
      "Current prediction:  61.740848541259766 \n",
      "\n",
      "Iteration 9364, Loss: 36.00370788574219, L1: 9.334399223327637, L3: 26.669309616088867\n",
      "Current prediction:  61.44562911987305 \n",
      "\n",
      "Iteration 9365, Loss: 36.007999420166016, L1: 9.433222770690918, L3: 26.57477569580078\n",
      "Current prediction:  60.76850128173828 \n",
      "\n",
      "Iteration 9366, Loss: 35.927589416503906, L1: 9.435240745544434, L3: 26.492347717285156\n",
      "Current prediction:  60.68191909790039 \n",
      "\n",
      "Iteration 9367, Loss: 35.257415771484375, L1: 9.478670120239258, L3: 25.778743743896484\n",
      "Current prediction:  60.687400817871094 \n",
      "\n",
      "Iteration 9368, Loss: 36.54246139526367, L1: 9.509322166442871, L3: 27.033140182495117\n",
      "Current prediction:  60.73041534423828 \n",
      "\n",
      "Iteration 9369, Loss: 36.25398254394531, L1: 9.469072341918945, L3: 26.784910202026367\n",
      "Current prediction:  61.12823486328125 \n",
      "\n",
      "Iteration 9370, Loss: 34.08366775512695, L1: 9.259591102600098, L3: 24.82407569885254\n",
      "Current prediction:  61.7148323059082 \n",
      "\n",
      "Iteration 9371, Loss: 36.38243103027344, L1: 9.313379287719727, L3: 27.069053649902344\n",
      "Current prediction:  61.78643798828125 \n",
      "\n",
      "Iteration 9372, Loss: 35.67849349975586, L1: 9.393150329589844, L3: 26.285343170166016\n",
      "Current prediction:  61.791709899902344 \n",
      "\n",
      "Iteration 9373, Loss: 35.297847747802734, L1: 9.378195762634277, L3: 25.91965103149414\n",
      "Current prediction:  61.6590461730957 \n",
      "\n",
      "Iteration 9374, Loss: 35.65031051635742, L1: 9.328348159790039, L3: 26.321962356567383\n",
      "Current prediction:  61.03676223754883 \n",
      "\n",
      "Iteration 9375, Loss: 35.51667022705078, L1: 9.339187622070312, L3: 26.17748260498047\n",
      "Current prediction:  60.81576919555664 \n",
      "\n",
      "Iteration 9376, Loss: 35.569801330566406, L1: 9.339557647705078, L3: 26.230243682861328\n",
      "Current prediction:  60.80079650878906 \n",
      "\n",
      "Iteration 9377, Loss: 35.07499694824219, L1: 9.410369873046875, L3: 25.664628982543945\n",
      "Current prediction:  61.08964538574219 \n",
      "\n",
      "Iteration 9378, Loss: 35.258338928222656, L1: 9.393235206604004, L3: 25.865102767944336\n",
      "Current prediction:  61.35308074951172 \n",
      "\n",
      "Iteration 9379, Loss: 35.13492202758789, L1: 9.36266040802002, L3: 25.772260665893555\n",
      "Current prediction:  61.48835372924805 \n",
      "\n",
      "Iteration 9380, Loss: 36.19757080078125, L1: 9.374340057373047, L3: 26.82322883605957\n",
      "Current prediction:  61.17707443237305 \n",
      "\n",
      "Iteration 9381, Loss: 35.321746826171875, L1: 9.369396209716797, L3: 25.95235252380371\n",
      "Current prediction:  60.98731231689453 \n",
      "\n",
      "Iteration 9382, Loss: 34.56938552856445, L1: 9.323065757751465, L3: 25.246320724487305\n",
      "Current prediction:  61.267269134521484 \n",
      "\n",
      "Iteration 9383, Loss: 35.10410690307617, L1: 9.27957820892334, L3: 25.824527740478516\n",
      "Current prediction:  61.471290588378906 \n",
      "\n",
      "Iteration 9384, Loss: 36.09228515625, L1: 9.354510307312012, L3: 26.737775802612305\n",
      "Current prediction:  61.64200210571289 \n",
      "\n",
      "Iteration 9385, Loss: 36.54304122924805, L1: 9.430532455444336, L3: 27.11250877380371\n",
      "Current prediction:  61.53055191040039 \n",
      "\n",
      "Iteration 9386, Loss: 36.05074691772461, L1: 9.357893943786621, L3: 26.692853927612305\n",
      "Current prediction:  61.279117584228516 \n",
      "\n",
      "Iteration 9387, Loss: 35.22282791137695, L1: 9.413668632507324, L3: 25.809160232543945\n",
      "Current prediction:  60.89895248413086 \n",
      "\n",
      "Iteration 9388, Loss: 35.804134368896484, L1: 9.471878051757812, L3: 26.332256317138672\n",
      "Current prediction:  60.940792083740234 \n",
      "\n",
      "Iteration 9389, Loss: 36.46335220336914, L1: 9.454310417175293, L3: 27.00904083251953\n",
      "Current prediction:  61.212013244628906 \n",
      "\n",
      "Iteration 9390, Loss: 35.84231948852539, L1: 9.431893348693848, L3: 26.410425186157227\n",
      "Current prediction:  61.41120910644531 \n",
      "\n",
      "Iteration 9391, Loss: 35.15758514404297, L1: 9.393452644348145, L3: 25.76413345336914\n",
      "Current prediction:  61.55971145629883 \n",
      "\n",
      "Iteration 9392, Loss: 34.800689697265625, L1: 9.459914207458496, L3: 25.340774536132812\n",
      "Current prediction:  61.458946228027344 \n",
      "\n",
      "Iteration 9393, Loss: 36.2391471862793, L1: 9.434403419494629, L3: 26.80474281311035\n",
      "Current prediction:  61.1123161315918 \n",
      "\n",
      "Iteration 9394, Loss: 37.2342529296875, L1: 9.486533164978027, L3: 27.747718811035156\n",
      "Current prediction:  60.87885284423828 \n",
      "\n",
      "Iteration 9395, Loss: 36.386962890625, L1: 9.481443405151367, L3: 26.905521392822266\n",
      "Current prediction:  60.742366790771484 \n",
      "\n",
      "Iteration 9396, Loss: 35.529998779296875, L1: 9.503653526306152, L3: 26.02634620666504\n",
      "Current prediction:  61.30646896362305 \n",
      "\n",
      "Iteration 9397, Loss: 34.489288330078125, L1: 9.481095314025879, L3: 25.008193969726562\n",
      "Current prediction:  61.38969039916992 \n",
      "\n",
      "Iteration 9398, Loss: 36.53700637817383, L1: 9.512825012207031, L3: 27.024181365966797\n",
      "Current prediction:  61.138946533203125 \n",
      "\n",
      "Iteration 9399, Loss: 36.3586540222168, L1: 9.482258796691895, L3: 26.87639617919922\n",
      "Current prediction:  61.04035949707031 \n",
      "\n",
      "Iteration 9400, Loss: 36.893089294433594, L1: 9.474225997924805, L3: 27.41886329650879\n",
      "Current prediction:  61.11330032348633 \n",
      "\n",
      "Iteration 9401, Loss: 35.724525451660156, L1: 9.570025444030762, L3: 26.154499053955078\n",
      "Current prediction:  61.365604400634766 \n",
      "\n",
      "Iteration 9402, Loss: 35.887603759765625, L1: 9.53315258026123, L3: 26.354450225830078\n",
      "Current prediction:  61.23172378540039 \n",
      "\n",
      "Iteration 9403, Loss: 35.60417556762695, L1: 9.483677864074707, L3: 26.120498657226562\n",
      "Current prediction:  61.10403060913086 \n",
      "\n",
      "Iteration 9404, Loss: 35.25096130371094, L1: 9.51279067993164, L3: 25.73817253112793\n",
      "Current prediction:  60.90663146972656 \n",
      "\n",
      "Iteration 9405, Loss: 35.27806854248047, L1: 9.592077255249023, L3: 25.685989379882812\n",
      "Current prediction:  61.05289077758789 \n",
      "\n",
      "Iteration 9406, Loss: 36.03803253173828, L1: 9.45403003692627, L3: 26.584003448486328\n",
      "Current prediction:  61.25450897216797 \n",
      "\n",
      "Iteration 9407, Loss: 36.3281135559082, L1: 9.428574562072754, L3: 26.899538040161133\n",
      "Current prediction:  60.98675537109375 \n",
      "\n",
      "Iteration 9408, Loss: 35.35912322998047, L1: 9.409425735473633, L3: 25.94969940185547\n",
      "Current prediction:  60.77745819091797 \n",
      "\n",
      "Iteration 9409, Loss: 35.47802734375, L1: 9.308740615844727, L3: 26.16928482055664\n",
      "Current prediction:  60.84661102294922 \n",
      "\n",
      "Iteration 9410, Loss: 36.56608200073242, L1: 9.331731796264648, L3: 27.234350204467773\n",
      "Current prediction:  60.92822265625 \n",
      "\n",
      "Iteration 9411, Loss: 34.671966552734375, L1: 9.332266807556152, L3: 25.33970069885254\n",
      "Current prediction:  61.43271255493164 \n",
      "\n",
      "Iteration 9412, Loss: 35.76002883911133, L1: 9.367324829101562, L3: 26.392704010009766\n",
      "Current prediction:  61.44660568237305 \n",
      "\n",
      "Iteration 9413, Loss: 35.31631851196289, L1: 9.313288688659668, L3: 26.003028869628906\n",
      "Current prediction:  61.27241134643555 \n",
      "\n",
      "Iteration 9414, Loss: 35.70471954345703, L1: 9.33913803100586, L3: 26.365581512451172\n",
      "Current prediction:  60.93746566772461 \n",
      "\n",
      "Iteration 9415, Loss: 35.353485107421875, L1: 9.343062400817871, L3: 26.010421752929688\n",
      "Current prediction:  60.83678436279297 \n",
      "\n",
      "Iteration 9416, Loss: 34.933780670166016, L1: 9.36368465423584, L3: 25.57009506225586\n",
      "Current prediction:  60.82903289794922 \n",
      "\n",
      "Iteration 9417, Loss: 35.63629150390625, L1: 9.375279426574707, L3: 26.26101303100586\n",
      "Current prediction:  61.131961822509766 \n",
      "\n",
      "Iteration 9418, Loss: 35.33281326293945, L1: 9.430438995361328, L3: 25.902374267578125\n",
      "Current prediction:  61.30540084838867 \n",
      "\n",
      "Iteration 9419, Loss: 35.82605743408203, L1: 9.323715209960938, L3: 26.50234031677246\n",
      "Current prediction:  61.25642013549805 \n",
      "\n",
      "Iteration 9420, Loss: 36.68358612060547, L1: 9.40097427368164, L3: 27.282609939575195\n",
      "Current prediction:  60.966041564941406 \n",
      "\n",
      "Iteration 9421, Loss: 35.88481140136719, L1: 9.37472915649414, L3: 26.51008415222168\n",
      "Current prediction:  60.837730407714844 \n",
      "\n",
      "Iteration 9422, Loss: 34.30561447143555, L1: 9.342901229858398, L3: 24.96271324157715\n",
      "Current prediction:  60.93011474609375 \n",
      "\n",
      "Iteration 9423, Loss: 35.27179718017578, L1: 9.414950370788574, L3: 25.85684585571289\n",
      "Current prediction:  60.982025146484375 \n",
      "\n",
      "Iteration 9424, Loss: 34.90617752075195, L1: 9.417855262756348, L3: 25.48832130432129\n",
      "Current prediction:  61.37112808227539 \n",
      "\n",
      "Iteration 9425, Loss: 35.14018249511719, L1: 9.455599784851074, L3: 25.68458366394043\n",
      "Current prediction:  61.609344482421875 \n",
      "\n",
      "Iteration 9426, Loss: 35.158321380615234, L1: 9.387154579162598, L3: 25.771167755126953\n",
      "Current prediction:  61.61037826538086 \n",
      "\n",
      "Iteration 9427, Loss: 35.672027587890625, L1: 9.371938705444336, L3: 26.300086975097656\n",
      "Current prediction:  61.53333282470703 \n",
      "\n",
      "Iteration 9428, Loss: 36.0761604309082, L1: 9.377568244934082, L3: 26.698593139648438\n",
      "Current prediction:  60.82080841064453 \n",
      "\n",
      "Iteration 9429, Loss: 35.39085388183594, L1: 9.457012176513672, L3: 25.933839797973633\n",
      "Current prediction:  60.51177978515625 \n",
      "\n",
      "Iteration 9430, Loss: 35.91042709350586, L1: 9.562029838562012, L3: 26.34839630126953\n",
      "Current prediction:  60.51154327392578 \n",
      "\n",
      "Iteration 9431, Loss: 35.77642822265625, L1: 9.528558731079102, L3: 26.24786949157715\n",
      "Current prediction:  60.643089294433594 \n",
      "\n",
      "Iteration 9432, Loss: 35.75627517700195, L1: 9.369101524353027, L3: 26.38717269897461\n",
      "Current prediction:  61.0960578918457 \n",
      "\n",
      "Iteration 9433, Loss: 35.112274169921875, L1: 9.34768295288086, L3: 25.76459312438965\n",
      "Current prediction:  61.43785095214844 \n",
      "\n",
      "Iteration 9434, Loss: 35.73883056640625, L1: 9.37537670135498, L3: 26.363452911376953\n",
      "Current prediction:  61.52528762817383 \n",
      "\n",
      "Iteration 9435, Loss: 35.95478057861328, L1: 9.439244270324707, L3: 26.51553726196289\n",
      "Current prediction:  61.523948669433594 \n",
      "\n",
      "Iteration 9436, Loss: 36.710899353027344, L1: 9.43472957611084, L3: 27.276168823242188\n",
      "Current prediction:  61.294189453125 \n",
      "\n",
      "Iteration 9437, Loss: 35.573123931884766, L1: 9.44012451171875, L3: 26.132999420166016\n",
      "Current prediction:  60.85024642944336 \n",
      "\n",
      "Iteration 9438, Loss: 35.080352783203125, L1: 9.338966369628906, L3: 25.741384506225586\n",
      "Current prediction:  60.798614501953125 \n",
      "\n",
      "Iteration 9439, Loss: 35.94353485107422, L1: 9.40132999420166, L3: 26.542203903198242\n",
      "Current prediction:  60.931278228759766 \n",
      "\n",
      "Iteration 9440, Loss: 34.92481231689453, L1: 9.347063064575195, L3: 25.577747344970703\n",
      "Current prediction:  61.225616455078125 \n",
      "\n",
      "Iteration 9441, Loss: 35.21112060546875, L1: 9.361394882202148, L3: 25.84972381591797\n",
      "Current prediction:  61.36069869995117 \n",
      "\n",
      "Iteration 9442, Loss: 34.805198669433594, L1: 9.377142906188965, L3: 25.428056716918945\n",
      "Current prediction:  61.139949798583984 \n",
      "\n",
      "Iteration 9443, Loss: 35.90752410888672, L1: 9.374008178710938, L3: 26.533517837524414\n",
      "Current prediction:  61.15977096557617 \n",
      "\n",
      "Iteration 9444, Loss: 35.07024383544922, L1: 9.414368629455566, L3: 25.655874252319336\n",
      "Current prediction:  61.00112533569336 \n",
      "\n",
      "Iteration 9445, Loss: 35.4480094909668, L1: 9.399761199951172, L3: 26.048248291015625\n",
      "Current prediction:  61.03789138793945 \n",
      "\n",
      "Iteration 9446, Loss: 35.16714859008789, L1: 9.3452730178833, L3: 25.821876525878906\n",
      "Current prediction:  61.079505920410156 \n",
      "\n",
      "Iteration 9447, Loss: 35.519412994384766, L1: 9.364789009094238, L3: 26.154624938964844\n",
      "Current prediction:  61.26893615722656 \n",
      "\n",
      "Iteration 9448, Loss: 35.79983139038086, L1: 9.33558177947998, L3: 26.464250564575195\n",
      "Current prediction:  61.047462463378906 \n",
      "\n",
      "Iteration 9449, Loss: 35.978050231933594, L1: 9.352596282958984, L3: 26.62545394897461\n",
      "Current prediction:  60.993595123291016 \n",
      "\n",
      "Iteration 9450, Loss: 34.84098434448242, L1: 9.351874351501465, L3: 25.489110946655273\n",
      "Current prediction:  61.32684326171875 \n",
      "\n",
      "Iteration 9451, Loss: 35.06229782104492, L1: 9.294662475585938, L3: 25.767635345458984\n",
      "Current prediction:  61.40908432006836 \n",
      "\n",
      "Iteration 9452, Loss: 36.84222412109375, L1: 9.285055160522461, L3: 27.55716896057129\n",
      "Current prediction:  61.458614349365234 \n",
      "\n",
      "Iteration 9453, Loss: 36.64189910888672, L1: 9.32947826385498, L3: 27.312421798706055\n",
      "Current prediction:  61.124366760253906 \n",
      "\n",
      "Iteration 9454, Loss: 35.77410125732422, L1: 9.332267761230469, L3: 26.44183349609375\n",
      "Current prediction:  60.62953567504883 \n",
      "\n",
      "Iteration 9455, Loss: 36.91838073730469, L1: 9.400935173034668, L3: 27.517446517944336\n",
      "Current prediction:  60.48490524291992 \n",
      "\n",
      "Iteration 9456, Loss: 35.611331939697266, L1: 9.511031150817871, L3: 26.10030174255371\n",
      "Current prediction:  60.54969024658203 \n",
      "\n",
      "Iteration 9457, Loss: 35.881126403808594, L1: 9.46157455444336, L3: 26.419553756713867\n",
      "Current prediction:  60.63561248779297 \n",
      "\n",
      "Iteration 9458, Loss: 35.920196533203125, L1: 9.43626594543457, L3: 26.483928680419922\n",
      "Current prediction:  61.19452667236328 \n",
      "\n",
      "Iteration 9459, Loss: 35.65025329589844, L1: 9.357637405395508, L3: 26.29261589050293\n",
      "Current prediction:  61.5520133972168 \n",
      "\n",
      "Iteration 9460, Loss: 36.00253677368164, L1: 9.44205379486084, L3: 26.560482025146484\n",
      "Current prediction:  61.571651458740234 \n",
      "\n",
      "Iteration 9461, Loss: 34.76786804199219, L1: 9.437605857849121, L3: 25.330263137817383\n",
      "Current prediction:  61.51921463012695 \n",
      "\n",
      "Iteration 9462, Loss: 36.04387283325195, L1: 9.403037071228027, L3: 26.640836715698242\n",
      "Current prediction:  61.141971588134766 \n",
      "\n",
      "Iteration 9463, Loss: 36.39537048339844, L1: 9.394868850708008, L3: 27.000503540039062\n",
      "Current prediction:  60.724143981933594 \n",
      "\n",
      "Iteration 9464, Loss: 36.27521514892578, L1: 9.412802696228027, L3: 26.86241340637207\n",
      "Current prediction:  60.48421096801758 \n",
      "\n",
      "Iteration 9465, Loss: 35.712249755859375, L1: 9.478477478027344, L3: 26.233774185180664\n",
      "Current prediction:  60.52161407470703 \n",
      "\n",
      "Iteration 9466, Loss: 36.55805587768555, L1: 9.46377182006836, L3: 27.094284057617188\n",
      "Current prediction:  60.780914306640625 \n",
      "\n",
      "Iteration 9467, Loss: 36.15904998779297, L1: 9.351489067077637, L3: 26.80756187438965\n",
      "Current prediction:  61.2364501953125 \n",
      "\n",
      "Iteration 9468, Loss: 35.21171569824219, L1: 9.333110809326172, L3: 25.87860679626465\n",
      "Current prediction:  61.60790252685547 \n",
      "\n",
      "Iteration 9469, Loss: 34.703678131103516, L1: 9.334647178649902, L3: 25.36903190612793\n",
      "Current prediction:  61.66472244262695 \n",
      "\n",
      "Iteration 9470, Loss: 35.42466735839844, L1: 9.313310623168945, L3: 26.111358642578125\n",
      "Current prediction:  61.39495849609375 \n",
      "\n",
      "Iteration 9471, Loss: 34.687713623046875, L1: 9.379131317138672, L3: 25.308584213256836\n",
      "Current prediction:  60.80475616455078 \n",
      "\n",
      "Iteration 9472, Loss: 35.57481384277344, L1: 9.260390281677246, L3: 26.314422607421875\n",
      "Current prediction:  60.631431579589844 \n",
      "\n",
      "Iteration 9473, Loss: 36.445899963378906, L1: 9.392945289611816, L3: 27.052955627441406\n",
      "Current prediction:  60.558738708496094 \n",
      "\n",
      "Iteration 9474, Loss: 36.7071418762207, L1: 9.372857093811035, L3: 27.33428382873535\n",
      "Current prediction:  60.5102653503418 \n",
      "\n",
      "Iteration 9475, Loss: 35.05345916748047, L1: 9.452829360961914, L3: 25.600627899169922\n",
      "Current prediction:  60.531978607177734 \n",
      "\n",
      "Iteration 9476, Loss: 35.71192932128906, L1: 9.466276168823242, L3: 26.24565315246582\n",
      "Current prediction:  61.158042907714844 \n",
      "\n",
      "Iteration 9477, Loss: 35.16167068481445, L1: 9.366501808166504, L3: 25.795167922973633\n",
      "Current prediction:  61.618202209472656 \n",
      "\n",
      "Iteration 9478, Loss: 36.43171310424805, L1: 9.60600757598877, L3: 26.825706481933594\n",
      "Current prediction:  61.61652755737305 \n",
      "\n",
      "Iteration 9479, Loss: 35.90138244628906, L1: 9.52857780456543, L3: 26.372802734375\n",
      "Current prediction:  61.59550857543945 \n",
      "\n",
      "Iteration 9480, Loss: 36.48122787475586, L1: 9.513334274291992, L3: 26.967893600463867\n",
      "Current prediction:  61.52082824707031 \n",
      "\n",
      "Iteration 9481, Loss: 36.31566619873047, L1: 9.570440292358398, L3: 26.74522590637207\n",
      "Current prediction:  60.73794937133789 \n",
      "\n",
      "Iteration 9482, Loss: 35.9647102355957, L1: 9.48830795288086, L3: 26.476402282714844\n",
      "Current prediction:  60.38819122314453 \n",
      "\n",
      "Iteration 9483, Loss: 36.35978698730469, L1: 9.504334449768066, L3: 26.855453491210938\n",
      "Current prediction:  60.35670852661133 \n",
      "\n",
      "Iteration 9484, Loss: 35.566734313964844, L1: 9.612958908081055, L3: 25.953777313232422\n",
      "Current prediction:  60.36789321899414 \n",
      "\n",
      "Iteration 9485, Loss: 35.65920639038086, L1: 9.598150253295898, L3: 26.06105613708496\n",
      "Current prediction:  60.42502975463867 \n",
      "\n",
      "Iteration 9486, Loss: 34.482765197753906, L1: 9.494718551635742, L3: 24.988046646118164\n",
      "Current prediction:  60.749183654785156 \n",
      "\n",
      "Iteration 9487, Loss: 35.94218826293945, L1: 9.436810493469238, L3: 26.50537872314453\n",
      "Current prediction:  61.61499786376953 \n",
      "\n",
      "Iteration 9488, Loss: 35.99381637573242, L1: 9.40689468383789, L3: 26.58692169189453\n",
      "Current prediction:  61.686431884765625 \n",
      "\n",
      "Iteration 9489, Loss: 35.795684814453125, L1: 9.51519775390625, L3: 26.280487060546875\n",
      "Current prediction:  61.703147888183594 \n",
      "\n",
      "Iteration 9490, Loss: 35.93458557128906, L1: 9.501017570495605, L3: 26.433568954467773\n",
      "Current prediction:  61.69308090209961 \n",
      "\n",
      "Iteration 9491, Loss: 36.41975402832031, L1: 9.338257789611816, L3: 27.081497192382812\n",
      "Current prediction:  61.194557189941406 \n",
      "\n",
      "Iteration 9492, Loss: 35.42448425292969, L1: 9.358768463134766, L3: 26.06571388244629\n",
      "Current prediction:  60.50408172607422 \n",
      "\n",
      "Iteration 9493, Loss: 35.222774505615234, L1: 9.553206443786621, L3: 25.66956901550293\n",
      "Current prediction:  60.490020751953125 \n",
      "\n",
      "Iteration 9494, Loss: 36.189178466796875, L1: 9.549861907958984, L3: 26.63931655883789\n",
      "Current prediction:  60.49237823486328 \n",
      "\n",
      "Iteration 9495, Loss: 36.22927474975586, L1: 9.652456283569336, L3: 26.576818466186523\n",
      "Current prediction:  60.507347106933594 \n",
      "\n",
      "Iteration 9496, Loss: 35.64042282104492, L1: 9.497200965881348, L3: 26.14322280883789\n",
      "Current prediction:  60.59270095825195 \n",
      "\n",
      "Iteration 9497, Loss: 36.29206466674805, L1: 9.43399715423584, L3: 26.858068466186523\n",
      "Current prediction:  61.63193893432617 \n",
      "\n",
      "Iteration 9498, Loss: 35.8818473815918, L1: 9.260281562805176, L3: 26.621564865112305\n",
      "Current prediction:  61.76435089111328 \n",
      "\n",
      "Iteration 9499, Loss: 35.46348571777344, L1: 9.402751922607422, L3: 26.060731887817383\n",
      "Current prediction:  61.76199722290039 \n",
      "\n",
      "Iteration 9500, Loss: 36.635093688964844, L1: 9.418927192687988, L3: 27.21616554260254\n",
      "Current prediction:  61.73393249511719 \n",
      "\n",
      "Iteration 9501, Loss: 35.911712646484375, L1: 9.346061706542969, L3: 26.565650939941406\n",
      "Current prediction:  61.595054626464844 \n",
      "\n",
      "Iteration 9502, Loss: 35.21723175048828, L1: 9.258667945861816, L3: 25.95856475830078\n",
      "Current prediction:  60.58668518066406 \n",
      "\n",
      "Iteration 9503, Loss: 36.01818084716797, L1: 9.4429292678833, L3: 26.57525062561035\n",
      "Current prediction:  60.45280838012695 \n",
      "\n",
      "Iteration 9504, Loss: 34.76243591308594, L1: 9.598397254943848, L3: 25.164039611816406\n",
      "Current prediction:  60.43220901489258 \n",
      "\n",
      "Iteration 9505, Loss: 36.17361068725586, L1: 9.597917556762695, L3: 26.575693130493164\n",
      "Current prediction:  60.45122146606445 \n",
      "\n",
      "Iteration 9506, Loss: 35.34446334838867, L1: 9.447236061096191, L3: 25.897226333618164\n",
      "Current prediction:  61.168296813964844 \n",
      "\n",
      "Iteration 9507, Loss: 35.799442291259766, L1: 9.36557674407959, L3: 26.433866500854492\n",
      "Current prediction:  61.6206169128418 \n",
      "\n",
      "Iteration 9508, Loss: 35.54517364501953, L1: 9.555231094360352, L3: 25.989944458007812\n",
      "Current prediction:  61.63024139404297 \n",
      "\n",
      "Iteration 9509, Loss: 34.475921630859375, L1: 9.555817604064941, L3: 24.920103073120117\n",
      "Current prediction:  61.63404846191406 \n",
      "\n",
      "Iteration 9510, Loss: 35.939369201660156, L1: 9.634589195251465, L3: 26.304780960083008\n",
      "Current prediction:  61.62088394165039 \n",
      "\n",
      "Iteration 9511, Loss: 35.664817810058594, L1: 9.481165885925293, L3: 26.183652877807617\n",
      "Current prediction:  61.4012565612793 \n",
      "\n",
      "Iteration 9512, Loss: 34.88135528564453, L1: 9.402251243591309, L3: 25.47910499572754\n",
      "Current prediction:  60.67448043823242 \n",
      "\n",
      "Iteration 9513, Loss: 35.85777282714844, L1: 9.423173904418945, L3: 26.43459701538086\n",
      "Current prediction:  60.41844177246094 \n",
      "\n",
      "Iteration 9514, Loss: 36.16556930541992, L1: 9.55412769317627, L3: 26.611440658569336\n",
      "Current prediction:  60.40874481201172 \n",
      "\n",
      "Iteration 9515, Loss: 35.189056396484375, L1: 9.536989212036133, L3: 25.652067184448242\n",
      "Current prediction:  60.599300384521484 \n",
      "\n",
      "Iteration 9516, Loss: 35.232215881347656, L1: 9.38579273223877, L3: 25.846424102783203\n",
      "Current prediction:  61.42219161987305 \n",
      "\n",
      "Iteration 9517, Loss: 35.26408386230469, L1: 9.438459396362305, L3: 25.82562255859375\n",
      "Current prediction:  61.562828063964844 \n",
      "\n",
      "Iteration 9518, Loss: 34.65931701660156, L1: 9.421615600585938, L3: 25.237703323364258\n",
      "Current prediction:  61.58447265625 \n",
      "\n",
      "Iteration 9519, Loss: 36.3453369140625, L1: 9.397652626037598, L3: 26.94768524169922\n",
      "Current prediction:  61.51692581176758 \n",
      "\n",
      "Iteration 9520, Loss: 35.59453582763672, L1: 9.330156326293945, L3: 26.264381408691406\n",
      "Current prediction:  61.142799377441406 \n",
      "\n",
      "Iteration 9521, Loss: 35.734710693359375, L1: 9.393486022949219, L3: 26.341224670410156\n",
      "Current prediction:  60.59259796142578 \n",
      "\n",
      "Iteration 9522, Loss: 36.06723403930664, L1: 9.350172996520996, L3: 26.71706199645996\n",
      "Current prediction:  60.529541015625 \n",
      "\n",
      "Iteration 9523, Loss: 36.0643424987793, L1: 9.40438175201416, L3: 26.659961700439453\n",
      "Current prediction:  60.63007354736328 \n",
      "\n",
      "Iteration 9524, Loss: 34.987091064453125, L1: 9.343196868896484, L3: 25.643896102905273\n",
      "Current prediction:  61.149837493896484 \n",
      "\n",
      "Iteration 9525, Loss: 36.45729446411133, L1: 9.284807205200195, L3: 27.172487258911133\n",
      "Current prediction:  61.47490310668945 \n",
      "\n",
      "Iteration 9526, Loss: 36.35042953491211, L1: 9.320291519165039, L3: 27.03013801574707\n",
      "Current prediction:  61.37164306640625 \n",
      "\n",
      "Iteration 9527, Loss: 36.395362854003906, L1: 9.37962532043457, L3: 27.015737533569336\n",
      "Current prediction:  61.00999069213867 \n",
      "\n",
      "Iteration 9528, Loss: 35.74578094482422, L1: 9.377059936523438, L3: 26.36871910095215\n",
      "Current prediction:  60.65558624267578 \n",
      "\n",
      "Iteration 9529, Loss: 35.94330596923828, L1: 9.353567123413086, L3: 26.589736938476562\n",
      "Current prediction:  60.65833282470703 \n",
      "\n",
      "Iteration 9530, Loss: 35.91109085083008, L1: 9.387314796447754, L3: 26.523775100708008\n",
      "Current prediction:  61.07776641845703 \n",
      "\n",
      "Iteration 9531, Loss: 35.487937927246094, L1: 9.354095458984375, L3: 26.13384437561035\n",
      "Current prediction:  61.5433235168457 \n",
      "\n",
      "Iteration 9532, Loss: 37.903297424316406, L1: 11.398473739624023, L3: 26.504825592041016\n",
      "Current prediction:  61.565895080566406 \n",
      "\n",
      "Iteration 9533, Loss: 36.243770599365234, L1: 9.405426979064941, L3: 26.83834457397461\n",
      "Current prediction:  61.538509368896484 \n",
      "\n",
      "Iteration 9534, Loss: 36.746971130371094, L1: 9.480535507202148, L3: 27.266433715820312\n",
      "Current prediction:  61.40651321411133 \n",
      "\n",
      "Iteration 9535, Loss: 35.99820327758789, L1: 9.466803550720215, L3: 26.531400680541992\n",
      "Current prediction:  60.81856155395508 \n",
      "\n",
      "Iteration 9536, Loss: 34.75856018066406, L1: 9.450538635253906, L3: 25.308019638061523\n",
      "Current prediction:  60.51002883911133 \n",
      "\n",
      "Iteration 9537, Loss: 35.5465087890625, L1: 9.441164016723633, L3: 26.1053466796875\n",
      "Current prediction:  60.429168701171875 \n",
      "\n",
      "Iteration 9538, Loss: 35.829978942871094, L1: 9.479131698608398, L3: 26.350847244262695\n",
      "Current prediction:  60.587646484375 \n",
      "\n",
      "Iteration 9539, Loss: 36.13693618774414, L1: 9.501147270202637, L3: 26.635787963867188\n",
      "Current prediction:  61.022457122802734 \n",
      "\n",
      "Iteration 9540, Loss: 35.412925720214844, L1: 9.424934387207031, L3: 25.987991333007812\n",
      "Current prediction:  61.508460998535156 \n",
      "\n",
      "Iteration 9541, Loss: 35.24396514892578, L1: 9.396106719970703, L3: 25.847858428955078\n",
      "Current prediction:  61.61188888549805 \n",
      "\n",
      "Iteration 9542, Loss: 35.88724136352539, L1: 9.436782836914062, L3: 26.450458526611328\n",
      "Current prediction:  61.582275390625 \n",
      "\n",
      "Iteration 9543, Loss: 35.69178009033203, L1: 9.312861442565918, L3: 26.37891960144043\n",
      "Current prediction:  61.06130599975586 \n",
      "\n",
      "Iteration 9544, Loss: 35.52853775024414, L1: 9.32237434387207, L3: 26.20616340637207\n",
      "Current prediction:  60.79255294799805 \n",
      "\n",
      "Iteration 9545, Loss: 35.8179931640625, L1: 9.375633239746094, L3: 26.44236183166504\n",
      "Current prediction:  60.5693473815918 \n",
      "\n",
      "Iteration 9546, Loss: 35.434791564941406, L1: 9.333736419677734, L3: 26.10105323791504\n",
      "Current prediction:  60.831607818603516 \n",
      "\n",
      "Iteration 9547, Loss: 36.09025573730469, L1: 9.300896644592285, L3: 26.78936004638672\n",
      "Current prediction:  61.532291412353516 \n",
      "\n",
      "Iteration 9548, Loss: 35.78961181640625, L1: 9.422003746032715, L3: 26.36760902404785\n",
      "Current prediction:  61.655601501464844 \n",
      "\n",
      "Iteration 9549, Loss: 35.1312255859375, L1: 9.400803565979004, L3: 25.730422973632812\n",
      "Current prediction:  61.653404235839844 \n",
      "\n",
      "Iteration 9550, Loss: 35.92171859741211, L1: 9.445769309997559, L3: 26.475950241088867\n",
      "Current prediction:  61.55452346801758 \n",
      "\n",
      "Iteration 9551, Loss: 35.49256896972656, L1: 9.410868644714355, L3: 26.08169937133789\n",
      "Current prediction:  61.0015754699707 \n",
      "\n",
      "Iteration 9552, Loss: 35.198883056640625, L1: 9.358297348022461, L3: 25.840587615966797\n",
      "Current prediction:  60.51097869873047 \n",
      "\n",
      "Iteration 9553, Loss: 35.75767517089844, L1: 9.392986297607422, L3: 26.36469078063965\n",
      "Current prediction:  60.42013931274414 \n",
      "\n",
      "Iteration 9554, Loss: 36.20801544189453, L1: 9.531813621520996, L3: 26.67620086669922\n",
      "Current prediction:  60.40037536621094 \n",
      "\n",
      "Iteration 9555, Loss: 35.357322692871094, L1: 9.602649688720703, L3: 25.754671096801758\n",
      "Current prediction:  60.419132232666016 \n",
      "\n",
      "Iteration 9556, Loss: 35.12652587890625, L1: 9.539478302001953, L3: 25.58704948425293\n",
      "Current prediction:  60.9683952331543 \n",
      "\n",
      "Iteration 9557, Loss: 35.808048248291016, L1: 9.436762809753418, L3: 26.37128448486328\n",
      "Current prediction:  61.597145080566406 \n",
      "\n",
      "Iteration 9558, Loss: 36.2248420715332, L1: 9.453509330749512, L3: 26.771331787109375\n",
      "Current prediction:  61.628780364990234 \n",
      "\n",
      "Iteration 9559, Loss: 35.91091537475586, L1: 9.541309356689453, L3: 26.369606018066406\n",
      "Current prediction:  61.62165069580078 \n",
      "\n",
      "Iteration 9560, Loss: 35.83620071411133, L1: 9.572294235229492, L3: 26.263906478881836\n",
      "Current prediction:  61.60860061645508 \n",
      "\n",
      "Iteration 9561, Loss: 35.320884704589844, L1: 9.51051139831543, L3: 25.81037139892578\n",
      "Current prediction:  61.59104537963867 \n",
      "\n",
      "Iteration 9562, Loss: 36.400943756103516, L1: 9.459330558776855, L3: 26.941612243652344\n",
      "Current prediction:  61.42552947998047 \n",
      "\n",
      "Iteration 9563, Loss: 35.47309494018555, L1: 9.447803497314453, L3: 26.025291442871094\n",
      "Current prediction:  60.56729507446289 \n",
      "\n",
      "Iteration 9564, Loss: 35.73350524902344, L1: 9.417984962463379, L3: 26.315519332885742\n",
      "Current prediction:  60.450557708740234 \n",
      "\n",
      "Iteration 9565, Loss: 35.78230285644531, L1: 9.514697074890137, L3: 26.267606735229492\n",
      "Current prediction:  60.458927154541016 \n",
      "\n",
      "Iteration 9566, Loss: 36.21796417236328, L1: 9.476086616516113, L3: 26.741878509521484\n",
      "Current prediction:  60.7493896484375 \n",
      "\n",
      "Iteration 9567, Loss: 34.793907165527344, L1: 9.468866348266602, L3: 25.325040817260742\n",
      "Current prediction:  61.31785202026367 \n",
      "\n",
      "Iteration 9568, Loss: 35.54328155517578, L1: 9.37420654296875, L3: 26.1690731048584\n",
      "Current prediction:  61.587825775146484 \n",
      "\n",
      "Iteration 9569, Loss: 35.816551208496094, L1: 9.41901969909668, L3: 26.397531509399414\n",
      "Current prediction:  61.57221984863281 \n",
      "\n",
      "Iteration 9570, Loss: 36.84136199951172, L1: 9.384729385375977, L3: 27.45663070678711\n",
      "Current prediction:  61.36700439453125 \n",
      "\n",
      "Iteration 9571, Loss: 35.59150314331055, L1: 9.301173210144043, L3: 26.29033088684082\n",
      "Current prediction:  60.90391159057617 \n",
      "\n",
      "Iteration 9572, Loss: 36.208003997802734, L1: 9.331345558166504, L3: 26.876659393310547\n",
      "Current prediction:  60.77550506591797 \n",
      "\n",
      "Iteration 9573, Loss: 35.89621353149414, L1: 9.305977821350098, L3: 26.59023666381836\n",
      "Current prediction:  61.33735275268555 \n",
      "\n",
      "Iteration 9574, Loss: 36.0860710144043, L1: 9.333308219909668, L3: 26.752761840820312\n",
      "Current prediction:  61.43282699584961 \n",
      "\n",
      "Iteration 9575, Loss: 35.510257720947266, L1: 9.286808967590332, L3: 26.223447799682617\n",
      "Current prediction:  61.4908332824707 \n",
      "\n",
      "Iteration 9576, Loss: 35.63433837890625, L1: 9.238866806030273, L3: 26.395471572875977\n",
      "Current prediction:  61.32054901123047 \n",
      "\n",
      "Iteration 9577, Loss: 35.571815490722656, L1: 9.277384757995605, L3: 26.294431686401367\n",
      "Current prediction:  61.027130126953125 \n",
      "\n",
      "Iteration 9578, Loss: 34.417335510253906, L1: 9.355005264282227, L3: 25.062332153320312\n",
      "Current prediction:  60.85568618774414 \n",
      "\n",
      "Iteration 9579, Loss: 34.89078140258789, L1: 9.331270217895508, L3: 25.559511184692383\n",
      "Current prediction:  60.66382598876953 \n",
      "\n",
      "Iteration 9580, Loss: 35.295616149902344, L1: 9.356925010681152, L3: 25.938692092895508\n",
      "Current prediction:  60.68094253540039 \n",
      "\n",
      "Iteration 9581, Loss: 34.873287200927734, L1: 9.35285758972168, L3: 25.520429611206055\n",
      "Current prediction:  61.16926574707031 \n",
      "\n",
      "Iteration 9582, Loss: 35.35807418823242, L1: 9.291681289672852, L3: 26.06639289855957\n",
      "Current prediction:  61.629493713378906 \n",
      "\n",
      "Iteration 9583, Loss: 36.39438247680664, L1: 9.305768013000488, L3: 27.08861541748047\n",
      "Current prediction:  61.65715789794922 \n",
      "\n",
      "Iteration 9584, Loss: 36.23167419433594, L1: 9.354938507080078, L3: 26.876733779907227\n",
      "Current prediction:  61.55303955078125 \n",
      "\n",
      "Iteration 9585, Loss: 36.09617614746094, L1: 9.365045547485352, L3: 26.731130599975586\n",
      "Current prediction:  61.02644348144531 \n",
      "\n",
      "Iteration 9586, Loss: 35.18894958496094, L1: 9.391500473022461, L3: 25.79745101928711\n",
      "Current prediction:  60.57346725463867 \n",
      "\n",
      "Iteration 9587, Loss: 35.24154281616211, L1: 9.438603401184082, L3: 25.802940368652344\n",
      "Current prediction:  60.4517707824707 \n",
      "\n",
      "Iteration 9588, Loss: 37.254554748535156, L1: 9.511375427246094, L3: 27.74317741394043\n",
      "Current prediction:  60.541282653808594 \n",
      "\n",
      "Iteration 9589, Loss: 35.76588821411133, L1: 9.417132377624512, L3: 26.348756790161133\n",
      "Current prediction:  60.8935546875 \n",
      "\n",
      "Iteration 9590, Loss: 37.32670974731445, L1: 9.396982192993164, L3: 27.92972755432129\n",
      "Current prediction:  61.214107513427734 \n",
      "\n",
      "Iteration 9591, Loss: 35.81696319580078, L1: 9.408075332641602, L3: 26.408889770507812\n",
      "Current prediction:  61.43364334106445 \n",
      "\n",
      "Iteration 9592, Loss: 35.08714294433594, L1: 9.355945587158203, L3: 25.731199264526367\n",
      "Current prediction:  61.527278900146484 \n",
      "\n",
      "Iteration 9593, Loss: 37.70999526977539, L1: 9.431608200073242, L3: 28.27838706970215\n",
      "Current prediction:  61.39899826049805 \n",
      "\n",
      "Iteration 9594, Loss: 35.83250427246094, L1: 9.352827072143555, L3: 26.479677200317383\n",
      "Current prediction:  60.986968994140625 \n",
      "\n",
      "Iteration 9595, Loss: 35.8985481262207, L1: 9.294172286987305, L3: 26.6043758392334\n",
      "Current prediction:  60.46942138671875 \n",
      "\n",
      "Iteration 9596, Loss: 35.9432487487793, L1: 9.508787155151367, L3: 26.43446159362793\n",
      "Current prediction:  60.431236267089844 \n",
      "\n",
      "Iteration 9597, Loss: 36.76935577392578, L1: 9.497446060180664, L3: 27.271909713745117\n",
      "Current prediction:  60.74315643310547 \n",
      "\n",
      "Iteration 9598, Loss: 35.068504333496094, L1: 9.338098526000977, L3: 25.730403900146484\n",
      "Current prediction:  61.25545120239258 \n",
      "\n",
      "Iteration 9599, Loss: 36.44038391113281, L1: 9.300577163696289, L3: 27.139806747436523\n",
      "Current prediction:  61.32401657104492 \n",
      "\n",
      "Iteration 9600, Loss: 34.81743621826172, L1: 9.342072486877441, L3: 25.475364685058594\n",
      "Current prediction:  61.18731689453125 \n",
      "\n",
      "Iteration 9601, Loss: 36.01203155517578, L1: 9.275967597961426, L3: 26.736064910888672\n",
      "Current prediction:  60.70765686035156 \n",
      "\n",
      "Iteration 9602, Loss: 35.04024124145508, L1: 9.285733222961426, L3: 25.754507064819336\n",
      "Current prediction:  60.538047790527344 \n",
      "\n",
      "Iteration 9603, Loss: 34.5899543762207, L1: 9.372236251831055, L3: 25.21771812438965\n",
      "Current prediction:  60.61259078979492 \n",
      "\n",
      "Iteration 9604, Loss: 35.12907791137695, L1: 9.363913536071777, L3: 25.765165328979492\n",
      "Current prediction:  60.88987731933594 \n",
      "\n",
      "Iteration 9605, Loss: 35.59530258178711, L1: 9.250165939331055, L3: 26.345136642456055\n",
      "Current prediction:  60.9411506652832 \n",
      "\n",
      "Iteration 9606, Loss: 35.417179107666016, L1: 9.30983829498291, L3: 26.10733985900879\n",
      "Current prediction:  61.26395797729492 \n",
      "\n",
      "Iteration 9607, Loss: 34.60829544067383, L1: 9.302163124084473, L3: 25.30613136291504\n",
      "Current prediction:  61.45962142944336 \n",
      "\n",
      "Iteration 9608, Loss: 35.110862731933594, L1: 9.35732364654541, L3: 25.753538131713867\n",
      "Current prediction:  61.252723693847656 \n",
      "\n",
      "Iteration 9609, Loss: 36.37763595581055, L1: 9.310153007507324, L3: 27.067481994628906\n",
      "Current prediction:  60.91608810424805 \n",
      "\n",
      "Iteration 9610, Loss: 35.101905822753906, L1: 9.239144325256348, L3: 25.862762451171875\n",
      "Current prediction:  60.73543930053711 \n",
      "\n",
      "Iteration 9611, Loss: 35.45010757446289, L1: 9.317766189575195, L3: 26.132341384887695\n",
      "Current prediction:  60.84724044799805 \n",
      "\n",
      "Iteration 9612, Loss: 35.258182525634766, L1: 9.337668418884277, L3: 25.920513153076172\n",
      "Current prediction:  61.27962112426758 \n",
      "\n",
      "Iteration 9613, Loss: 35.361907958984375, L1: 9.273622512817383, L3: 26.088287353515625\n",
      "Current prediction:  61.46356201171875 \n",
      "\n",
      "Iteration 9614, Loss: 35.698795318603516, L1: 9.309027671813965, L3: 26.389768600463867\n",
      "Current prediction:  61.32717514038086 \n",
      "\n",
      "Iteration 9615, Loss: 36.369773864746094, L1: 9.300825119018555, L3: 27.068946838378906\n",
      "Current prediction:  61.247291564941406 \n",
      "\n",
      "Iteration 9616, Loss: 35.229156494140625, L1: 9.259876251220703, L3: 25.96927833557129\n",
      "Current prediction:  60.95045852661133 \n",
      "\n",
      "Iteration 9617, Loss: 35.32463455200195, L1: 9.26729965209961, L3: 26.057334899902344\n",
      "Current prediction:  60.782718658447266 \n",
      "\n",
      "Iteration 9618, Loss: 36.102516174316406, L1: 9.34872055053711, L3: 26.75379753112793\n",
      "Current prediction:  60.70257568359375 \n",
      "\n",
      "Iteration 9619, Loss: 35.176761627197266, L1: 9.369370460510254, L3: 25.807392120361328\n",
      "Current prediction:  60.98100280761719 \n",
      "\n",
      "Iteration 9620, Loss: 36.227783203125, L1: 9.289328575134277, L3: 26.93845558166504\n",
      "Current prediction:  61.220245361328125 \n",
      "\n",
      "Iteration 9621, Loss: 35.56071090698242, L1: 9.273711204528809, L3: 26.28700065612793\n",
      "Current prediction:  61.30885696411133 \n",
      "\n",
      "Iteration 9622, Loss: 36.518550872802734, L1: 9.299556732177734, L3: 27.218994140625\n",
      "Current prediction:  60.76648712158203 \n",
      "\n",
      "Iteration 9623, Loss: 34.75987243652344, L1: 9.301608085632324, L3: 25.45826530456543\n",
      "Current prediction:  60.553382873535156 \n",
      "\n",
      "Iteration 9624, Loss: 35.71328353881836, L1: 9.384170532226562, L3: 26.329113006591797\n",
      "Current prediction:  60.55707550048828 \n",
      "\n",
      "Iteration 9625, Loss: 35.96087646484375, L1: 9.372365951538086, L3: 26.58850860595703\n",
      "Current prediction:  61.03590774536133 \n",
      "\n",
      "Iteration 9626, Loss: 35.56090545654297, L1: 9.305352210998535, L3: 26.255552291870117\n",
      "Current prediction:  61.46172332763672 \n",
      "\n",
      "Iteration 9627, Loss: 34.84405517578125, L1: 9.321901321411133, L3: 25.522153854370117\n",
      "Current prediction:  61.55430221557617 \n",
      "\n",
      "Iteration 9628, Loss: 36.30450439453125, L1: 9.416301727294922, L3: 26.888200759887695\n",
      "Current prediction:  61.47235107421875 \n",
      "\n",
      "Iteration 9629, Loss: 35.71760559082031, L1: 9.388723373413086, L3: 26.32888412475586\n",
      "Current prediction:  60.99810028076172 \n",
      "\n",
      "Iteration 9630, Loss: 36.08509063720703, L1: 9.35210132598877, L3: 26.732988357543945\n",
      "Current prediction:  60.474159240722656 \n",
      "\n",
      "Iteration 9631, Loss: 35.97053146362305, L1: 9.410422325134277, L3: 26.560110092163086\n",
      "Current prediction:  60.45833969116211 \n",
      "\n",
      "Iteration 9632, Loss: 35.67771530151367, L1: 9.514060020446777, L3: 26.16365623474121\n",
      "Current prediction:  60.75678634643555 \n",
      "\n",
      "Iteration 9633, Loss: 36.670406341552734, L1: 9.36816120147705, L3: 27.30224609375\n",
      "Current prediction:  61.36085510253906 \n",
      "\n",
      "Iteration 9634, Loss: 35.8546142578125, L1: 9.319819450378418, L3: 26.534793853759766\n",
      "Current prediction:  61.558135986328125 \n",
      "\n",
      "Iteration 9635, Loss: 35.598388671875, L1: 9.43758487701416, L3: 26.160802841186523\n",
      "Current prediction:  61.562320709228516 \n",
      "\n",
      "Iteration 9636, Loss: 36.678321838378906, L1: 9.386430740356445, L3: 27.291893005371094\n",
      "Current prediction:  61.52830505371094 \n",
      "\n",
      "Iteration 9637, Loss: 36.58498001098633, L1: 9.446645736694336, L3: 27.138334274291992\n",
      "Current prediction:  60.99747085571289 \n",
      "\n",
      "Iteration 9638, Loss: 36.20684814453125, L1: 9.328705787658691, L3: 26.878141403198242\n",
      "Current prediction:  60.48003387451172 \n",
      "\n",
      "Iteration 9639, Loss: 34.96318435668945, L1: 9.394383430480957, L3: 25.56879997253418\n",
      "Current prediction:  60.50163269042969 \n",
      "\n",
      "Iteration 9640, Loss: 34.91355514526367, L1: 9.44039249420166, L3: 25.473161697387695\n",
      "Current prediction:  60.65791702270508 \n",
      "\n",
      "Iteration 9641, Loss: 37.36621856689453, L1: 9.327085494995117, L3: 28.03913116455078\n",
      "Current prediction:  60.92656326293945 \n",
      "\n",
      "Iteration 9642, Loss: 34.8499641418457, L1: 9.397636413574219, L3: 25.452327728271484\n",
      "Current prediction:  61.296146392822266 \n",
      "\n",
      "Iteration 9643, Loss: 35.979610443115234, L1: 9.269876480102539, L3: 26.709733963012695\n",
      "Current prediction:  61.08549118041992 \n",
      "\n",
      "Iteration 9644, Loss: 35.632530212402344, L1: 9.279937744140625, L3: 26.35259437561035\n",
      "Current prediction:  60.9876823425293 \n",
      "\n",
      "Iteration 9645, Loss: 35.46514129638672, L1: 9.280350685119629, L3: 26.184789657592773\n",
      "Current prediction:  60.7344856262207 \n",
      "\n",
      "Iteration 9646, Loss: 35.539756774902344, L1: 9.306295394897461, L3: 26.23345947265625\n",
      "Current prediction:  60.66204071044922 \n",
      "\n",
      "Iteration 9647, Loss: 36.01710891723633, L1: 9.321629524230957, L3: 26.695478439331055\n",
      "Current prediction:  60.89883041381836 \n",
      "\n",
      "Iteration 9648, Loss: 35.84779357910156, L1: 9.304801940917969, L3: 26.54298973083496\n",
      "Current prediction:  61.0512580871582 \n",
      "\n",
      "Iteration 9649, Loss: 35.21632385253906, L1: 9.266955375671387, L3: 25.949369430541992\n",
      "Current prediction:  61.47970962524414 \n",
      "\n",
      "Iteration 9650, Loss: 35.417198181152344, L1: 9.284139633178711, L3: 26.133058547973633\n",
      "Current prediction:  61.554100036621094 \n",
      "\n",
      "Iteration 9651, Loss: 37.82466125488281, L1: 10.443007469177246, L3: 27.38165283203125\n",
      "Current prediction:  61.362815856933594 \n",
      "\n",
      "Iteration 9652, Loss: 35.07394790649414, L1: 9.382460594177246, L3: 25.69148826599121\n",
      "Current prediction:  60.86393356323242 \n",
      "\n",
      "Iteration 9653, Loss: 42.49857711791992, L1: 15.000945091247559, L3: 27.497631072998047\n",
      "Current prediction:  60.50167465209961 \n",
      "\n",
      "Iteration 9654, Loss: 38.62212371826172, L1: 12.028453826904297, L3: 26.59366798400879\n",
      "Current prediction:  60.50608825683594 \n",
      "\n",
      "Iteration 9655, Loss: 38.50557327270508, L1: 10.497594833374023, L3: 28.007978439331055\n",
      "Current prediction:  58.37333679199219 \n",
      "\n",
      "Iteration 9656, Loss: 41.01327133178711, L1: 10.731112480163574, L3: 30.28215980529785\n",
      "Current prediction:  58.0355339050293 \n",
      "\n",
      "Iteration 9657, Loss: 41.497230529785156, L1: 10.89273452758789, L3: 30.604496002197266\n",
      "Current prediction:  59.00058364868164 \n",
      "\n",
      "Iteration 9658, Loss: 43.293609619140625, L1: 12.16125774383545, L3: 31.13235092163086\n",
      "Current prediction:  59.345951080322266 \n",
      "\n",
      "Iteration 9659, Loss: 42.5173454284668, L1: 12.677306175231934, L3: 29.840038299560547\n",
      "Current prediction:  59.713966369628906 \n",
      "\n",
      "Iteration 9660, Loss: 40.498680114746094, L1: 13.536255836486816, L3: 26.962425231933594\n",
      "Current prediction:  60.11085891723633 \n",
      "\n",
      "Iteration 9661, Loss: 41.6663932800293, L1: 15.127542495727539, L3: 26.538850784301758\n",
      "Current prediction:  60.51593780517578 \n",
      "\n",
      "Iteration 9662, Loss: 41.544395446777344, L1: 15.149619102478027, L3: 26.394777297973633\n",
      "Current prediction:  60.91844940185547 \n",
      "\n",
      "Iteration 9663, Loss: 40.247467041015625, L1: 13.558104515075684, L3: 26.689361572265625\n",
      "Current prediction:  61.29817581176758 \n",
      "\n",
      "Iteration 9664, Loss: 40.1783561706543, L1: 12.964539527893066, L3: 27.213815689086914\n",
      "Current prediction:  61.63722610473633 \n",
      "\n",
      "Iteration 9665, Loss: 39.52056121826172, L1: 13.264178276062012, L3: 26.256383895874023\n",
      "Current prediction:  61.92416000366211 \n",
      "\n",
      "Iteration 9666, Loss: 42.41119384765625, L1: 14.177051544189453, L3: 28.23414421081543\n",
      "Current prediction:  62.148616790771484 \n",
      "\n",
      "Iteration 9667, Loss: 41.3826904296875, L1: 10.639842987060547, L3: 30.742849349975586\n",
      "Current prediction:  62.28558349609375 \n",
      "\n",
      "Iteration 9668, Loss: 39.711769104003906, L1: 8.860147476196289, L3: 30.851621627807617\n",
      "Current prediction:  62.34555435180664 \n",
      "\n",
      "Iteration 9669, Loss: 43.53485107421875, L1: 8.183694839477539, L3: 35.351158142089844\n",
      "Current prediction:  64.62641143798828 \n",
      "\n",
      "Iteration 9670, Loss: 43.28440856933594, L1: 7.577128887176514, L3: 35.707279205322266\n",
      "Current prediction:  63.73003005981445 \n",
      "\n",
      "Iteration 9671, Loss: 43.66401672363281, L1: 8.050860404968262, L3: 35.613155364990234\n",
      "Current prediction:  63.50810623168945 \n",
      "\n",
      "Iteration 9672, Loss: 42.571449279785156, L1: 8.791540145874023, L3: 33.7799072265625\n",
      "Current prediction:  63.23952102661133 \n",
      "\n",
      "Iteration 9673, Loss: 41.34981155395508, L1: 9.585885047912598, L3: 31.763925552368164\n",
      "Current prediction:  62.93515396118164 \n",
      "\n",
      "Iteration 9674, Loss: 41.88237762451172, L1: 10.785233497619629, L3: 31.097143173217773\n",
      "Current prediction:  62.61224365234375 \n",
      "\n",
      "Iteration 9675, Loss: 41.15721130371094, L1: 11.648127555847168, L3: 29.509084701538086\n",
      "Current prediction:  62.28113555908203 \n",
      "\n",
      "Iteration 9676, Loss: 39.20480728149414, L1: 11.488870620727539, L3: 27.7159366607666\n",
      "Current prediction:  61.953060150146484 \n",
      "\n",
      "Iteration 9677, Loss: 38.657310485839844, L1: 11.87244987487793, L3: 26.784862518310547\n",
      "Current prediction:  61.63960266113281 \n",
      "\n",
      "Iteration 9678, Loss: 39.588584899902344, L1: 12.745546340942383, L3: 26.843040466308594\n",
      "Current prediction:  61.34596633911133 \n",
      "\n",
      "Iteration 9679, Loss: 39.03154373168945, L1: 12.78144359588623, L3: 26.250099182128906\n",
      "Current prediction:  61.08650207519531 \n",
      "\n",
      "Iteration 9680, Loss: 38.201171875, L1: 12.49655818939209, L3: 25.704614639282227\n",
      "Current prediction:  60.86581802368164 \n",
      "\n",
      "Iteration 9681, Loss: 39.404136657714844, L1: 13.546566009521484, L3: 25.857572555541992\n",
      "Current prediction:  60.68536376953125 \n",
      "\n",
      "Iteration 9682, Loss: 40.31139373779297, L1: 13.20631217956543, L3: 27.10508155822754\n",
      "Current prediction:  60.54443359375 \n",
      "\n",
      "Iteration 9683, Loss: 39.7647590637207, L1: 13.422146797180176, L3: 26.342613220214844\n",
      "Current prediction:  60.448062896728516 \n",
      "\n",
      "Iteration 9684, Loss: 38.4159049987793, L1: 12.941751480102539, L3: 25.474153518676758\n",
      "Current prediction:  60.396568298339844 \n",
      "\n",
      "Iteration 9685, Loss: 41.499916076660156, L1: 14.123910903930664, L3: 27.376007080078125\n",
      "Current prediction:  60.386539459228516 \n",
      "\n",
      "Iteration 9686, Loss: 40.90123748779297, L1: 13.786656379699707, L3: 27.114582061767578\n",
      "Current prediction:  60.40439987182617 \n",
      "\n",
      "Iteration 9687, Loss: 40.07341766357422, L1: 14.082080841064453, L3: 25.991336822509766\n",
      "Current prediction:  60.449581146240234 \n",
      "\n",
      "Iteration 9688, Loss: 40.482826232910156, L1: 13.805644035339355, L3: 26.677181243896484\n",
      "Current prediction:  60.51360321044922 \n",
      "\n",
      "Iteration 9689, Loss: 39.21805953979492, L1: 13.547525405883789, L3: 25.670534133911133\n",
      "Current prediction:  60.59599685668945 \n",
      "\n",
      "Iteration 9690, Loss: 38.66209411621094, L1: 12.691308975219727, L3: 25.97078514099121\n",
      "Current prediction:  60.69398880004883 \n",
      "\n",
      "Iteration 9691, Loss: 37.91038131713867, L1: 12.332337379455566, L3: 25.578044891357422\n",
      "Current prediction:  60.80685043334961 \n",
      "\n",
      "Iteration 9692, Loss: 39.88075637817383, L1: 13.323075294494629, L3: 26.557682037353516\n",
      "Current prediction:  60.92142868041992 \n",
      "\n",
      "Iteration 9693, Loss: 39.55046081542969, L1: 12.927448272705078, L3: 26.62301254272461\n",
      "Current prediction:  61.03427505493164 \n",
      "\n",
      "Iteration 9694, Loss: 38.915836334228516, L1: 12.62966537475586, L3: 26.286170959472656\n",
      "Current prediction:  61.15031051635742 \n",
      "\n",
      "Iteration 9695, Loss: 39.298377990722656, L1: 11.983299255371094, L3: 27.315078735351562\n",
      "Current prediction:  61.26544189453125 \n",
      "\n",
      "Iteration 9696, Loss: 37.07422637939453, L1: 11.20594310760498, L3: 25.868284225463867\n",
      "Current prediction:  61.38151168823242 \n",
      "\n",
      "Iteration 9697, Loss: 37.77164840698242, L1: 11.118376731872559, L3: 26.65327262878418\n",
      "Current prediction:  61.496185302734375 \n",
      "\n",
      "Iteration 9698, Loss: 38.38729476928711, L1: 10.930249214172363, L3: 27.45704460144043\n",
      "Current prediction:  61.61484909057617 \n",
      "\n",
      "Iteration 9699, Loss: 37.30547332763672, L1: 11.04580020904541, L3: 26.259674072265625\n",
      "Current prediction:  61.725990295410156 \n",
      "\n",
      "Iteration 9700, Loss: 37.86293411254883, L1: 11.155930519104004, L3: 26.707002639770508\n",
      "Current prediction:  60.68141555786133 \n",
      "\n",
      "Iteration 9701, Loss: 37.64535903930664, L1: 11.021875381469727, L3: 26.623483657836914\n",
      "Current prediction:  59.783355712890625 \n",
      "\n",
      "Iteration 9702, Loss: 37.80998229980469, L1: 11.0657958984375, L3: 26.74418830871582\n",
      "Current prediction:  59.911231994628906 \n",
      "\n",
      "Iteration 9703, Loss: 37.584190368652344, L1: 10.995797157287598, L3: 26.588394165039062\n",
      "Current prediction:  60.04979705810547 \n",
      "\n",
      "Iteration 9704, Loss: 37.74190139770508, L1: 10.902596473693848, L3: 26.839303970336914\n",
      "Current prediction:  60.18552017211914 \n",
      "\n",
      "Iteration 9705, Loss: 38.53931427001953, L1: 10.855936050415039, L3: 27.683378219604492\n",
      "Current prediction:  60.315452575683594 \n",
      "\n",
      "Iteration 9706, Loss: 38.330631256103516, L1: 10.837852478027344, L3: 27.492778778076172\n",
      "Current prediction:  60.43601989746094 \n",
      "\n",
      "Iteration 9707, Loss: 36.38102722167969, L1: 10.753774642944336, L3: 25.627254486083984\n",
      "Current prediction:  60.55218505859375 \n",
      "\n",
      "Iteration 9708, Loss: 37.60539627075195, L1: 10.754010200500488, L3: 26.85138702392578\n",
      "Current prediction:  60.6553955078125 \n",
      "\n",
      "Iteration 9709, Loss: 37.48396682739258, L1: 10.732826232910156, L3: 26.751140594482422\n",
      "Current prediction:  60.74729919433594 \n",
      "\n",
      "Iteration 9710, Loss: 37.85688018798828, L1: 10.710677146911621, L3: 27.146203994750977\n",
      "Current prediction:  60.8293342590332 \n",
      "\n",
      "Iteration 9711, Loss: 38.50889587402344, L1: 10.642400741577148, L3: 27.866493225097656\n",
      "Current prediction:  60.901119232177734 \n",
      "\n",
      "Iteration 9712, Loss: 37.16122055053711, L1: 10.644835472106934, L3: 26.51638412475586\n",
      "Current prediction:  60.9605827331543 \n",
      "\n",
      "Iteration 9713, Loss: 38.48761749267578, L1: 10.617761611938477, L3: 27.869853973388672\n",
      "Current prediction:  61.004791259765625 \n",
      "\n",
      "Iteration 9714, Loss: 37.54989242553711, L1: 10.703003883361816, L3: 26.846887588500977\n",
      "Current prediction:  61.030853271484375 \n",
      "\n",
      "Iteration 9715, Loss: 37.96512985229492, L1: 10.611403465270996, L3: 27.353727340698242\n",
      "Current prediction:  61.04396057128906 \n",
      "\n",
      "Iteration 9716, Loss: 38.52108383178711, L1: 10.7047758102417, L3: 27.816308975219727\n",
      "Current prediction:  61.04160690307617 \n",
      "\n",
      "Iteration 9717, Loss: 37.30410385131836, L1: 10.662903785705566, L3: 26.641199111938477\n",
      "Current prediction:  61.029541015625 \n",
      "\n",
      "Iteration 9718, Loss: 38.52444839477539, L1: 10.82882308959961, L3: 27.69562530517578\n",
      "Current prediction:  61.00785446166992 \n",
      "\n",
      "Iteration 9719, Loss: 37.438995361328125, L1: 10.74590015411377, L3: 26.69309425354004\n",
      "Current prediction:  60.98082733154297 \n",
      "\n",
      "Iteration 9720, Loss: 37.36845397949219, L1: 10.921128273010254, L3: 26.447324752807617\n",
      "Current prediction:  60.94757080078125 \n",
      "\n",
      "Iteration 9721, Loss: 37.856536865234375, L1: 10.896604537963867, L3: 26.95993423461914\n",
      "Current prediction:  60.91170883178711 \n",
      "\n",
      "Iteration 9722, Loss: 36.60399627685547, L1: 10.887285232543945, L3: 25.716711044311523\n",
      "Current prediction:  60.879112243652344 \n",
      "\n",
      "Iteration 9723, Loss: 38.74292755126953, L1: 11.005635261535645, L3: 27.737293243408203\n",
      "Current prediction:  60.846099853515625 \n",
      "\n",
      "Iteration 9724, Loss: 36.82826232910156, L1: 10.895983695983887, L3: 25.93227767944336\n",
      "Current prediction:  60.816162109375 \n",
      "\n",
      "Iteration 9725, Loss: 38.16743850708008, L1: 10.941712379455566, L3: 27.225725173950195\n",
      "Current prediction:  60.791282653808594 \n",
      "\n",
      "Iteration 9726, Loss: 38.49747848510742, L1: 10.982744216918945, L3: 27.514734268188477\n",
      "Current prediction:  60.767059326171875 \n",
      "\n",
      "Iteration 9727, Loss: 37.94339370727539, L1: 10.963502883911133, L3: 26.979890823364258\n",
      "Current prediction:  60.74698257446289 \n",
      "\n",
      "Iteration 9728, Loss: 37.65045928955078, L1: 10.91472053527832, L3: 26.735740661621094\n",
      "Current prediction:  60.72854995727539 \n",
      "\n",
      "Iteration 9729, Loss: 38.04299545288086, L1: 10.926310539245605, L3: 27.116683959960938\n",
      "Current prediction:  60.71492004394531 \n",
      "\n",
      "Iteration 9730, Loss: 38.44659423828125, L1: 10.977450370788574, L3: 27.46914291381836\n",
      "Current prediction:  60.69661331176758 \n",
      "\n",
      "Iteration 9731, Loss: 37.931583404541016, L1: 10.926262855529785, L3: 27.005321502685547\n",
      "Current prediction:  60.68163299560547 \n",
      "\n",
      "Iteration 9732, Loss: 38.21071243286133, L1: 10.893856048583984, L3: 27.316856384277344\n",
      "Current prediction:  60.6705207824707 \n",
      "\n",
      "Iteration 9733, Loss: 37.01350402832031, L1: 10.944192886352539, L3: 26.06930923461914\n",
      "Current prediction:  60.667625427246094 \n",
      "\n",
      "Iteration 9734, Loss: 37.94944763183594, L1: 10.876493453979492, L3: 27.072952270507812\n",
      "Current prediction:  60.667144775390625 \n",
      "\n",
      "Iteration 9735, Loss: 38.486732482910156, L1: 10.842146873474121, L3: 27.64458465576172\n",
      "Current prediction:  60.66609573364258 \n",
      "\n",
      "Iteration 9736, Loss: 37.0872917175293, L1: 10.878924369812012, L3: 26.20836639404297\n",
      "Current prediction:  60.669742584228516 \n",
      "\n",
      "Iteration 9737, Loss: 37.52779769897461, L1: 10.83043384552002, L3: 26.697362899780273\n",
      "Current prediction:  60.67786407470703 \n",
      "\n",
      "Iteration 9738, Loss: 38.04994201660156, L1: 10.772956848144531, L3: 27.27698516845703\n",
      "Current prediction:  60.68757247924805 \n",
      "\n",
      "Iteration 9739, Loss: 37.17705535888672, L1: 10.746665954589844, L3: 26.430387496948242\n",
      "Current prediction:  60.69969177246094 \n",
      "\n",
      "Iteration 9740, Loss: 36.803260803222656, L1: 10.70740032196045, L3: 26.09585952758789\n",
      "Current prediction:  60.714881896972656 \n",
      "\n",
      "Iteration 9741, Loss: 37.07830047607422, L1: 10.65257453918457, L3: 26.42572593688965\n",
      "Current prediction:  60.72907257080078 \n",
      "\n",
      "Iteration 9742, Loss: 36.16154861450195, L1: 10.651576042175293, L3: 25.509973526000977\n",
      "Current prediction:  60.7419319152832 \n",
      "\n",
      "Iteration 9743, Loss: 36.72988510131836, L1: 10.596940994262695, L3: 26.132944107055664\n",
      "Current prediction:  60.75126647949219 \n",
      "\n",
      "Iteration 9744, Loss: 37.54435348510742, L1: 10.57569408416748, L3: 26.968658447265625\n",
      "Current prediction:  60.75939178466797 \n",
      "\n",
      "Iteration 9745, Loss: 38.30923080444336, L1: 10.570767402648926, L3: 27.738462448120117\n",
      "Current prediction:  60.763309478759766 \n",
      "\n",
      "Iteration 9746, Loss: 37.58784866333008, L1: 10.546103477478027, L3: 27.041746139526367\n",
      "Current prediction:  60.765037536621094 \n",
      "\n",
      "Iteration 9747, Loss: 38.032981872558594, L1: 10.5117769241333, L3: 27.52120590209961\n",
      "Current prediction:  60.76376724243164 \n",
      "\n",
      "Iteration 9748, Loss: 37.98291778564453, L1: 10.525296211242676, L3: 27.457622528076172\n",
      "Current prediction:  60.758480072021484 \n",
      "\n",
      "Iteration 9749, Loss: 38.537479400634766, L1: 10.508397102355957, L3: 28.029081344604492\n",
      "Current prediction:  60.74932098388672 \n",
      "\n",
      "Iteration 9750, Loss: 37.413841247558594, L1: 10.535937309265137, L3: 26.87790298461914\n",
      "Current prediction:  60.73549270629883 \n",
      "\n",
      "Iteration 9751, Loss: 37.11592102050781, L1: 10.540057182312012, L3: 26.575862884521484\n",
      "Current prediction:  60.724769592285156 \n",
      "\n",
      "Iteration 9752, Loss: 36.99591827392578, L1: 10.551756858825684, L3: 26.444162368774414\n",
      "Current prediction:  60.71156692504883 \n",
      "\n",
      "Iteration 9753, Loss: 37.28775405883789, L1: 10.549149513244629, L3: 26.738603591918945\n",
      "Current prediction:  60.69721603393555 \n",
      "\n",
      "Iteration 9754, Loss: 37.69868469238281, L1: 10.567590713500977, L3: 27.131093978881836\n",
      "Current prediction:  60.68142318725586 \n",
      "\n",
      "Iteration 9755, Loss: 37.455101013183594, L1: 10.569730758666992, L3: 26.885372161865234\n",
      "Current prediction:  60.66507339477539 \n",
      "\n",
      "Iteration 9756, Loss: 37.925662994384766, L1: 10.598115921020508, L3: 27.327547073364258\n",
      "Current prediction:  60.650665283203125 \n",
      "\n",
      "Iteration 9757, Loss: 37.06094741821289, L1: 10.605096817016602, L3: 26.45585060119629\n",
      "Current prediction:  60.637184143066406 \n",
      "\n",
      "Iteration 9758, Loss: 37.378517150878906, L1: 10.59345817565918, L3: 26.785057067871094\n",
      "Current prediction:  60.623226165771484 \n",
      "\n",
      "Iteration 9759, Loss: 36.69841384887695, L1: 10.60749340057373, L3: 26.09092140197754\n",
      "Current prediction:  60.6165885925293 \n",
      "\n",
      "Iteration 9760, Loss: 37.282981872558594, L1: 10.633684158325195, L3: 26.6492977142334\n",
      "Current prediction:  60.607948303222656 \n",
      "\n",
      "Iteration 9761, Loss: 37.357093811035156, L1: 10.615629196166992, L3: 26.741466522216797\n",
      "Current prediction:  60.603736877441406 \n",
      "\n",
      "Iteration 9762, Loss: 37.76893615722656, L1: 10.635470390319824, L3: 27.133464813232422\n",
      "Current prediction:  60.600860595703125 \n",
      "\n",
      "Iteration 9763, Loss: 37.72254943847656, L1: 10.629194259643555, L3: 27.09335708618164\n",
      "Current prediction:  60.60002136230469 \n",
      "\n",
      "Iteration 9764, Loss: 38.15863800048828, L1: 10.624908447265625, L3: 27.533727645874023\n",
      "Current prediction:  60.59661865234375 \n",
      "\n",
      "Iteration 9765, Loss: 36.83675003051758, L1: 10.645806312561035, L3: 26.190942764282227\n",
      "Current prediction:  60.595458984375 \n",
      "\n",
      "Iteration 9766, Loss: 36.8333740234375, L1: 10.642975807189941, L3: 26.190399169921875\n",
      "Current prediction:  60.59600830078125 \n",
      "\n",
      "Iteration 9767, Loss: 36.89378356933594, L1: 10.630159378051758, L3: 26.263622283935547\n",
      "Current prediction:  60.60022735595703 \n",
      "\n",
      "Iteration 9768, Loss: 36.244956970214844, L1: 10.627175331115723, L3: 25.617782592773438\n",
      "Current prediction:  60.60962677001953 \n",
      "\n",
      "Iteration 9769, Loss: 36.85076904296875, L1: 10.633444786071777, L3: 26.21732521057129\n",
      "Current prediction:  60.61791229248047 \n",
      "\n",
      "Iteration 9770, Loss: 37.052757263183594, L1: 10.631936073303223, L3: 26.420822143554688\n",
      "Current prediction:  60.626094818115234 \n",
      "\n",
      "Iteration 9771, Loss: 37.95539093017578, L1: 10.585652351379395, L3: 27.36973762512207\n",
      "Current prediction:  60.632259368896484 \n",
      "\n",
      "Iteration 9772, Loss: 37.08087921142578, L1: 10.591875076293945, L3: 26.489002227783203\n",
      "Current prediction:  60.637577056884766 \n",
      "\n",
      "Iteration 9773, Loss: 36.637428283691406, L1: 10.574862480163574, L3: 26.06256675720215\n",
      "Current prediction:  60.64109420776367 \n",
      "\n",
      "Iteration 9774, Loss: 36.310333251953125, L1: 10.581527709960938, L3: 25.728805541992188\n",
      "Current prediction:  60.64716339111328 \n",
      "\n",
      "Iteration 9775, Loss: 37.61443328857422, L1: 10.55644702911377, L3: 27.057987213134766\n",
      "Current prediction:  60.652984619140625 \n",
      "\n",
      "Iteration 9776, Loss: 37.51824951171875, L1: 10.555441856384277, L3: 26.962806701660156\n",
      "Current prediction:  60.65394592285156 \n",
      "\n",
      "Iteration 9777, Loss: 37.709774017333984, L1: 10.57006549835205, L3: 27.13970947265625\n",
      "Current prediction:  60.65234375 \n",
      "\n",
      "Iteration 9778, Loss: 38.42926025390625, L1: 10.567811012268066, L3: 27.8614501953125\n",
      "Current prediction:  60.648502349853516 \n",
      "\n",
      "Iteration 9779, Loss: 36.95863342285156, L1: 10.555042266845703, L3: 26.403593063354492\n",
      "Current prediction:  60.645729064941406 \n",
      "\n",
      "Iteration 9780, Loss: 37.803985595703125, L1: 10.580181121826172, L3: 27.22380256652832\n",
      "Current prediction:  60.64024353027344 \n",
      "\n",
      "Iteration 9781, Loss: 35.98405456542969, L1: 10.582530975341797, L3: 25.401525497436523\n",
      "Current prediction:  60.63206481933594 \n",
      "\n",
      "Iteration 9782, Loss: 37.041343688964844, L1: 10.606142044067383, L3: 26.435203552246094\n",
      "Current prediction:  60.62614440917969 \n",
      "\n",
      "Iteration 9783, Loss: 37.65772247314453, L1: 10.584989547729492, L3: 27.072734832763672\n",
      "Current prediction:  60.619171142578125 \n",
      "\n",
      "Iteration 9784, Loss: 37.60590744018555, L1: 10.597020149230957, L3: 27.008888244628906\n",
      "Current prediction:  60.611572265625 \n",
      "\n",
      "Iteration 9785, Loss: 37.66155242919922, L1: 10.591886520385742, L3: 27.06966781616211\n",
      "Current prediction:  60.60456085205078 \n",
      "\n",
      "Iteration 9786, Loss: 36.34307098388672, L1: 10.584527969360352, L3: 25.758543014526367\n",
      "Current prediction:  60.5985221862793 \n",
      "\n",
      "Iteration 9787, Loss: 37.76811599731445, L1: 10.60944652557373, L3: 27.158668518066406\n",
      "Current prediction:  60.594322204589844 \n",
      "\n",
      "Iteration 9788, Loss: 37.494773864746094, L1: 10.587728500366211, L3: 26.907047271728516\n",
      "Current prediction:  60.59315490722656 \n",
      "\n",
      "Iteration 9789, Loss: 37.7413444519043, L1: 10.594165802001953, L3: 27.147178649902344\n",
      "Current prediction:  60.587886810302734 \n",
      "\n",
      "Iteration 9790, Loss: 36.667869567871094, L1: 10.616445541381836, L3: 26.05142593383789\n",
      "Current prediction:  60.58530044555664 \n",
      "\n",
      "Iteration 9791, Loss: 37.66999053955078, L1: 10.593055725097656, L3: 27.076934814453125\n",
      "Current prediction:  60.584083557128906 \n",
      "\n",
      "Iteration 9792, Loss: 37.04755783081055, L1: 10.613396644592285, L3: 26.434160232543945\n",
      "Current prediction:  60.58133316040039 \n",
      "\n",
      "Iteration 9793, Loss: 37.03788375854492, L1: 10.635440826416016, L3: 26.402442932128906\n",
      "Current prediction:  60.5805549621582 \n",
      "\n",
      "Iteration 9794, Loss: 37.18043518066406, L1: 10.619392395019531, L3: 26.561044692993164\n",
      "Current prediction:  60.5794677734375 \n",
      "\n",
      "Iteration 9795, Loss: 37.97797393798828, L1: 10.62153434753418, L3: 27.35643768310547\n",
      "Current prediction:  60.578277587890625 \n",
      "\n",
      "Iteration 9796, Loss: 36.61927795410156, L1: 10.613445281982422, L3: 26.005834579467773\n",
      "Current prediction:  60.5792236328125 \n",
      "\n",
      "Iteration 9797, Loss: 37.37295150756836, L1: 10.638561248779297, L3: 26.734390258789062\n",
      "Current prediction:  60.580718994140625 \n",
      "\n",
      "Iteration 9798, Loss: 38.06028366088867, L1: 10.817093849182129, L3: 27.24319076538086\n",
      "Current prediction:  60.57891082763672 \n",
      "\n",
      "Iteration 9799, Loss: 36.142974853515625, L1: 10.594538688659668, L3: 25.54843521118164\n",
      "Current prediction:  60.57840347290039 \n",
      "\n",
      "Iteration 9800, Loss: 37.517913818359375, L1: 10.672555923461914, L3: 26.845359802246094\n",
      "Current prediction:  60.57339859008789 \n",
      "\n",
      "Iteration 9801, Loss: 37.60051727294922, L1: 10.643099784851074, L3: 26.95741844177246\n",
      "Current prediction:  60.5659294128418 \n",
      "\n",
      "Iteration 9802, Loss: 37.27836608886719, L1: 10.688587188720703, L3: 26.589778900146484\n",
      "Current prediction:  60.55949401855469 \n",
      "\n",
      "Iteration 9803, Loss: 37.865909576416016, L1: 10.778522491455078, L3: 27.087387084960938\n",
      "Current prediction:  60.558570861816406 \n",
      "\n",
      "Iteration 9804, Loss: 36.83230972290039, L1: 10.722832679748535, L3: 26.109477996826172\n",
      "Current prediction:  60.55852127075195 \n",
      "\n",
      "Iteration 9805, Loss: 36.91798400878906, L1: 10.770251274108887, L3: 26.147733688354492\n",
      "Current prediction:  60.56690216064453 \n",
      "\n",
      "Iteration 9806, Loss: 36.86695861816406, L1: 10.696319580078125, L3: 26.170637130737305\n",
      "Current prediction:  60.579490661621094 \n",
      "\n",
      "Iteration 9807, Loss: 36.90234375, L1: 10.72001838684082, L3: 26.182327270507812\n",
      "Current prediction:  60.5963134765625 \n",
      "\n",
      "Iteration 9808, Loss: 36.88572311401367, L1: 10.737082481384277, L3: 26.14864158630371\n",
      "Current prediction:  60.59764862060547 \n",
      "\n",
      "Iteration 9809, Loss: 36.90946578979492, L1: 10.731995582580566, L3: 26.177471160888672\n",
      "Current prediction:  60.28615951538086 \n",
      "\n",
      "Iteration 9810, Loss: 37.02112579345703, L1: 10.730233192443848, L3: 26.2908935546875\n",
      "Current prediction:  60.319915771484375 \n",
      "\n",
      "Iteration 9811, Loss: 36.238521575927734, L1: 10.767269134521484, L3: 25.47125244140625\n",
      "Current prediction:  60.35673522949219 \n",
      "\n",
      "Iteration 9812, Loss: 37.319801330566406, L1: 10.724808692932129, L3: 26.594993591308594\n",
      "Current prediction:  60.39692687988281 \n",
      "\n",
      "Iteration 9813, Loss: 36.377628326416016, L1: 10.796174049377441, L3: 25.581453323364258\n",
      "Current prediction:  60.43627166748047 \n",
      "\n",
      "Iteration 9814, Loss: 36.17546081542969, L1: 10.765634536743164, L3: 25.409828186035156\n",
      "Current prediction:  60.484107971191406 \n",
      "\n",
      "Iteration 9815, Loss: 37.071685791015625, L1: 10.702510833740234, L3: 26.36917495727539\n",
      "Current prediction:  60.531036376953125 \n",
      "\n",
      "Iteration 9816, Loss: 36.007789611816406, L1: 10.717926979064941, L3: 25.28986358642578\n",
      "Current prediction:  60.576812744140625 \n",
      "\n",
      "Iteration 9817, Loss: 37.397491455078125, L1: 10.676499366760254, L3: 26.720993041992188\n",
      "Current prediction:  60.62223815917969 \n",
      "\n",
      "Iteration 9818, Loss: 37.11748504638672, L1: 10.68413257598877, L3: 26.433353424072266\n",
      "Current prediction:  60.65532302856445 \n",
      "\n",
      "Iteration 9819, Loss: 37.1611213684082, L1: 10.600172996520996, L3: 26.560949325561523\n",
      "Current prediction:  60.68134307861328 \n",
      "\n",
      "Iteration 9820, Loss: 36.71160888671875, L1: 10.665266990661621, L3: 26.046342849731445\n",
      "Current prediction:  60.70771789550781 \n",
      "\n",
      "Iteration 9821, Loss: 37.50804138183594, L1: 10.577096939086914, L3: 26.930944442749023\n",
      "Current prediction:  60.72840118408203 \n",
      "\n",
      "Iteration 9822, Loss: 36.671234130859375, L1: 10.563384056091309, L3: 26.107851028442383\n",
      "Current prediction:  60.74046325683594 \n",
      "\n",
      "Iteration 9823, Loss: 36.73402404785156, L1: 10.584806442260742, L3: 26.14921760559082\n",
      "Current prediction:  60.75611877441406 \n",
      "\n",
      "Iteration 9824, Loss: 36.02938461303711, L1: 10.585780143737793, L3: 25.443605422973633\n",
      "Current prediction:  60.771766662597656 \n",
      "\n",
      "Iteration 9825, Loss: 36.017120361328125, L1: 10.528724670410156, L3: 25.488393783569336\n",
      "Current prediction:  60.7802619934082 \n",
      "\n",
      "Iteration 9826, Loss: 36.35633087158203, L1: 10.57139778137207, L3: 25.78493309020996\n",
      "Current prediction:  60.78374099731445 \n",
      "\n",
      "Iteration 9827, Loss: 35.682655334472656, L1: 10.547444343566895, L3: 25.135211944580078\n",
      "Current prediction:  60.7998161315918 \n",
      "\n",
      "Iteration 9828, Loss: 36.46752166748047, L1: 10.587580680847168, L3: 25.879940032958984\n",
      "Current prediction:  60.79938507080078 \n",
      "\n",
      "Iteration 9829, Loss: 36.77383804321289, L1: 10.52957534790039, L3: 26.2442626953125\n",
      "Current prediction:  60.7973518371582 \n",
      "\n",
      "Iteration 9830, Loss: 36.3228645324707, L1: 10.574917793273926, L3: 25.747947692871094\n",
      "Current prediction:  60.793678283691406 \n",
      "\n",
      "Iteration 9831, Loss: 37.46312713623047, L1: 10.575746536254883, L3: 26.887380599975586\n",
      "Current prediction:  60.79378128051758 \n",
      "\n",
      "Iteration 9832, Loss: 36.23129653930664, L1: 10.656740188598633, L3: 25.574556350708008\n",
      "Current prediction:  60.78745651245117 \n",
      "\n",
      "Iteration 9833, Loss: 37.38239669799805, L1: 10.593504905700684, L3: 26.788890838623047\n",
      "Current prediction:  60.81317901611328 \n",
      "\n",
      "Iteration 9834, Loss: 38.034423828125, L1: 10.699538230895996, L3: 27.33488655090332\n",
      "Current prediction:  60.83755874633789 \n",
      "\n",
      "Iteration 9835, Loss: 37.13837432861328, L1: 10.616154670715332, L3: 26.522218704223633\n",
      "Current prediction:  60.848106384277344 \n",
      "\n",
      "Iteration 9836, Loss: 37.0616569519043, L1: 10.725451469421387, L3: 26.336206436157227\n",
      "Current prediction:  60.83315658569336 \n",
      "\n",
      "Iteration 9837, Loss: 35.49037551879883, L1: 10.618651390075684, L3: 24.87172508239746\n",
      "Current prediction:  60.83561706542969 \n",
      "\n",
      "Iteration 9838, Loss: 36.806854248046875, L1: 10.763656616210938, L3: 26.043197631835938\n",
      "Current prediction:  60.83415603637695 \n",
      "\n",
      "Iteration 9839, Loss: 36.04273986816406, L1: 10.69624137878418, L3: 25.34649658203125\n",
      "Current prediction:  60.844383239746094 \n",
      "\n",
      "Iteration 9840, Loss: 36.09236145019531, L1: 10.718125343322754, L3: 25.374235153198242\n",
      "Current prediction:  60.824710845947266 \n",
      "\n",
      "Iteration 9841, Loss: 36.318016052246094, L1: 10.69903564453125, L3: 25.618980407714844\n",
      "Current prediction:  60.80006408691406 \n",
      "\n",
      "Iteration 9842, Loss: 36.25947570800781, L1: 10.796156883239746, L3: 25.463319778442383\n",
      "Current prediction:  60.789730072021484 \n",
      "\n",
      "Iteration 9843, Loss: 37.14360046386719, L1: 10.717924118041992, L3: 26.425674438476562\n",
      "Current prediction:  60.79066848754883 \n",
      "\n",
      "Iteration 9844, Loss: 36.978153228759766, L1: 10.674015045166016, L3: 26.30413818359375\n",
      "Current prediction:  60.790348052978516 \n",
      "\n",
      "Iteration 9845, Loss: 36.00170135498047, L1: 10.668785095214844, L3: 25.332918167114258\n",
      "Current prediction:  60.7802848815918 \n",
      "\n",
      "Iteration 9846, Loss: 36.6987190246582, L1: 10.749910354614258, L3: 25.948808670043945\n",
      "Current prediction:  60.76466369628906 \n",
      "\n",
      "Iteration 9847, Loss: 36.6823616027832, L1: 10.68684196472168, L3: 25.995519638061523\n",
      "Current prediction:  60.75852966308594 \n",
      "\n",
      "Iteration 9848, Loss: 37.45638656616211, L1: 10.70299243927002, L3: 26.753393173217773\n",
      "Current prediction:  60.75030517578125 \n",
      "\n",
      "Iteration 9849, Loss: 36.39113235473633, L1: 10.677477836608887, L3: 25.713655471801758\n",
      "Current prediction:  60.74992752075195 \n",
      "\n",
      "Iteration 9850, Loss: 37.11013412475586, L1: 10.60789966583252, L3: 26.502233505249023\n",
      "Current prediction:  60.75336837768555 \n",
      "\n",
      "Iteration 9851, Loss: 37.06489181518555, L1: 10.617640495300293, L3: 26.447250366210938\n",
      "Current prediction:  60.761600494384766 \n",
      "\n",
      "Iteration 9852, Loss: 36.66691589355469, L1: 10.649801254272461, L3: 26.01711654663086\n",
      "Current prediction:  60.76774215698242 \n",
      "\n",
      "Iteration 9853, Loss: 36.366512298583984, L1: 10.606664657592773, L3: 25.75984764099121\n",
      "Current prediction:  60.77204513549805 \n",
      "\n",
      "Iteration 9854, Loss: 36.74440383911133, L1: 10.580098152160645, L3: 26.164306640625\n",
      "Current prediction:  60.774322509765625 \n",
      "\n",
      "Iteration 9855, Loss: 35.72106170654297, L1: 10.573980331420898, L3: 25.14708137512207\n",
      "Current prediction:  60.77463912963867 \n",
      "\n",
      "Iteration 9856, Loss: 35.886627197265625, L1: 10.582416534423828, L3: 25.304210662841797\n",
      "Current prediction:  60.77737808227539 \n",
      "\n",
      "Iteration 9857, Loss: 36.58095932006836, L1: 10.58398723602295, L3: 25.996971130371094\n",
      "Current prediction:  60.77915954589844 \n",
      "\n",
      "Iteration 9858, Loss: 37.277442932128906, L1: 10.617635726928711, L3: 26.659809112548828\n",
      "Current prediction:  60.781246185302734 \n",
      "\n",
      "Iteration 9859, Loss: 36.24716567993164, L1: 10.570664405822754, L3: 25.67650032043457\n",
      "Current prediction:  60.782894134521484 \n",
      "\n",
      "Iteration 9860, Loss: 36.15773010253906, L1: 10.543935775756836, L3: 25.613794326782227\n",
      "Current prediction:  60.78276824951172 \n",
      "\n",
      "Iteration 9861, Loss: 37.037452697753906, L1: 10.56649112701416, L3: 26.470962524414062\n",
      "Current prediction:  60.783145904541016 \n",
      "\n",
      "Iteration 9862, Loss: 37.59442138671875, L1: 10.549427032470703, L3: 27.044994354248047\n",
      "Current prediction:  60.784908294677734 \n",
      "\n",
      "Iteration 9863, Loss: 37.29838562011719, L1: 10.552830696105957, L3: 26.745555877685547\n",
      "Current prediction:  60.788124084472656 \n",
      "\n",
      "Iteration 9864, Loss: 37.066131591796875, L1: 10.566425323486328, L3: 26.499706268310547\n",
      "Current prediction:  60.79029083251953 \n",
      "\n",
      "Iteration 9865, Loss: 37.409671783447266, L1: 10.56460952758789, L3: 26.845062255859375\n",
      "Current prediction:  60.79344940185547 \n",
      "\n",
      "Iteration 9866, Loss: 37.21531677246094, L1: 10.556609153747559, L3: 26.658708572387695\n",
      "Current prediction:  60.79812240600586 \n",
      "\n",
      "Iteration 9867, Loss: 36.86058807373047, L1: 10.567061424255371, L3: 26.293527603149414\n",
      "Current prediction:  60.79930114746094 \n",
      "\n",
      "Iteration 9868, Loss: 37.7589111328125, L1: 10.562657356262207, L3: 27.196252822875977\n",
      "Current prediction:  60.80268096923828 \n",
      "\n",
      "Iteration 9869, Loss: 37.6732177734375, L1: 10.572710037231445, L3: 27.100509643554688\n",
      "Current prediction:  60.808311462402344 \n",
      "\n",
      "Iteration 9870, Loss: 36.65715408325195, L1: 10.586722373962402, L3: 26.070432662963867\n",
      "Current prediction:  60.80625915527344 \n",
      "\n",
      "Iteration 9871, Loss: 37.28062438964844, L1: 10.567209243774414, L3: 26.713415145874023\n",
      "Current prediction:  60.81663513183594 \n",
      "\n",
      "Iteration 9872, Loss: 36.602516174316406, L1: 10.603730201721191, L3: 25.9987850189209\n",
      "Current prediction:  60.82510757446289 \n",
      "\n",
      "Iteration 9873, Loss: 36.19670867919922, L1: 10.569329261779785, L3: 25.62738037109375\n",
      "Current prediction:  60.838470458984375 \n",
      "\n",
      "Iteration 9874, Loss: 36.42886734008789, L1: 10.59430980682373, L3: 25.834556579589844\n",
      "Current prediction:  60.851280212402344 \n",
      "\n",
      "Iteration 9875, Loss: 36.405372619628906, L1: 10.58126449584961, L3: 25.824108123779297\n",
      "Current prediction:  60.86955261230469 \n",
      "\n",
      "Iteration 9876, Loss: 36.833866119384766, L1: 10.593148231506348, L3: 26.240718841552734\n",
      "Current prediction:  60.895381927490234 \n",
      "\n",
      "Iteration 9877, Loss: 36.80636215209961, L1: 10.587706565856934, L3: 26.218656539916992\n",
      "Current prediction:  60.90973663330078 \n",
      "\n",
      "Iteration 9878, Loss: 36.76193618774414, L1: 10.570615768432617, L3: 26.191320419311523\n",
      "Current prediction:  60.90887451171875 \n",
      "\n",
      "Iteration 9879, Loss: 37.22109603881836, L1: 10.569808006286621, L3: 26.651287078857422\n",
      "Current prediction:  60.900630950927734 \n",
      "\n",
      "Iteration 9880, Loss: 37.43032455444336, L1: 10.552254676818848, L3: 26.878068923950195\n",
      "Current prediction:  60.91452407836914 \n",
      "\n",
      "Iteration 9881, Loss: 36.849151611328125, L1: 10.543715476989746, L3: 26.305437088012695\n",
      "Current prediction:  60.92695617675781 \n",
      "\n",
      "Iteration 9882, Loss: 36.83946228027344, L1: 10.548676490783691, L3: 26.290786743164062\n",
      "Current prediction:  60.93252182006836 \n",
      "\n",
      "Iteration 9883, Loss: 38.28504943847656, L1: 10.54688835144043, L3: 27.7381591796875\n",
      "Current prediction:  60.938838958740234 \n",
      "\n",
      "Iteration 9884, Loss: 37.3360595703125, L1: 10.547792434692383, L3: 26.78826904296875\n",
      "Current prediction:  60.946475982666016 \n",
      "\n",
      "Iteration 9885, Loss: 37.29560089111328, L1: 10.5673246383667, L3: 26.728275299072266\n",
      "Current prediction:  60.92644500732422 \n",
      "\n",
      "Iteration 9886, Loss: 36.92108154296875, L1: 10.56631851196289, L3: 26.35476303100586\n",
      "Current prediction:  60.91557312011719 \n",
      "\n",
      "Iteration 9887, Loss: 37.204833984375, L1: 10.572761535644531, L3: 26.63207244873047\n",
      "Current prediction:  60.91054153442383 \n",
      "\n",
      "Iteration 9888, Loss: 36.85698699951172, L1: 10.602302551269531, L3: 26.254684448242188\n",
      "Current prediction:  60.90313720703125 \n",
      "\n",
      "Iteration 9889, Loss: 37.00150680541992, L1: 10.588251113891602, L3: 26.41325569152832\n",
      "Current prediction:  60.89974594116211 \n",
      "\n",
      "Iteration 9890, Loss: 36.81512451171875, L1: 10.59562873840332, L3: 26.219497680664062\n",
      "Current prediction:  60.89065933227539 \n",
      "\n",
      "Iteration 9891, Loss: 37.41978073120117, L1: 10.590837478637695, L3: 26.828943252563477\n",
      "Current prediction:  60.88262939453125 \n",
      "\n",
      "Iteration 9892, Loss: 37.34883499145508, L1: 10.608527183532715, L3: 26.740306854248047\n",
      "Current prediction:  60.88222885131836 \n",
      "\n",
      "Iteration 9893, Loss: 36.409767150878906, L1: 10.606762886047363, L3: 25.803003311157227\n",
      "Current prediction:  60.887760162353516 \n",
      "\n",
      "Iteration 9894, Loss: 37.87382125854492, L1: 10.622326850891113, L3: 27.251493453979492\n",
      "Current prediction:  60.89253616333008 \n",
      "\n",
      "Iteration 9895, Loss: 37.619972229003906, L1: 10.631204605102539, L3: 26.98876953125\n",
      "Current prediction:  60.91183853149414 \n",
      "\n",
      "Iteration 9896, Loss: 38.28291702270508, L1: 10.634943962097168, L3: 27.647972106933594\n",
      "Current prediction:  60.919857025146484 \n",
      "\n",
      "Iteration 9897, Loss: 36.62480926513672, L1: 10.651249885559082, L3: 25.97355842590332\n",
      "Current prediction:  60.94306564331055 \n",
      "\n",
      "Iteration 9898, Loss: 36.237342834472656, L1: 10.634251594543457, L3: 25.603090286254883\n",
      "Current prediction:  60.935829162597656 \n",
      "\n",
      "Iteration 9899, Loss: 36.688697814941406, L1: 10.645955085754395, L3: 26.042741775512695\n",
      "Current prediction:  60.9503173828125 \n",
      "\n",
      "Iteration 9900, Loss: 35.599365234375, L1: 10.65023136138916, L3: 24.949134826660156\n",
      "Current prediction:  60.98890686035156 \n",
      "\n",
      "Iteration 9901, Loss: 37.387489318847656, L1: 10.632678985595703, L3: 26.754812240600586\n",
      "Current prediction:  61.0343132019043 \n",
      "\n",
      "Iteration 9902, Loss: 36.13019561767578, L1: 10.640514373779297, L3: 25.489681243896484\n",
      "Current prediction:  61.091304779052734 \n",
      "\n",
      "Iteration 9903, Loss: 36.48798370361328, L1: 10.64305591583252, L3: 25.844928741455078\n",
      "Current prediction:  61.16890335083008 \n",
      "\n",
      "Iteration 9904, Loss: 37.577205657958984, L1: 10.611130714416504, L3: 26.966075897216797\n",
      "Current prediction:  61.251441955566406 \n",
      "\n",
      "Iteration 9905, Loss: 37.408905029296875, L1: 10.589937210083008, L3: 26.818967819213867\n",
      "Current prediction:  61.30968475341797 \n",
      "\n",
      "Iteration 9906, Loss: 36.29536437988281, L1: 10.580378532409668, L3: 25.71498680114746\n",
      "Current prediction:  61.34379577636719 \n",
      "\n",
      "Iteration 9907, Loss: 37.64811706542969, L1: 10.577034950256348, L3: 27.071081161499023\n",
      "Current prediction:  61.36094284057617 \n",
      "\n",
      "Iteration 9908, Loss: 36.280094146728516, L1: 10.576515197753906, L3: 25.70357894897461\n",
      "Current prediction:  61.39287185668945 \n",
      "\n",
      "Iteration 9909, Loss: 37.40830993652344, L1: 10.545042991638184, L3: 26.863265991210938\n",
      "Current prediction:  61.41057586669922 \n",
      "\n",
      "Iteration 9910, Loss: 37.705718994140625, L1: 10.539043426513672, L3: 27.166675567626953\n",
      "Current prediction:  61.411258697509766 \n",
      "\n",
      "Iteration 9911, Loss: 37.100738525390625, L1: 10.543617248535156, L3: 26.55712127685547\n",
      "Current prediction:  61.413753509521484 \n",
      "\n",
      "Iteration 9912, Loss: 36.02942657470703, L1: 10.551804542541504, L3: 25.47762107849121\n",
      "Current prediction:  61.41214370727539 \n",
      "\n",
      "Iteration 9913, Loss: 36.90882873535156, L1: 10.561878204345703, L3: 26.346952438354492\n",
      "Current prediction:  61.400814056396484 \n",
      "\n",
      "Iteration 9914, Loss: 37.161224365234375, L1: 10.556241035461426, L3: 26.604984283447266\n",
      "Current prediction:  61.387611389160156 \n",
      "\n",
      "Iteration 9915, Loss: 36.65290832519531, L1: 10.547523498535156, L3: 26.105384826660156\n",
      "Current prediction:  61.36984634399414 \n",
      "\n",
      "Iteration 9916, Loss: 37.00260925292969, L1: 10.535014152526855, L3: 26.46759605407715\n",
      "Current prediction:  61.36612319946289 \n",
      "\n",
      "Iteration 9917, Loss: 36.866607666015625, L1: 10.58407211303711, L3: 26.282533645629883\n",
      "Current prediction:  61.361663818359375 \n",
      "\n",
      "Iteration 9918, Loss: 37.51709747314453, L1: 10.586529731750488, L3: 26.93056869506836\n",
      "Current prediction:  61.35982131958008 \n",
      "\n",
      "Iteration 9919, Loss: 36.930023193359375, L1: 10.595312118530273, L3: 26.33470916748047\n",
      "Current prediction:  61.36000061035156 \n",
      "\n",
      "Iteration 9920, Loss: 36.368675231933594, L1: 10.585372924804688, L3: 25.783302307128906\n",
      "Current prediction:  61.371097564697266 \n",
      "\n",
      "Iteration 9921, Loss: 38.18233871459961, L1: 10.587214469909668, L3: 27.595125198364258\n",
      "Current prediction:  61.395668029785156 \n",
      "\n",
      "Iteration 9922, Loss: 35.63773727416992, L1: 10.581814765930176, L3: 25.05592155456543\n",
      "Current prediction:  61.41834259033203 \n",
      "\n",
      "Iteration 9923, Loss: 37.737457275390625, L1: 10.5931978225708, L3: 27.144258499145508\n",
      "Current prediction:  61.426944732666016 \n",
      "\n",
      "Iteration 9924, Loss: 36.870262145996094, L1: 10.601253509521484, L3: 26.269006729125977\n",
      "Current prediction:  61.428890228271484 \n",
      "\n",
      "Iteration 9925, Loss: 36.23989486694336, L1: 10.619539260864258, L3: 25.6203556060791\n",
      "Current prediction:  61.42835998535156 \n",
      "\n",
      "Iteration 9926, Loss: 36.069976806640625, L1: 10.621712684631348, L3: 25.44826316833496\n",
      "Current prediction:  61.4328727722168 \n",
      "\n",
      "Iteration 9927, Loss: 37.0865592956543, L1: 10.595516204833984, L3: 26.491043090820312\n",
      "Current prediction:  61.44087600708008 \n",
      "\n",
      "Iteration 9928, Loss: 37.21003723144531, L1: 10.60030746459961, L3: 26.609731674194336\n",
      "Current prediction:  61.45185089111328 \n",
      "\n",
      "Iteration 9929, Loss: 36.88136672973633, L1: 10.584892272949219, L3: 26.29647445678711\n",
      "Current prediction:  61.46133041381836 \n",
      "\n",
      "Iteration 9930, Loss: 36.64795684814453, L1: 10.572027206420898, L3: 26.075929641723633\n",
      "Current prediction:  61.4725341796875 \n",
      "\n",
      "Iteration 9931, Loss: 36.916786193847656, L1: 10.576001167297363, L3: 26.340784072875977\n",
      "Current prediction:  61.4798698425293 \n",
      "\n",
      "Iteration 9932, Loss: 36.84031677246094, L1: 10.567201614379883, L3: 26.273113250732422\n",
      "Current prediction:  61.48488998413086 \n",
      "\n",
      "Iteration 9933, Loss: 35.85687255859375, L1: 10.555673599243164, L3: 25.301197052001953\n",
      "Current prediction:  61.48963928222656 \n",
      "\n",
      "Iteration 9934, Loss: 37.158050537109375, L1: 10.555671691894531, L3: 26.602380752563477\n",
      "Current prediction:  61.49320983886719 \n",
      "\n",
      "Iteration 9935, Loss: 37.42792510986328, L1: 10.568082809448242, L3: 26.859844207763672\n",
      "Current prediction:  61.494598388671875 \n",
      "\n",
      "Iteration 9936, Loss: 36.36632537841797, L1: 10.539894104003906, L3: 25.826433181762695\n",
      "Current prediction:  61.49566650390625 \n",
      "\n",
      "Iteration 9937, Loss: 37.097694396972656, L1: 10.54323673248291, L3: 26.55445671081543\n",
      "Current prediction:  61.49652862548828 \n",
      "\n",
      "Iteration 9938, Loss: 35.75994873046875, L1: 10.547178268432617, L3: 25.212772369384766\n",
      "Current prediction:  61.498069763183594 \n",
      "\n",
      "Iteration 9939, Loss: 36.411468505859375, L1: 10.532613754272461, L3: 25.878854751586914\n",
      "Current prediction:  61.4981803894043 \n",
      "\n",
      "Iteration 9940, Loss: 35.94069290161133, L1: 10.537080764770508, L3: 25.40361213684082\n",
      "Current prediction:  61.49834060668945 \n",
      "\n",
      "Iteration 9941, Loss: 36.92912673950195, L1: 10.534852027893066, L3: 26.394275665283203\n",
      "Current prediction:  61.500606536865234 \n",
      "\n",
      "Iteration 9942, Loss: 36.135948181152344, L1: 10.531719207763672, L3: 25.604228973388672\n",
      "Current prediction:  61.502784729003906 \n",
      "\n",
      "Iteration 9943, Loss: 37.55783462524414, L1: 10.530762672424316, L3: 27.027070999145508\n",
      "Current prediction:  61.50527572631836 \n",
      "\n",
      "Iteration 9944, Loss: 36.25644302368164, L1: 10.527312278747559, L3: 25.7291316986084\n",
      "Current prediction:  61.50520324707031 \n",
      "\n",
      "Iteration 9945, Loss: 37.34097671508789, L1: 10.52445125579834, L3: 26.816524505615234\n",
      "Current prediction:  61.49870681762695 \n",
      "\n",
      "Iteration 9946, Loss: 37.142677307128906, L1: 10.521378517150879, L3: 26.621299743652344\n",
      "Current prediction:  61.491905212402344 \n",
      "\n",
      "Iteration 9947, Loss: 36.51848602294922, L1: 10.516227722167969, L3: 26.002256393432617\n",
      "Current prediction:  61.4860725402832 \n",
      "\n",
      "Iteration 9948, Loss: 36.3726806640625, L1: 10.526262283325195, L3: 25.846416473388672\n",
      "Current prediction:  61.47783660888672 \n",
      "\n",
      "Iteration 9949, Loss: 36.87001037597656, L1: 10.528535842895508, L3: 26.341474533081055\n",
      "Current prediction:  61.47096252441406 \n",
      "\n",
      "Iteration 9950, Loss: 35.97570037841797, L1: 10.53558349609375, L3: 25.44011878967285\n",
      "Current prediction:  61.463348388671875 \n",
      "\n",
      "Iteration 9951, Loss: 35.91817092895508, L1: 10.54231071472168, L3: 25.3758602142334\n",
      "Current prediction:  61.458404541015625 \n",
      "\n",
      "Iteration 9952, Loss: 37.60572814941406, L1: 10.542326927185059, L3: 27.063400268554688\n",
      "Current prediction:  61.45064163208008 \n",
      "\n",
      "Iteration 9953, Loss: 36.03544616699219, L1: 10.543303489685059, L3: 25.492141723632812\n",
      "Current prediction:  61.447696685791016 \n",
      "\n",
      "Iteration 9954, Loss: 37.025333404541016, L1: 10.549671173095703, L3: 26.475662231445312\n",
      "Current prediction:  61.442649841308594 \n",
      "\n",
      "Iteration 9955, Loss: 37.05681228637695, L1: 10.541558265686035, L3: 26.5152530670166\n",
      "Current prediction:  61.44270706176758 \n",
      "\n",
      "Iteration 9956, Loss: 36.394134521484375, L1: 10.547887802124023, L3: 25.84624671936035\n",
      "Current prediction:  61.437889099121094 \n",
      "\n",
      "Iteration 9957, Loss: 36.848236083984375, L1: 10.54429817199707, L3: 26.303937911987305\n",
      "Current prediction:  61.43210983276367 \n",
      "\n",
      "Iteration 9958, Loss: 36.2966423034668, L1: 10.5440092086792, L3: 25.75263214111328\n",
      "Current prediction:  61.427738189697266 \n",
      "\n",
      "Iteration 9959, Loss: 36.851966857910156, L1: 10.54861831665039, L3: 26.3033504486084\n",
      "Current prediction:  61.423763275146484 \n",
      "\n",
      "Iteration 9960, Loss: 36.86000442504883, L1: 10.557801246643066, L3: 26.302202224731445\n",
      "Current prediction:  61.42079162597656 \n",
      "\n",
      "Iteration 9961, Loss: 36.418922424316406, L1: 10.548611640930176, L3: 25.870311737060547\n",
      "Current prediction:  61.4189453125 \n",
      "\n",
      "Iteration 9962, Loss: 35.05841064453125, L1: 10.54743766784668, L3: 24.510971069335938\n",
      "Current prediction:  61.41953659057617 \n",
      "\n",
      "Iteration 9963, Loss: 36.96234130859375, L1: 10.544631958007812, L3: 26.417709350585938\n",
      "Current prediction:  61.42259979248047 \n",
      "\n",
      "Iteration 9964, Loss: 36.43643569946289, L1: 10.539289474487305, L3: 25.897146224975586\n",
      "Current prediction:  61.42666244506836 \n",
      "\n",
      "Iteration 9965, Loss: 36.2490234375, L1: 10.542476654052734, L3: 25.7065486907959\n",
      "Current prediction:  61.42927169799805 \n",
      "\n",
      "Iteration 9966, Loss: 35.29463577270508, L1: 10.53166675567627, L3: 24.762968063354492\n",
      "Current prediction:  61.432411193847656 \n",
      "\n",
      "Iteration 9967, Loss: 37.11155700683594, L1: 10.526235580444336, L3: 26.5853214263916\n",
      "Current prediction:  61.43217849731445 \n",
      "\n",
      "Iteration 9968, Loss: 37.15536880493164, L1: 10.52774429321289, L3: 26.62762451171875\n",
      "Current prediction:  61.43168640136719 \n",
      "\n",
      "Iteration 9969, Loss: 36.46184158325195, L1: 10.523391723632812, L3: 25.93844985961914\n",
      "Current prediction:  61.42953872680664 \n",
      "\n",
      "Iteration 9970, Loss: 36.02342224121094, L1: 10.515795707702637, L3: 25.507627487182617\n",
      "Current prediction:  61.42879104614258 \n",
      "\n",
      "Iteration 9971, Loss: 35.94252395629883, L1: 10.514153480529785, L3: 25.42837142944336\n",
      "Current prediction:  61.42945098876953 \n",
      "\n",
      "Iteration 9972, Loss: 37.49972152709961, L1: 10.513699531555176, L3: 26.986021041870117\n",
      "Current prediction:  61.42699432373047 \n",
      "\n",
      "Iteration 9973, Loss: 36.646507263183594, L1: 10.516473770141602, L3: 26.13003158569336\n",
      "Current prediction:  61.423072814941406 \n",
      "\n",
      "Iteration 9974, Loss: 36.26215362548828, L1: 10.513684272766113, L3: 25.74846839904785\n",
      "Current prediction:  61.4194221496582 \n",
      "\n",
      "Iteration 9975, Loss: 36.629310607910156, L1: 10.513092994689941, L3: 26.11621856689453\n",
      "Current prediction:  61.41261291503906 \n",
      "\n",
      "Iteration 9976, Loss: 36.22947311401367, L1: 10.527043342590332, L3: 25.702430725097656\n",
      "Current prediction:  61.405818939208984 \n",
      "\n",
      "Iteration 9977, Loss: 36.27893829345703, L1: 10.525226593017578, L3: 25.753713607788086\n",
      "Current prediction:  61.39899826049805 \n",
      "\n",
      "Iteration 9978, Loss: 37.14944839477539, L1: 10.533921241760254, L3: 26.615528106689453\n",
      "Current prediction:  61.392738342285156 \n",
      "\n",
      "Iteration 9979, Loss: 35.6407585144043, L1: 10.535658836364746, L3: 25.105100631713867\n",
      "Current prediction:  61.39047622680664 \n",
      "\n",
      "Iteration 9980, Loss: 37.291648864746094, L1: 10.53432846069336, L3: 26.757322311401367\n",
      "Current prediction:  61.3859748840332 \n",
      "\n",
      "Iteration 9981, Loss: 37.671295166015625, L1: 10.541543960571289, L3: 27.129749298095703\n",
      "Current prediction:  61.379425048828125 \n",
      "\n",
      "Iteration 9982, Loss: 36.1587028503418, L1: 10.537972450256348, L3: 25.620729446411133\n",
      "Current prediction:  61.37606430053711 \n",
      "\n",
      "Iteration 9983, Loss: 36.385353088378906, L1: 10.545977592468262, L3: 25.839374542236328\n",
      "Current prediction:  61.37590408325195 \n",
      "\n",
      "Iteration 9984, Loss: 36.92471694946289, L1: 10.526629447937012, L3: 26.398086547851562\n",
      "Current prediction:  61.3768424987793 \n",
      "\n",
      "Iteration 9985, Loss: 36.96159744262695, L1: 10.537813186645508, L3: 26.423784255981445\n",
      "Current prediction:  61.37465286254883 \n",
      "\n",
      "Iteration 9986, Loss: 37.61211395263672, L1: 10.538782119750977, L3: 27.073331832885742\n",
      "Current prediction:  61.37017059326172 \n",
      "\n",
      "Iteration 9987, Loss: 37.779754638671875, L1: 10.545953750610352, L3: 27.233800888061523\n",
      "Current prediction:  61.359092712402344 \n",
      "\n",
      "Iteration 9988, Loss: 37.86362075805664, L1: 10.716233253479004, L3: 27.14738655090332\n",
      "Current prediction:  61.35123062133789 \n",
      "\n",
      "Iteration 9989, Loss: 37.29944610595703, L1: 10.557204246520996, L3: 26.74224090576172\n",
      "Current prediction:  61.3431396484375 \n",
      "\n",
      "Iteration 9990, Loss: 36.68390655517578, L1: 10.550938606262207, L3: 26.132966995239258\n",
      "Current prediction:  61.33829879760742 \n",
      "\n",
      "Iteration 9991, Loss: 35.88895797729492, L1: 10.56218433380127, L3: 25.32677459716797\n",
      "Current prediction:  61.33259201049805 \n",
      "\n",
      "Iteration 9992, Loss: 37.711692810058594, L1: 10.568408966064453, L3: 27.14328384399414\n",
      "Current prediction:  61.324195861816406 \n",
      "\n",
      "Iteration 9993, Loss: 37.266357421875, L1: 10.569002151489258, L3: 26.69735336303711\n",
      "Current prediction:  61.31828308105469 \n",
      "\n",
      "Iteration 9994, Loss: 36.67414474487305, L1: 10.57474422454834, L3: 26.099401473999023\n",
      "Current prediction:  61.316131591796875 \n",
      "\n",
      "Iteration 9995, Loss: 36.84964370727539, L1: 10.580668449401855, L3: 26.26897430419922\n",
      "Current prediction:  61.31862258911133 \n",
      "\n",
      "Iteration 9996, Loss: 37.048954010009766, L1: 10.578065872192383, L3: 26.470888137817383\n",
      "Current prediction:  61.32554244995117 \n",
      "\n",
      "Iteration 9997, Loss: 36.55767059326172, L1: 10.571279525756836, L3: 25.986392974853516\n",
      "Current prediction:  61.33649444580078 \n",
      "\n",
      "Iteration 9998, Loss: 36.62671661376953, L1: 10.569817543029785, L3: 26.056900024414062\n",
      "Current prediction:  61.3465576171875 \n",
      "\n",
      "â†³ LR reduced to 2.5e-04 at iteration 10000 \n",
      "\n",
      "Iteration 9999, Loss: 36.571807861328125, L1: 10.542292594909668, L3: 26.029516220092773\n",
      "Current prediction:  61.360050201416016 \n",
      "\n",
      "Iteration 10000, Loss: 36.53757095336914, L1: 10.534724235534668, L3: 26.00284767150879\n",
      "Current prediction:  61.36664962768555 \n",
      "\n",
      "Iteration 10001, Loss: 37.272186279296875, L1: 10.526692390441895, L3: 26.745492935180664\n",
      "Current prediction:  61.37263870239258 \n",
      "\n",
      "Iteration 10002, Loss: 36.62267303466797, L1: 10.53663444519043, L3: 26.086040496826172\n",
      "Current prediction:  61.37605667114258 \n",
      "\n",
      "Iteration 10003, Loss: 37.1168327331543, L1: 10.526162147521973, L3: 26.59067153930664\n",
      "Current prediction:  61.37825012207031 \n",
      "\n",
      "Iteration 10004, Loss: 36.276817321777344, L1: 10.522396087646484, L3: 25.75442123413086\n",
      "Current prediction:  61.37896728515625 \n",
      "\n",
      "Iteration 10005, Loss: 36.23371887207031, L1: 10.518967628479004, L3: 25.714752197265625\n",
      "Current prediction:  61.379722595214844 \n",
      "\n",
      "Iteration 10006, Loss: 36.79066467285156, L1: 10.515558242797852, L3: 26.27510643005371\n",
      "Current prediction:  61.38288116455078 \n",
      "\n",
      "Iteration 10007, Loss: 36.220672607421875, L1: 10.507354736328125, L3: 25.71331787109375\n",
      "Current prediction:  61.38611602783203 \n",
      "\n",
      "Iteration 10008, Loss: 35.86962890625, L1: 10.515480041503906, L3: 25.35414695739746\n",
      "Current prediction:  61.389137268066406 \n",
      "\n",
      "Iteration 10009, Loss: 35.91320037841797, L1: 10.503058433532715, L3: 25.41014289855957\n",
      "Current prediction:  61.39159393310547 \n",
      "\n",
      "Iteration 10010, Loss: 36.363563537597656, L1: 10.491883277893066, L3: 25.871681213378906\n",
      "Current prediction:  61.39352798461914 \n",
      "\n",
      "Iteration 10011, Loss: 36.49082565307617, L1: 10.5005521774292, L3: 25.99027442932129\n",
      "Current prediction:  61.394248962402344 \n",
      "\n",
      "Iteration 10012, Loss: 35.66377258300781, L1: 10.507792472839355, L3: 25.155981063842773\n",
      "Current prediction:  61.39379119873047 \n",
      "\n",
      "Iteration 10013, Loss: 37.90903091430664, L1: 10.497450828552246, L3: 27.41158103942871\n",
      "Current prediction:  61.39109802246094 \n",
      "\n",
      "Iteration 10014, Loss: 36.09174346923828, L1: 10.491985321044922, L3: 25.59975814819336\n",
      "Current prediction:  61.386817932128906 \n",
      "\n",
      "Iteration 10015, Loss: 36.41826629638672, L1: 10.553498268127441, L3: 25.864768981933594\n",
      "Current prediction:  61.38347244262695 \n",
      "\n",
      "Iteration 10016, Loss: 37.00810241699219, L1: 10.496882438659668, L3: 26.511219024658203\n",
      "Current prediction:  61.37989807128906 \n",
      "\n",
      "Iteration 10017, Loss: 36.644920349121094, L1: 10.503789901733398, L3: 26.141128540039062\n",
      "Current prediction:  61.37599563598633 \n",
      "\n",
      "Iteration 10018, Loss: 37.58421325683594, L1: 10.508304595947266, L3: 27.075908660888672\n",
      "Current prediction:  61.37195587158203 \n",
      "\n",
      "Iteration 10019, Loss: 36.387306213378906, L1: 10.51209545135498, L3: 25.875211715698242\n",
      "Current prediction:  61.36558151245117 \n",
      "\n",
      "Iteration 10020, Loss: 36.66342544555664, L1: 10.530582427978516, L3: 26.132843017578125\n",
      "Current prediction:  61.361270904541016 \n",
      "\n",
      "Iteration 10021, Loss: 36.230003356933594, L1: 10.522838592529297, L3: 25.707162857055664\n",
      "Current prediction:  61.35628890991211 \n",
      "\n",
      "Iteration 10022, Loss: 36.03955841064453, L1: 10.520246505737305, L3: 25.519311904907227\n",
      "Current prediction:  61.35505676269531 \n",
      "\n",
      "Iteration 10023, Loss: 37.373321533203125, L1: 10.530630111694336, L3: 26.842693328857422\n",
      "Current prediction:  61.35208511352539 \n",
      "\n",
      "Iteration 10024, Loss: 36.9034423828125, L1: 10.521742820739746, L3: 26.38170051574707\n",
      "Current prediction:  61.349891662597656 \n",
      "\n",
      "Iteration 10025, Loss: 37.125728607177734, L1: 10.533110618591309, L3: 26.592618942260742\n",
      "Current prediction:  61.34693145751953 \n",
      "\n",
      "Iteration 10026, Loss: 36.76097106933594, L1: 10.53918170928955, L3: 26.22178840637207\n",
      "Current prediction:  61.34172439575195 \n",
      "\n",
      "Iteration 10027, Loss: 37.515106201171875, L1: 10.538040161132812, L3: 26.977067947387695\n",
      "Current prediction:  61.33698272705078 \n",
      "\n",
      "Iteration 10028, Loss: 37.045989990234375, L1: 10.550039291381836, L3: 26.49595069885254\n",
      "Current prediction:  61.33595657348633 \n",
      "\n",
      "Iteration 10029, Loss: 35.82915115356445, L1: 10.540513038635254, L3: 25.288637161254883\n",
      "Current prediction:  61.33543014526367 \n",
      "\n",
      "Iteration 10030, Loss: 37.270286560058594, L1: 10.54063606262207, L3: 26.729652404785156\n",
      "Current prediction:  61.33464431762695 \n",
      "\n",
      "Iteration 10031, Loss: 37.45396041870117, L1: 10.546873092651367, L3: 26.907087326049805\n",
      "Current prediction:  61.3338508605957 \n",
      "\n",
      "Iteration 10032, Loss: 36.71311950683594, L1: 10.534748077392578, L3: 26.178373336791992\n",
      "Current prediction:  61.33254623413086 \n",
      "\n",
      "Iteration 10033, Loss: 36.0832633972168, L1: 10.540019989013672, L3: 25.543243408203125\n",
      "Current prediction:  61.33106994628906 \n",
      "\n",
      "Iteration 10034, Loss: 36.38664245605469, L1: 10.537941932678223, L3: 25.84869956970215\n",
      "Current prediction:  61.331851959228516 \n",
      "\n",
      "Iteration 10035, Loss: 36.015403747558594, L1: 10.531867980957031, L3: 25.483535766601562\n",
      "Current prediction:  61.33270263671875 \n",
      "\n",
      "Iteration 10036, Loss: 36.77891540527344, L1: 10.540140151977539, L3: 26.2387752532959\n",
      "Current prediction:  61.333740234375 \n",
      "\n",
      "Iteration 10037, Loss: 36.75447082519531, L1: 10.532706260681152, L3: 26.221765518188477\n",
      "Current prediction:  61.332054138183594 \n",
      "\n",
      "Iteration 10038, Loss: 36.54350280761719, L1: 10.5464448928833, L3: 25.99705696105957\n",
      "Current prediction:  61.329681396484375 \n",
      "\n",
      "Iteration 10039, Loss: 36.183860778808594, L1: 10.534530639648438, L3: 25.64933204650879\n",
      "Current prediction:  61.327880859375 \n",
      "\n",
      "Iteration 10040, Loss: 37.02617263793945, L1: 10.539613723754883, L3: 26.48655891418457\n",
      "Current prediction:  61.328182220458984 \n",
      "\n",
      "Iteration 10041, Loss: 36.572391510009766, L1: 10.546512603759766, L3: 26.02587890625\n",
      "Current prediction:  61.32849884033203 \n",
      "\n",
      "Iteration 10042, Loss: 35.7228889465332, L1: 10.54286003112793, L3: 25.180028915405273\n",
      "Current prediction:  61.32972717285156 \n",
      "\n",
      "Iteration 10043, Loss: 36.610198974609375, L1: 10.540794372558594, L3: 26.069406509399414\n",
      "Current prediction:  61.33005905151367 \n",
      "\n",
      "Iteration 10044, Loss: 35.650821685791016, L1: 10.549434661865234, L3: 25.10138702392578\n",
      "Current prediction:  61.332271575927734 \n",
      "\n",
      "Iteration 10045, Loss: 35.58079528808594, L1: 10.539752960205078, L3: 25.041044235229492\n",
      "Current prediction:  61.33501052856445 \n",
      "\n",
      "Iteration 10046, Loss: 36.88868713378906, L1: 10.529064178466797, L3: 26.3596248626709\n",
      "Current prediction:  61.334815979003906 \n",
      "\n",
      "Iteration 10047, Loss: 36.934383392333984, L1: 10.529977798461914, L3: 26.40440559387207\n",
      "Current prediction:  61.3337287902832 \n",
      "\n",
      "Iteration 10048, Loss: 36.78875732421875, L1: 10.539077758789062, L3: 26.249677658081055\n",
      "Current prediction:  61.333553314208984 \n",
      "\n",
      "Iteration 10049, Loss: 36.074012756347656, L1: 10.537623405456543, L3: 25.536388397216797\n",
      "Current prediction:  61.33415222167969 \n",
      "\n",
      "Iteration 10050, Loss: 36.399757385253906, L1: 10.540143966674805, L3: 25.859615325927734\n",
      "Current prediction:  61.33403778076172 \n",
      "\n",
      "Iteration 10051, Loss: 37.720359802246094, L1: 10.534262657165527, L3: 27.18609619140625\n",
      "Current prediction:  61.33253860473633 \n",
      "\n",
      "Iteration 10052, Loss: 35.51689910888672, L1: 10.540360450744629, L3: 24.976537704467773\n",
      "Current prediction:  61.332847595214844 \n",
      "\n",
      "Iteration 10053, Loss: 36.55744171142578, L1: 10.534079551696777, L3: 26.02336311340332\n",
      "Current prediction:  61.332176208496094 \n",
      "\n",
      "Iteration 10054, Loss: 37.23670959472656, L1: 10.534445762634277, L3: 26.7022647857666\n",
      "Current prediction:  61.33075714111328 \n",
      "\n",
      "Iteration 10055, Loss: 36.38585662841797, L1: 10.528207778930664, L3: 25.857648849487305\n",
      "Current prediction:  61.32785415649414 \n",
      "\n",
      "Iteration 10056, Loss: 36.42047119140625, L1: 10.53419017791748, L3: 25.886281967163086\n",
      "Current prediction:  61.32675552368164 \n",
      "\n",
      "Iteration 10057, Loss: 37.028385162353516, L1: 10.528873443603516, L3: 26.49951171875\n",
      "Current prediction:  61.323795318603516 \n",
      "\n",
      "Iteration 10058, Loss: 37.87028884887695, L1: 10.532604217529297, L3: 27.337684631347656\n",
      "Current prediction:  61.31725311279297 \n",
      "\n",
      "Iteration 10059, Loss: 37.90898895263672, L1: 10.535697937011719, L3: 27.373292922973633\n",
      "Current prediction:  61.310829162597656 \n",
      "\n",
      "Iteration 10060, Loss: 36.24530792236328, L1: 10.553653717041016, L3: 25.691654205322266\n",
      "Current prediction:  61.30781936645508 \n",
      "\n",
      "Iteration 10061, Loss: 37.47727966308594, L1: 10.548094749450684, L3: 26.929183959960938\n",
      "Current prediction:  61.30469512939453 \n",
      "\n",
      "Iteration 10062, Loss: 36.91012191772461, L1: 10.549508094787598, L3: 26.360612869262695\n",
      "Current prediction:  61.30088424682617 \n",
      "\n",
      "Iteration 10063, Loss: 36.61846923828125, L1: 10.566093444824219, L3: 26.052377700805664\n",
      "Current prediction:  61.2973747253418 \n",
      "\n",
      "Iteration 10064, Loss: 36.930564880371094, L1: 10.564014434814453, L3: 26.366552352905273\n",
      "Current prediction:  61.294464111328125 \n",
      "\n",
      "Iteration 10065, Loss: 37.71563720703125, L1: 10.558614730834961, L3: 27.157024383544922\n",
      "Current prediction:  61.29227066040039 \n",
      "\n",
      "Iteration 10066, Loss: 37.261409759521484, L1: 10.55573558807373, L3: 26.70567512512207\n",
      "Current prediction:  61.289939880371094 \n",
      "\n",
      "Iteration 10067, Loss: 36.38282775878906, L1: 10.55941390991211, L3: 25.823415756225586\n",
      "Current prediction:  61.28817367553711 \n",
      "\n",
      "Iteration 10068, Loss: 36.989192962646484, L1: 10.560425758361816, L3: 26.428768157958984\n",
      "Current prediction:  61.2886962890625 \n",
      "\n",
      "Iteration 10069, Loss: 37.84977340698242, L1: 10.55809211730957, L3: 27.29168128967285\n",
      "Current prediction:  61.289554595947266 \n",
      "\n",
      "Iteration 10070, Loss: 36.337100982666016, L1: 10.567495346069336, L3: 25.76960563659668\n",
      "Current prediction:  61.291934967041016 \n",
      "\n",
      "Iteration 10071, Loss: 36.86068344116211, L1: 10.556771278381348, L3: 26.303913116455078\n",
      "Current prediction:  61.294677734375 \n",
      "\n",
      "Iteration 10072, Loss: 36.04603958129883, L1: 10.576248168945312, L3: 25.469791412353516\n",
      "Current prediction:  61.29795837402344 \n",
      "\n",
      "Iteration 10073, Loss: 36.41514587402344, L1: 10.562813758850098, L3: 25.852331161499023\n",
      "Current prediction:  61.301143646240234 \n",
      "\n",
      "Iteration 10074, Loss: 35.865047454833984, L1: 10.550108909606934, L3: 25.314937591552734\n",
      "Current prediction:  61.30418395996094 \n",
      "\n",
      "Iteration 10075, Loss: 36.741939544677734, L1: 10.549129486083984, L3: 26.19281005859375\n",
      "Current prediction:  61.30629348754883 \n",
      "\n",
      "Iteration 10076, Loss: 37.304500579833984, L1: 10.54284381866455, L3: 26.761655807495117\n",
      "Current prediction:  61.309051513671875 \n",
      "\n",
      "Iteration 10077, Loss: 37.0612678527832, L1: 10.540131568908691, L3: 26.521137237548828\n",
      "Current prediction:  61.31169509887695 \n",
      "\n",
      "Iteration 10078, Loss: 37.46266174316406, L1: 10.542724609375, L3: 26.919937133789062\n",
      "Current prediction:  61.31276321411133 \n",
      "\n",
      "Iteration 10079, Loss: 35.99772262573242, L1: 10.5381498336792, L3: 25.45957374572754\n",
      "Current prediction:  61.3126106262207 \n",
      "\n",
      "Iteration 10080, Loss: 36.10383605957031, L1: 10.549854278564453, L3: 25.553979873657227\n",
      "Current prediction:  61.313777923583984 \n",
      "\n",
      "Iteration 10081, Loss: 36.22782516479492, L1: 10.548134803771973, L3: 25.679691314697266\n",
      "Current prediction:  61.316307067871094 \n",
      "\n",
      "Iteration 10082, Loss: 36.69205093383789, L1: 10.535731315612793, L3: 26.15631866455078\n",
      "Current prediction:  61.31897735595703 \n",
      "\n",
      "Iteration 10083, Loss: 36.462013244628906, L1: 10.53339672088623, L3: 25.92861557006836\n",
      "Current prediction:  61.322689056396484 \n",
      "\n",
      "Iteration 10084, Loss: 36.355045318603516, L1: 10.528799057006836, L3: 25.82624626159668\n",
      "Current prediction:  61.32625198364258 \n",
      "\n",
      "Iteration 10085, Loss: 37.18212127685547, L1: 10.5239839553833, L3: 26.658138275146484\n",
      "Current prediction:  61.33049774169922 \n",
      "\n",
      "Iteration 10086, Loss: 36.32164764404297, L1: 10.526969909667969, L3: 25.794675827026367\n",
      "Current prediction:  61.33332061767578 \n",
      "\n",
      "Iteration 10087, Loss: 37.86980056762695, L1: 10.520015716552734, L3: 27.34978485107422\n",
      "Current prediction:  61.33647155761719 \n",
      "\n",
      "Iteration 10088, Loss: 36.983856201171875, L1: 10.51834774017334, L3: 26.46550941467285\n",
      "Current prediction:  61.33808898925781 \n",
      "\n",
      "Iteration 10089, Loss: 37.70949172973633, L1: 10.526451110839844, L3: 27.183040618896484\n",
      "Current prediction:  61.33949279785156 \n",
      "\n",
      "Iteration 10090, Loss: 35.338905334472656, L1: 10.523000717163086, L3: 24.81590461730957\n",
      "Current prediction:  61.34162139892578 \n",
      "\n",
      "Iteration 10091, Loss: 36.66114044189453, L1: 10.509607315063477, L3: 26.151535034179688\n",
      "Current prediction:  61.3442497253418 \n",
      "\n",
      "Iteration 10092, Loss: 37.44725036621094, L1: 10.520596504211426, L3: 26.926652908325195\n",
      "Current prediction:  61.34764862060547 \n",
      "\n",
      "Iteration 10093, Loss: 36.396480560302734, L1: 10.506128311157227, L3: 25.890352249145508\n",
      "Current prediction:  61.349884033203125 \n",
      "\n",
      "Iteration 10094, Loss: 36.91481399536133, L1: 10.519645690917969, L3: 26.39516830444336\n",
      "Current prediction:  61.35368728637695 \n",
      "\n",
      "Iteration 10095, Loss: 36.805908203125, L1: 10.512925148010254, L3: 26.292984008789062\n",
      "Current prediction:  61.357242584228516 \n",
      "\n",
      "Iteration 10096, Loss: 36.64836120605469, L1: 10.499395370483398, L3: 26.14896583557129\n",
      "Current prediction:  61.359825134277344 \n",
      "\n",
      "Iteration 10097, Loss: 36.566001892089844, L1: 10.511353492736816, L3: 26.054649353027344\n",
      "Current prediction:  61.360321044921875 \n",
      "\n",
      "Iteration 10098, Loss: 36.679725646972656, L1: 10.506566047668457, L3: 26.173160552978516\n",
      "Current prediction:  61.361610412597656 \n",
      "\n",
      "Iteration 10099, Loss: 36.33151626586914, L1: 10.505287170410156, L3: 25.826229095458984\n",
      "Current prediction:  61.36472702026367 \n",
      "\n",
      "Iteration 10100, Loss: 35.795928955078125, L1: 10.48909854888916, L3: 25.30682945251465\n",
      "Current prediction:  61.36601638793945 \n",
      "\n",
      "Iteration 10101, Loss: 37.38204574584961, L1: 10.487250328063965, L3: 26.89479637145996\n",
      "Current prediction:  61.36430740356445 \n",
      "\n",
      "Iteration 10102, Loss: 36.907901763916016, L1: 10.485404968261719, L3: 26.422496795654297\n",
      "Current prediction:  61.362613677978516 \n",
      "\n",
      "Iteration 10103, Loss: 36.47557067871094, L1: 10.492011070251465, L3: 25.983558654785156\n",
      "Current prediction:  61.36274337768555 \n",
      "\n",
      "Iteration 10104, Loss: 36.00300216674805, L1: 10.493332862854004, L3: 25.50967025756836\n",
      "Current prediction:  61.362083435058594 \n",
      "\n",
      "Iteration 10105, Loss: 36.371368408203125, L1: 10.50468921661377, L3: 25.86667823791504\n",
      "Current prediction:  61.357635498046875 \n",
      "\n",
      "Iteration 10106, Loss: 36.71277618408203, L1: 10.504484176635742, L3: 26.208293914794922\n",
      "Current prediction:  61.353424072265625 \n",
      "\n",
      "Iteration 10107, Loss: 36.90912628173828, L1: 10.50894546508789, L3: 26.40018081665039\n",
      "Current prediction:  61.351253509521484 \n",
      "\n",
      "Iteration 10108, Loss: 37.157562255859375, L1: 10.514029502868652, L3: 26.64353370666504\n",
      "Current prediction:  61.34743881225586 \n",
      "\n",
      "Iteration 10109, Loss: 36.806434631347656, L1: 10.50047492980957, L3: 26.30596160888672\n",
      "Current prediction:  61.34223937988281 \n",
      "\n",
      "Iteration 10110, Loss: 37.361732482910156, L1: 10.613779067993164, L3: 26.747955322265625\n",
      "Current prediction:  61.33845901489258 \n",
      "\n",
      "Iteration 10111, Loss: 37.50855255126953, L1: 10.508089065551758, L3: 27.000465393066406\n",
      "Current prediction:  61.333683013916016 \n",
      "\n",
      "Iteration 10112, Loss: 36.51875305175781, L1: 10.521124839782715, L3: 25.997629165649414\n",
      "Current prediction:  61.32867431640625 \n",
      "\n",
      "Iteration 10113, Loss: 37.325069427490234, L1: 10.525202751159668, L3: 26.79986572265625\n",
      "Current prediction:  61.3222770690918 \n",
      "\n",
      "Iteration 10114, Loss: 36.23746109008789, L1: 10.526220321655273, L3: 25.711240768432617\n",
      "Current prediction:  61.316612243652344 \n",
      "\n",
      "Iteration 10115, Loss: 36.06684112548828, L1: 10.530085563659668, L3: 25.53675651550293\n",
      "Current prediction:  61.31160354614258 \n",
      "\n",
      "Iteration 10116, Loss: 37.18423843383789, L1: 10.54392147064209, L3: 26.640317916870117\n",
      "Current prediction:  61.305850982666016 \n",
      "\n",
      "Iteration 10117, Loss: 36.28687286376953, L1: 10.531405448913574, L3: 25.75546646118164\n",
      "Current prediction:  61.301605224609375 \n",
      "\n",
      "Iteration 10118, Loss: 36.858192443847656, L1: 10.541646957397461, L3: 26.316547393798828\n",
      "Current prediction:  61.297847747802734 \n",
      "\n",
      "Iteration 10119, Loss: 37.08381652832031, L1: 10.543306350708008, L3: 26.540510177612305\n",
      "Current prediction:  61.29253387451172 \n",
      "\n",
      "Iteration 10120, Loss: 37.402095794677734, L1: 10.555066108703613, L3: 26.847028732299805\n",
      "Current prediction:  61.28795623779297 \n",
      "\n",
      "Iteration 10121, Loss: 36.3400764465332, L1: 10.555590629577637, L3: 25.78448486328125\n",
      "Current prediction:  61.28615951538086 \n",
      "\n",
      "Iteration 10122, Loss: 36.474483489990234, L1: 10.548310279846191, L3: 25.926172256469727\n",
      "Current prediction:  61.28474044799805 \n",
      "\n",
      "Iteration 10123, Loss: 36.925514221191406, L1: 10.558331489562988, L3: 26.367183685302734\n",
      "Current prediction:  61.28318405151367 \n",
      "\n",
      "Iteration 10124, Loss: 36.4200553894043, L1: 10.566023826599121, L3: 25.85403060913086\n",
      "Current prediction:  61.284114837646484 \n",
      "\n",
      "Iteration 10125, Loss: 37.17744827270508, L1: 10.561885833740234, L3: 26.615562438964844\n",
      "Current prediction:  61.28384017944336 \n",
      "\n",
      "Iteration 10126, Loss: 35.24345016479492, L1: 10.553025245666504, L3: 24.690425872802734\n",
      "Current prediction:  61.28716278076172 \n",
      "\n",
      "Iteration 10127, Loss: 36.588783264160156, L1: 10.541438102722168, L3: 26.047346115112305\n",
      "Current prediction:  61.29080581665039 \n",
      "\n",
      "Iteration 10128, Loss: 36.737796783447266, L1: 10.540962219238281, L3: 26.196834564208984\n",
      "Current prediction:  61.29367446899414 \n",
      "\n",
      "Iteration 10129, Loss: 35.478816986083984, L1: 10.551796913146973, L3: 24.927019119262695\n",
      "Current prediction:  61.29567337036133 \n",
      "\n",
      "Iteration 10130, Loss: 36.55031204223633, L1: 10.55617618560791, L3: 25.994136810302734\n",
      "Current prediction:  61.29807662963867 \n",
      "\n",
      "Iteration 10131, Loss: 35.679813385009766, L1: 10.542381286621094, L3: 25.137432098388672\n",
      "Current prediction:  61.302330017089844 \n",
      "\n",
      "Iteration 10132, Loss: 36.62504577636719, L1: 10.53720474243164, L3: 26.087839126586914\n",
      "Current prediction:  61.30588150024414 \n",
      "\n",
      "Iteration 10133, Loss: 36.851417541503906, L1: 10.539400100708008, L3: 26.31201934814453\n",
      "Current prediction:  61.307945251464844 \n",
      "\n",
      "Iteration 10134, Loss: 35.969810485839844, L1: 10.527833938598633, L3: 25.441978454589844\n",
      "Current prediction:  61.30718994140625 \n",
      "\n",
      "Iteration 10135, Loss: 36.276222229003906, L1: 10.529121398925781, L3: 25.747102737426758\n",
      "Current prediction:  61.307804107666016 \n",
      "\n",
      "Iteration 10136, Loss: 36.302215576171875, L1: 10.530387878417969, L3: 25.771825790405273\n",
      "Current prediction:  61.308128356933594 \n",
      "\n",
      "Iteration 10137, Loss: 36.96546173095703, L1: 10.538131713867188, L3: 26.427331924438477\n",
      "Current prediction:  61.308807373046875 \n",
      "\n",
      "Iteration 10138, Loss: 36.67821502685547, L1: 10.525045394897461, L3: 26.153167724609375\n",
      "Current prediction:  61.31019973754883 \n",
      "\n",
      "Iteration 10139, Loss: 37.56079864501953, L1: 10.527666091918945, L3: 27.033130645751953\n",
      "Current prediction:  61.31064224243164 \n",
      "\n",
      "Iteration 10140, Loss: 37.27513885498047, L1: 10.53132152557373, L3: 26.743818283081055\n",
      "Current prediction:  61.310150146484375 \n",
      "\n",
      "Iteration 10141, Loss: 36.117950439453125, L1: 10.524947166442871, L3: 25.59300422668457\n",
      "Current prediction:  61.31142807006836 \n",
      "\n",
      "Iteration 10142, Loss: 36.644622802734375, L1: 10.5269775390625, L3: 26.117643356323242\n",
      "Current prediction:  61.31173324584961 \n",
      "\n",
      "Iteration 10143, Loss: 36.95555114746094, L1: 10.535143852233887, L3: 26.420406341552734\n",
      "Current prediction:  61.312442779541016 \n",
      "\n",
      "Iteration 10144, Loss: 36.24422836303711, L1: 10.521941184997559, L3: 25.722288131713867\n",
      "Current prediction:  61.312191009521484 \n",
      "\n",
      "Iteration 10145, Loss: 36.68265914916992, L1: 10.541645050048828, L3: 26.141014099121094\n",
      "Current prediction:  61.31279754638672 \n",
      "\n",
      "Iteration 10146, Loss: 37.60127258300781, L1: 10.530855178833008, L3: 27.070415496826172\n",
      "Current prediction:  61.3138542175293 \n",
      "\n",
      "Iteration 10147, Loss: 35.99168014526367, L1: 10.518149375915527, L3: 25.47353172302246\n",
      "Current prediction:  61.316776275634766 \n",
      "\n",
      "Iteration 10148, Loss: 36.462074279785156, L1: 10.517467498779297, L3: 25.944608688354492\n",
      "Current prediction:  61.319374084472656 \n",
      "\n",
      "Iteration 10149, Loss: 36.241764068603516, L1: 10.503830909729004, L3: 25.737934112548828\n",
      "Current prediction:  61.321964263916016 \n",
      "\n",
      "Iteration 10150, Loss: 35.86578369140625, L1: 10.533119201660156, L3: 25.332666397094727\n",
      "Current prediction:  61.32448959350586 \n",
      "\n",
      "Iteration 10151, Loss: 35.64735794067383, L1: 10.503561973571777, L3: 25.143795013427734\n",
      "Current prediction:  61.3266487121582 \n",
      "\n",
      "Iteration 10152, Loss: 36.399681091308594, L1: 10.508726119995117, L3: 25.89095687866211\n",
      "Current prediction:  61.32844543457031 \n",
      "\n",
      "Iteration 10153, Loss: 37.093475341796875, L1: 10.513459205627441, L3: 26.580015182495117\n",
      "Current prediction:  61.32981491088867 \n",
      "\n",
      "Iteration 10154, Loss: 36.44711685180664, L1: 10.518956184387207, L3: 25.92816162109375\n",
      "Current prediction:  61.33027648925781 \n",
      "\n",
      "Iteration 10155, Loss: 36.53090286254883, L1: 10.518484115600586, L3: 26.012418746948242\n",
      "Current prediction:  61.32937240600586 \n",
      "\n",
      "Iteration 10156, Loss: 37.8336067199707, L1: 10.506611824035645, L3: 27.326993942260742\n",
      "Current prediction:  61.32868576049805 \n",
      "\n",
      "Iteration 10157, Loss: 36.3332633972168, L1: 10.505685806274414, L3: 25.827577590942383\n",
      "Current prediction:  61.32574462890625 \n",
      "\n",
      "Iteration 10158, Loss: 36.987342834472656, L1: 10.532764434814453, L3: 26.454578399658203\n",
      "Current prediction:  61.321861267089844 \n",
      "\n",
      "Iteration 10159, Loss: 36.46767044067383, L1: 10.526019096374512, L3: 25.941650390625\n",
      "Current prediction:  61.31663513183594 \n",
      "\n",
      "Iteration 10160, Loss: 35.958473205566406, L1: 10.509490966796875, L3: 25.44898223876953\n",
      "Current prediction:  61.3140983581543 \n",
      "\n",
      "Iteration 10161, Loss: 36.725730895996094, L1: 10.514646530151367, L3: 26.211082458496094\n",
      "Current prediction:  61.30923843383789 \n",
      "\n",
      "Iteration 10162, Loss: 36.70085906982422, L1: 10.519752502441406, L3: 26.181108474731445\n",
      "Current prediction:  61.30397033691406 \n",
      "\n",
      "Iteration 10163, Loss: 36.850181579589844, L1: 10.517895698547363, L3: 26.332286834716797\n",
      "Current prediction:  61.2967414855957 \n",
      "\n",
      "Iteration 10164, Loss: 35.879005432128906, L1: 10.538734436035156, L3: 25.340269088745117\n",
      "Current prediction:  61.289798736572266 \n",
      "\n",
      "Iteration 10165, Loss: 36.66316223144531, L1: 10.544975280761719, L3: 26.118186950683594\n",
      "Current prediction:  61.283355712890625 \n",
      "\n",
      "Iteration 10166, Loss: 36.844520568847656, L1: 10.564384460449219, L3: 26.28013801574707\n",
      "Current prediction:  61.27823257446289 \n",
      "\n",
      "Iteration 10167, Loss: 37.2518424987793, L1: 10.54745864868164, L3: 26.704383850097656\n",
      "Current prediction:  61.27323532104492 \n",
      "\n",
      "Iteration 10168, Loss: 35.90422439575195, L1: 10.550688743591309, L3: 25.35353660583496\n",
      "Current prediction:  61.268592834472656 \n",
      "\n",
      "Iteration 10169, Loss: 36.899410247802734, L1: 10.565753936767578, L3: 26.333656311035156\n",
      "Current prediction:  61.266021728515625 \n",
      "\n",
      "Iteration 10170, Loss: 35.46175003051758, L1: 10.547141075134277, L3: 24.914608001708984\n",
      "Current prediction:  61.26612854003906 \n",
      "\n",
      "Iteration 10171, Loss: 36.609745025634766, L1: 10.566740989685059, L3: 26.043004989624023\n",
      "Current prediction:  61.2679557800293 \n",
      "\n",
      "Iteration 10172, Loss: 37.381351470947266, L1: 10.544371604919434, L3: 26.83698081970215\n",
      "Current prediction:  61.26924133300781 \n",
      "\n",
      "Iteration 10173, Loss: 35.86829376220703, L1: 10.546483993530273, L3: 25.32181167602539\n",
      "Current prediction:  61.27180480957031 \n",
      "\n",
      "Iteration 10174, Loss: 37.787933349609375, L1: 10.557351112365723, L3: 27.230581283569336\n",
      "Current prediction:  61.27633285522461 \n",
      "\n",
      "Iteration 10175, Loss: 35.89669418334961, L1: 10.555333137512207, L3: 25.34136199951172\n",
      "Current prediction:  61.27876281738281 \n",
      "\n",
      "Iteration 10176, Loss: 36.94261932373047, L1: 10.596357345581055, L3: 26.34626007080078\n",
      "Current prediction:  61.28322982788086 \n",
      "\n",
      "Iteration 10177, Loss: 36.289466857910156, L1: 10.545781135559082, L3: 25.74368667602539\n",
      "Current prediction:  61.28836441040039 \n",
      "\n",
      "Iteration 10178, Loss: 35.84755325317383, L1: 10.541784286499023, L3: 25.305768966674805\n",
      "Current prediction:  61.29460525512695 \n",
      "\n",
      "Iteration 10179, Loss: 36.726627349853516, L1: 10.541548728942871, L3: 26.185077667236328\n",
      "Current prediction:  61.299888610839844 \n",
      "\n",
      "Iteration 10180, Loss: 37.366371154785156, L1: 10.533397674560547, L3: 26.832975387573242\n",
      "Current prediction:  61.30123519897461 \n",
      "\n",
      "Iteration 10181, Loss: 36.327789306640625, L1: 10.52714729309082, L3: 25.800643920898438\n",
      "Current prediction:  61.30241012573242 \n",
      "\n",
      "Iteration 10182, Loss: 36.177650451660156, L1: 10.525569915771484, L3: 25.65207862854004\n",
      "Current prediction:  61.30270767211914 \n",
      "\n",
      "Iteration 10183, Loss: 35.5458984375, L1: 10.516563415527344, L3: 25.02933692932129\n",
      "Current prediction:  61.30527877807617 \n",
      "\n",
      "Iteration 10184, Loss: 35.88081741333008, L1: 10.542511940002441, L3: 25.338306427001953\n",
      "Current prediction:  61.30576705932617 \n",
      "\n",
      "Iteration 10185, Loss: 37.23799514770508, L1: 10.517729759216309, L3: 26.720266342163086\n",
      "Current prediction:  61.30630111694336 \n",
      "\n",
      "Iteration 10186, Loss: 36.509742736816406, L1: 10.505634307861328, L3: 26.004108428955078\n",
      "Current prediction:  61.30582046508789 \n",
      "\n",
      "Iteration 10187, Loss: 36.15387725830078, L1: 10.523850440979004, L3: 25.63002586364746\n",
      "Current prediction:  61.3068733215332 \n",
      "\n",
      "Iteration 10188, Loss: 36.642459869384766, L1: 10.533562660217285, L3: 26.108898162841797\n",
      "Current prediction:  61.308467864990234 \n",
      "\n",
      "Iteration 10189, Loss: 36.80290985107422, L1: 10.538002014160156, L3: 26.26490592956543\n",
      "Current prediction:  61.30917739868164 \n",
      "\n",
      "Iteration 10190, Loss: 36.71376037597656, L1: 10.519671440124512, L3: 26.194089889526367\n",
      "Current prediction:  61.30936813354492 \n",
      "\n",
      "Iteration 10191, Loss: 36.446380615234375, L1: 10.524994850158691, L3: 25.92138671875\n",
      "Current prediction:  61.3098258972168 \n",
      "\n",
      "Iteration 10192, Loss: 35.900733947753906, L1: 10.499085426330566, L3: 25.401647567749023\n",
      "Current prediction:  61.31208801269531 \n",
      "\n",
      "Iteration 10193, Loss: 36.99015808105469, L1: 10.503665924072266, L3: 26.486494064331055\n",
      "Current prediction:  61.312744140625 \n",
      "\n",
      "Iteration 10194, Loss: 35.481040954589844, L1: 10.50749397277832, L3: 24.973548889160156\n",
      "Current prediction:  61.31578063964844 \n",
      "\n",
      "Iteration 10195, Loss: 35.94081115722656, L1: 10.495320320129395, L3: 25.445491790771484\n",
      "Current prediction:  61.31663513183594 \n",
      "\n",
      "Iteration 10196, Loss: 36.981136322021484, L1: 10.51843547821045, L3: 26.46269989013672\n",
      "Current prediction:  61.31623840332031 \n",
      "\n",
      "Iteration 10197, Loss: 37.82991027832031, L1: 10.497875213623047, L3: 27.3320369720459\n",
      "Current prediction:  61.31544876098633 \n",
      "\n",
      "Iteration 10198, Loss: 36.129783630371094, L1: 10.51789665222168, L3: 25.61188507080078\n",
      "Current prediction:  61.312191009521484 \n",
      "\n",
      "Iteration 10199, Loss: 36.40129089355469, L1: 10.501769065856934, L3: 25.899520874023438\n",
      "Current prediction:  61.306312561035156 \n",
      "\n",
      "Iteration 10200, Loss: 36.82278823852539, L1: 10.510605812072754, L3: 26.31218147277832\n",
      "Current prediction:  61.301116943359375 \n",
      "\n",
      "Iteration 10201, Loss: 35.27204513549805, L1: 10.544405937194824, L3: 24.72764015197754\n",
      "Current prediction:  61.29405212402344 \n",
      "\n",
      "Iteration 10202, Loss: 36.593379974365234, L1: 10.520491600036621, L3: 26.07288932800293\n",
      "Current prediction:  61.28977584838867 \n",
      "\n",
      "Iteration 10203, Loss: 36.7628173828125, L1: 10.532581329345703, L3: 26.23023796081543\n",
      "Current prediction:  61.28527069091797 \n",
      "\n",
      "Iteration 10204, Loss: 36.28327941894531, L1: 10.524002075195312, L3: 25.75927734375\n",
      "Current prediction:  61.28107452392578 \n",
      "\n",
      "Iteration 10205, Loss: 37.25843811035156, L1: 10.53941822052002, L3: 26.719018936157227\n",
      "Current prediction:  61.27576446533203 \n",
      "\n",
      "Iteration 10206, Loss: 38.02139663696289, L1: 10.533105850219727, L3: 27.488290786743164\n",
      "Current prediction:  61.26810836791992 \n",
      "\n",
      "Iteration 10207, Loss: 36.39712905883789, L1: 10.553781509399414, L3: 25.843347549438477\n",
      "Current prediction:  61.261722564697266 \n",
      "\n",
      "Iteration 10208, Loss: 37.48872375488281, L1: 10.56660270690918, L3: 26.922119140625\n",
      "Current prediction:  61.25448226928711 \n",
      "\n",
      "Iteration 10209, Loss: 36.550662994384766, L1: 10.564607620239258, L3: 25.986055374145508\n",
      "Current prediction:  61.25297927856445 \n",
      "\n",
      "Iteration 10210, Loss: 36.43395233154297, L1: 10.56110668182373, L3: 25.872846603393555\n",
      "Current prediction:  61.253170013427734 \n",
      "\n",
      "Iteration 10211, Loss: 37.10466384887695, L1: 10.592763900756836, L3: 26.511899948120117\n",
      "Current prediction:  61.25120544433594 \n",
      "\n",
      "Iteration 10212, Loss: 37.06489562988281, L1: 10.564579010009766, L3: 26.50031852722168\n",
      "Current prediction:  61.24915313720703 \n",
      "\n",
      "Iteration 10213, Loss: 37.099246978759766, L1: 10.579066276550293, L3: 26.52018165588379\n",
      "Current prediction:  61.24932861328125 \n",
      "\n",
      "Iteration 10214, Loss: 36.573219299316406, L1: 10.556195259094238, L3: 26.01702308654785\n",
      "Current prediction:  61.25128173828125 \n",
      "\n",
      "Iteration 10215, Loss: 36.063018798828125, L1: 10.552879333496094, L3: 25.51013946533203\n",
      "Current prediction:  61.25387191772461 \n",
      "\n",
      "Iteration 10216, Loss: 37.0996208190918, L1: 10.559435844421387, L3: 26.540184020996094\n",
      "Current prediction:  61.254486083984375 \n",
      "\n",
      "Iteration 10217, Loss: 37.504150390625, L1: 10.588496208190918, L3: 26.915653228759766\n",
      "Current prediction:  61.255069732666016 \n",
      "\n",
      "Iteration 10218, Loss: 35.49937057495117, L1: 10.5525484085083, L3: 24.946821212768555\n",
      "Current prediction:  61.25834655761719 \n",
      "\n",
      "Iteration 10219, Loss: 37.07896423339844, L1: 10.556188583374023, L3: 26.522777557373047\n",
      "Current prediction:  61.2614631652832 \n",
      "\n",
      "Iteration 10220, Loss: 37.30723571777344, L1: 10.555198669433594, L3: 26.752037048339844\n",
      "Current prediction:  61.26388931274414 \n",
      "\n",
      "Iteration 10221, Loss: 36.04456329345703, L1: 10.549825668334961, L3: 25.49473762512207\n",
      "Current prediction:  61.26619338989258 \n",
      "\n",
      "Iteration 10222, Loss: 36.25257110595703, L1: 10.560894012451172, L3: 25.691679000854492\n",
      "Current prediction:  61.26698684692383 \n",
      "\n",
      "Iteration 10223, Loss: 37.09403610229492, L1: 10.566689491271973, L3: 26.527347564697266\n",
      "Current prediction:  61.26879119873047 \n",
      "\n",
      "Iteration 10224, Loss: 36.19688415527344, L1: 10.554051399230957, L3: 25.642833709716797\n",
      "Current prediction:  61.274383544921875 \n",
      "\n",
      "Iteration 10225, Loss: 36.475860595703125, L1: 10.546250343322754, L3: 25.929611206054688\n",
      "Current prediction:  61.27976989746094 \n",
      "\n",
      "Iteration 10226, Loss: 36.96691131591797, L1: 10.535430908203125, L3: 26.431480407714844\n",
      "Current prediction:  61.2858772277832 \n",
      "\n",
      "Iteration 10227, Loss: 36.810401916503906, L1: 10.539958000183105, L3: 26.270442962646484\n",
      "Current prediction:  61.29147720336914 \n",
      "\n",
      "Iteration 10228, Loss: 35.86627197265625, L1: 10.545405387878418, L3: 25.320865631103516\n",
      "Current prediction:  61.29738998413086 \n",
      "\n",
      "Iteration 10229, Loss: 37.72702407836914, L1: 10.520098686218262, L3: 27.206924438476562\n",
      "Current prediction:  61.299373626708984 \n",
      "\n",
      "Iteration 10230, Loss: 35.982940673828125, L1: 10.535684585571289, L3: 25.447254180908203\n",
      "Current prediction:  61.30124282836914 \n",
      "\n",
      "Iteration 10231, Loss: 37.719932556152344, L1: 10.544770240783691, L3: 27.17516326904297\n",
      "Current prediction:  61.303043365478516 \n",
      "\n",
      "Iteration 10232, Loss: 36.55204391479492, L1: 10.512450218200684, L3: 26.039594650268555\n",
      "Current prediction:  61.303565979003906 \n",
      "\n",
      "Iteration 10233, Loss: 36.31181335449219, L1: 10.534841537475586, L3: 25.776973724365234\n",
      "Current prediction:  61.304630279541016 \n",
      "\n",
      "Iteration 10234, Loss: 36.04600143432617, L1: 10.518379211425781, L3: 25.52762222290039\n",
      "Current prediction:  61.30485534667969 \n",
      "\n",
      "Iteration 10235, Loss: 36.243080139160156, L1: 10.527618408203125, L3: 25.715463638305664\n",
      "Current prediction:  61.30670928955078 \n",
      "\n",
      "Iteration 10236, Loss: 35.418357849121094, L1: 10.499043464660645, L3: 24.919315338134766\n",
      "Current prediction:  61.3095703125 \n",
      "\n",
      "Iteration 10237, Loss: 36.33134460449219, L1: 10.508735656738281, L3: 25.822608947753906\n",
      "Current prediction:  61.31147766113281 \n",
      "\n",
      "Iteration 10238, Loss: 36.70537185668945, L1: 10.518431663513184, L3: 26.186941146850586\n",
      "Current prediction:  61.310794830322266 \n",
      "\n",
      "Iteration 10239, Loss: 36.62513732910156, L1: 10.502628326416016, L3: 26.122507095336914\n",
      "Current prediction:  61.30910110473633 \n",
      "\n",
      "Iteration 10240, Loss: 36.442108154296875, L1: 10.527902603149414, L3: 25.91420555114746\n",
      "Current prediction:  61.30582809448242 \n",
      "\n",
      "Iteration 10241, Loss: 36.469505310058594, L1: 10.521594047546387, L3: 25.94791030883789\n",
      "Current prediction:  61.300235748291016 \n",
      "\n",
      "Iteration 10242, Loss: 35.10659408569336, L1: 10.504398345947266, L3: 24.602195739746094\n",
      "Current prediction:  61.295753479003906 \n",
      "\n",
      "Iteration 10243, Loss: 36.690460205078125, L1: 10.551629066467285, L3: 26.138832092285156\n",
      "Current prediction:  61.28986358642578 \n",
      "\n",
      "Iteration 10244, Loss: 37.276241302490234, L1: 10.528267860412598, L3: 26.747974395751953\n",
      "Current prediction:  61.28451919555664 \n",
      "\n",
      "Iteration 10245, Loss: 36.41847610473633, L1: 10.531352996826172, L3: 25.887123107910156\n",
      "Current prediction:  61.28316116333008 \n",
      "\n",
      "Iteration 10246, Loss: 36.74887466430664, L1: 10.532815933227539, L3: 26.2160587310791\n",
      "Current prediction:  61.281272888183594 \n",
      "\n",
      "Iteration 10247, Loss: 36.29191589355469, L1: 10.537522315979004, L3: 25.75439453125\n",
      "Current prediction:  61.28274154663086 \n",
      "\n",
      "Iteration 10248, Loss: 36.24844741821289, L1: 10.551523208618164, L3: 25.696924209594727\n",
      "Current prediction:  61.284420013427734 \n",
      "\n",
      "Iteration 10249, Loss: 36.8103141784668, L1: 10.534424781799316, L3: 26.275890350341797\n",
      "Current prediction:  61.28473663330078 \n",
      "\n",
      "Iteration 10250, Loss: 37.48408126831055, L1: 10.526801109313965, L3: 26.9572811126709\n",
      "Current prediction:  61.28242874145508 \n",
      "\n",
      "Iteration 10251, Loss: 36.86378479003906, L1: 10.543355941772461, L3: 26.3204288482666\n",
      "Current prediction:  61.28097152709961 \n",
      "\n",
      "Iteration 10252, Loss: 36.77132797241211, L1: 10.53752613067627, L3: 26.233800888061523\n",
      "Current prediction:  61.279762268066406 \n",
      "\n",
      "Iteration 10253, Loss: 36.67424011230469, L1: 10.526156425476074, L3: 26.148082733154297\n",
      "Current prediction:  61.278282165527344 \n",
      "\n",
      "Iteration 10254, Loss: 36.31670379638672, L1: 10.535998344421387, L3: 25.780704498291016\n",
      "Current prediction:  61.27877426147461 \n",
      "\n",
      "Iteration 10255, Loss: 36.318138122558594, L1: 10.512157440185547, L3: 25.805980682373047\n",
      "Current prediction:  61.277793884277344 \n",
      "\n",
      "Iteration 10256, Loss: 36.74781799316406, L1: 10.542339324951172, L3: 26.20547866821289\n",
      "Current prediction:  61.27729415893555 \n",
      "\n",
      "Iteration 10257, Loss: 37.168975830078125, L1: 10.529520988464355, L3: 26.639453887939453\n",
      "Current prediction:  61.276371002197266 \n",
      "\n",
      "Iteration 10258, Loss: 36.1947021484375, L1: 10.531156539916992, L3: 25.663543701171875\n",
      "Current prediction:  61.27529525756836 \n",
      "\n",
      "Iteration 10259, Loss: 36.31194305419922, L1: 10.532899856567383, L3: 25.779041290283203\n",
      "Current prediction:  61.27498245239258 \n",
      "\n",
      "Iteration 10260, Loss: 36.970550537109375, L1: 10.553634643554688, L3: 26.41691780090332\n",
      "Current prediction:  61.27717208862305 \n",
      "\n",
      "Iteration 10261, Loss: 37.040504455566406, L1: 10.53293514251709, L3: 26.507568359375\n",
      "Current prediction:  61.280094146728516 \n",
      "\n",
      "Iteration 10262, Loss: 36.24898147583008, L1: 10.54100513458252, L3: 25.707977294921875\n",
      "Current prediction:  61.28644561767578 \n",
      "\n",
      "Iteration 10263, Loss: 37.33660125732422, L1: 10.514249801635742, L3: 26.822349548339844\n",
      "Current prediction:  61.29145812988281 \n",
      "\n",
      "Iteration 10264, Loss: 36.44957733154297, L1: 10.52495002746582, L3: 25.924625396728516\n",
      "Current prediction:  61.294795989990234 \n",
      "\n",
      "Iteration 10265, Loss: 35.79752731323242, L1: 10.520359992980957, L3: 25.27716827392578\n",
      "Current prediction:  61.29791259765625 \n",
      "\n",
      "Iteration 10266, Loss: 37.361045837402344, L1: 10.52906608581543, L3: 26.831981658935547\n",
      "Current prediction:  61.29964828491211 \n",
      "\n",
      "Iteration 10267, Loss: 36.776832580566406, L1: 10.508722305297852, L3: 26.268112182617188\n",
      "Current prediction:  61.30107498168945 \n",
      "\n",
      "Iteration 10268, Loss: 36.95555114746094, L1: 10.523771286010742, L3: 26.431777954101562\n",
      "Current prediction:  61.30078887939453 \n",
      "\n",
      "Iteration 10269, Loss: 37.830360412597656, L1: 10.513772964477539, L3: 27.316587448120117\n",
      "Current prediction:  61.29928970336914 \n",
      "\n",
      "Iteration 10270, Loss: 37.22698211669922, L1: 10.518795013427734, L3: 26.70818519592285\n",
      "Current prediction:  61.29597854614258 \n",
      "\n",
      "Iteration 10271, Loss: 35.292091369628906, L1: 10.519560813903809, L3: 24.77252960205078\n",
      "Current prediction:  61.2940559387207 \n",
      "\n",
      "Iteration 10272, Loss: 36.711639404296875, L1: 10.517925262451172, L3: 26.193714141845703\n",
      "Current prediction:  61.29127883911133 \n",
      "\n",
      "Iteration 10273, Loss: 36.47309875488281, L1: 10.532578468322754, L3: 25.940521240234375\n",
      "Current prediction:  61.288848876953125 \n",
      "\n",
      "Iteration 10274, Loss: 36.928714752197266, L1: 10.517922401428223, L3: 26.410791397094727\n",
      "Current prediction:  61.28879928588867 \n",
      "\n",
      "Iteration 10275, Loss: 35.76062774658203, L1: 10.522135734558105, L3: 25.238492965698242\n",
      "Current prediction:  61.289306640625 \n",
      "\n",
      "Iteration 10276, Loss: 36.86840057373047, L1: 10.517051696777344, L3: 26.351350784301758\n",
      "Current prediction:  61.29159927368164 \n",
      "\n",
      "Iteration 10277, Loss: 36.0855827331543, L1: 10.527064323425293, L3: 25.558517456054688\n",
      "Current prediction:  61.2955322265625 \n",
      "\n",
      "Iteration 10278, Loss: 36.01216506958008, L1: 10.527165412902832, L3: 25.485000610351562\n",
      "Current prediction:  61.29960632324219 \n",
      "\n",
      "Iteration 10279, Loss: 36.649436950683594, L1: 10.514776229858398, L3: 26.134662628173828\n",
      "Current prediction:  61.30380630493164 \n",
      "\n",
      "Iteration 10280, Loss: 36.09918212890625, L1: 10.515661239624023, L3: 25.58352279663086\n",
      "Current prediction:  61.30683517456055 \n",
      "\n",
      "Iteration 10281, Loss: 36.72125244140625, L1: 10.516489028930664, L3: 26.204763412475586\n",
      "Current prediction:  61.30971908569336 \n",
      "\n",
      "Iteration 10282, Loss: 37.051414489746094, L1: 10.491854667663574, L3: 26.559558868408203\n",
      "Current prediction:  61.311492919921875 \n",
      "\n",
      "Iteration 10283, Loss: 36.57820510864258, L1: 10.497197151184082, L3: 26.081008911132812\n",
      "Current prediction:  61.311981201171875 \n",
      "\n",
      "Iteration 10284, Loss: 35.732261657714844, L1: 10.489557266235352, L3: 25.242704391479492\n",
      "Current prediction:  61.31492233276367 \n",
      "\n",
      "Iteration 10285, Loss: 37.34238052368164, L1: 10.497689247131348, L3: 26.844690322875977\n",
      "Current prediction:  61.313995361328125 \n",
      "\n",
      "Iteration 10286, Loss: 37.09672546386719, L1: 10.500046730041504, L3: 26.596677780151367\n",
      "Current prediction:  61.313114166259766 \n",
      "\n",
      "Iteration 10287, Loss: 37.25846481323242, L1: 10.509200096130371, L3: 26.749265670776367\n",
      "Current prediction:  61.30942916870117 \n",
      "\n",
      "Iteration 10288, Loss: 35.363197326660156, L1: 10.502849578857422, L3: 24.860347747802734\n",
      "Current prediction:  61.30620193481445 \n",
      "\n",
      "Iteration 10289, Loss: 36.8192024230957, L1: 10.509352684020996, L3: 26.309850692749023\n",
      "Current prediction:  61.30030059814453 \n",
      "\n",
      "Iteration 10290, Loss: 35.9444465637207, L1: 10.516825675964355, L3: 25.427621841430664\n",
      "Current prediction:  61.29579544067383 \n",
      "\n",
      "Iteration 10291, Loss: 35.39140701293945, L1: 10.521100044250488, L3: 24.87030792236328\n",
      "Current prediction:  61.29312515258789 \n",
      "\n",
      "Iteration 10292, Loss: 35.61231231689453, L1: 10.536484718322754, L3: 25.07582664489746\n",
      "Current prediction:  61.291542053222656 \n",
      "\n",
      "Iteration 10293, Loss: 36.535682678222656, L1: 10.533912658691406, L3: 26.001771926879883\n",
      "Current prediction:  61.28783416748047 \n",
      "\n",
      "Iteration 10294, Loss: 36.631736755371094, L1: 10.529425621032715, L3: 26.102310180664062\n",
      "Current prediction:  61.283870697021484 \n",
      "\n",
      "Iteration 10295, Loss: 36.333126068115234, L1: 10.521537780761719, L3: 25.811588287353516\n",
      "Current prediction:  61.281166076660156 \n",
      "\n",
      "Iteration 10296, Loss: 36.58095169067383, L1: 10.537198066711426, L3: 26.043752670288086\n",
      "Current prediction:  61.27764129638672 \n",
      "\n",
      "Iteration 10297, Loss: 36.65000534057617, L1: 10.550215721130371, L3: 26.099788665771484\n",
      "Current prediction:  61.271812438964844 \n",
      "\n",
      "Iteration 10298, Loss: 36.04941177368164, L1: 10.552556037902832, L3: 25.496856689453125\n",
      "Current prediction:  61.26776885986328 \n",
      "\n",
      "Iteration 10299, Loss: 37.319881439208984, L1: 10.555275917053223, L3: 26.764606475830078\n",
      "Current prediction:  61.261539459228516 \n",
      "\n",
      "Iteration 10300, Loss: 36.84465789794922, L1: 10.552597045898438, L3: 26.29205894470215\n",
      "Current prediction:  61.256813049316406 \n",
      "\n",
      "Iteration 10301, Loss: 35.61809539794922, L1: 10.54365348815918, L3: 25.074440002441406\n",
      "Current prediction:  61.25389862060547 \n",
      "\n",
      "Iteration 10302, Loss: 37.360511779785156, L1: 10.564178466796875, L3: 26.796335220336914\n",
      "Current prediction:  61.25199508666992 \n",
      "\n",
      "Iteration 10303, Loss: 37.05375671386719, L1: 10.560301780700684, L3: 26.49345588684082\n",
      "Current prediction:  61.25229263305664 \n",
      "\n",
      "Iteration 10304, Loss: 37.167236328125, L1: 10.56863021850586, L3: 26.59860610961914\n",
      "Current prediction:  61.25259017944336 \n",
      "\n",
      "Iteration 10305, Loss: 35.54503631591797, L1: 10.558213233947754, L3: 24.98682403564453\n",
      "Current prediction:  61.25584411621094 \n",
      "\n",
      "Iteration 10306, Loss: 36.36597442626953, L1: 10.560592651367188, L3: 25.805381774902344\n",
      "Current prediction:  61.25880813598633 \n",
      "\n",
      "Iteration 10307, Loss: 36.131622314453125, L1: 10.541987419128418, L3: 25.58963394165039\n",
      "Current prediction:  61.26259231567383 \n",
      "\n",
      "Iteration 10308, Loss: 37.197425842285156, L1: 10.554600715637207, L3: 26.642824172973633\n",
      "Current prediction:  61.26573944091797 \n",
      "\n",
      "Iteration 10309, Loss: 36.78447723388672, L1: 10.54571533203125, L3: 26.238759994506836\n",
      "Current prediction:  61.27041244506836 \n",
      "\n",
      "Iteration 10310, Loss: 35.807151794433594, L1: 10.557241439819336, L3: 25.249908447265625\n",
      "Current prediction:  61.2765998840332 \n",
      "\n",
      "Iteration 10311, Loss: 36.96104431152344, L1: 10.532186508178711, L3: 26.428855895996094\n",
      "Current prediction:  61.28171157836914 \n",
      "\n",
      "Iteration 10312, Loss: 36.791587829589844, L1: 10.530649185180664, L3: 26.260940551757812\n",
      "Current prediction:  61.285789489746094 \n",
      "\n",
      "Iteration 10313, Loss: 37.087928771972656, L1: 10.531282424926758, L3: 26.556644439697266\n",
      "Current prediction:  61.289756774902344 \n",
      "\n",
      "Iteration 10314, Loss: 36.43894958496094, L1: 10.517699241638184, L3: 25.921249389648438\n",
      "Current prediction:  61.29591369628906 \n",
      "\n",
      "Iteration 10315, Loss: 36.59164810180664, L1: 10.533467292785645, L3: 26.058181762695312\n",
      "Current prediction:  61.30268859863281 \n",
      "\n",
      "Iteration 10316, Loss: 36.30500793457031, L1: 10.516768455505371, L3: 25.788240432739258\n",
      "Current prediction:  61.308067321777344 \n",
      "\n",
      "Iteration 10317, Loss: 36.35516357421875, L1: 10.514699935913086, L3: 25.840465545654297\n",
      "Current prediction:  61.31452941894531 \n",
      "\n",
      "Iteration 10318, Loss: 37.51862335205078, L1: 10.509648323059082, L3: 27.008974075317383\n",
      "Current prediction:  61.31966018676758 \n",
      "\n",
      "Iteration 10319, Loss: 35.972862243652344, L1: 10.498673439025879, L3: 25.47418975830078\n",
      "Current prediction:  61.32410430908203 \n",
      "\n",
      "Iteration 10320, Loss: 36.52127456665039, L1: 10.492720603942871, L3: 26.028553009033203\n",
      "Current prediction:  61.328094482421875 \n",
      "\n",
      "Iteration 10321, Loss: 36.50174331665039, L1: 10.487462043762207, L3: 26.014280319213867\n",
      "Current prediction:  61.3327522277832 \n",
      "\n",
      "Iteration 10322, Loss: 35.89556121826172, L1: 10.474703788757324, L3: 25.420856475830078\n",
      "Current prediction:  61.33769226074219 \n",
      "\n",
      "Iteration 10323, Loss: 36.38182067871094, L1: 10.472088813781738, L3: 25.909730911254883\n",
      "Current prediction:  61.341739654541016 \n",
      "\n",
      "Iteration 10324, Loss: 37.03255844116211, L1: 10.473226547241211, L3: 26.5593318939209\n",
      "Current prediction:  61.34515380859375 \n",
      "\n",
      "Iteration 10325, Loss: 36.66034698486328, L1: 10.478302955627441, L3: 26.182044982910156\n",
      "Current prediction:  61.34645080566406 \n",
      "\n",
      "Iteration 10326, Loss: 35.95827102661133, L1: 10.474797248840332, L3: 25.48347282409668\n",
      "Current prediction:  61.346435546875 \n",
      "\n",
      "Iteration 10327, Loss: 36.903907775878906, L1: 10.480976104736328, L3: 26.422931671142578\n",
      "Current prediction:  61.3441276550293 \n",
      "\n",
      "Iteration 10328, Loss: 36.54706954956055, L1: 10.479525566101074, L3: 26.067543029785156\n",
      "Current prediction:  61.34037399291992 \n",
      "\n",
      "Iteration 10329, Loss: 36.518741607666016, L1: 10.484711647033691, L3: 26.034029006958008\n",
      "Current prediction:  61.336387634277344 \n",
      "\n",
      "Iteration 10330, Loss: 36.56990051269531, L1: 10.476350784301758, L3: 26.093549728393555\n",
      "Current prediction:  61.33103561401367 \n",
      "\n",
      "Iteration 10331, Loss: 36.25883483886719, L1: 10.481032371520996, L3: 25.777803421020508\n",
      "Current prediction:  61.32292556762695 \n",
      "\n",
      "Iteration 10332, Loss: 37.09905242919922, L1: 10.485132217407227, L3: 26.613922119140625\n",
      "Current prediction:  61.31651306152344 \n",
      "\n",
      "Iteration 10333, Loss: 36.24421310424805, L1: 10.499361991882324, L3: 25.74485206604004\n",
      "Current prediction:  61.30854797363281 \n",
      "\n",
      "Iteration 10334, Loss: 36.61457061767578, L1: 10.497198104858398, L3: 26.117372512817383\n",
      "Current prediction:  61.299476623535156 \n",
      "\n",
      "Iteration 10335, Loss: 37.470184326171875, L1: 10.513567924499512, L3: 26.956615447998047\n",
      "Current prediction:  61.29144287109375 \n",
      "\n",
      "Iteration 10336, Loss: 37.119327545166016, L1: 10.528186798095703, L3: 26.591140747070312\n",
      "Current prediction:  61.2845344543457 \n",
      "\n",
      "Iteration 10337, Loss: 36.734046936035156, L1: 10.530914306640625, L3: 26.203134536743164\n",
      "Current prediction:  61.2787971496582 \n",
      "\n",
      "Iteration 10338, Loss: 37.726585388183594, L1: 10.539532661437988, L3: 27.187053680419922\n",
      "Current prediction:  61.273765563964844 \n",
      "\n",
      "Iteration 10339, Loss: 36.19186019897461, L1: 10.53886604309082, L3: 25.65299415588379\n",
      "Current prediction:  61.27008056640625 \n",
      "\n",
      "Iteration 10340, Loss: 35.69253921508789, L1: 10.539202690124512, L3: 25.153335571289062\n",
      "Current prediction:  61.26797103881836 \n",
      "\n",
      "Iteration 10341, Loss: 36.49309539794922, L1: 10.528888702392578, L3: 25.96420669555664\n",
      "Current prediction:  61.26791000366211 \n",
      "\n",
      "Iteration 10342, Loss: 36.36203384399414, L1: 10.560662269592285, L3: 25.801372528076172\n",
      "Current prediction:  61.267330169677734 \n",
      "\n",
      "Iteration 10343, Loss: 35.992027282714844, L1: 10.551212310791016, L3: 25.440814971923828\n",
      "Current prediction:  61.267757415771484 \n",
      "\n",
      "Iteration 10344, Loss: 36.79094696044922, L1: 10.541096687316895, L3: 26.249849319458008\n",
      "Current prediction:  61.266624450683594 \n",
      "\n",
      "Iteration 10345, Loss: 37.28578186035156, L1: 10.54859733581543, L3: 26.737184524536133\n",
      "Current prediction:  61.26418685913086 \n",
      "\n",
      "Iteration 10346, Loss: 35.66215515136719, L1: 10.559370040893555, L3: 25.102785110473633\n",
      "Current prediction:  61.26259994506836 \n",
      "\n",
      "Iteration 10347, Loss: 36.338279724121094, L1: 10.546319007873535, L3: 25.791961669921875\n",
      "Current prediction:  61.26006317138672 \n",
      "\n",
      "Iteration 10348, Loss: 36.34342956542969, L1: 10.544811248779297, L3: 25.79861831665039\n",
      "Current prediction:  61.25760269165039 \n",
      "\n",
      "Iteration 10349, Loss: 36.53063201904297, L1: 10.552908897399902, L3: 25.977724075317383\n",
      "Current prediction:  61.25678253173828 \n",
      "\n",
      "Iteration 10350, Loss: 36.99546432495117, L1: 10.557332992553711, L3: 26.43813133239746\n",
      "Current prediction:  61.25589370727539 \n",
      "\n",
      "Iteration 10351, Loss: 36.63471221923828, L1: 10.560688972473145, L3: 26.07402229309082\n",
      "Current prediction:  61.25700759887695 \n",
      "\n",
      "Iteration 10352, Loss: 36.88716506958008, L1: 10.55405330657959, L3: 26.333112716674805\n",
      "Current prediction:  61.258399963378906 \n",
      "\n",
      "Iteration 10353, Loss: 37.023773193359375, L1: 10.527079582214355, L3: 26.496694564819336\n",
      "Current prediction:  61.26081848144531 \n",
      "\n",
      "Iteration 10354, Loss: 35.204254150390625, L1: 10.547450065612793, L3: 24.656803131103516\n",
      "Current prediction:  61.26416778564453 \n",
      "\n",
      "Iteration 10355, Loss: 35.909034729003906, L1: 10.5469970703125, L3: 25.362037658691406\n",
      "Current prediction:  61.26797103881836 \n",
      "\n",
      "Iteration 10356, Loss: 36.44535827636719, L1: 10.538768768310547, L3: 25.90658950805664\n",
      "Current prediction:  61.26974868774414 \n",
      "\n",
      "Iteration 10357, Loss: 37.52903747558594, L1: 10.525982856750488, L3: 27.003055572509766\n",
      "Current prediction:  61.27265548706055 \n",
      "\n",
      "Iteration 10358, Loss: 37.105987548828125, L1: 10.541851997375488, L3: 26.56413459777832\n",
      "Current prediction:  61.2758674621582 \n",
      "\n",
      "Iteration 10359, Loss: 36.927207946777344, L1: 10.54715347290039, L3: 26.38005256652832\n",
      "Current prediction:  61.28019714355469 \n",
      "\n",
      "Iteration 10360, Loss: 35.66059112548828, L1: 10.549651145935059, L3: 25.11094093322754\n",
      "Current prediction:  61.2857666015625 \n",
      "\n",
      "Iteration 10361, Loss: 37.495079040527344, L1: 10.5416898727417, L3: 26.95339012145996\n",
      "Current prediction:  61.288177490234375 \n",
      "\n",
      "Iteration 10362, Loss: 36.167144775390625, L1: 10.51455307006836, L3: 25.652589797973633\n",
      "Current prediction:  61.289222717285156 \n",
      "\n",
      "Iteration 10363, Loss: 36.6171760559082, L1: 10.524398803710938, L3: 26.092777252197266\n",
      "Current prediction:  61.28835678100586 \n",
      "\n",
      "Iteration 10364, Loss: 37.473812103271484, L1: 10.527344703674316, L3: 26.94646644592285\n",
      "Current prediction:  61.28619384765625 \n",
      "\n",
      "Iteration 10365, Loss: 37.12159729003906, L1: 10.525008201599121, L3: 26.596590042114258\n",
      "Current prediction:  61.282257080078125 \n",
      "\n",
      "Iteration 10366, Loss: 35.34910202026367, L1: 10.530729293823242, L3: 24.81837272644043\n",
      "Current prediction:  61.281227111816406 \n",
      "\n",
      "Iteration 10367, Loss: 38.151771545410156, L1: 11.715913772583008, L3: 26.435855865478516\n",
      "Current prediction:  61.28083038330078 \n",
      "\n",
      "Iteration 10368, Loss: 36.45107650756836, L1: 10.5288667678833, L3: 25.922208786010742\n",
      "Current prediction:  61.282264709472656 \n",
      "\n",
      "Iteration 10369, Loss: 36.853450775146484, L1: 10.528477668762207, L3: 26.32497215270996\n",
      "Current prediction:  61.28010177612305 \n",
      "\n",
      "Iteration 10370, Loss: 35.98023223876953, L1: 10.53398323059082, L3: 25.446247100830078\n",
      "Current prediction:  61.278968811035156 \n",
      "\n",
      "Iteration 10371, Loss: 36.02793884277344, L1: 10.533455848693848, L3: 25.494482040405273\n",
      "Current prediction:  61.277957916259766 \n",
      "\n",
      "Iteration 10372, Loss: 37.45597457885742, L1: 10.5308256149292, L3: 26.92514991760254\n",
      "Current prediction:  61.27653503417969 \n",
      "\n",
      "Iteration 10373, Loss: 36.178592681884766, L1: 10.540641784667969, L3: 25.637950897216797\n",
      "Current prediction:  61.2752685546875 \n",
      "\n",
      "Iteration 10374, Loss: 37.0721321105957, L1: 10.540881156921387, L3: 26.53125\n",
      "Current prediction:  61.27509307861328 \n",
      "\n",
      "Iteration 10375, Loss: 36.219146728515625, L1: 10.526883125305176, L3: 25.692264556884766\n",
      "Current prediction:  61.274757385253906 \n",
      "\n",
      "Iteration 10376, Loss: 36.00950622558594, L1: 10.539545059204102, L3: 25.469959259033203\n",
      "Current prediction:  61.274017333984375 \n",
      "\n",
      "Iteration 10377, Loss: 37.3684196472168, L1: 10.530235290527344, L3: 26.838184356689453\n",
      "Current prediction:  61.27256393432617 \n",
      "\n",
      "Iteration 10378, Loss: 36.24458312988281, L1: 10.537269592285156, L3: 25.70731544494629\n",
      "Current prediction:  61.27265167236328 \n",
      "\n",
      "Iteration 10379, Loss: 36.11378479003906, L1: 10.540190696716309, L3: 25.573593139648438\n",
      "Current prediction:  61.27121353149414 \n",
      "\n",
      "Iteration 10380, Loss: 36.81061553955078, L1: 10.53835678100586, L3: 26.27225685119629\n",
      "Current prediction:  61.27098083496094 \n",
      "\n",
      "Iteration 10381, Loss: 36.48511505126953, L1: 10.53634262084961, L3: 25.948772430419922\n",
      "Current prediction:  61.271263122558594 \n",
      "\n",
      "Iteration 10382, Loss: 36.774810791015625, L1: 10.550620079040527, L3: 26.22418975830078\n",
      "Current prediction:  61.27153778076172 \n",
      "\n",
      "Iteration 10383, Loss: 36.03483200073242, L1: 10.537131309509277, L3: 25.497699737548828\n",
      "Current prediction:  61.271018981933594 \n",
      "\n",
      "Iteration 10384, Loss: 36.46479797363281, L1: 10.541603088378906, L3: 25.923192977905273\n",
      "Current prediction:  61.26948165893555 \n",
      "\n",
      "Iteration 10385, Loss: 37.06127166748047, L1: 10.539731979370117, L3: 26.52153968811035\n",
      "Current prediction:  61.26826095581055 \n",
      "\n",
      "Iteration 10386, Loss: 36.62821960449219, L1: 10.543954849243164, L3: 26.084264755249023\n",
      "Current prediction:  61.26728820800781 \n",
      "\n",
      "Iteration 10387, Loss: 36.59018325805664, L1: 10.541924476623535, L3: 26.048259735107422\n",
      "Current prediction:  61.26765060424805 \n",
      "\n",
      "Iteration 10388, Loss: 36.662078857421875, L1: 10.535786628723145, L3: 26.126291275024414\n",
      "Current prediction:  61.26639175415039 \n",
      "\n",
      "Iteration 10389, Loss: 36.81019592285156, L1: 10.542581558227539, L3: 26.267616271972656\n",
      "Current prediction:  61.26470947265625 \n",
      "\n",
      "Iteration 10390, Loss: 37.17231750488281, L1: 10.541497230529785, L3: 26.63081932067871\n",
      "Current prediction:  61.26581954956055 \n",
      "\n",
      "Iteration 10391, Loss: 36.3983154296875, L1: 10.55665111541748, L3: 25.841663360595703\n",
      "Current prediction:  61.26620864868164 \n",
      "\n",
      "Iteration 10392, Loss: 37.06886672973633, L1: 10.549090385437012, L3: 26.519775390625\n",
      "Current prediction:  61.26749801635742 \n",
      "\n",
      "Iteration 10393, Loss: 37.41047668457031, L1: 10.552409172058105, L3: 26.858068466186523\n",
      "Current prediction:  61.26925277709961 \n",
      "\n",
      "Iteration 10394, Loss: 37.573951721191406, L1: 10.550239562988281, L3: 27.023710250854492\n",
      "Current prediction:  61.2700080871582 \n",
      "\n",
      "Iteration 10395, Loss: 37.39215087890625, L1: 10.534457206726074, L3: 26.857694625854492\n",
      "Current prediction:  61.272911071777344 \n",
      "\n",
      "Iteration 10396, Loss: 36.11939239501953, L1: 10.560579299926758, L3: 25.558815002441406\n",
      "Current prediction:  61.2785758972168 \n",
      "\n",
      "Iteration 10397, Loss: 36.59116744995117, L1: 10.544107437133789, L3: 26.047060012817383\n",
      "Current prediction:  61.28540802001953 \n",
      "\n",
      "Iteration 10398, Loss: 36.236305236816406, L1: 10.540745735168457, L3: 25.695558547973633\n",
      "Current prediction:  61.291805267333984 \n",
      "\n",
      "Iteration 10399, Loss: 36.188289642333984, L1: 10.525456428527832, L3: 25.662832260131836\n",
      "Current prediction:  61.300437927246094 \n",
      "\n",
      "Iteration 10400, Loss: 37.04086685180664, L1: 10.528040885925293, L3: 26.51282501220703\n",
      "Current prediction:  61.3078727722168 \n",
      "\n",
      "Iteration 10401, Loss: 36.1196174621582, L1: 10.513693809509277, L3: 25.60592269897461\n",
      "Current prediction:  61.31556701660156 \n",
      "\n",
      "Iteration 10402, Loss: 36.641990661621094, L1: 10.499696731567383, L3: 26.14229393005371\n",
      "Current prediction:  61.31959915161133 \n",
      "\n",
      "Iteration 10403, Loss: 35.651493072509766, L1: 10.493239402770996, L3: 25.158254623413086\n",
      "Current prediction:  61.32033920288086 \n",
      "\n",
      "Iteration 10404, Loss: 37.23284912109375, L1: 10.502912521362305, L3: 26.729936599731445\n",
      "Current prediction:  61.32107162475586 \n",
      "\n",
      "Iteration 10405, Loss: 35.723934173583984, L1: 10.495017051696777, L3: 25.22891616821289\n",
      "Current prediction:  61.323516845703125 \n",
      "\n",
      "Iteration 10406, Loss: 36.46855926513672, L1: 10.496476173400879, L3: 25.972082138061523\n",
      "Current prediction:  61.3250846862793 \n",
      "\n",
      "Iteration 10407, Loss: 36.059234619140625, L1: 10.487279891967773, L3: 25.571956634521484\n",
      "Current prediction:  61.324951171875 \n",
      "\n",
      "Iteration 10408, Loss: 36.956058502197266, L1: 10.50037670135498, L3: 26.45568084716797\n",
      "Current prediction:  61.3227653503418 \n",
      "\n",
      "Iteration 10409, Loss: 36.34894561767578, L1: 10.488321304321289, L3: 25.86062240600586\n",
      "Current prediction:  61.3202018737793 \n",
      "\n",
      "Iteration 10410, Loss: 37.40272903442383, L1: 10.501032829284668, L3: 26.901697158813477\n",
      "Current prediction:  61.31814956665039 \n",
      "\n",
      "Iteration 10411, Loss: 36.61759948730469, L1: 10.50956916809082, L3: 26.108030319213867\n",
      "Current prediction:  61.31586456298828 \n",
      "\n",
      "Iteration 10412, Loss: 36.91757583618164, L1: 10.503338813781738, L3: 26.41423797607422\n",
      "Current prediction:  61.31180953979492 \n",
      "\n",
      "Iteration 10413, Loss: 35.29395294189453, L1: 10.510674476623535, L3: 24.78327751159668\n",
      "Current prediction:  61.308349609375 \n",
      "\n",
      "Iteration 10414, Loss: 36.70561218261719, L1: 10.523298263549805, L3: 26.182315826416016\n",
      "Current prediction:  61.30205154418945 \n",
      "\n",
      "Iteration 10415, Loss: 36.42741012573242, L1: 10.52315616607666, L3: 25.904253005981445\n",
      "Current prediction:  61.297061920166016 \n",
      "\n",
      "Iteration 10416, Loss: 35.56380844116211, L1: 10.516106605529785, L3: 25.04770278930664\n",
      "Current prediction:  61.29368591308594 \n",
      "\n",
      "Iteration 10417, Loss: 37.59513473510742, L1: 10.526509284973145, L3: 27.06862449645996\n",
      "Current prediction:  61.28870391845703 \n",
      "\n",
      "Iteration 10418, Loss: 36.901153564453125, L1: 10.527859687805176, L3: 26.373292922973633\n",
      "Current prediction:  61.283878326416016 \n",
      "\n",
      "Iteration 10419, Loss: 36.96720504760742, L1: 10.523322105407715, L3: 26.443883895874023\n",
      "Current prediction:  61.280582427978516 \n",
      "\n",
      "Iteration 10420, Loss: 36.383697509765625, L1: 10.529781341552734, L3: 25.853918075561523\n",
      "Current prediction:  61.276729583740234 \n",
      "\n",
      "Iteration 10421, Loss: 36.14029312133789, L1: 10.530474662780762, L3: 25.609817504882812\n",
      "Current prediction:  61.275360107421875 \n",
      "\n",
      "Iteration 10422, Loss: 35.708770751953125, L1: 10.544556617736816, L3: 25.164213180541992\n",
      "Current prediction:  61.274845123291016 \n",
      "\n",
      "Iteration 10423, Loss: 35.81103515625, L1: 10.537151336669922, L3: 25.273883819580078\n",
      "Current prediction:  61.27690505981445 \n",
      "\n",
      "Iteration 10424, Loss: 36.52389907836914, L1: 10.53607177734375, L3: 25.98782730102539\n",
      "Current prediction:  61.27964782714844 \n",
      "\n",
      "Iteration 10425, Loss: 35.82475280761719, L1: 10.541988372802734, L3: 25.28276252746582\n",
      "Current prediction:  61.28397750854492 \n",
      "\n",
      "Iteration 10426, Loss: 35.9399528503418, L1: 10.538990020751953, L3: 25.400962829589844\n",
      "Current prediction:  61.288352966308594 \n",
      "\n",
      "Iteration 10427, Loss: 36.77086639404297, L1: 10.536834716796875, L3: 26.234031677246094\n",
      "Current prediction:  61.29125213623047 \n",
      "\n",
      "Iteration 10428, Loss: 36.9407844543457, L1: 10.525015830993652, L3: 26.415769577026367\n",
      "Current prediction:  61.29376220703125 \n",
      "\n",
      "Iteration 10429, Loss: 36.850486755371094, L1: 10.52839469909668, L3: 26.32209014892578\n",
      "Current prediction:  61.2923583984375 \n",
      "\n",
      "Iteration 10430, Loss: 37.58799362182617, L1: 10.526061058044434, L3: 27.061931610107422\n",
      "Current prediction:  61.28973388671875 \n",
      "\n",
      "Iteration 10431, Loss: 37.39959716796875, L1: 10.535167694091797, L3: 26.864431381225586\n",
      "Current prediction:  61.28630828857422 \n",
      "\n",
      "Iteration 10432, Loss: 36.35760498046875, L1: 10.537891387939453, L3: 25.819711685180664\n",
      "Current prediction:  61.28372573852539 \n",
      "\n",
      "Iteration 10433, Loss: 35.513397216796875, L1: 10.54021167755127, L3: 24.97318458557129\n",
      "Current prediction:  61.2844352722168 \n",
      "\n",
      "Iteration 10434, Loss: 37.489070892333984, L1: 10.540047645568848, L3: 26.94902229309082\n",
      "Current prediction:  61.2830696105957 \n",
      "\n",
      "Iteration 10435, Loss: 36.21458435058594, L1: 10.54237174987793, L3: 25.672212600708008\n",
      "Current prediction:  61.281219482421875 \n",
      "\n",
      "Iteration 10436, Loss: 36.139671325683594, L1: 10.528604507446289, L3: 25.611068725585938\n",
      "Current prediction:  61.28015899658203 \n",
      "\n",
      "Iteration 10437, Loss: 36.664466857910156, L1: 10.546142578125, L3: 26.118322372436523\n",
      "Current prediction:  61.27836608886719 \n",
      "\n",
      "Iteration 10438, Loss: 36.98052978515625, L1: 10.538087844848633, L3: 26.44244384765625\n",
      "Current prediction:  61.27771759033203 \n",
      "\n",
      "Iteration 10439, Loss: 36.25355911254883, L1: 10.539496421813965, L3: 25.71406364440918\n",
      "Current prediction:  61.28001022338867 \n",
      "\n",
      "Iteration 10440, Loss: 36.446563720703125, L1: 10.5349760055542, L3: 25.911588668823242\n",
      "Current prediction:  61.283485412597656 \n",
      "\n",
      "Iteration 10441, Loss: 36.613624572753906, L1: 10.534135818481445, L3: 26.07948875427246\n",
      "Current prediction:  61.286712646484375 \n",
      "\n",
      "Iteration 10442, Loss: 36.6336669921875, L1: 10.528741836547852, L3: 26.10492706298828\n",
      "Current prediction:  61.2868766784668 \n",
      "\n",
      "Iteration 10443, Loss: 36.53980255126953, L1: 10.532025337219238, L3: 26.00777816772461\n",
      "Current prediction:  61.28805160522461 \n",
      "\n",
      "Iteration 10444, Loss: 37.167449951171875, L1: 10.534680366516113, L3: 26.632768630981445\n",
      "Current prediction:  61.29070281982422 \n",
      "\n",
      "Iteration 10445, Loss: 36.66716384887695, L1: 10.521559715270996, L3: 26.145605087280273\n",
      "Current prediction:  61.289886474609375 \n",
      "\n",
      "Iteration 10446, Loss: 36.671112060546875, L1: 10.525236129760742, L3: 26.145875930786133\n",
      "Current prediction:  61.292808532714844 \n",
      "\n",
      "Iteration 10447, Loss: 37.02025604248047, L1: 10.51042652130127, L3: 26.509830474853516\n",
      "Current prediction:  61.29624938964844 \n",
      "\n",
      "Iteration 10448, Loss: 36.87202072143555, L1: 10.516072273254395, L3: 26.35594940185547\n",
      "Current prediction:  61.29691696166992 \n",
      "\n",
      "Iteration 10449, Loss: 36.194725036621094, L1: 10.518194198608398, L3: 25.676530838012695\n",
      "Current prediction:  61.29737091064453 \n",
      "\n",
      "Iteration 10450, Loss: 35.89867401123047, L1: 10.522659301757812, L3: 25.376012802124023\n",
      "Current prediction:  61.29732131958008 \n",
      "\n",
      "Iteration 10451, Loss: 36.298606872558594, L1: 10.53121566772461, L3: 25.767391204833984\n",
      "Current prediction:  61.2952880859375 \n",
      "\n",
      "Iteration 10452, Loss: 36.910728454589844, L1: 10.525394439697266, L3: 26.385332107543945\n",
      "Current prediction:  61.294795989990234 \n",
      "\n",
      "Iteration 10453, Loss: 36.193885803222656, L1: 10.527921676635742, L3: 25.665966033935547\n",
      "Current prediction:  61.295196533203125 \n",
      "\n",
      "Iteration 10454, Loss: 36.271759033203125, L1: 10.52511978149414, L3: 25.746639251708984\n",
      "Current prediction:  61.29607009887695 \n",
      "\n",
      "Iteration 10455, Loss: 37.099205017089844, L1: 10.5223970413208, L3: 26.576807022094727\n",
      "Current prediction:  61.296913146972656 \n",
      "\n",
      "Iteration 10456, Loss: 35.8897590637207, L1: 10.518534660339355, L3: 25.371225357055664\n",
      "Current prediction:  61.29768753051758 \n",
      "\n",
      "Iteration 10457, Loss: 36.143375396728516, L1: 10.529301643371582, L3: 25.61407470703125\n",
      "Current prediction:  61.29616165161133 \n",
      "\n",
      "Iteration 10458, Loss: 37.76072311401367, L1: 10.520846366882324, L3: 27.23987579345703\n",
      "Current prediction:  61.29145431518555 \n",
      "\n",
      "Iteration 10459, Loss: 36.2623291015625, L1: 10.530669212341309, L3: 25.731660842895508\n",
      "Current prediction:  61.28902053833008 \n",
      "\n",
      "Iteration 10460, Loss: 36.65598678588867, L1: 10.52177619934082, L3: 26.13421058654785\n",
      "Current prediction:  61.28512954711914 \n",
      "\n",
      "Iteration 10461, Loss: 37.39632034301758, L1: 10.535025596618652, L3: 26.86129379272461\n",
      "Current prediction:  61.28017807006836 \n",
      "\n",
      "Iteration 10462, Loss: 36.98966979980469, L1: 10.535219192504883, L3: 26.454452514648438\n",
      "Current prediction:  61.27338790893555 \n",
      "\n",
      "Iteration 10463, Loss: 36.27831268310547, L1: 10.541830062866211, L3: 25.736482620239258\n",
      "Current prediction:  61.26820373535156 \n",
      "\n",
      "Iteration 10464, Loss: 36.73011779785156, L1: 10.54268741607666, L3: 26.18743133544922\n",
      "Current prediction:  61.263404846191406 \n",
      "\n",
      "Iteration 10465, Loss: 36.7825813293457, L1: 10.547006607055664, L3: 26.23557472229004\n",
      "Current prediction:  61.25455093383789 \n",
      "\n",
      "Iteration 10466, Loss: 36.033775329589844, L1: 10.561430931091309, L3: 25.47234344482422\n",
      "Current prediction:  61.24909973144531 \n",
      "\n",
      "Iteration 10467, Loss: 37.15042495727539, L1: 10.554149627685547, L3: 26.596275329589844\n",
      "Current prediction:  61.24382781982422 \n",
      "\n",
      "Iteration 10468, Loss: 36.59349060058594, L1: 10.559814453125, L3: 26.033676147460938\n",
      "Current prediction:  61.2361946105957 \n",
      "\n",
      "Iteration 10469, Loss: 37.14946746826172, L1: 10.561444282531738, L3: 26.588022232055664\n",
      "Current prediction:  61.227500915527344 \n",
      "\n",
      "Iteration 10470, Loss: 36.648399353027344, L1: 10.573918342590332, L3: 26.074481964111328\n",
      "Current prediction:  61.219932556152344 \n",
      "\n",
      "Iteration 10471, Loss: 36.36066818237305, L1: 10.579975128173828, L3: 25.78069305419922\n",
      "Current prediction:  61.21501159667969 \n",
      "\n",
      "Iteration 10472, Loss: 36.54892349243164, L1: 10.582679748535156, L3: 25.966243743896484\n",
      "Current prediction:  61.21146011352539 \n",
      "\n",
      "Iteration 10473, Loss: 36.44279479980469, L1: 10.592473030090332, L3: 25.850322723388672\n",
      "Current prediction:  61.20823669433594 \n",
      "\n",
      "Iteration 10474, Loss: 36.183162689208984, L1: 10.598934173583984, L3: 25.584228515625\n",
      "Current prediction:  61.20734405517578 \n",
      "\n",
      "Iteration 10475, Loss: 36.475704193115234, L1: 10.600686073303223, L3: 25.875019073486328\n",
      "Current prediction:  61.20931625366211 \n",
      "\n",
      "Iteration 10476, Loss: 36.50446319580078, L1: 10.594049453735352, L3: 25.910411834716797\n",
      "Current prediction:  61.214351654052734 \n",
      "\n",
      "Iteration 10477, Loss: 36.92500305175781, L1: 10.596992492675781, L3: 26.328012466430664\n",
      "Current prediction:  61.21721267700195 \n",
      "\n",
      "Iteration 10478, Loss: 35.83658218383789, L1: 10.591139793395996, L3: 25.245441436767578\n",
      "Current prediction:  61.222591400146484 \n",
      "\n",
      "Iteration 10479, Loss: 36.41339874267578, L1: 10.580074310302734, L3: 25.833322525024414\n",
      "Current prediction:  61.227783203125 \n",
      "\n",
      "Iteration 10480, Loss: 38.09001922607422, L1: 10.571165084838867, L3: 27.51885223388672\n",
      "Current prediction:  61.232608795166016 \n",
      "\n",
      "Iteration 10481, Loss: 35.9695930480957, L1: 10.56528377532959, L3: 25.404308319091797\n",
      "Current prediction:  61.23859405517578 \n",
      "\n",
      "Iteration 10482, Loss: 35.025081634521484, L1: 10.574748039245605, L3: 24.450334548950195\n",
      "Current prediction:  61.2483024597168 \n",
      "\n",
      "Iteration 10483, Loss: 36.28152847290039, L1: 10.558505058288574, L3: 25.7230224609375\n",
      "Current prediction:  61.2569465637207 \n",
      "\n",
      "Iteration 10484, Loss: 36.245140075683594, L1: 10.546981811523438, L3: 25.69816017150879\n",
      "Current prediction:  61.26542282104492 \n",
      "\n",
      "Iteration 10485, Loss: 37.157405853271484, L1: 10.54610538482666, L3: 26.611299514770508\n",
      "Current prediction:  61.271087646484375 \n",
      "\n",
      "Iteration 10486, Loss: 36.83753204345703, L1: 10.533198356628418, L3: 26.30433464050293\n",
      "Current prediction:  61.27952194213867 \n",
      "\n",
      "Iteration 10487, Loss: 36.71053695678711, L1: 10.52643871307373, L3: 26.184099197387695\n",
      "Current prediction:  61.2877082824707 \n",
      "\n",
      "Iteration 10488, Loss: 36.64449691772461, L1: 10.524989128112793, L3: 26.1195068359375\n",
      "Current prediction:  61.29245376586914 \n",
      "\n",
      "Iteration 10489, Loss: 36.25164031982422, L1: 10.5225830078125, L3: 25.72905731201172\n",
      "Current prediction:  61.29688262939453 \n",
      "\n",
      "Iteration 10490, Loss: 36.605499267578125, L1: 10.517446517944336, L3: 26.088054656982422\n",
      "Current prediction:  61.30061721801758 \n",
      "\n",
      "Iteration 10491, Loss: 37.1736946105957, L1: 10.508048057556152, L3: 26.665647506713867\n",
      "Current prediction:  61.30036926269531 \n",
      "\n",
      "Iteration 10492, Loss: 35.35841369628906, L1: 10.507735252380371, L3: 24.850677490234375\n",
      "Current prediction:  61.29975509643555 \n",
      "\n",
      "Iteration 10493, Loss: 35.924774169921875, L1: 10.519731521606445, L3: 25.405040740966797\n",
      "Current prediction:  61.2970085144043 \n",
      "\n",
      "Iteration 10494, Loss: 36.93931579589844, L1: 10.50710678100586, L3: 26.432207107543945\n",
      "Current prediction:  61.2946891784668 \n",
      "\n",
      "Iteration 10495, Loss: 36.827789306640625, L1: 10.507537841796875, L3: 26.320253372192383\n",
      "Current prediction:  61.29140090942383 \n",
      "\n",
      "Iteration 10496, Loss: 35.77808380126953, L1: 10.518465042114258, L3: 25.259618759155273\n",
      "Current prediction:  61.287681579589844 \n",
      "\n",
      "Iteration 10497, Loss: 37.35934829711914, L1: 10.520977020263672, L3: 26.83837127685547\n",
      "Current prediction:  61.2845458984375 \n",
      "\n",
      "Iteration 10498, Loss: 37.46794891357422, L1: 10.524297714233398, L3: 26.943649291992188\n",
      "Current prediction:  61.27902603149414 \n",
      "\n",
      "Iteration 10499, Loss: 37.0073127746582, L1: 10.530193328857422, L3: 26.47711944580078\n",
      "Current prediction:  61.27376937866211 \n",
      "\n",
      "Iteration 10500, Loss: 36.602970123291016, L1: 10.536331176757812, L3: 26.066638946533203\n",
      "Current prediction:  61.26654815673828 \n",
      "\n",
      "Iteration 10501, Loss: 36.3602294921875, L1: 10.548052787780762, L3: 25.812177658081055\n",
      "Current prediction:  61.26093292236328 \n",
      "\n",
      "Iteration 10502, Loss: 36.09914779663086, L1: 10.543896675109863, L3: 25.55525016784668\n",
      "Current prediction:  61.256935119628906 \n",
      "\n",
      "Iteration 10503, Loss: 36.130714416503906, L1: 10.551074981689453, L3: 25.57963752746582\n",
      "Current prediction:  61.253143310546875 \n",
      "\n",
      "Iteration 10504, Loss: 35.94169998168945, L1: 10.555346488952637, L3: 25.386354446411133\n",
      "Current prediction:  61.25102996826172 \n",
      "\n",
      "Iteration 10505, Loss: 35.340362548828125, L1: 10.54205322265625, L3: 24.798309326171875\n",
      "Current prediction:  61.250911712646484 \n",
      "\n",
      "Iteration 10506, Loss: 35.83314895629883, L1: 10.539348602294922, L3: 25.293800354003906\n",
      "Current prediction:  61.25017547607422 \n",
      "\n",
      "Iteration 10507, Loss: 36.47431182861328, L1: 10.552546501159668, L3: 25.92176628112793\n",
      "Current prediction:  61.251121520996094 \n",
      "\n",
      "Iteration 10508, Loss: 36.020301818847656, L1: 10.558870315551758, L3: 25.4614315032959\n",
      "Current prediction:  61.254634857177734 \n",
      "\n",
      "Iteration 10509, Loss: 37.00824737548828, L1: 10.547866821289062, L3: 26.46038055419922\n",
      "Current prediction:  61.258583068847656 \n",
      "\n",
      "Iteration 10510, Loss: 36.46060562133789, L1: 10.545557975769043, L3: 25.91504669189453\n",
      "Current prediction:  61.26021194458008 \n",
      "\n",
      "Iteration 10511, Loss: 36.81285858154297, L1: 10.549053192138672, L3: 26.263805389404297\n",
      "Current prediction:  61.261783599853516 \n",
      "\n",
      "Iteration 10512, Loss: 36.33980941772461, L1: 10.539284706115723, L3: 25.800525665283203\n",
      "Current prediction:  61.26083755493164 \n",
      "\n",
      "Iteration 10513, Loss: 36.963619232177734, L1: 10.536595344543457, L3: 26.427024841308594\n",
      "Current prediction:  61.25990676879883 \n",
      "\n",
      "Iteration 10514, Loss: 37.01116180419922, L1: 10.545839309692383, L3: 26.465322494506836\n",
      "Current prediction:  61.25950622558594 \n",
      "\n",
      "Iteration 10515, Loss: 35.901649475097656, L1: 10.545818328857422, L3: 25.355833053588867\n",
      "Current prediction:  61.263885498046875 \n",
      "\n",
      "Iteration 10516, Loss: 36.221988677978516, L1: 10.540179252624512, L3: 25.681808471679688\n",
      "Current prediction:  61.26896286010742 \n",
      "\n",
      "Iteration 10517, Loss: 37.239315032958984, L1: 10.52443790435791, L3: 26.71487808227539\n",
      "Current prediction:  61.274261474609375 \n",
      "\n",
      "Iteration 10518, Loss: 36.7809944152832, L1: 10.534420013427734, L3: 26.24657440185547\n",
      "Current prediction:  61.28046798706055 \n",
      "\n",
      "Iteration 10519, Loss: 36.04629135131836, L1: 10.513026237487793, L3: 25.533266067504883\n",
      "Current prediction:  61.28624725341797 \n",
      "\n",
      "Iteration 10520, Loss: 36.601680755615234, L1: 10.512707710266113, L3: 26.088972091674805\n",
      "Current prediction:  61.29226303100586 \n",
      "\n",
      "Iteration 10521, Loss: 36.596771240234375, L1: 10.49943733215332, L3: 26.097332000732422\n",
      "Current prediction:  61.299068450927734 \n",
      "\n",
      "Iteration 10522, Loss: 36.05039978027344, L1: 10.500144958496094, L3: 25.550254821777344\n",
      "Current prediction:  61.30659484863281 \n",
      "\n",
      "Iteration 10523, Loss: 36.293052673339844, L1: 10.486753463745117, L3: 25.806299209594727\n",
      "Current prediction:  61.31001281738281 \n",
      "\n",
      "Iteration 10524, Loss: 36.87662887573242, L1: 10.491595268249512, L3: 26.385034561157227\n",
      "Current prediction:  61.31155776977539 \n",
      "\n",
      "Iteration 10525, Loss: 36.27149963378906, L1: 10.501514434814453, L3: 25.769987106323242\n",
      "Current prediction:  61.314640045166016 \n",
      "\n",
      "Iteration 10526, Loss: 36.840240478515625, L1: 10.486273765563965, L3: 26.353965759277344\n",
      "Current prediction:  61.315853118896484 \n",
      "\n",
      "Iteration 10527, Loss: 36.01252365112305, L1: 10.485067367553711, L3: 25.527456283569336\n",
      "Current prediction:  61.31289291381836 \n",
      "\n",
      "Iteration 10528, Loss: 37.46452331542969, L1: 10.50288200378418, L3: 26.961641311645508\n",
      "Current prediction:  61.30830383300781 \n",
      "\n",
      "Iteration 10529, Loss: 36.12510681152344, L1: 10.507061004638672, L3: 25.618043899536133\n",
      "Current prediction:  61.30366134643555 \n",
      "\n",
      "Iteration 10530, Loss: 36.575531005859375, L1: 10.492621421813965, L3: 26.082910537719727\n",
      "Current prediction:  61.29872131347656 \n",
      "\n",
      "Iteration 10531, Loss: 36.83055877685547, L1: 10.501453399658203, L3: 26.329105377197266\n",
      "Current prediction:  61.29338073730469 \n",
      "\n",
      "Iteration 10532, Loss: 36.26692199707031, L1: 10.5010404586792, L3: 25.76588249206543\n",
      "Current prediction:  61.29035949707031 \n",
      "\n",
      "Iteration 10533, Loss: 35.32209777832031, L1: 10.507493019104004, L3: 24.814603805541992\n",
      "Current prediction:  61.288291931152344 \n",
      "\n",
      "Iteration 10534, Loss: 35.9245719909668, L1: 10.52230167388916, L3: 25.40226936340332\n",
      "Current prediction:  61.284881591796875 \n",
      "\n",
      "Iteration 10535, Loss: 37.77208709716797, L1: 10.515869140625, L3: 27.25621795654297\n",
      "Current prediction:  61.278385162353516 \n",
      "\n",
      "Iteration 10536, Loss: 36.65513229370117, L1: 10.530285835266113, L3: 26.124847412109375\n",
      "Current prediction:  61.27231216430664 \n",
      "\n",
      "Iteration 10537, Loss: 35.98725891113281, L1: 10.533135414123535, L3: 25.454124450683594\n",
      "Current prediction:  61.2647819519043 \n",
      "\n",
      "Iteration 10538, Loss: 36.380653381347656, L1: 10.541280746459961, L3: 25.839370727539062\n",
      "Current prediction:  61.25871276855469 \n",
      "\n",
      "Iteration 10539, Loss: 36.6688232421875, L1: 10.552536010742188, L3: 26.11628532409668\n",
      "Current prediction:  61.253238677978516 \n",
      "\n",
      "Iteration 10540, Loss: 36.36161804199219, L1: 10.534893989562988, L3: 25.826725006103516\n",
      "Current prediction:  61.250362396240234 \n",
      "\n",
      "Iteration 10541, Loss: 37.388389587402344, L1: 10.54699420928955, L3: 26.841394424438477\n",
      "Current prediction:  61.2481689453125 \n",
      "\n",
      "Iteration 10542, Loss: 36.18012237548828, L1: 10.53658390045166, L3: 25.643539428710938\n",
      "Current prediction:  61.24805450439453 \n",
      "\n",
      "Iteration 10543, Loss: 35.952964782714844, L1: 10.534425735473633, L3: 25.418540954589844\n",
      "Current prediction:  61.25063705444336 \n",
      "\n",
      "Iteration 10544, Loss: 36.498802185058594, L1: 10.546463012695312, L3: 25.95233726501465\n",
      "Current prediction:  61.25149154663086 \n",
      "\n",
      "Iteration 10545, Loss: 36.45225524902344, L1: 10.548372268676758, L3: 25.903884887695312\n",
      "Current prediction:  61.25353240966797 \n",
      "\n",
      "Iteration 10546, Loss: 36.294551849365234, L1: 10.53591251373291, L3: 25.758638381958008\n",
      "Current prediction:  61.2585563659668 \n",
      "\n",
      "Iteration 10547, Loss: 36.89369583129883, L1: 10.54735279083252, L3: 26.346343994140625\n",
      "Current prediction:  61.26350784301758 \n",
      "\n",
      "Iteration 10548, Loss: 36.06026840209961, L1: 10.535443305969238, L3: 25.524824142456055\n",
      "Current prediction:  61.26958465576172 \n",
      "\n",
      "Iteration 10549, Loss: 36.431827545166016, L1: 10.529739379882812, L3: 25.902088165283203\n",
      "Current prediction:  61.274559020996094 \n",
      "\n",
      "Iteration 10550, Loss: 37.33375549316406, L1: 10.527119636535645, L3: 26.806636810302734\n",
      "Current prediction:  61.27872085571289 \n",
      "\n",
      "Iteration 10551, Loss: 37.37606430053711, L1: 10.523402214050293, L3: 26.852663040161133\n",
      "Current prediction:  61.28124237060547 \n",
      "\n",
      "Iteration 10552, Loss: 37.250885009765625, L1: 10.514961242675781, L3: 26.73592185974121\n",
      "Current prediction:  61.282379150390625 \n",
      "\n",
      "Iteration 10553, Loss: 35.89663314819336, L1: 10.526453971862793, L3: 25.37017822265625\n",
      "Current prediction:  61.28479766845703 \n",
      "\n",
      "Iteration 10554, Loss: 36.77021789550781, L1: 10.518790245056152, L3: 26.251428604125977\n",
      "Current prediction:  61.28557205200195 \n",
      "\n",
      "Iteration 10555, Loss: 35.13951873779297, L1: 10.497376441955566, L3: 24.642141342163086\n",
      "Current prediction:  61.289119720458984 \n",
      "\n",
      "Iteration 10556, Loss: 37.00882339477539, L1: 10.504910469055176, L3: 26.5039119720459\n",
      "Current prediction:  61.28822326660156 \n",
      "\n",
      "Iteration 10557, Loss: 36.3943977355957, L1: 10.509387969970703, L3: 25.885009765625\n",
      "Current prediction:  61.28604507446289 \n",
      "\n",
      "Iteration 10558, Loss: 36.54693603515625, L1: 10.509654998779297, L3: 26.03727912902832\n",
      "Current prediction:  61.28343963623047 \n",
      "\n",
      "Iteration 10559, Loss: 35.9171028137207, L1: 10.502886772155762, L3: 25.414215087890625\n",
      "Current prediction:  61.28031539916992 \n",
      "\n",
      "Iteration 10560, Loss: 36.47213363647461, L1: 10.519424438476562, L3: 25.952709197998047\n",
      "Current prediction:  61.277015686035156 \n",
      "\n",
      "Iteration 10561, Loss: 36.915653228759766, L1: 10.52971363067627, L3: 26.385940551757812\n",
      "Current prediction:  61.27534484863281 \n",
      "\n",
      "Iteration 10562, Loss: 37.186119079589844, L1: 10.535711288452148, L3: 26.650409698486328\n",
      "Current prediction:  61.27234649658203 \n",
      "\n",
      "Iteration 10563, Loss: 35.677757263183594, L1: 10.529067993164062, L3: 25.14868927001953\n",
      "Current prediction:  61.27009201049805 \n",
      "\n",
      "Iteration 10564, Loss: 37.50266647338867, L1: 10.523645401000977, L3: 26.979021072387695\n",
      "Current prediction:  61.2682991027832 \n",
      "\n",
      "Iteration 10565, Loss: 37.01206588745117, L1: 10.52779769897461, L3: 26.484268188476562\n",
      "Current prediction:  61.26691436767578 \n",
      "\n",
      "Iteration 10566, Loss: 35.983909606933594, L1: 10.533416748046875, L3: 25.450490951538086\n",
      "Current prediction:  61.26528549194336 \n",
      "\n",
      "Iteration 10567, Loss: 37.66145324707031, L1: 10.544271469116211, L3: 27.1171817779541\n",
      "Current prediction:  61.26205825805664 \n",
      "\n",
      "Iteration 10568, Loss: 36.89295196533203, L1: 10.538125991821289, L3: 26.35482406616211\n",
      "Current prediction:  61.25982666015625 \n",
      "\n",
      "Iteration 10569, Loss: 36.513999938964844, L1: 10.527259826660156, L3: 25.986738204956055\n",
      "Current prediction:  61.25992202758789 \n",
      "\n",
      "Iteration 10570, Loss: 35.837188720703125, L1: 10.538228034973145, L3: 25.298961639404297\n",
      "Current prediction:  61.25872802734375 \n",
      "\n",
      "Iteration 10571, Loss: 38.18410110473633, L1: 10.536702156066895, L3: 27.64739990234375\n",
      "Current prediction:  61.258766174316406 \n",
      "\n",
      "Iteration 10572, Loss: 36.34098434448242, L1: 10.535868644714355, L3: 25.805116653442383\n",
      "Current prediction:  61.25950622558594 \n",
      "\n",
      "Iteration 10573, Loss: 35.70232391357422, L1: 10.54271411895752, L3: 25.159608840942383\n",
      "Current prediction:  61.259822845458984 \n",
      "\n",
      "Iteration 10574, Loss: 37.37846374511719, L1: 10.555339813232422, L3: 26.823122024536133\n",
      "Current prediction:  61.26226806640625 \n",
      "\n",
      "Iteration 10575, Loss: 36.722591400146484, L1: 10.529763221740723, L3: 26.192827224731445\n",
      "Current prediction:  61.2647819519043 \n",
      "\n",
      "Iteration 10576, Loss: 36.66486358642578, L1: 10.540616035461426, L3: 26.12424659729004\n",
      "Current prediction:  61.266605377197266 \n",
      "\n",
      "Iteration 10577, Loss: 36.6092643737793, L1: 10.54137134552002, L3: 26.067893981933594\n",
      "Current prediction:  61.26841735839844 \n",
      "\n",
      "Iteration 10578, Loss: 35.89310836791992, L1: 10.525376319885254, L3: 25.367733001708984\n",
      "Current prediction:  61.267940521240234 \n",
      "\n",
      "Iteration 10579, Loss: 35.805381774902344, L1: 10.536559104919434, L3: 25.268821716308594\n",
      "Current prediction:  61.27011489868164 \n",
      "\n",
      "Iteration 10580, Loss: 37.21282196044922, L1: 10.536768913269043, L3: 26.676054000854492\n",
      "Current prediction:  61.27045822143555 \n",
      "\n",
      "Iteration 10581, Loss: 36.76496887207031, L1: 10.518241882324219, L3: 26.246728897094727\n",
      "Current prediction:  61.268165588378906 \n",
      "\n",
      "Iteration 10582, Loss: 37.660552978515625, L1: 10.529186248779297, L3: 27.13136863708496\n",
      "Current prediction:  61.26423645019531 \n",
      "\n",
      "Iteration 10583, Loss: 36.94317626953125, L1: 10.527360916137695, L3: 26.415815353393555\n",
      "Current prediction:  61.25876235961914 \n",
      "\n",
      "Iteration 10584, Loss: 36.176334381103516, L1: 10.529763221740723, L3: 25.64657211303711\n",
      "Current prediction:  61.25136184692383 \n",
      "\n",
      "Iteration 10585, Loss: 37.47095489501953, L1: 10.538347244262695, L3: 26.93260955810547\n",
      "Current prediction:  61.24383544921875 \n",
      "\n",
      "Iteration 10586, Loss: 35.68619155883789, L1: 10.55611515045166, L3: 25.130077362060547\n",
      "Current prediction:  61.23821258544922 \n",
      "\n",
      "Iteration 10587, Loss: 35.14733123779297, L1: 10.544121742248535, L3: 24.60321044921875\n",
      "Current prediction:  61.23564529418945 \n",
      "\n",
      "Iteration 10588, Loss: 36.92875671386719, L1: 10.55861759185791, L3: 26.37013816833496\n",
      "Current prediction:  61.23222351074219 \n",
      "\n",
      "Iteration 10589, Loss: 36.39453125, L1: 10.546674728393555, L3: 25.847858428955078\n",
      "Current prediction:  61.2306022644043 \n",
      "\n",
      "Iteration 10590, Loss: 36.43672561645508, L1: 10.56143569946289, L3: 25.875289916992188\n",
      "Current prediction:  61.22864532470703 \n",
      "\n",
      "Iteration 10591, Loss: 35.07994842529297, L1: 10.562177658081055, L3: 24.517772674560547\n",
      "Current prediction:  61.2309684753418 \n",
      "\n",
      "Iteration 10592, Loss: 37.08449172973633, L1: 10.56766128540039, L3: 26.516830444335938\n",
      "Current prediction:  61.2349967956543 \n",
      "\n",
      "Iteration 10593, Loss: 37.0306510925293, L1: 10.542393684387207, L3: 26.488256454467773\n",
      "Current prediction:  61.23614501953125 \n",
      "\n",
      "Iteration 10594, Loss: 37.33628845214844, L1: 10.549114227294922, L3: 26.787172317504883\n",
      "Current prediction:  61.23715591430664 \n",
      "\n",
      "Iteration 10595, Loss: 36.3866081237793, L1: 10.551020622253418, L3: 25.835586547851562\n",
      "Current prediction:  61.23865509033203 \n",
      "\n",
      "Iteration 10596, Loss: 35.948219299316406, L1: 10.540071487426758, L3: 25.408145904541016\n",
      "Current prediction:  61.2391471862793 \n",
      "\n",
      "Iteration 10597, Loss: 37.58171844482422, L1: 10.549718856811523, L3: 27.031999588012695\n",
      "Current prediction:  61.23823928833008 \n",
      "\n",
      "Iteration 10598, Loss: 37.119476318359375, L1: 10.54419231414795, L3: 26.575284957885742\n",
      "Current prediction:  61.2349967956543 \n",
      "\n",
      "Iteration 10599, Loss: 36.56778335571289, L1: 10.564626693725586, L3: 26.003156661987305\n",
      "Current prediction:  61.23395538330078 \n",
      "\n",
      "Iteration 10600, Loss: 37.02005386352539, L1: 10.537818908691406, L3: 26.482234954833984\n",
      "Current prediction:  61.23420715332031 \n",
      "\n",
      "Iteration 10601, Loss: 35.8963737487793, L1: 10.550836563110352, L3: 25.345537185668945\n",
      "Current prediction:  61.232887268066406 \n",
      "\n",
      "Iteration 10602, Loss: 36.938446044921875, L1: 10.551572799682617, L3: 26.38687515258789\n",
      "Current prediction:  61.231040954589844 \n",
      "\n",
      "Iteration 10603, Loss: 35.27790069580078, L1: 10.561548233032227, L3: 24.716350555419922\n",
      "Current prediction:  61.232887268066406 \n",
      "\n",
      "Iteration 10604, Loss: 37.066566467285156, L1: 10.555124282836914, L3: 26.51144027709961\n",
      "Current prediction:  61.232852935791016 \n",
      "\n",
      "Iteration 10605, Loss: 36.385589599609375, L1: 10.554292678833008, L3: 25.831296920776367\n",
      "Current prediction:  61.23468780517578 \n",
      "\n",
      "Iteration 10606, Loss: 37.487037658691406, L1: 10.552393913269043, L3: 26.93464469909668\n",
      "Current prediction:  61.234500885009766 \n",
      "\n",
      "Iteration 10607, Loss: 36.74097442626953, L1: 10.545002937316895, L3: 26.195972442626953\n",
      "Current prediction:  61.23515319824219 \n",
      "\n",
      "Iteration 10608, Loss: 36.15971755981445, L1: 10.562687873840332, L3: 25.597030639648438\n",
      "Current prediction:  61.24003601074219 \n",
      "\n",
      "Iteration 10609, Loss: 36.382381439208984, L1: 10.548087120056152, L3: 25.83429527282715\n",
      "Current prediction:  61.24647521972656 \n",
      "\n",
      "Iteration 10610, Loss: 36.205345153808594, L1: 10.550142288208008, L3: 25.65520477294922\n",
      "Current prediction:  61.251285552978516 \n",
      "\n",
      "Iteration 10611, Loss: 37.00132369995117, L1: 10.53914737701416, L3: 26.462177276611328\n",
      "Current prediction:  61.256614685058594 \n",
      "\n",
      "Iteration 10612, Loss: 35.7951545715332, L1: 10.532496452331543, L3: 25.262657165527344\n",
      "Current prediction:  61.26176071166992 \n",
      "\n",
      "Iteration 10613, Loss: 36.91541290283203, L1: 10.535612106323242, L3: 26.379802703857422\n",
      "Current prediction:  61.26565933227539 \n",
      "\n",
      "Iteration 10614, Loss: 36.208160400390625, L1: 10.53741455078125, L3: 25.670747756958008\n",
      "Current prediction:  61.269107818603516 \n",
      "\n",
      "Iteration 10615, Loss: 36.165977478027344, L1: 10.530315399169922, L3: 25.635662078857422\n",
      "Current prediction:  61.26976013183594 \n",
      "\n",
      "Iteration 10616, Loss: 36.92737579345703, L1: 10.526225090026855, L3: 26.40114974975586\n",
      "Current prediction:  61.27286148071289 \n",
      "\n",
      "Iteration 10617, Loss: 36.34834289550781, L1: 10.516927719116211, L3: 25.83141326904297\n",
      "Current prediction:  61.273094177246094 \n",
      "\n",
      "Iteration 10618, Loss: 37.51072692871094, L1: 10.523298263549805, L3: 26.987430572509766\n",
      "Current prediction:  61.27144241333008 \n",
      "\n",
      "Iteration 10619, Loss: 36.03360366821289, L1: 10.531119346618652, L3: 25.502483367919922\n",
      "Current prediction:  61.26929473876953 \n",
      "\n",
      "Iteration 10620, Loss: 36.72404479980469, L1: 10.530689239501953, L3: 26.193355560302734\n",
      "Current prediction:  61.26912307739258 \n",
      "\n",
      "Iteration 10621, Loss: 36.972171783447266, L1: 10.529668807983398, L3: 26.442502975463867\n",
      "Current prediction:  61.27082061767578 \n",
      "\n",
      "Iteration 10622, Loss: 37.582584381103516, L1: 10.52054500579834, L3: 27.06203842163086\n",
      "Current prediction:  61.27392578125 \n",
      "\n",
      "Iteration 10623, Loss: 37.63444137573242, L1: 10.52354907989502, L3: 27.110891342163086\n",
      "Current prediction:  61.27324676513672 \n",
      "\n",
      "Iteration 10624, Loss: 36.06156539916992, L1: 10.52065372467041, L3: 25.540910720825195\n",
      "Current prediction:  61.27399444580078 \n",
      "\n",
      "Iteration 10625, Loss: 36.93680953979492, L1: 10.519512176513672, L3: 26.41729736328125\n",
      "Current prediction:  61.27294158935547 \n",
      "\n",
      "Iteration 10626, Loss: 36.50909423828125, L1: 10.528870582580566, L3: 25.980222702026367\n",
      "Current prediction:  61.274925231933594 \n",
      "\n",
      "Iteration 10627, Loss: 36.507476806640625, L1: 10.526138305664062, L3: 25.98133659362793\n",
      "Current prediction:  61.27695083618164 \n",
      "\n",
      "Iteration 10628, Loss: 36.61363983154297, L1: 10.525103569030762, L3: 26.08853530883789\n",
      "Current prediction:  61.27970886230469 \n",
      "\n",
      "Iteration 10629, Loss: 36.9434928894043, L1: 10.524731636047363, L3: 26.418760299682617\n",
      "Current prediction:  61.28007125854492 \n",
      "\n",
      "Iteration 10630, Loss: 36.9604606628418, L1: 10.514927864074707, L3: 26.445531845092773\n",
      "Current prediction:  61.281272888183594 \n",
      "\n",
      "Iteration 10631, Loss: 35.726417541503906, L1: 10.517921447753906, L3: 25.208494186401367\n",
      "Current prediction:  61.28145980834961 \n",
      "\n",
      "Iteration 10632, Loss: 36.37346649169922, L1: 10.52598762512207, L3: 25.84747886657715\n",
      "Current prediction:  61.28062438964844 \n",
      "\n",
      "Iteration 10633, Loss: 37.21351623535156, L1: 10.528762817382812, L3: 26.684755325317383\n",
      "Current prediction:  61.27934265136719 \n",
      "\n",
      "Iteration 10634, Loss: 36.116432189941406, L1: 10.517593383789062, L3: 25.598840713500977\n",
      "Current prediction:  61.27756881713867 \n",
      "\n",
      "Iteration 10635, Loss: 37.690860748291016, L1: 10.519801139831543, L3: 27.171058654785156\n",
      "Current prediction:  61.274906158447266 \n",
      "\n",
      "Iteration 10636, Loss: 36.362709045410156, L1: 10.50961685180664, L3: 25.853092193603516\n",
      "Current prediction:  61.27024841308594 \n",
      "\n",
      "Iteration 10637, Loss: 36.65951919555664, L1: 10.520000457763672, L3: 26.13951873779297\n",
      "Current prediction:  61.267269134521484 \n",
      "\n",
      "Iteration 10638, Loss: 36.86811065673828, L1: 10.541827201843262, L3: 26.326284408569336\n",
      "Current prediction:  61.2666015625 \n",
      "\n",
      "Iteration 10639, Loss: 36.487857818603516, L1: 10.517007827758789, L3: 25.970849990844727\n",
      "Current prediction:  61.265872955322266 \n",
      "\n",
      "Iteration 10640, Loss: 37.056270599365234, L1: 10.541677474975586, L3: 26.51459312438965\n",
      "Current prediction:  61.26288986206055 \n",
      "\n",
      "Iteration 10641, Loss: 37.247161865234375, L1: 10.533565521240234, L3: 26.713594436645508\n",
      "Current prediction:  61.26021194458008 \n",
      "\n",
      "Iteration 10642, Loss: 36.61299133300781, L1: 10.52714729309082, L3: 26.085844039916992\n",
      "Current prediction:  61.259490966796875 \n",
      "\n",
      "Iteration 10643, Loss: 36.22449493408203, L1: 10.534384727478027, L3: 25.690109252929688\n",
      "Current prediction:  61.25779342651367 \n",
      "\n",
      "Iteration 10644, Loss: 36.62221145629883, L1: 10.530037879943848, L3: 26.092172622680664\n",
      "Current prediction:  61.2565803527832 \n",
      "\n",
      "Iteration 10645, Loss: 37.09789276123047, L1: 10.539371490478516, L3: 26.55851936340332\n",
      "Current prediction:  61.25707244873047 \n",
      "\n",
      "Iteration 10646, Loss: 36.55701446533203, L1: 10.536558151245117, L3: 26.020458221435547\n",
      "Current prediction:  61.258262634277344 \n",
      "\n",
      "Iteration 10647, Loss: 37.66077423095703, L1: 10.546087265014648, L3: 27.11468505859375\n",
      "Current prediction:  61.26118087768555 \n",
      "\n",
      "Iteration 10648, Loss: 37.105491638183594, L1: 10.540351867675781, L3: 26.565139770507812\n",
      "Current prediction:  61.26359176635742 \n",
      "\n",
      "Iteration 10649, Loss: 36.56650924682617, L1: 10.544261932373047, L3: 26.022247314453125\n",
      "Current prediction:  61.26763916015625 \n",
      "\n",
      "Iteration 10650, Loss: 36.521263122558594, L1: 10.513623237609863, L3: 26.007638931274414\n",
      "Current prediction:  61.26941680908203 \n",
      "\n",
      "Iteration 10651, Loss: 36.33060073852539, L1: 10.521785736083984, L3: 25.808815002441406\n",
      "Current prediction:  61.27194595336914 \n",
      "\n",
      "Iteration 10652, Loss: 36.66923141479492, L1: 10.514310836791992, L3: 26.15492057800293\n",
      "Current prediction:  61.27423858642578 \n",
      "\n",
      "Iteration 10653, Loss: 37.18896484375, L1: 10.536182403564453, L3: 26.652780532836914\n",
      "Current prediction:  61.2755126953125 \n",
      "\n",
      "Iteration 10654, Loss: 38.13405990600586, L1: 10.524178504943848, L3: 27.609880447387695\n",
      "Current prediction:  61.273292541503906 \n",
      "\n",
      "Iteration 10655, Loss: 37.36467742919922, L1: 10.516741752624512, L3: 26.847936630249023\n",
      "Current prediction:  61.26887130737305 \n",
      "\n",
      "Iteration 10656, Loss: 37.31499099731445, L1: 10.523816108703613, L3: 26.791175842285156\n",
      "Current prediction:  61.26338195800781 \n",
      "\n",
      "Iteration 10657, Loss: 36.52344512939453, L1: 10.540237426757812, L3: 25.98320770263672\n",
      "Current prediction:  61.258113861083984 \n",
      "\n",
      "Iteration 10658, Loss: 35.98304748535156, L1: 10.53872299194336, L3: 25.44432258605957\n",
      "Current prediction:  61.252777099609375 \n",
      "\n",
      "Iteration 10659, Loss: 36.79149627685547, L1: 10.537117958068848, L3: 26.254377365112305\n",
      "Current prediction:  61.24724578857422 \n",
      "\n",
      "Iteration 10660, Loss: 35.62847137451172, L1: 10.543617248535156, L3: 25.084854125976562\n",
      "Current prediction:  61.244205474853516 \n",
      "\n",
      "Iteration 10661, Loss: 37.37971878051758, L1: 10.54889965057373, L3: 26.83081817626953\n",
      "Current prediction:  61.240997314453125 \n",
      "\n",
      "Iteration 10662, Loss: 36.162994384765625, L1: 10.55329704284668, L3: 25.609699249267578\n",
      "Current prediction:  61.239830017089844 \n",
      "\n",
      "Iteration 10663, Loss: 36.138816833496094, L1: 10.555856704711914, L3: 25.58296012878418\n",
      "Current prediction:  61.239810943603516 \n",
      "\n",
      "Iteration 10664, Loss: 35.866390228271484, L1: 10.552334785461426, L3: 25.314056396484375\n",
      "Current prediction:  61.2415771484375 \n",
      "\n",
      "Iteration 10665, Loss: 35.664588928222656, L1: 10.53553295135498, L3: 25.129056930541992\n",
      "Current prediction:  61.24578094482422 \n",
      "\n",
      "Iteration 10666, Loss: 36.29214096069336, L1: 10.552837371826172, L3: 25.739303588867188\n",
      "Current prediction:  61.246795654296875 \n",
      "\n",
      "Iteration 10667, Loss: 36.81788635253906, L1: 10.553226470947266, L3: 26.26466178894043\n",
      "Current prediction:  61.24828338623047 \n",
      "\n",
      "Iteration 10668, Loss: 36.61257553100586, L1: 10.550031661987305, L3: 26.062543869018555\n",
      "Current prediction:  61.24995803833008 \n",
      "\n",
      "Iteration 10669, Loss: 35.704689025878906, L1: 10.54710578918457, L3: 25.15758514404297\n",
      "Current prediction:  61.25157165527344 \n",
      "\n",
      "Iteration 10670, Loss: 37.40692901611328, L1: 10.528075218200684, L3: 26.87885284423828\n",
      "Current prediction:  61.25437545776367 \n",
      "\n",
      "Iteration 10671, Loss: 36.33689498901367, L1: 10.534686088562012, L3: 25.802207946777344\n",
      "Current prediction:  61.25575637817383 \n",
      "\n",
      "Iteration 10672, Loss: 36.66147232055664, L1: 10.540419578552246, L3: 26.12105369567871\n",
      "Current prediction:  61.259647369384766 \n",
      "\n",
      "Iteration 10673, Loss: 36.24705505371094, L1: 10.535871505737305, L3: 25.711183547973633\n",
      "Current prediction:  61.26254653930664 \n",
      "\n",
      "Iteration 10674, Loss: 36.33915328979492, L1: 10.545403480529785, L3: 25.793750762939453\n",
      "Current prediction:  61.267333984375 \n",
      "\n",
      "Iteration 10675, Loss: 37.561920166015625, L1: 10.520150184631348, L3: 27.041770935058594\n",
      "Current prediction:  61.27033233642578 \n",
      "\n",
      "Iteration 10676, Loss: 36.44182205200195, L1: 10.533777236938477, L3: 25.908044815063477\n",
      "Current prediction:  61.27248001098633 \n",
      "\n",
      "Iteration 10677, Loss: 37.37469482421875, L1: 10.52807331085205, L3: 26.846622467041016\n",
      "Current prediction:  61.27180480957031 \n",
      "\n",
      "Iteration 10678, Loss: 35.945228576660156, L1: 10.519685745239258, L3: 25.4255428314209\n",
      "Current prediction:  61.27077102661133 \n",
      "\n",
      "Iteration 10679, Loss: 37.410377502441406, L1: 10.51710319519043, L3: 26.893274307250977\n",
      "Current prediction:  61.26607894897461 \n",
      "\n",
      "Iteration 10680, Loss: 36.308448791503906, L1: 10.53072452545166, L3: 25.77772331237793\n",
      "Current prediction:  61.26328659057617 \n",
      "\n",
      "Iteration 10681, Loss: 35.77936553955078, L1: 10.530234336853027, L3: 25.24913215637207\n",
      "Current prediction:  61.26123809814453 \n",
      "\n",
      "Iteration 10682, Loss: 36.5059928894043, L1: 10.536393165588379, L3: 25.9695987701416\n",
      "Current prediction:  61.25911331176758 \n",
      "\n",
      "Iteration 10683, Loss: 36.834075927734375, L1: 11.062877655029297, L3: 25.77120018005371\n",
      "Current prediction:  61.25764465332031 \n",
      "\n",
      "Iteration 10684, Loss: 36.76536560058594, L1: 10.536707878112793, L3: 26.228656768798828\n",
      "Current prediction:  61.25841522216797 \n",
      "\n",
      "Iteration 10685, Loss: 36.86173629760742, L1: 10.543960571289062, L3: 26.31777572631836\n",
      "Current prediction:  61.25825881958008 \n",
      "\n",
      "Iteration 10686, Loss: 36.99127197265625, L1: 10.533187866210938, L3: 26.458084106445312\n",
      "Current prediction:  61.25482940673828 \n",
      "\n",
      "Iteration 10687, Loss: 36.78245162963867, L1: 10.53099536895752, L3: 26.251455307006836\n",
      "Current prediction:  61.24811553955078 \n",
      "\n",
      "Iteration 10688, Loss: 36.97279357910156, L1: 10.554985046386719, L3: 26.41780662536621\n",
      "Current prediction:  61.24286651611328 \n",
      "\n",
      "Iteration 10689, Loss: 36.8506965637207, L1: 10.549216270446777, L3: 26.301481246948242\n",
      "Current prediction:  61.23652267456055 \n",
      "\n",
      "Iteration 10690, Loss: 37.149017333984375, L1: 10.557127952575684, L3: 26.591888427734375\n",
      "Current prediction:  61.23016357421875 \n",
      "\n",
      "Iteration 10691, Loss: 36.96364212036133, L1: 10.569218635559082, L3: 26.394424438476562\n",
      "Current prediction:  61.224430084228516 \n",
      "\n",
      "Iteration 10692, Loss: 35.85207748413086, L1: 10.56763744354248, L3: 25.284440994262695\n",
      "Current prediction:  61.21999740600586 \n",
      "\n",
      "Iteration 10693, Loss: 36.41011047363281, L1: 10.57079029083252, L3: 25.83932113647461\n",
      "Current prediction:  61.21763229370117 \n",
      "\n",
      "Iteration 10694, Loss: 37.259883880615234, L1: 10.573525428771973, L3: 26.686359405517578\n",
      "Current prediction:  61.21772766113281 \n",
      "\n",
      "Iteration 10695, Loss: 36.646820068359375, L1: 10.574625968933105, L3: 26.072193145751953\n",
      "Current prediction:  61.21971130371094 \n",
      "\n",
      "Iteration 10696, Loss: 35.939064025878906, L1: 10.574152946472168, L3: 25.364910125732422\n",
      "Current prediction:  61.220619201660156 \n",
      "\n",
      "Iteration 10697, Loss: 36.36488342285156, L1: 10.771596908569336, L3: 25.593286514282227\n",
      "Current prediction:  61.22575759887695 \n",
      "\n",
      "Iteration 10698, Loss: 36.98597717285156, L1: 10.560019493103027, L3: 26.42595672607422\n",
      "Current prediction:  61.2303352355957 \n",
      "\n",
      "Iteration 10699, Loss: 36.87068176269531, L1: 10.570690155029297, L3: 26.299989700317383\n",
      "Current prediction:  61.2332649230957 \n",
      "\n",
      "Iteration 10700, Loss: 36.57587814331055, L1: 10.554095268249512, L3: 26.02178192138672\n",
      "Current prediction:  61.239288330078125 \n",
      "\n",
      "Iteration 10701, Loss: 35.46543502807617, L1: 10.550641059875488, L3: 24.914793014526367\n",
      "Current prediction:  61.246124267578125 \n",
      "\n",
      "Iteration 10702, Loss: 37.330474853515625, L1: 10.537251472473145, L3: 26.793222427368164\n",
      "Current prediction:  61.25244903564453 \n",
      "\n",
      "Iteration 10703, Loss: 35.99774169921875, L1: 10.542386054992676, L3: 25.45535659790039\n",
      "Current prediction:  61.25593948364258 \n",
      "\n",
      "Iteration 10704, Loss: 36.57740020751953, L1: 10.543144226074219, L3: 26.03425407409668\n",
      "Current prediction:  61.25948715209961 \n",
      "\n",
      "Iteration 10705, Loss: 38.13273239135742, L1: 10.533455848693848, L3: 27.59927749633789\n",
      "Current prediction:  61.26155090332031 \n",
      "\n",
      "Iteration 10706, Loss: 35.09677505493164, L1: 10.534951210021973, L3: 24.56182289123535\n",
      "Current prediction:  61.26177978515625 \n",
      "\n",
      "Iteration 10707, Loss: 36.675899505615234, L1: 10.526321411132812, L3: 26.149578094482422\n",
      "Current prediction:  61.26048278808594 \n",
      "\n",
      "Iteration 10708, Loss: 36.971351623535156, L1: 10.535240173339844, L3: 26.43610954284668\n",
      "Current prediction:  61.26265335083008 \n",
      "\n",
      "Iteration 10709, Loss: 35.77091979980469, L1: 10.529184341430664, L3: 25.241735458374023\n",
      "Current prediction:  61.265350341796875 \n",
      "\n",
      "Iteration 10710, Loss: 36.36842346191406, L1: 10.534432411193848, L3: 25.8339900970459\n",
      "Current prediction:  61.26740264892578 \n",
      "\n",
      "Iteration 10711, Loss: 36.41340637207031, L1: 10.522294044494629, L3: 25.89111328125\n",
      "Current prediction:  61.2712287902832 \n",
      "\n",
      "Iteration 10712, Loss: 35.2175178527832, L1: 10.524373054504395, L3: 24.693143844604492\n",
      "Current prediction:  61.27741622924805 \n",
      "\n",
      "Iteration 10713, Loss: 36.85875701904297, L1: 10.516124725341797, L3: 26.34263038635254\n",
      "Current prediction:  61.283348083496094 \n",
      "\n",
      "Iteration 10714, Loss: 36.43354415893555, L1: 10.513620376586914, L3: 25.919923782348633\n",
      "Current prediction:  61.2915153503418 \n",
      "\n",
      "Iteration 10715, Loss: 37.676753997802734, L1: 10.507257461547852, L3: 27.169496536254883\n",
      "Current prediction:  61.30068588256836 \n",
      "\n",
      "Iteration 10716, Loss: 36.500511169433594, L1: 10.497295379638672, L3: 26.00321388244629\n",
      "Current prediction:  61.309120178222656 \n",
      "\n",
      "Iteration 10717, Loss: 37.028785705566406, L1: 10.497825622558594, L3: 26.530961990356445\n",
      "Current prediction:  61.31673812866211 \n",
      "\n",
      "Iteration 10718, Loss: 36.25310516357422, L1: 10.473345756530762, L3: 25.779760360717773\n",
      "Current prediction:  61.32417297363281 \n",
      "\n",
      "Iteration 10719, Loss: 36.17512130737305, L1: 10.484871864318848, L3: 25.690250396728516\n",
      "Current prediction:  61.33051681518555 \n",
      "\n",
      "Iteration 10720, Loss: 36.47220993041992, L1: 10.466769218444824, L3: 26.005441665649414\n",
      "Current prediction:  61.338600158691406 \n",
      "\n",
      "Iteration 10721, Loss: 37.16655731201172, L1: 10.474489212036133, L3: 26.692066192626953\n",
      "Current prediction:  61.342247009277344 \n",
      "\n",
      "Iteration 10722, Loss: 36.821529388427734, L1: 10.456971168518066, L3: 26.364559173583984\n",
      "Current prediction:  61.34437561035156 \n",
      "\n",
      "Iteration 10723, Loss: 36.50122833251953, L1: 10.469871520996094, L3: 26.03135871887207\n",
      "Current prediction:  61.344581604003906 \n",
      "\n",
      "Iteration 10724, Loss: 36.75383758544922, L1: 10.468255043029785, L3: 26.285581588745117\n",
      "Current prediction:  61.34431838989258 \n",
      "\n",
      "Iteration 10725, Loss: 37.12611389160156, L1: 10.450560569763184, L3: 26.675552368164062\n",
      "Current prediction:  61.34181594848633 \n",
      "\n",
      "Iteration 10726, Loss: 36.38715744018555, L1: 10.465057373046875, L3: 25.922100067138672\n",
      "Current prediction:  61.33488464355469 \n",
      "\n",
      "Iteration 10727, Loss: 36.45355224609375, L1: 10.454535484313965, L3: 25.99901580810547\n",
      "Current prediction:  61.32724380493164 \n",
      "\n",
      "Iteration 10728, Loss: 36.155147552490234, L1: 10.473909378051758, L3: 25.681238174438477\n",
      "Current prediction:  61.31953048706055 \n",
      "\n",
      "Iteration 10729, Loss: 36.56462478637695, L1: 10.476137161254883, L3: 26.08848762512207\n",
      "Current prediction:  61.31096267700195 \n",
      "\n",
      "Iteration 10730, Loss: 37.123477935791016, L1: 10.485259056091309, L3: 26.63821792602539\n",
      "Current prediction:  61.301761627197266 \n",
      "\n",
      "Iteration 10731, Loss: 35.92305374145508, L1: 10.49348258972168, L3: 25.4295711517334\n",
      "Current prediction:  61.28977966308594 \n",
      "\n",
      "Iteration 10732, Loss: 37.6018180847168, L1: 10.519254684448242, L3: 27.082563400268555\n",
      "Current prediction:  61.2750358581543 \n",
      "\n",
      "Iteration 10733, Loss: 37.647300720214844, L1: 10.520279884338379, L3: 27.12702178955078\n",
      "Current prediction:  61.265708923339844 \n",
      "\n",
      "Iteration 10734, Loss: 35.69715118408203, L1: 10.526418685913086, L3: 25.170730590820312\n",
      "Current prediction:  61.26078414916992 \n",
      "\n",
      "Iteration 10735, Loss: 36.27393341064453, L1: 10.528849601745605, L3: 25.74508285522461\n",
      "Current prediction:  61.25552749633789 \n",
      "\n",
      "Iteration 10736, Loss: 36.755619049072266, L1: 10.534128189086914, L3: 26.22149085998535\n",
      "Current prediction:  61.24934768676758 \n",
      "\n",
      "Iteration 10737, Loss: 36.40370178222656, L1: 10.549345016479492, L3: 25.854358673095703\n",
      "Current prediction:  61.243839263916016 \n",
      "\n",
      "Iteration 10738, Loss: 36.654335021972656, L1: 10.552888870239258, L3: 26.10144805908203\n",
      "Current prediction:  61.23933029174805 \n",
      "\n",
      "Iteration 10739, Loss: 36.90970993041992, L1: 10.546967506408691, L3: 26.362743377685547\n",
      "Current prediction:  61.23576736450195 \n",
      "\n",
      "Iteration 10740, Loss: 37.17692184448242, L1: 10.551607131958008, L3: 26.625314712524414\n",
      "Current prediction:  61.23394775390625 \n",
      "\n",
      "Iteration 10741, Loss: 36.773719787597656, L1: 10.557389259338379, L3: 26.216331481933594\n",
      "Current prediction:  61.231040954589844 \n",
      "\n",
      "Iteration 10742, Loss: 35.834251403808594, L1: 10.55504035949707, L3: 25.279212951660156\n",
      "Current prediction:  61.230690002441406 \n",
      "\n",
      "Iteration 10743, Loss: 36.75545120239258, L1: 10.56224250793457, L3: 26.193208694458008\n",
      "Current prediction:  61.23016357421875 \n",
      "\n",
      "Iteration 10744, Loss: 36.51832580566406, L1: 10.54954719543457, L3: 25.968780517578125\n",
      "Current prediction:  61.22976303100586 \n",
      "\n",
      "Iteration 10745, Loss: 35.63898849487305, L1: 10.5571870803833, L3: 25.081802368164062\n",
      "Current prediction:  61.229644775390625 \n",
      "\n",
      "Iteration 10746, Loss: 37.41983413696289, L1: 10.5506591796875, L3: 26.86917495727539\n",
      "Current prediction:  61.22719192504883 \n",
      "\n",
      "Iteration 10747, Loss: 36.5821418762207, L1: 10.563484191894531, L3: 26.018657684326172\n",
      "Current prediction:  61.2254753112793 \n",
      "\n",
      "Iteration 10748, Loss: 36.133087158203125, L1: 10.558525085449219, L3: 25.57456398010254\n",
      "Current prediction:  61.2243537902832 \n",
      "\n",
      "Iteration 10749, Loss: 37.35523223876953, L1: 10.561931610107422, L3: 26.793302536010742\n",
      "Current prediction:  61.2220573425293 \n",
      "\n",
      "Iteration 10750, Loss: 36.08929443359375, L1: 10.568463325500488, L3: 25.520832061767578\n",
      "Current prediction:  61.220726013183594 \n",
      "\n",
      "Iteration 10751, Loss: 37.862823486328125, L1: 10.576274871826172, L3: 27.28654670715332\n",
      "Current prediction:  61.218650817871094 \n",
      "\n",
      "Iteration 10752, Loss: 36.328983306884766, L1: 10.572667121887207, L3: 25.756317138671875\n",
      "Current prediction:  61.22049331665039 \n",
      "\n",
      "Iteration 10753, Loss: 36.90346145629883, L1: 10.569595336914062, L3: 26.333866119384766\n",
      "Current prediction:  61.22442626953125 \n",
      "\n",
      "Iteration 10754, Loss: 37.729366302490234, L1: 10.57174015045166, L3: 27.157625198364258\n",
      "Current prediction:  61.2237548828125 \n",
      "\n",
      "Iteration 10755, Loss: 35.62651062011719, L1: 10.555093765258789, L3: 25.071414947509766\n",
      "Current prediction:  61.22618103027344 \n",
      "\n",
      "Iteration 10756, Loss: 36.81045150756836, L1: 10.567530632019043, L3: 26.242921829223633\n",
      "Current prediction:  61.229339599609375 \n",
      "\n",
      "Iteration 10757, Loss: 37.78588104248047, L1: 10.557916641235352, L3: 27.227962493896484\n",
      "Current prediction:  61.23041915893555 \n",
      "\n",
      "Iteration 10758, Loss: 38.29370880126953, L1: 11.809391021728516, L3: 26.48431968688965\n",
      "Current prediction:  61.231109619140625 \n",
      "\n",
      "Iteration 10759, Loss: 36.98091125488281, L1: 10.800983428955078, L3: 26.179929733276367\n",
      "Current prediction:  61.23454284667969 \n",
      "\n",
      "Iteration 10760, Loss: 36.551849365234375, L1: 10.5421142578125, L3: 26.009733200073242\n",
      "Current prediction:  61.241455078125 \n",
      "\n",
      "Iteration 10761, Loss: 36.11378479003906, L1: 10.537325859069824, L3: 25.576459884643555\n",
      "Current prediction:  61.24958038330078 \n",
      "\n",
      "Iteration 10762, Loss: 37.82642364501953, L1: 10.53605842590332, L3: 27.290363311767578\n",
      "Current prediction:  61.25491714477539 \n",
      "\n",
      "Iteration 10763, Loss: 36.066864013671875, L1: 10.537216186523438, L3: 25.52964973449707\n",
      "Current prediction:  61.26115036010742 \n",
      "\n",
      "Iteration 10764, Loss: 36.74106216430664, L1: 11.524103164672852, L3: 25.21695899963379\n",
      "Current prediction:  61.26689529418945 \n",
      "\n",
      "Iteration 10765, Loss: 37.404876708984375, L1: 10.53073501586914, L3: 26.8741397857666\n",
      "Current prediction:  61.2712516784668 \n",
      "\n",
      "Iteration 10766, Loss: 36.91313934326172, L1: 10.530811309814453, L3: 26.382328033447266\n",
      "Current prediction:  61.27894973754883 \n",
      "\n",
      "Iteration 10767, Loss: 36.335426330566406, L1: 10.532124519348145, L3: 25.803300857543945\n",
      "Current prediction:  61.28275680541992 \n",
      "\n",
      "Iteration 10768, Loss: 36.19287109375, L1: 10.516351699829102, L3: 25.6765193939209\n",
      "Current prediction:  61.28386688232422 \n",
      "\n",
      "Iteration 10769, Loss: 36.32664489746094, L1: 10.519105911254883, L3: 25.807537078857422\n",
      "Current prediction:  61.28472900390625 \n",
      "\n",
      "Iteration 10770, Loss: 36.465187072753906, L1: 10.507617950439453, L3: 25.957569122314453\n",
      "Current prediction:  61.28406524658203 \n",
      "\n",
      "Iteration 10771, Loss: 35.989009857177734, L1: 10.521279335021973, L3: 25.467731475830078\n",
      "Current prediction:  61.28407287597656 \n",
      "\n",
      "Iteration 10772, Loss: 37.388641357421875, L1: 10.511225700378418, L3: 26.87741470336914\n",
      "Current prediction:  61.28358459472656 \n",
      "\n",
      "Iteration 10773, Loss: 36.87054443359375, L1: 10.504949569702148, L3: 26.36559295654297\n",
      "Current prediction:  61.2813606262207 \n",
      "\n",
      "Iteration 10774, Loss: 35.99038314819336, L1: 10.511500358581543, L3: 25.478883743286133\n",
      "Current prediction:  61.28060531616211 \n",
      "\n",
      "Iteration 10775, Loss: 37.15031051635742, L1: 10.52259349822998, L3: 26.627717971801758\n",
      "Current prediction:  61.27854537963867 \n",
      "\n",
      "Iteration 10776, Loss: 37.10381317138672, L1: 10.526788711547852, L3: 26.577024459838867\n",
      "Current prediction:  61.272132873535156 \n",
      "\n",
      "Iteration 10777, Loss: 36.25637435913086, L1: 10.51612377166748, L3: 25.740251541137695\n",
      "Current prediction:  61.26680374145508 \n",
      "\n",
      "Iteration 10778, Loss: 37.15583801269531, L1: 10.522826194763184, L3: 26.633012771606445\n",
      "Current prediction:  61.25659942626953 \n",
      "\n",
      "Iteration 10779, Loss: 36.36176300048828, L1: 10.530135154724121, L3: 25.831628799438477\n",
      "Current prediction:  61.24971008300781 \n",
      "\n",
      "Iteration 10780, Loss: 36.18637466430664, L1: 10.533909797668457, L3: 25.6524658203125\n",
      "Current prediction:  61.24347686767578 \n",
      "\n",
      "Iteration 10781, Loss: 37.25920104980469, L1: 10.554606437683105, L3: 26.704593658447266\n",
      "Current prediction:  61.239505767822266 \n",
      "\n",
      "Iteration 10782, Loss: 35.7022705078125, L1: 10.541341781616211, L3: 25.160930633544922\n",
      "Current prediction:  61.234832763671875 \n",
      "\n",
      "Iteration 10783, Loss: 35.8618049621582, L1: 10.547648429870605, L3: 25.31415557861328\n",
      "Current prediction:  61.233428955078125 \n",
      "\n",
      "Iteration 10784, Loss: 36.42718505859375, L1: 10.557921409606934, L3: 25.8692626953125\n",
      "Current prediction:  61.23373031616211 \n",
      "\n",
      "Iteration 10785, Loss: 36.50273132324219, L1: 10.557548522949219, L3: 25.945180892944336\n",
      "Current prediction:  61.233123779296875 \n",
      "\n",
      "Iteration 10786, Loss: 36.43171691894531, L1: 10.554448127746582, L3: 25.877267837524414\n",
      "Current prediction:  61.2329216003418 \n",
      "\n",
      "Iteration 10787, Loss: 36.379844665527344, L1: 10.558974266052246, L3: 25.82086944580078\n",
      "Current prediction:  61.23292541503906 \n",
      "\n",
      "Iteration 10788, Loss: 36.012184143066406, L1: 10.556238174438477, L3: 25.45594596862793\n",
      "Current prediction:  61.23417663574219 \n",
      "\n",
      "Iteration 10789, Loss: 36.80046463012695, L1: 10.5619535446167, L3: 26.238510131835938\n",
      "Current prediction:  61.237796783447266 \n",
      "\n",
      "Iteration 10790, Loss: 36.50391387939453, L1: 10.549173355102539, L3: 25.954742431640625\n",
      "Current prediction:  61.23834991455078 \n",
      "\n",
      "Iteration 10791, Loss: 36.15642547607422, L1: 10.553630828857422, L3: 25.60279655456543\n",
      "Current prediction:  61.23955535888672 \n",
      "\n",
      "Iteration 10792, Loss: 36.879241943359375, L1: 10.550040245056152, L3: 26.329200744628906\n",
      "Current prediction:  61.240299224853516 \n",
      "\n",
      "Iteration 10793, Loss: 37.33286666870117, L1: 10.545979499816895, L3: 26.786888122558594\n",
      "Current prediction:  61.24007034301758 \n",
      "\n",
      "Iteration 10794, Loss: 35.75479507446289, L1: 10.553146362304688, L3: 25.201648712158203\n",
      "Current prediction:  61.241153717041016 \n",
      "\n",
      "Iteration 10795, Loss: 37.17356872558594, L1: 10.558225631713867, L3: 26.615341186523438\n",
      "Current prediction:  61.24129104614258 \n",
      "\n",
      "Iteration 10796, Loss: 37.21977615356445, L1: 10.556052207946777, L3: 26.663724899291992\n",
      "Current prediction:  61.240020751953125 \n",
      "\n",
      "Iteration 10797, Loss: 35.97959899902344, L1: 10.55830192565918, L3: 25.421297073364258\n",
      "Current prediction:  61.24017333984375 \n",
      "\n",
      "Iteration 10798, Loss: 37.28327178955078, L1: 10.553589820861816, L3: 26.72968101501465\n",
      "Current prediction:  61.2408561706543 \n",
      "\n",
      "Iteration 10799, Loss: 37.38587951660156, L1: 10.548399925231934, L3: 26.837480545043945\n",
      "Current prediction:  61.24104309082031 \n",
      "\n",
      "Iteration 10800, Loss: 36.97421646118164, L1: 10.550869941711426, L3: 26.42334747314453\n",
      "Current prediction:  61.24073028564453 \n",
      "\n",
      "Iteration 10801, Loss: 36.25627899169922, L1: 10.545234680175781, L3: 25.71104621887207\n",
      "Current prediction:  61.239662170410156 \n",
      "\n",
      "Iteration 10802, Loss: 36.4203987121582, L1: 10.550347328186035, L3: 25.87005043029785\n",
      "Current prediction:  61.239315032958984 \n",
      "\n",
      "Iteration 10803, Loss: 35.70362091064453, L1: 10.549673080444336, L3: 25.153945922851562\n",
      "Current prediction:  61.23851013183594 \n",
      "\n",
      "Iteration 10804, Loss: 36.76232147216797, L1: 10.537221908569336, L3: 26.225099563598633\n",
      "Current prediction:  61.23978805541992 \n",
      "\n",
      "Iteration 10805, Loss: 36.468326568603516, L1: 10.547776222229004, L3: 25.920551300048828\n",
      "Current prediction:  61.24146270751953 \n",
      "\n",
      "Iteration 10806, Loss: 36.796382904052734, L1: 10.553828239440918, L3: 26.2425537109375\n",
      "Current prediction:  61.241493225097656 \n",
      "\n",
      "Iteration 10807, Loss: 36.782936096191406, L1: 10.551769256591797, L3: 26.231164932250977\n",
      "Current prediction:  61.23921203613281 \n",
      "\n",
      "Iteration 10808, Loss: 36.440818786621094, L1: 10.540501594543457, L3: 25.900318145751953\n",
      "Current prediction:  61.24016189575195 \n",
      "\n",
      "Iteration 10809, Loss: 36.717811584472656, L1: 10.549772262573242, L3: 26.16803741455078\n",
      "Current prediction:  61.24179458618164 \n",
      "\n",
      "Iteration 10810, Loss: 35.756568908691406, L1: 10.542776107788086, L3: 25.21379280090332\n",
      "Current prediction:  61.244346618652344 \n",
      "\n",
      "Iteration 10811, Loss: 37.63911437988281, L1: 10.536148071289062, L3: 27.10296630859375\n",
      "Current prediction:  61.24724197387695 \n",
      "\n",
      "Iteration 10812, Loss: 36.72734069824219, L1: 10.54489517211914, L3: 26.182445526123047\n",
      "Current prediction:  61.25242233276367 \n",
      "\n",
      "Iteration 10813, Loss: 37.14055633544922, L1: 10.533388137817383, L3: 26.607168197631836\n",
      "Current prediction:  61.25502395629883 \n",
      "\n",
      "Iteration 10814, Loss: 36.73582077026367, L1: 10.54135799407959, L3: 26.1944637298584\n",
      "Current prediction:  61.256954193115234 \n",
      "\n",
      "Iteration 10815, Loss: 36.32796859741211, L1: 10.530547142028809, L3: 25.797422409057617\n",
      "Current prediction:  61.258113861083984 \n",
      "\n",
      "Iteration 10816, Loss: 38.059814453125, L1: 10.537196159362793, L3: 27.522619247436523\n",
      "Current prediction:  61.25801086425781 \n",
      "\n",
      "Iteration 10817, Loss: 37.09959411621094, L1: 10.538658142089844, L3: 26.56093406677246\n",
      "Current prediction:  61.25868606567383 \n",
      "\n",
      "Iteration 10818, Loss: 37.13862609863281, L1: 10.52971076965332, L3: 26.608915328979492\n",
      "Current prediction:  61.260032653808594 \n",
      "\n",
      "Iteration 10819, Loss: 36.59005355834961, L1: 10.525947570800781, L3: 26.064105987548828\n",
      "Current prediction:  61.26388168334961 \n",
      "\n",
      "Iteration 10820, Loss: 36.149845123291016, L1: 10.524070739746094, L3: 25.625774383544922\n",
      "Current prediction:  61.26596450805664 \n",
      "\n",
      "Iteration 10821, Loss: 36.692474365234375, L1: 10.522947311401367, L3: 26.16952896118164\n",
      "Current prediction:  61.26893615722656 \n",
      "\n",
      "Iteration 10822, Loss: 36.365604400634766, L1: 10.525824546813965, L3: 25.839778900146484\n",
      "Current prediction:  61.271183013916016 \n",
      "\n",
      "Iteration 10823, Loss: 36.95216369628906, L1: 10.52879810333252, L3: 26.42336654663086\n",
      "Current prediction:  61.27115249633789 \n",
      "\n",
      "Iteration 10824, Loss: 35.56632614135742, L1: 10.530951499938965, L3: 25.03537368774414\n",
      "Current prediction:  61.27186584472656 \n",
      "\n",
      "Iteration 10825, Loss: 37.4427375793457, L1: 10.523041725158691, L3: 26.919696807861328\n",
      "Current prediction:  61.26839065551758 \n",
      "\n",
      "Iteration 10826, Loss: 35.58906555175781, L1: 10.519339561462402, L3: 25.069726943969727\n",
      "Current prediction:  61.267127990722656 \n",
      "\n",
      "Iteration 10827, Loss: 36.33158874511719, L1: 10.519552230834961, L3: 25.812034606933594\n",
      "Current prediction:  61.266563415527344 \n",
      "\n",
      "Iteration 10828, Loss: 36.157535552978516, L1: 10.523242950439453, L3: 25.634292602539062\n",
      "Current prediction:  61.26569366455078 \n",
      "\n",
      "Iteration 10829, Loss: 36.94408416748047, L1: 10.512213706970215, L3: 26.431869506835938\n",
      "Current prediction:  61.264732360839844 \n",
      "\n",
      "Iteration 10830, Loss: 37.6653938293457, L1: 10.519070625305176, L3: 27.146324157714844\n",
      "Current prediction:  61.26120376586914 \n",
      "\n",
      "Iteration 10831, Loss: 37.090328216552734, L1: 10.52780818939209, L3: 26.562519073486328\n",
      "Current prediction:  61.25679016113281 \n",
      "\n",
      "Iteration 10832, Loss: 36.31414031982422, L1: 10.527023315429688, L3: 25.78711700439453\n",
      "Current prediction:  61.25431823730469 \n",
      "\n",
      "Iteration 10833, Loss: 36.69197463989258, L1: 10.54080581665039, L3: 26.151168823242188\n",
      "Current prediction:  61.252803802490234 \n",
      "\n",
      "Iteration 10834, Loss: 37.1431770324707, L1: 10.54220199584961, L3: 26.600975036621094\n",
      "Current prediction:  61.249473571777344 \n",
      "\n",
      "Iteration 10835, Loss: 36.52400207519531, L1: 10.524576187133789, L3: 25.999425888061523\n",
      "Current prediction:  61.24419021606445 \n",
      "\n",
      "Iteration 10836, Loss: 37.126304626464844, L1: 10.55286693572998, L3: 26.573436737060547\n",
      "Current prediction:  61.24102783203125 \n",
      "\n",
      "Iteration 10837, Loss: 37.06029510498047, L1: 10.552470207214355, L3: 26.50782585144043\n",
      "Current prediction:  61.23876190185547 \n",
      "\n",
      "Iteration 10838, Loss: 37.59166717529297, L1: 10.542283058166504, L3: 27.04938507080078\n",
      "Current prediction:  61.23711013793945 \n",
      "\n",
      "Iteration 10839, Loss: 36.98201370239258, L1: 10.55988883972168, L3: 26.4221248626709\n",
      "Current prediction:  61.23509216308594 \n",
      "\n",
      "Iteration 10840, Loss: 36.837364196777344, L1: 10.552968978881836, L3: 26.284393310546875\n",
      "Current prediction:  61.23246383666992 \n",
      "\n",
      "Iteration 10841, Loss: 36.68484878540039, L1: 10.553446769714355, L3: 26.13140296936035\n",
      "Current prediction:  61.22989273071289 \n",
      "\n",
      "Iteration 10842, Loss: 37.80632781982422, L1: 10.545372009277344, L3: 27.260955810546875\n",
      "Current prediction:  61.224876403808594 \n",
      "\n",
      "Iteration 10843, Loss: 37.613624572753906, L1: 10.570764541625977, L3: 27.042861938476562\n",
      "Current prediction:  61.21800231933594 \n",
      "\n",
      "Iteration 10844, Loss: 36.88253402709961, L1: 10.577366828918457, L3: 26.30516815185547\n",
      "Current prediction:  61.21075439453125 \n",
      "\n",
      "Iteration 10845, Loss: 36.39389419555664, L1: 10.566439628601074, L3: 25.827455520629883\n",
      "Current prediction:  61.20338821411133 \n",
      "\n",
      "Iteration 10846, Loss: 35.90044403076172, L1: 10.583459854125977, L3: 25.31698226928711\n",
      "Current prediction:  61.199161529541016 \n",
      "\n",
      "Iteration 10847, Loss: 37.301910400390625, L1: 10.581787109375, L3: 26.720125198364258\n",
      "Current prediction:  61.19849395751953 \n",
      "\n",
      "Iteration 10848, Loss: 36.700279235839844, L1: 10.573410034179688, L3: 26.12687110900879\n",
      "Current prediction:  61.20042037963867 \n",
      "\n",
      "Iteration 10849, Loss: 35.89973068237305, L1: 10.564654350280762, L3: 25.3350772857666\n",
      "Current prediction:  61.201812744140625 \n",
      "\n",
      "Iteration 10850, Loss: 36.63323974609375, L1: 10.57644271850586, L3: 26.05679702758789\n",
      "Current prediction:  61.205589294433594 \n",
      "\n",
      "Iteration 10851, Loss: 36.82712173461914, L1: 10.580224990844727, L3: 26.246896743774414\n",
      "Current prediction:  61.21004867553711 \n",
      "\n",
      "Iteration 10852, Loss: 36.8986930847168, L1: 10.572518348693848, L3: 26.326175689697266\n",
      "Current prediction:  61.2180290222168 \n",
      "\n",
      "Iteration 10853, Loss: 35.991859436035156, L1: 10.562932014465332, L3: 25.42892837524414\n",
      "Current prediction:  61.22734451293945 \n",
      "\n",
      "Iteration 10854, Loss: 36.04826736450195, L1: 10.57009220123291, L3: 25.478174209594727\n",
      "Current prediction:  61.23943328857422 \n",
      "\n",
      "Iteration 10855, Loss: 35.84124755859375, L1: 10.546424865722656, L3: 25.294822692871094\n",
      "Current prediction:  61.25079345703125 \n",
      "\n",
      "Iteration 10856, Loss: 36.51845169067383, L1: 10.532137870788574, L3: 25.986312866210938\n",
      "Current prediction:  61.263282775878906 \n",
      "\n",
      "Iteration 10857, Loss: 36.72948455810547, L1: 10.522385597229004, L3: 26.20709800720215\n",
      "Current prediction:  61.274620056152344 \n",
      "\n",
      "Iteration 10858, Loss: 36.38634490966797, L1: 10.520998001098633, L3: 25.865346908569336\n",
      "Current prediction:  61.285972595214844 \n",
      "\n",
      "Iteration 10859, Loss: 37.607383728027344, L1: 10.508712768554688, L3: 27.098669052124023\n",
      "Current prediction:  61.29419708251953 \n",
      "\n",
      "Iteration 10860, Loss: 36.40746307373047, L1: 10.489082336425781, L3: 25.91838264465332\n",
      "Current prediction:  61.299503326416016 \n",
      "\n",
      "Iteration 10861, Loss: 36.57661056518555, L1: 10.5010347366333, L3: 26.07557487487793\n",
      "Current prediction:  61.301727294921875 \n",
      "\n",
      "Iteration 10862, Loss: 36.46990203857422, L1: 10.479870796203613, L3: 25.990032196044922\n",
      "Current prediction:  61.30141830444336 \n",
      "\n",
      "Iteration 10863, Loss: 36.1348991394043, L1: 10.493709564208984, L3: 25.641189575195312\n",
      "Current prediction:  61.30312728881836 \n",
      "\n",
      "Iteration 10864, Loss: 36.1851692199707, L1: 10.479551315307617, L3: 25.705617904663086\n",
      "Current prediction:  61.30670166015625 \n",
      "\n",
      "Iteration 10865, Loss: 36.03195571899414, L1: 10.47865104675293, L3: 25.55330467224121\n",
      "Current prediction:  61.30869674682617 \n",
      "\n",
      "Iteration 10866, Loss: 36.19062805175781, L1: 10.489066123962402, L3: 25.701562881469727\n",
      "Current prediction:  61.30857467651367 \n",
      "\n",
      "Iteration 10867, Loss: 36.59929656982422, L1: 10.487067222595215, L3: 26.11223030090332\n",
      "Current prediction:  61.30904769897461 \n",
      "\n",
      "Iteration 10868, Loss: 37.083683013916016, L1: 10.484993934631348, L3: 26.598690032958984\n",
      "Current prediction:  61.30753707885742 \n",
      "\n",
      "Iteration 10869, Loss: 36.66936111450195, L1: 10.488446235656738, L3: 26.18091583251953\n",
      "Current prediction:  61.30765151977539 \n",
      "\n",
      "Iteration 10870, Loss: 35.12269973754883, L1: 10.493317604064941, L3: 24.629383087158203\n",
      "Current prediction:  61.308494567871094 \n",
      "\n",
      "Iteration 10871, Loss: 36.483741760253906, L1: 10.480337142944336, L3: 26.00340461730957\n",
      "Current prediction:  61.309078216552734 \n",
      "\n",
      "Iteration 10872, Loss: 36.00961685180664, L1: 10.488274574279785, L3: 25.52134132385254\n",
      "Current prediction:  61.30919647216797 \n",
      "\n",
      "Iteration 10873, Loss: 36.97683334350586, L1: 10.472046852111816, L3: 26.50478744506836\n",
      "Current prediction:  61.30757522583008 \n",
      "\n",
      "Iteration 10874, Loss: 36.77793884277344, L1: 10.49603271484375, L3: 26.281906127929688\n",
      "Current prediction:  61.303977966308594 \n",
      "\n",
      "Iteration 10875, Loss: 36.35009765625, L1: 10.49271011352539, L3: 25.85738754272461\n",
      "Current prediction:  61.30008316040039 \n",
      "\n",
      "Iteration 10876, Loss: 36.038516998291016, L1: 10.489080429077148, L3: 25.549436569213867\n",
      "Current prediction:  61.29395294189453 \n",
      "\n",
      "Iteration 10877, Loss: 36.69694137573242, L1: 10.50402545928955, L3: 26.192914962768555\n",
      "Current prediction:  61.28828430175781 \n",
      "\n",
      "Iteration 10878, Loss: 36.844703674316406, L1: 10.503034591674805, L3: 26.3416690826416\n",
      "Current prediction:  61.27883529663086 \n",
      "\n",
      "Iteration 10879, Loss: 35.97810363769531, L1: 10.512434005737305, L3: 25.46567153930664\n",
      "Current prediction:  61.267845153808594 \n",
      "\n",
      "Iteration 10880, Loss: 36.06459045410156, L1: 10.524615287780762, L3: 25.539976119995117\n",
      "Current prediction:  61.25978469848633 \n",
      "\n",
      "Iteration 10881, Loss: 35.90679168701172, L1: 10.518536567687988, L3: 25.388256072998047\n",
      "Current prediction:  61.25369644165039 \n",
      "\n",
      "Iteration 10882, Loss: 35.547080993652344, L1: 10.53284740447998, L3: 25.014232635498047\n",
      "Current prediction:  61.249359130859375 \n",
      "\n",
      "Iteration 10883, Loss: 35.835811614990234, L1: 10.539554595947266, L3: 25.29625701904297\n",
      "Current prediction:  61.2457275390625 \n",
      "\n",
      "Iteration 10884, Loss: 36.624454498291016, L1: 10.540627479553223, L3: 26.083826065063477\n",
      "Current prediction:  61.2449836730957 \n",
      "\n",
      "Iteration 10885, Loss: 36.80821228027344, L1: 10.528685569763184, L3: 26.27952766418457\n",
      "Current prediction:  61.245059967041016 \n",
      "\n",
      "Iteration 10886, Loss: 35.75978088378906, L1: 10.547826766967773, L3: 25.211952209472656\n",
      "Current prediction:  61.24454116821289 \n",
      "\n",
      "Iteration 10887, Loss: 35.58464431762695, L1: 10.53476619720459, L3: 25.04987907409668\n",
      "Current prediction:  61.24700927734375 \n",
      "\n",
      "Iteration 10888, Loss: 36.89948272705078, L1: 10.538901329040527, L3: 26.360580444335938\n",
      "Current prediction:  61.24705123901367 \n",
      "\n",
      "Iteration 10889, Loss: 36.88002395629883, L1: 10.53836727142334, L3: 26.341655731201172\n",
      "Current prediction:  61.25105285644531 \n",
      "\n",
      "Iteration 10890, Loss: 36.70842361450195, L1: 10.545211791992188, L3: 26.163211822509766\n",
      "Current prediction:  61.256038665771484 \n",
      "\n",
      "Iteration 10891, Loss: 36.84153747558594, L1: 10.524575233459473, L3: 26.31696319580078\n",
      "Current prediction:  61.26150131225586 \n",
      "\n",
      "Iteration 10892, Loss: 36.950767517089844, L1: 10.526559829711914, L3: 26.424209594726562\n",
      "Current prediction:  61.26741409301758 \n",
      "\n",
      "Iteration 10893, Loss: 37.64898681640625, L1: 10.512762069702148, L3: 27.1362247467041\n",
      "Current prediction:  61.2717399597168 \n",
      "\n",
      "Iteration 10894, Loss: 37.43135070800781, L1: 10.513452529907227, L3: 26.91790008544922\n",
      "Current prediction:  61.274845123291016 \n",
      "\n",
      "Iteration 10895, Loss: 36.248199462890625, L1: 10.518807411193848, L3: 25.72939109802246\n",
      "Current prediction:  61.27688980102539 \n",
      "\n",
      "Iteration 10896, Loss: 36.90547561645508, L1: 10.509563446044922, L3: 26.395912170410156\n",
      "Current prediction:  61.2792854309082 \n",
      "\n",
      "Iteration 10897, Loss: 35.408172607421875, L1: 10.505843162536621, L3: 24.90233039855957\n",
      "Current prediction:  61.28085708618164 \n",
      "\n",
      "Iteration 10898, Loss: 36.74553680419922, L1: 10.51305866241455, L3: 26.23247718811035\n",
      "Current prediction:  61.2806510925293 \n",
      "\n",
      "Iteration 10899, Loss: 36.441646575927734, L1: 10.511958122253418, L3: 25.929689407348633\n",
      "Current prediction:  61.28078079223633 \n",
      "\n",
      "Iteration 10900, Loss: 37.16474151611328, L1: 10.507745742797852, L3: 26.656997680664062\n",
      "Current prediction:  61.275047302246094 \n",
      "\n",
      "Iteration 10901, Loss: 36.18553161621094, L1: 10.507494926452637, L3: 25.678037643432617\n",
      "Current prediction:  61.269805908203125 \n",
      "\n",
      "Iteration 10902, Loss: 36.40168762207031, L1: 10.504579544067383, L3: 25.89710807800293\n",
      "Current prediction:  61.26513671875 \n",
      "\n",
      "Iteration 10903, Loss: 36.524742126464844, L1: 10.522921562194824, L3: 26.001819610595703\n",
      "Current prediction:  61.26047134399414 \n",
      "\n",
      "Iteration 10904, Loss: 35.92100524902344, L1: 10.524500846862793, L3: 25.39650535583496\n",
      "Current prediction:  61.258541107177734 \n",
      "\n",
      "Iteration 10905, Loss: 35.799949645996094, L1: 10.516483306884766, L3: 25.283466339111328\n",
      "Current prediction:  61.25398254394531 \n",
      "\n",
      "Iteration 10906, Loss: 37.05640411376953, L1: 10.530823707580566, L3: 26.52558135986328\n",
      "Current prediction:  61.25148391723633 \n",
      "\n",
      "Iteration 10907, Loss: 36.68891143798828, L1: 10.527538299560547, L3: 26.1613712310791\n",
      "Current prediction:  61.24873733520508 \n",
      "\n",
      "Iteration 10908, Loss: 36.6170654296875, L1: 10.5381498336792, L3: 26.078914642333984\n",
      "Current prediction:  61.248355865478516 \n",
      "\n",
      "Iteration 10909, Loss: 36.467872619628906, L1: 10.524908065795898, L3: 25.94296646118164\n",
      "Current prediction:  61.24740219116211 \n",
      "\n",
      "Iteration 10910, Loss: 35.79004669189453, L1: 10.533491134643555, L3: 25.256555557250977\n",
      "Current prediction:  61.246952056884766 \n",
      "\n",
      "Iteration 10911, Loss: 36.875144958496094, L1: 10.540332794189453, L3: 26.334810256958008\n",
      "Current prediction:  61.24407958984375 \n",
      "\n",
      "Iteration 10912, Loss: 36.57891082763672, L1: 10.543424606323242, L3: 26.03548812866211\n",
      "Current prediction:  61.24350357055664 \n",
      "\n",
      "Iteration 10913, Loss: 36.36518478393555, L1: 10.530884742736816, L3: 25.834299087524414\n",
      "Current prediction:  61.244407653808594 \n",
      "\n",
      "Iteration 10914, Loss: 37.09023666381836, L1: 10.544478416442871, L3: 26.545759201049805\n",
      "Current prediction:  61.245361328125 \n",
      "\n",
      "Iteration 10915, Loss: 37.09973907470703, L1: 10.549046516418457, L3: 26.55069351196289\n",
      "Current prediction:  61.2459831237793 \n",
      "\n",
      "Iteration 10916, Loss: 36.37149429321289, L1: 10.543519973754883, L3: 25.827974319458008\n",
      "Current prediction:  61.24599075317383 \n",
      "\n",
      "Iteration 10917, Loss: 35.92744445800781, L1: 10.536697387695312, L3: 25.3907470703125\n",
      "Current prediction:  61.24630355834961 \n",
      "\n",
      "Iteration 10918, Loss: 36.74346160888672, L1: 10.539436340332031, L3: 26.204025268554688\n",
      "Current prediction:  61.24721908569336 \n",
      "\n",
      "Iteration 10919, Loss: 36.032859802246094, L1: 10.537019729614258, L3: 25.495838165283203\n",
      "Current prediction:  61.24953079223633 \n",
      "\n",
      "Iteration 10920, Loss: 36.81751251220703, L1: 10.546360969543457, L3: 26.271150588989258\n",
      "Current prediction:  61.252838134765625 \n",
      "\n",
      "Iteration 10921, Loss: 37.33453369140625, L1: 10.535669326782227, L3: 26.79886245727539\n",
      "Current prediction:  61.25650405883789 \n",
      "\n",
      "Iteration 10922, Loss: 37.26816177368164, L1: 10.538318634033203, L3: 26.729843139648438\n",
      "Current prediction:  61.259307861328125 \n",
      "\n",
      "Iteration 10923, Loss: 36.07094955444336, L1: 10.534173011779785, L3: 25.53677749633789\n",
      "Current prediction:  61.260719299316406 \n",
      "\n",
      "Iteration 10924, Loss: 37.566627502441406, L1: 10.523091316223145, L3: 27.043535232543945\n",
      "Current prediction:  61.26093673706055 \n",
      "\n",
      "Iteration 10925, Loss: 35.6428337097168, L1: 10.534317970275879, L3: 25.1085147857666\n",
      "Current prediction:  61.261016845703125 \n",
      "\n",
      "Iteration 10926, Loss: 36.62028503417969, L1: 10.519184112548828, L3: 26.101099014282227\n",
      "Current prediction:  61.26390838623047 \n",
      "\n",
      "Iteration 10927, Loss: 36.60283660888672, L1: 10.521796226501465, L3: 26.08104133605957\n",
      "Current prediction:  61.26749038696289 \n",
      "\n",
      "Iteration 10928, Loss: 36.9622688293457, L1: 10.518671035766602, L3: 26.4435977935791\n",
      "Current prediction:  61.26572036743164 \n",
      "\n",
      "Iteration 10929, Loss: 36.464412689208984, L1: 10.529424667358398, L3: 25.934988021850586\n",
      "Current prediction:  61.26435852050781 \n",
      "\n",
      "Iteration 10930, Loss: 36.79298400878906, L1: 10.525277137756348, L3: 26.2677059173584\n",
      "Current prediction:  61.26250076293945 \n",
      "\n",
      "Iteration 10931, Loss: 36.6427116394043, L1: 10.519759178161621, L3: 26.12295150756836\n",
      "Current prediction:  61.26183319091797 \n",
      "\n",
      "Iteration 10932, Loss: 36.74494171142578, L1: 10.516036987304688, L3: 26.228906631469727\n",
      "Current prediction:  61.261573791503906 \n",
      "\n",
      "Iteration 10933, Loss: 37.10185241699219, L1: 10.528970718383789, L3: 26.57288360595703\n",
      "Current prediction:  61.259830474853516 \n",
      "\n",
      "Iteration 10934, Loss: 35.463253021240234, L1: 10.531054496765137, L3: 24.932199478149414\n",
      "Current prediction:  61.259525299072266 \n",
      "\n",
      "Iteration 10935, Loss: 37.901031494140625, L1: 10.535970687866211, L3: 27.365062713623047\n",
      "Current prediction:  61.259334564208984 \n",
      "\n",
      "Iteration 10936, Loss: 37.007347106933594, L1: 10.545257568359375, L3: 26.462087631225586\n",
      "Current prediction:  61.2629508972168 \n",
      "\n",
      "Iteration 10937, Loss: 37.3812141418457, L1: 10.532912254333496, L3: 26.848302841186523\n",
      "Current prediction:  61.26551055908203 \n",
      "\n",
      "Iteration 10938, Loss: 37.70170974731445, L1: 10.521146774291992, L3: 27.18056297302246\n",
      "Current prediction:  61.2661247253418 \n",
      "\n",
      "Iteration 10939, Loss: 37.484954833984375, L1: 10.521149635314941, L3: 26.96380615234375\n",
      "Current prediction:  61.26472854614258 \n",
      "\n",
      "Iteration 10940, Loss: 37.11142349243164, L1: 10.53332233428955, L3: 26.578100204467773\n",
      "Current prediction:  61.26294708251953 \n",
      "\n",
      "Iteration 10941, Loss: 36.37577819824219, L1: 10.526073455810547, L3: 25.84970474243164\n",
      "Current prediction:  61.26034927368164 \n",
      "\n",
      "Iteration 10942, Loss: 36.272491455078125, L1: 10.529228210449219, L3: 25.74326515197754\n",
      "Current prediction:  61.258853912353516 \n",
      "\n",
      "Iteration 10943, Loss: 36.1478271484375, L1: 10.537555694580078, L3: 25.61026954650879\n",
      "Current prediction:  61.257598876953125 \n",
      "\n",
      "Iteration 10944, Loss: 35.896602630615234, L1: 10.533032417297363, L3: 25.363571166992188\n",
      "Current prediction:  61.25503921508789 \n",
      "\n",
      "Iteration 10945, Loss: 35.804649353027344, L1: 10.523200035095215, L3: 25.281450271606445\n",
      "Current prediction:  61.25464630126953 \n",
      "\n",
      "Iteration 10946, Loss: 37.07495880126953, L1: 10.53665828704834, L3: 26.538299560546875\n",
      "Current prediction:  61.256797790527344 \n",
      "\n",
      "Iteration 10947, Loss: 36.93472671508789, L1: 10.527909278869629, L3: 26.406818389892578\n",
      "Current prediction:  61.26108932495117 \n",
      "\n",
      "Iteration 10948, Loss: 36.28947067260742, L1: 10.526835441589355, L3: 25.76263427734375\n",
      "Current prediction:  61.266239166259766 \n",
      "\n",
      "Iteration 10949, Loss: 36.197898864746094, L1: 10.53349494934082, L3: 25.664405822753906\n",
      "Current prediction:  61.273597717285156 \n",
      "\n",
      "Iteration 10950, Loss: 37.25274658203125, L1: 10.510432243347168, L3: 26.742313385009766\n",
      "Current prediction:  61.27460479736328 \n",
      "\n",
      "Iteration 10951, Loss: 36.96590042114258, L1: 10.51707935333252, L3: 26.448822021484375\n",
      "Current prediction:  61.275577545166016 \n",
      "\n",
      "Iteration 10952, Loss: 35.197837829589844, L1: 10.51323127746582, L3: 24.684606552124023\n",
      "Current prediction:  61.28023147583008 \n",
      "\n",
      "Iteration 10953, Loss: 36.14550018310547, L1: 10.513630867004395, L3: 25.631868362426758\n",
      "Current prediction:  61.2819938659668 \n",
      "\n",
      "Iteration 10954, Loss: 36.15615463256836, L1: 10.515290260314941, L3: 25.640865325927734\n",
      "Current prediction:  61.28410720825195 \n",
      "\n",
      "Iteration 10955, Loss: 36.59130859375, L1: 10.503448486328125, L3: 26.087862014770508\n",
      "Current prediction:  61.28584671020508 \n",
      "\n",
      "Iteration 10956, Loss: 36.575016021728516, L1: 10.515165328979492, L3: 26.059850692749023\n",
      "Current prediction:  61.287418365478516 \n",
      "\n",
      "Iteration 10957, Loss: 36.943241119384766, L1: 10.50761890411377, L3: 26.43562126159668\n",
      "Current prediction:  61.28970718383789 \n",
      "\n",
      "Iteration 10958, Loss: 36.01445007324219, L1: 10.50132942199707, L3: 25.51312255859375\n",
      "Current prediction:  61.29364013671875 \n",
      "\n",
      "Iteration 10959, Loss: 35.88782501220703, L1: 10.503954887390137, L3: 25.38387107849121\n",
      "Current prediction:  61.296077728271484 \n",
      "\n",
      "Iteration 10960, Loss: 35.8604736328125, L1: 10.496540069580078, L3: 25.36393165588379\n",
      "Current prediction:  61.29593276977539 \n",
      "\n",
      "Iteration 10961, Loss: 35.925498962402344, L1: 10.502758026123047, L3: 25.422739028930664\n",
      "Current prediction:  61.295684814453125 \n",
      "\n",
      "Iteration 10962, Loss: 36.54082489013672, L1: 10.49966049194336, L3: 26.041166305541992\n",
      "Current prediction:  61.29413604736328 \n",
      "\n",
      "Iteration 10963, Loss: 36.73698043823242, L1: 10.49752426147461, L3: 26.239456176757812\n",
      "Current prediction:  61.28883743286133 \n",
      "\n",
      "Iteration 10964, Loss: 35.86189270019531, L1: 10.5023832321167, L3: 25.359508514404297\n",
      "Current prediction:  61.282676696777344 \n",
      "\n",
      "Iteration 10965, Loss: 37.13224411010742, L1: 10.507043838500977, L3: 26.625200271606445\n",
      "Current prediction:  61.27550506591797 \n",
      "\n",
      "Iteration 10966, Loss: 36.509822845458984, L1: 10.515389442443848, L3: 25.994434356689453\n",
      "Current prediction:  61.268714904785156 \n",
      "\n",
      "Iteration 10967, Loss: 37.757015228271484, L1: 10.52121639251709, L3: 27.23579978942871\n",
      "Current prediction:  61.2586784362793 \n",
      "\n",
      "Iteration 10968, Loss: 36.790679931640625, L1: 10.527910232543945, L3: 26.26276969909668\n",
      "Current prediction:  61.25129318237305 \n",
      "\n",
      "Iteration 10969, Loss: 36.387603759765625, L1: 10.531678199768066, L3: 25.855926513671875\n",
      "Current prediction:  61.241825103759766 \n",
      "\n",
      "Iteration 10970, Loss: 37.299652099609375, L1: 10.548606872558594, L3: 26.751047134399414\n",
      "Current prediction:  61.23544692993164 \n",
      "\n",
      "Iteration 10971, Loss: 36.91157150268555, L1: 10.547137260437012, L3: 26.36443519592285\n",
      "Current prediction:  61.23001480102539 \n",
      "\n",
      "Iteration 10972, Loss: 36.71541976928711, L1: 10.55403995513916, L3: 26.161378860473633\n",
      "Current prediction:  61.223934173583984 \n",
      "\n",
      "Iteration 10973, Loss: 36.47007751464844, L1: 10.554019927978516, L3: 25.916059494018555\n",
      "Current prediction:  61.21978759765625 \n",
      "\n",
      "Iteration 10974, Loss: 37.30717849731445, L1: 10.559090614318848, L3: 26.74808692932129\n",
      "Current prediction:  61.21363067626953 \n",
      "\n",
      "Iteration 10975, Loss: 36.23274230957031, L1: 10.571183204650879, L3: 25.661558151245117\n",
      "Current prediction:  61.210594177246094 \n",
      "\n",
      "Iteration 10976, Loss: 36.072566986083984, L1: 10.572966575622559, L3: 25.49959945678711\n",
      "Current prediction:  61.20922088623047 \n",
      "\n",
      "Iteration 10977, Loss: 37.1204833984375, L1: 10.574467658996582, L3: 26.5460147857666\n",
      "Current prediction:  61.208038330078125 \n",
      "\n",
      "Iteration 10978, Loss: 36.864532470703125, L1: 10.577339172363281, L3: 26.287193298339844\n",
      "Current prediction:  61.20532989501953 \n",
      "\n",
      "Iteration 10979, Loss: 36.68534851074219, L1: 10.576045989990234, L3: 26.109302520751953\n",
      "Current prediction:  61.20444869995117 \n",
      "\n",
      "Iteration 10980, Loss: 36.11690902709961, L1: 10.579049110412598, L3: 25.537858963012695\n",
      "Current prediction:  61.20661544799805 \n",
      "\n",
      "Iteration 10981, Loss: 37.424522399902344, L1: 10.572006225585938, L3: 26.852516174316406\n",
      "Current prediction:  61.20903778076172 \n",
      "\n",
      "Iteration 10982, Loss: 36.840946197509766, L1: 10.569976806640625, L3: 26.27096939086914\n",
      "Current prediction:  61.210662841796875 \n",
      "\n",
      "Iteration 10983, Loss: 36.64899826049805, L1: 10.571733474731445, L3: 26.0772647857666\n",
      "Current prediction:  61.21384048461914 \n",
      "\n",
      "Iteration 10984, Loss: 36.18215560913086, L1: 10.56760311126709, L3: 25.614553451538086\n",
      "Current prediction:  61.21757125854492 \n",
      "\n",
      "Iteration 10985, Loss: 36.45931625366211, L1: 10.573655128479004, L3: 25.88566017150879\n",
      "Current prediction:  61.22319793701172 \n",
      "\n",
      "Iteration 10986, Loss: 35.56319046020508, L1: 10.568432807922363, L3: 24.9947566986084\n",
      "Current prediction:  61.23030471801758 \n",
      "\n",
      "Iteration 10987, Loss: 36.999664306640625, L1: 10.55241870880127, L3: 26.44724464416504\n",
      "Current prediction:  61.23534393310547 \n",
      "\n",
      "Iteration 10988, Loss: 36.957725524902344, L1: 10.552750587463379, L3: 26.40497589111328\n",
      "Current prediction:  61.23834228515625 \n",
      "\n",
      "Iteration 10989, Loss: 36.73073196411133, L1: 10.556500434875488, L3: 26.174232482910156\n",
      "Current prediction:  61.2375373840332 \n",
      "\n",
      "Iteration 10990, Loss: 37.59227752685547, L1: 10.55596923828125, L3: 27.036306381225586\n",
      "Current prediction:  61.23759841918945 \n",
      "\n",
      "Iteration 10991, Loss: 36.316715240478516, L1: 10.543272972106934, L3: 25.773441314697266\n",
      "Current prediction:  61.2398567199707 \n",
      "\n",
      "Iteration 10992, Loss: 36.27930450439453, L1: 10.542733192443848, L3: 25.736570358276367\n",
      "Current prediction:  61.24019241333008 \n",
      "\n",
      "Iteration 10993, Loss: 36.906341552734375, L1: 10.546186447143555, L3: 26.360157012939453\n",
      "Current prediction:  61.24348449707031 \n",
      "\n",
      "Iteration 10994, Loss: 36.01323699951172, L1: 10.548986434936523, L3: 25.464250564575195\n",
      "Current prediction:  61.24903869628906 \n",
      "\n",
      "Iteration 10995, Loss: 36.61580276489258, L1: 10.53886604309082, L3: 26.076936721801758\n",
      "Current prediction:  61.25303649902344 \n",
      "\n",
      "Iteration 10996, Loss: 36.07296371459961, L1: 10.534910202026367, L3: 25.538053512573242\n",
      "Current prediction:  61.25764465332031 \n",
      "\n",
      "Iteration 10997, Loss: 37.06657028198242, L1: 10.548796653747559, L3: 26.517772674560547\n",
      "Current prediction:  61.26060104370117 \n",
      "\n",
      "Iteration 10998, Loss: 37.03715896606445, L1: 10.527774810791016, L3: 26.509384155273438\n",
      "Current prediction:  61.26239013671875 \n",
      "\n",
      "â†³ LR reduced to 2.5e-04 at iteration 11000 \n",
      "\n",
      "Iteration 10999, Loss: 36.596675872802734, L1: 10.533553123474121, L3: 26.063121795654297\n",
      "Current prediction:  61.26520919799805 \n",
      "\n",
      "Iteration 11000, Loss: 36.67915725708008, L1: 10.537845611572266, L3: 26.141311645507812\n",
      "Current prediction:  61.26730728149414 \n",
      "\n",
      "Iteration 11001, Loss: 35.63316345214844, L1: 10.526626586914062, L3: 25.106536865234375\n",
      "Current prediction:  61.26921081542969 \n",
      "\n",
      "Iteration 11002, Loss: 35.75182342529297, L1: 10.522480010986328, L3: 25.22934341430664\n",
      "Current prediction:  61.270240783691406 \n",
      "\n",
      "Iteration 11003, Loss: 36.62853240966797, L1: 10.530831336975098, L3: 26.097702026367188\n",
      "Current prediction:  61.27199935913086 \n",
      "\n",
      "Iteration 11004, Loss: 35.925052642822266, L1: 10.528892517089844, L3: 25.396160125732422\n",
      "Current prediction:  61.27232360839844 \n",
      "\n",
      "Iteration 11005, Loss: 36.373538970947266, L1: 10.520448684692383, L3: 25.853090286254883\n",
      "Current prediction:  61.274749755859375 \n",
      "\n",
      "Iteration 11006, Loss: 36.08655548095703, L1: 10.523521423339844, L3: 25.56303596496582\n",
      "Current prediction:  61.27600860595703 \n",
      "\n",
      "Iteration 11007, Loss: 36.672786712646484, L1: 10.5122652053833, L3: 26.1605224609375\n",
      "Current prediction:  61.275360107421875 \n",
      "\n",
      "Iteration 11008, Loss: 36.67220687866211, L1: 10.514394760131836, L3: 26.157812118530273\n",
      "Current prediction:  61.27677536010742 \n",
      "\n",
      "Iteration 11009, Loss: 37.140541076660156, L1: 10.522209167480469, L3: 26.61833381652832\n",
      "Current prediction:  61.27542495727539 \n",
      "\n",
      "Iteration 11010, Loss: 36.54792785644531, L1: 10.518549919128418, L3: 26.02937889099121\n",
      "Current prediction:  61.27503204345703 \n",
      "\n",
      "Iteration 11011, Loss: 36.05111312866211, L1: 10.515875816345215, L3: 25.53523826599121\n",
      "Current prediction:  61.27469253540039 \n",
      "\n",
      "Iteration 11012, Loss: 36.42011260986328, L1: 10.522323608398438, L3: 25.89778709411621\n",
      "Current prediction:  61.277496337890625 \n",
      "\n",
      "Iteration 11013, Loss: 36.757625579833984, L1: 10.521368980407715, L3: 26.236255645751953\n",
      "Current prediction:  61.27992248535156 \n",
      "\n",
      "Iteration 11014, Loss: 36.56882095336914, L1: 10.506255149841309, L3: 26.06256675720215\n",
      "Current prediction:  61.280784606933594 \n",
      "\n",
      "Iteration 11015, Loss: 36.99254608154297, L1: 10.52027702331543, L3: 26.47226905822754\n",
      "Current prediction:  61.28108596801758 \n",
      "\n",
      "Iteration 11016, Loss: 36.657325744628906, L1: 10.507966041564941, L3: 26.14936065673828\n",
      "Current prediction:  61.27775955200195 \n",
      "\n",
      "Iteration 11017, Loss: 36.49200439453125, L1: 10.51823616027832, L3: 25.97376823425293\n",
      "Current prediction:  61.276771545410156 \n",
      "\n",
      "Iteration 11018, Loss: 36.93010711669922, L1: 10.520681381225586, L3: 26.409427642822266\n",
      "Current prediction:  61.27568435668945 \n",
      "\n",
      "Iteration 11019, Loss: 36.94044876098633, L1: 10.51927375793457, L3: 26.421175003051758\n",
      "Current prediction:  61.27342987060547 \n",
      "\n",
      "Iteration 11020, Loss: 35.21762466430664, L1: 10.523137092590332, L3: 24.694486618041992\n",
      "Current prediction:  61.27342987060547 \n",
      "\n",
      "Iteration 11021, Loss: 37.2133674621582, L1: 10.51077938079834, L3: 26.702587127685547\n",
      "Current prediction:  61.27114486694336 \n",
      "\n",
      "Iteration 11022, Loss: 37.00086212158203, L1: 10.521227836608887, L3: 26.47963523864746\n",
      "Current prediction:  61.268104553222656 \n",
      "\n",
      "Iteration 11023, Loss: 37.067222595214844, L1: 10.51710033416748, L3: 26.550121307373047\n",
      "Current prediction:  61.269596099853516 \n",
      "\n",
      "Iteration 11024, Loss: 37.30906677246094, L1: 10.510064125061035, L3: 26.79900360107422\n",
      "Current prediction:  61.269615173339844 \n",
      "\n",
      "Iteration 11025, Loss: 36.05976867675781, L1: 10.515067100524902, L3: 25.544700622558594\n",
      "Current prediction:  61.26741027832031 \n",
      "\n",
      "Iteration 11026, Loss: 36.96797180175781, L1: 10.524316787719727, L3: 26.443653106689453\n",
      "Current prediction:  61.2631950378418 \n",
      "\n",
      "Iteration 11027, Loss: 36.76593780517578, L1: 10.519251823425293, L3: 26.246686935424805\n",
      "Current prediction:  61.25716781616211 \n",
      "\n",
      "Iteration 11028, Loss: 35.778316497802734, L1: 10.532388687133789, L3: 25.245927810668945\n",
      "Current prediction:  61.25470733642578 \n",
      "\n",
      "Iteration 11029, Loss: 36.16736602783203, L1: 10.532865524291992, L3: 25.634502410888672\n",
      "Current prediction:  61.252872467041016 \n",
      "\n",
      "Iteration 11030, Loss: 36.38665008544922, L1: 10.538671493530273, L3: 25.847978591918945\n",
      "Current prediction:  61.25186538696289 \n",
      "\n",
      "Iteration 11031, Loss: 35.528282165527344, L1: 10.545297622680664, L3: 24.982986450195312\n",
      "Current prediction:  61.25266647338867 \n",
      "\n",
      "Iteration 11032, Loss: 36.66399383544922, L1: 10.537246704101562, L3: 26.126745223999023\n",
      "Current prediction:  61.255027770996094 \n",
      "\n",
      "Iteration 11033, Loss: 37.47732162475586, L1: 10.530560493469238, L3: 26.946762084960938\n",
      "Current prediction:  61.2573127746582 \n",
      "\n",
      "Iteration 11034, Loss: 36.11949920654297, L1: 10.528751373291016, L3: 25.590749740600586\n",
      "Current prediction:  61.256202697753906 \n",
      "\n",
      "Iteration 11035, Loss: 37.538997650146484, L1: 10.538784980773926, L3: 27.000211715698242\n",
      "Current prediction:  61.255985260009766 \n",
      "\n",
      "Iteration 11036, Loss: 37.609825134277344, L1: 10.527735710144043, L3: 27.082090377807617\n",
      "Current prediction:  61.25314712524414 \n",
      "\n",
      "Iteration 11037, Loss: 37.20512390136719, L1: 10.539012908935547, L3: 26.666112899780273\n",
      "Current prediction:  61.24916458129883 \n",
      "\n",
      "Iteration 11038, Loss: 36.797523498535156, L1: 10.537338256835938, L3: 26.26018714904785\n",
      "Current prediction:  61.24671936035156 \n",
      "\n",
      "Iteration 11039, Loss: 37.52934265136719, L1: 10.536921501159668, L3: 26.992422103881836\n",
      "Current prediction:  61.242671966552734 \n",
      "\n",
      "Iteration 11040, Loss: 36.27178192138672, L1: 10.549529075622559, L3: 25.722251892089844\n",
      "Current prediction:  61.23797607421875 \n",
      "\n",
      "Iteration 11041, Loss: 36.64701843261719, L1: 10.553217887878418, L3: 26.093799591064453\n",
      "Current prediction:  61.23592758178711 \n",
      "\n",
      "Iteration 11042, Loss: 36.812461853027344, L1: 10.542746543884277, L3: 26.269716262817383\n",
      "Current prediction:  61.23342514038086 \n",
      "\n",
      "Iteration 11043, Loss: 36.65037536621094, L1: 10.554888725280762, L3: 26.095487594604492\n",
      "Current prediction:  61.226444244384766 \n",
      "\n",
      "Iteration 11044, Loss: 37.00728988647461, L1: 10.563309669494629, L3: 26.443981170654297\n",
      "Current prediction:  61.216888427734375 \n",
      "\n",
      "Iteration 11045, Loss: 36.921112060546875, L1: 10.556365966796875, L3: 26.364744186401367\n",
      "Current prediction:  61.208160400390625 \n",
      "\n",
      "Iteration 11046, Loss: 36.523319244384766, L1: 10.576918601989746, L3: 25.946399688720703\n",
      "Current prediction:  61.20098876953125 \n",
      "\n",
      "Iteration 11047, Loss: 37.65557861328125, L1: 10.575179100036621, L3: 27.080400466918945\n",
      "Current prediction:  61.19480514526367 \n",
      "\n",
      "Iteration 11048, Loss: 36.818084716796875, L1: 10.598978996276855, L3: 26.219106674194336\n",
      "Current prediction:  61.190345764160156 \n",
      "\n",
      "Iteration 11049, Loss: 36.963157653808594, L1: 10.57560920715332, L3: 26.387550354003906\n",
      "Current prediction:  61.18695831298828 \n",
      "\n",
      "Iteration 11050, Loss: 36.70301055908203, L1: 10.58925724029541, L3: 26.113752365112305\n",
      "Current prediction:  61.18504333496094 \n",
      "\n",
      "Iteration 11051, Loss: 37.2440299987793, L1: 10.598700523376465, L3: 26.64533042907715\n",
      "Current prediction:  61.18351364135742 \n",
      "\n",
      "Iteration 11052, Loss: 36.10260772705078, L1: 10.589007377624512, L3: 25.513601303100586\n",
      "Current prediction:  61.1847038269043 \n",
      "\n",
      "Iteration 11053, Loss: 36.77009582519531, L1: 10.590200424194336, L3: 26.17989730834961\n",
      "Current prediction:  61.1909065246582 \n",
      "\n",
      "Iteration 11054, Loss: 36.57208251953125, L1: 10.593286514282227, L3: 25.97879409790039\n",
      "Current prediction:  61.19670867919922 \n",
      "\n",
      "Iteration 11055, Loss: 36.74296569824219, L1: 10.585399627685547, L3: 26.157564163208008\n",
      "Current prediction:  61.204227447509766 \n",
      "\n",
      "Iteration 11056, Loss: 36.750308990478516, L1: 10.575629234313965, L3: 26.174678802490234\n",
      "Current prediction:  61.21281814575195 \n",
      "\n",
      "Iteration 11057, Loss: 36.40409469604492, L1: 10.560461044311523, L3: 25.8436336517334\n",
      "Current prediction:  61.22064971923828 \n",
      "\n",
      "Iteration 11058, Loss: 35.614261627197266, L1: 10.565817832946777, L3: 25.048442840576172\n",
      "Current prediction:  61.231651306152344 \n",
      "\n",
      "Iteration 11059, Loss: 35.867103576660156, L1: 10.56701946258545, L3: 25.300085067749023\n",
      "Current prediction:  61.24224090576172 \n",
      "\n",
      "Iteration 11060, Loss: 36.54486083984375, L1: 10.547392845153809, L3: 25.997467041015625\n",
      "Current prediction:  61.25376510620117 \n",
      "\n",
      "Iteration 11061, Loss: 37.5814208984375, L1: 10.531960487365723, L3: 27.04945945739746\n",
      "Current prediction:  61.26105499267578 \n",
      "\n",
      "Iteration 11062, Loss: 36.37580490112305, L1: 10.531883239746094, L3: 25.843921661376953\n",
      "Current prediction:  61.2686767578125 \n",
      "\n",
      "Iteration 11063, Loss: 36.35477828979492, L1: 10.514297485351562, L3: 25.84048080444336\n",
      "Current prediction:  61.274681091308594 \n",
      "\n",
      "Iteration 11064, Loss: 36.62765884399414, L1: 10.52554702758789, L3: 26.10211181640625\n",
      "Current prediction:  61.278526306152344 \n",
      "\n",
      "Iteration 11065, Loss: 36.35780334472656, L1: 10.512173652648926, L3: 25.84562873840332\n",
      "Current prediction:  61.27765655517578 \n",
      "\n",
      "Iteration 11066, Loss: 35.660911560058594, L1: 10.515320777893066, L3: 25.14558982849121\n",
      "Current prediction:  61.2782096862793 \n",
      "\n",
      "Iteration 11067, Loss: 36.769962310791016, L1: 10.515273094177246, L3: 26.254688262939453\n",
      "Current prediction:  61.27566146850586 \n",
      "\n",
      "Iteration 11068, Loss: 37.38151550292969, L1: 10.516393661499023, L3: 26.865121841430664\n",
      "Current prediction:  61.273197174072266 \n",
      "\n",
      "Iteration 11069, Loss: 35.86140441894531, L1: 10.523992538452148, L3: 25.337413787841797\n",
      "Current prediction:  61.27198028564453 \n",
      "\n",
      "Iteration 11070, Loss: 35.802154541015625, L1: 10.518863677978516, L3: 25.28329086303711\n",
      "Current prediction:  61.272186279296875 \n",
      "\n",
      "Iteration 11071, Loss: 37.310997009277344, L1: 10.53328800201416, L3: 26.7777099609375\n",
      "Current prediction:  61.272212982177734 \n",
      "\n",
      "Iteration 11072, Loss: 36.8670654296875, L1: 10.525273323059082, L3: 26.3417911529541\n",
      "Current prediction:  61.27288055419922 \n",
      "\n",
      "Iteration 11073, Loss: 36.698890686035156, L1: 10.515682220458984, L3: 26.183208465576172\n",
      "Current prediction:  61.27286911010742 \n",
      "\n",
      "Iteration 11074, Loss: 37.08112335205078, L1: 10.523077964782715, L3: 26.55804443359375\n",
      "Current prediction:  61.27108383178711 \n",
      "\n",
      "Iteration 11075, Loss: 35.70113754272461, L1: 10.520551681518555, L3: 25.180585861206055\n",
      "Current prediction:  61.27231216430664 \n",
      "\n",
      "Iteration 11076, Loss: 36.8321533203125, L1: 10.521997451782227, L3: 26.310155868530273\n",
      "Current prediction:  61.27263641357422 \n",
      "\n",
      "Iteration 11077, Loss: 35.92464065551758, L1: 10.513880729675293, L3: 25.41075897216797\n",
      "Current prediction:  61.27506637573242 \n",
      "\n",
      "Iteration 11078, Loss: 36.516075134277344, L1: 10.518887519836426, L3: 25.9971866607666\n",
      "Current prediction:  61.274715423583984 \n",
      "\n",
      "Iteration 11079, Loss: 37.262447357177734, L1: 10.530797958374023, L3: 26.73164939880371\n",
      "Current prediction:  61.2733268737793 \n",
      "\n",
      "Iteration 11080, Loss: 38.21734619140625, L1: 10.513572692871094, L3: 27.70377540588379\n",
      "Current prediction:  61.26400375366211 \n",
      "\n",
      "Iteration 11081, Loss: 36.07204818725586, L1: 10.522439002990723, L3: 25.549610137939453\n",
      "Current prediction:  61.2555046081543 \n",
      "\n",
      "Iteration 11082, Loss: 36.01054382324219, L1: 10.537294387817383, L3: 25.473249435424805\n",
      "Current prediction:  61.24781036376953 \n",
      "\n",
      "Iteration 11083, Loss: 36.942996978759766, L1: 10.534393310546875, L3: 26.40860366821289\n",
      "Current prediction:  61.241764068603516 \n",
      "\n",
      "Iteration 11084, Loss: 36.43574523925781, L1: 10.543724060058594, L3: 25.89202117919922\n",
      "Current prediction:  61.23636245727539 \n",
      "\n",
      "Iteration 11085, Loss: 36.154296875, L1: 10.549448013305664, L3: 25.60485076904297\n",
      "Current prediction:  61.2338981628418 \n",
      "\n",
      "Iteration 11086, Loss: 36.562252044677734, L1: 10.559134483337402, L3: 26.00311851501465\n",
      "Current prediction:  61.2326545715332 \n",
      "\n",
      "Iteration 11087, Loss: 36.78388214111328, L1: 10.553506851196289, L3: 26.230377197265625\n",
      "Current prediction:  61.233558654785156 \n",
      "\n",
      "Iteration 11088, Loss: 36.75141525268555, L1: 10.547052383422852, L3: 26.204362869262695\n",
      "Current prediction:  61.23668670654297 \n",
      "\n",
      "Iteration 11089, Loss: 36.657554626464844, L1: 10.560855865478516, L3: 26.09670066833496\n",
      "Current prediction:  61.240638732910156 \n",
      "\n",
      "Iteration 11090, Loss: 36.82658767700195, L1: 10.544906616210938, L3: 26.281681060791016\n",
      "Current prediction:  61.24420166015625 \n",
      "\n",
      "Iteration 11091, Loss: 36.7401237487793, L1: 10.547699928283691, L3: 26.192424774169922\n",
      "Current prediction:  61.249271392822266 \n",
      "\n",
      "Iteration 11092, Loss: 37.320777893066406, L1: 10.55398178100586, L3: 26.766794204711914\n",
      "Current prediction:  61.2530517578125 \n",
      "\n",
      "Iteration 11093, Loss: 36.56253433227539, L1: 10.524921417236328, L3: 26.037612915039062\n",
      "Current prediction:  61.25714111328125 \n",
      "\n",
      "Iteration 11094, Loss: 36.63634490966797, L1: 10.527000427246094, L3: 26.109346389770508\n",
      "Current prediction:  61.260833740234375 \n",
      "\n",
      "Iteration 11095, Loss: 36.14966583251953, L1: 10.544025421142578, L3: 25.605640411376953\n",
      "Current prediction:  61.26426696777344 \n",
      "\n",
      "Iteration 11096, Loss: 36.529335021972656, L1: 10.519205093383789, L3: 26.010129928588867\n",
      "Current prediction:  61.26688003540039 \n",
      "\n",
      "Iteration 11097, Loss: 36.55400848388672, L1: 10.519988059997559, L3: 26.034019470214844\n",
      "Current prediction:  61.26940155029297 \n",
      "\n",
      "Iteration 11098, Loss: 35.991943359375, L1: 10.520689010620117, L3: 25.471254348754883\n",
      "Current prediction:  61.27158737182617 \n",
      "\n",
      "Iteration 11099, Loss: 36.80681610107422, L1: 10.524066925048828, L3: 26.282747268676758\n",
      "Current prediction:  61.271732330322266 \n",
      "\n",
      "Iteration 11100, Loss: 37.31489181518555, L1: 10.516587257385254, L3: 26.79830551147461\n",
      "Current prediction:  61.26969528198242 \n",
      "\n",
      "Iteration 11101, Loss: 36.41170883178711, L1: 10.529537200927734, L3: 25.882171630859375\n",
      "Current prediction:  61.26539611816406 \n",
      "\n",
      "Iteration 11102, Loss: 36.405303955078125, L1: 10.530763626098633, L3: 25.874540328979492\n",
      "Current prediction:  61.261383056640625 \n",
      "\n",
      "Iteration 11103, Loss: 35.89030456542969, L1: 10.523780822753906, L3: 25.366525650024414\n",
      "Current prediction:  61.25697326660156 \n",
      "\n",
      "Iteration 11104, Loss: 36.12612533569336, L1: 10.523863792419434, L3: 25.60226058959961\n",
      "Current prediction:  61.255802154541016 \n",
      "\n",
      "Iteration 11105, Loss: 36.56855392456055, L1: 10.534440994262695, L3: 26.03411293029785\n",
      "Current prediction:  61.25332260131836 \n",
      "\n",
      "Iteration 11106, Loss: 36.79405212402344, L1: 10.539368629455566, L3: 26.254684448242188\n",
      "Current prediction:  61.251033782958984 \n",
      "\n",
      "Iteration 11107, Loss: 36.31210708618164, L1: 10.530627250671387, L3: 25.78148078918457\n",
      "Current prediction:  61.24945068359375 \n",
      "\n",
      "Iteration 11108, Loss: 35.94053649902344, L1: 10.541919708251953, L3: 25.39861488342285\n",
      "Current prediction:  61.24695587158203 \n",
      "\n",
      "Iteration 11109, Loss: 36.56568908691406, L1: 10.541231155395508, L3: 26.024456024169922\n",
      "Current prediction:  61.24340057373047 \n",
      "\n",
      "Iteration 11110, Loss: 36.715484619140625, L1: 10.54315185546875, L3: 26.172330856323242\n",
      "Current prediction:  61.23920822143555 \n",
      "\n",
      "Iteration 11111, Loss: 36.52727127075195, L1: 10.550469398498535, L3: 25.976802825927734\n",
      "Current prediction:  61.23630142211914 \n",
      "\n",
      "Iteration 11112, Loss: 36.21808624267578, L1: 10.542793273925781, L3: 25.675291061401367\n",
      "Current prediction:  61.23358917236328 \n",
      "\n",
      "Iteration 11113, Loss: 35.6533203125, L1: 10.554553031921387, L3: 25.098766326904297\n",
      "Current prediction:  61.235294342041016 \n",
      "\n",
      "Iteration 11114, Loss: 36.900535583496094, L1: 10.554471969604492, L3: 26.34606170654297\n",
      "Current prediction:  61.23836898803711 \n",
      "\n",
      "Iteration 11115, Loss: 35.98406219482422, L1: 10.550697326660156, L3: 25.433364868164062\n",
      "Current prediction:  61.243045806884766 \n",
      "\n",
      "Iteration 11116, Loss: 35.57929992675781, L1: 10.543722152709961, L3: 25.03557586669922\n",
      "Current prediction:  61.25032043457031 \n",
      "\n",
      "Iteration 11117, Loss: 36.619117736816406, L1: 10.526761054992676, L3: 26.092357635498047\n",
      "Current prediction:  61.256080627441406 \n",
      "\n",
      "Iteration 11118, Loss: 37.60798263549805, L1: 10.536294937133789, L3: 27.071687698364258\n",
      "Current prediction:  61.262786865234375 \n",
      "\n",
      "Iteration 11119, Loss: 35.98887634277344, L1: 10.523159980773926, L3: 25.465717315673828\n",
      "Current prediction:  61.26919937133789 \n",
      "\n",
      "Iteration 11120, Loss: 36.520511627197266, L1: 10.517655372619629, L3: 26.002857208251953\n",
      "Current prediction:  61.270938873291016 \n",
      "\n",
      "Iteration 11121, Loss: 37.005516052246094, L1: 10.523679733276367, L3: 26.48183822631836\n",
      "Current prediction:  61.27250671386719 \n",
      "\n",
      "Iteration 11122, Loss: 36.918663024902344, L1: 10.506021499633789, L3: 26.412643432617188\n",
      "Current prediction:  61.2725944519043 \n",
      "\n",
      "Iteration 11123, Loss: 35.935630798339844, L1: 10.517263412475586, L3: 25.41836929321289\n",
      "Current prediction:  61.27408218383789 \n",
      "\n",
      "Iteration 11124, Loss: 36.4098014831543, L1: 10.520471572875977, L3: 25.88932991027832\n",
      "Current prediction:  61.27438735961914 \n",
      "\n",
      "Iteration 11125, Loss: 36.89996337890625, L1: 10.517921447753906, L3: 26.38204002380371\n",
      "Current prediction:  61.27193069458008 \n",
      "\n",
      "Iteration 11126, Loss: 36.91861343383789, L1: 10.521866798400879, L3: 26.396747589111328\n",
      "Current prediction:  61.26807403564453 \n",
      "\n",
      "Iteration 11127, Loss: 36.24397277832031, L1: 10.52486515045166, L3: 25.71910858154297\n",
      "Current prediction:  61.26308822631836 \n",
      "\n",
      "Iteration 11128, Loss: 36.046539306640625, L1: 10.571728706359863, L3: 25.474809646606445\n",
      "Current prediction:  61.257076263427734 \n",
      "\n",
      "Iteration 11129, Loss: 36.80303192138672, L1: 10.53628921508789, L3: 26.26674461364746\n",
      "Current prediction:  61.24717712402344 \n",
      "\n",
      "Iteration 11130, Loss: 37.32084655761719, L1: 10.533611297607422, L3: 26.7872371673584\n",
      "Current prediction:  61.23646545410156 \n",
      "\n",
      "Iteration 11131, Loss: 36.7843017578125, L1: 10.543441772460938, L3: 26.240859985351562\n",
      "Current prediction:  61.23072814941406 \n",
      "\n",
      "Iteration 11132, Loss: 36.125980377197266, L1: 10.553791999816895, L3: 25.572189331054688\n",
      "Current prediction:  61.22504806518555 \n",
      "\n",
      "Iteration 11133, Loss: 35.25239562988281, L1: 10.561086654663086, L3: 24.691308975219727\n",
      "Current prediction:  61.22727584838867 \n",
      "\n",
      "Iteration 11134, Loss: 36.05980682373047, L1: 10.553743362426758, L3: 25.506065368652344\n",
      "Current prediction:  61.228248596191406 \n",
      "\n",
      "Iteration 11135, Loss: 36.894386291503906, L1: 10.550236701965332, L3: 26.34415054321289\n",
      "Current prediction:  61.22587203979492 \n",
      "\n",
      "Iteration 11136, Loss: 36.0984001159668, L1: 10.556672096252441, L3: 25.541728973388672\n",
      "Current prediction:  61.22336196899414 \n",
      "\n",
      "Iteration 11137, Loss: 35.476829528808594, L1: 10.562017440795898, L3: 24.914810180664062\n",
      "Current prediction:  61.223140716552734 \n",
      "\n",
      "Iteration 11138, Loss: 36.50231170654297, L1: 10.561036109924316, L3: 25.94127655029297\n",
      "Current prediction:  61.22673034667969 \n",
      "\n",
      "Iteration 11139, Loss: 35.9403076171875, L1: 10.562055587768555, L3: 25.378253936767578\n",
      "Current prediction:  61.23249816894531 \n",
      "\n",
      "Iteration 11140, Loss: 35.81391906738281, L1: 10.56704044342041, L3: 25.24687957763672\n",
      "Current prediction:  61.23762512207031 \n",
      "\n",
      "Iteration 11141, Loss: 36.613861083984375, L1: 10.542377471923828, L3: 26.071483612060547\n",
      "Current prediction:  61.240562438964844 \n",
      "\n",
      "Iteration 11142, Loss: 37.5754280090332, L1: 10.550209045410156, L3: 27.025218963623047\n",
      "Current prediction:  61.24348831176758 \n",
      "\n",
      "Iteration 11143, Loss: 36.844390869140625, L1: 10.541531562805176, L3: 26.302860260009766\n",
      "Current prediction:  61.24571228027344 \n",
      "\n",
      "Iteration 11144, Loss: 37.0910530090332, L1: 10.539115905761719, L3: 26.551937103271484\n",
      "Current prediction:  61.245792388916016 \n",
      "\n",
      "Iteration 11145, Loss: 36.588199615478516, L1: 10.543203353881836, L3: 26.04499626159668\n",
      "Current prediction:  61.244266510009766 \n",
      "\n",
      "Iteration 11146, Loss: 36.987361907958984, L1: 10.541208267211914, L3: 26.44615364074707\n",
      "Current prediction:  61.24259948730469 \n",
      "\n",
      "Iteration 11147, Loss: 36.28010177612305, L1: 10.547666549682617, L3: 25.73243522644043\n",
      "Current prediction:  61.24217224121094 \n",
      "\n",
      "Iteration 11148, Loss: 35.80946350097656, L1: 10.537809371948242, L3: 25.271656036376953\n",
      "Current prediction:  61.24393844604492 \n",
      "\n",
      "Iteration 11149, Loss: 35.89606475830078, L1: 10.55208969116211, L3: 25.34397315979004\n",
      "Current prediction:  61.24580764770508 \n",
      "\n",
      "Iteration 11150, Loss: 36.521400451660156, L1: 10.54324722290039, L3: 25.978151321411133\n",
      "Current prediction:  61.247291564941406 \n",
      "\n",
      "Iteration 11151, Loss: 36.65107727050781, L1: 10.550443649291992, L3: 26.100631713867188\n",
      "Current prediction:  61.24723434448242 \n",
      "\n",
      "Iteration 11152, Loss: 37.081668853759766, L1: 10.538250923156738, L3: 26.543418884277344\n",
      "Current prediction:  61.24592971801758 \n",
      "\n",
      "Iteration 11153, Loss: 36.89823913574219, L1: 10.537630081176758, L3: 26.360607147216797\n",
      "Current prediction:  61.24481964111328 \n",
      "\n",
      "Iteration 11154, Loss: 37.5155029296875, L1: 10.538309097290039, L3: 26.97719383239746\n",
      "Current prediction:  61.243892669677734 \n",
      "\n",
      "Iteration 11155, Loss: 38.32643127441406, L1: 10.545795440673828, L3: 27.780637741088867\n",
      "Current prediction:  61.24049758911133 \n",
      "\n",
      "Iteration 11156, Loss: 37.13187026977539, L1: 10.554059028625488, L3: 26.577810287475586\n",
      "Current prediction:  61.237060546875 \n",
      "\n",
      "Iteration 11157, Loss: 36.318153381347656, L1: 10.55212688446045, L3: 25.76602554321289\n",
      "Current prediction:  61.233863830566406 \n",
      "\n",
      "Iteration 11158, Loss: 36.394657135009766, L1: 10.548812866210938, L3: 25.845844268798828\n",
      "Current prediction:  61.230796813964844 \n",
      "\n",
      "Iteration 11159, Loss: 37.051422119140625, L1: 10.560243606567383, L3: 26.491180419921875\n",
      "Current prediction:  61.229393005371094 \n",
      "\n",
      "Iteration 11160, Loss: 36.229000091552734, L1: 10.55052661895752, L3: 25.6784725189209\n",
      "Current prediction:  61.228599548339844 \n",
      "\n",
      "Iteration 11161, Loss: 35.968116760253906, L1: 10.557948112487793, L3: 25.41016960144043\n",
      "Current prediction:  61.230491638183594 \n",
      "\n",
      "Iteration 11162, Loss: 37.32570266723633, L1: 10.554682731628418, L3: 26.771018981933594\n",
      "Current prediction:  61.23186492919922 \n",
      "\n",
      "Iteration 11163, Loss: 36.53612518310547, L1: 10.553038597106934, L3: 25.98308563232422\n",
      "Current prediction:  61.232757568359375 \n",
      "\n",
      "Iteration 11164, Loss: 36.92784118652344, L1: 10.544626235961914, L3: 26.38321304321289\n",
      "Current prediction:  61.23329162597656 \n",
      "\n",
      "Iteration 11165, Loss: 37.0184211730957, L1: 10.559271812438965, L3: 26.459150314331055\n",
      "Current prediction:  61.23210525512695 \n",
      "\n",
      "Iteration 11166, Loss: 37.84284210205078, L1: 10.560739517211914, L3: 27.282100677490234\n",
      "Current prediction:  61.23133850097656 \n",
      "\n",
      "Iteration 11167, Loss: 36.35862731933594, L1: 10.560821533203125, L3: 25.79780387878418\n",
      "Current prediction:  61.2306022644043 \n",
      "\n",
      "Iteration 11168, Loss: 36.62739562988281, L1: 10.555156707763672, L3: 26.072240829467773\n",
      "Current prediction:  61.23087692260742 \n",
      "\n",
      "Iteration 11169, Loss: 37.08229064941406, L1: 10.559810638427734, L3: 26.52248191833496\n",
      "Current prediction:  61.23173522949219 \n",
      "\n",
      "Iteration 11170, Loss: 37.233924865722656, L1: 10.556928634643555, L3: 26.67699432373047\n",
      "Current prediction:  61.22926330566406 \n",
      "\n",
      "Iteration 11171, Loss: 37.816009521484375, L1: 10.56270980834961, L3: 27.253299713134766\n",
      "Current prediction:  61.228363037109375 \n",
      "\n",
      "Iteration 11172, Loss: 37.39336395263672, L1: 10.553730010986328, L3: 26.839632034301758\n",
      "Current prediction:  61.22848892211914 \n",
      "\n",
      "Iteration 11173, Loss: 37.28468704223633, L1: 10.55877685546875, L3: 26.725910186767578\n",
      "Current prediction:  61.23147201538086 \n",
      "\n",
      "Iteration 11174, Loss: 37.04863739013672, L1: 10.55469036102295, L3: 26.493946075439453\n",
      "Current prediction:  61.23213577270508 \n",
      "\n",
      "Iteration 11175, Loss: 36.80657196044922, L1: 10.556676864624023, L3: 26.249893188476562\n",
      "Current prediction:  61.23361587524414 \n",
      "\n",
      "Iteration 11176, Loss: 36.14100646972656, L1: 10.559401512145996, L3: 25.581605911254883\n",
      "Current prediction:  61.23656463623047 \n",
      "\n",
      "Iteration 11177, Loss: 36.8147087097168, L1: 10.557683944702148, L3: 26.25702476501465\n",
      "Current prediction:  61.23908996582031 \n",
      "\n",
      "Iteration 11178, Loss: 36.34054946899414, L1: 10.550707817077637, L3: 25.789840698242188\n",
      "Current prediction:  61.23907470703125 \n",
      "\n",
      "Iteration 11179, Loss: 36.468711853027344, L1: 10.545146942138672, L3: 25.923564910888672\n",
      "Current prediction:  61.24234390258789 \n",
      "\n",
      "Iteration 11180, Loss: 36.80881118774414, L1: 10.54938793182373, L3: 26.259424209594727\n",
      "Current prediction:  61.24435806274414 \n",
      "\n",
      "Iteration 11181, Loss: 36.69725036621094, L1: 10.540904998779297, L3: 26.156343460083008\n",
      "Current prediction:  61.244781494140625 \n",
      "\n",
      "Iteration 11182, Loss: 36.471981048583984, L1: 10.538727760314941, L3: 25.933252334594727\n",
      "Current prediction:  61.24660873413086 \n",
      "\n",
      "Iteration 11183, Loss: 36.04718017578125, L1: 10.549574851989746, L3: 25.497604370117188\n",
      "Current prediction:  61.250022888183594 \n",
      "\n",
      "Iteration 11184, Loss: 36.337928771972656, L1: 10.539785385131836, L3: 25.798145294189453\n",
      "Current prediction:  61.25135803222656 \n",
      "\n",
      "Iteration 11185, Loss: 37.49226379394531, L1: 10.545732498168945, L3: 26.946531295776367\n",
      "Current prediction:  61.251522064208984 \n",
      "\n",
      "Iteration 11186, Loss: 37.18852996826172, L1: 10.548049926757812, L3: 26.640478134155273\n",
      "Current prediction:  61.251956939697266 \n",
      "\n",
      "Iteration 11187, Loss: 36.57722854614258, L1: 10.535072326660156, L3: 26.042156219482422\n",
      "Current prediction:  61.254058837890625 \n",
      "\n",
      "Iteration 11188, Loss: 36.030948638916016, L1: 10.542167663574219, L3: 25.488780975341797\n",
      "Current prediction:  61.257843017578125 \n",
      "\n",
      "Iteration 11189, Loss: 36.38520812988281, L1: 10.531234741210938, L3: 25.853975296020508\n",
      "Current prediction:  61.26089859008789 \n",
      "\n",
      "Iteration 11190, Loss: 36.82377243041992, L1: 10.527138710021973, L3: 26.296634674072266\n",
      "Current prediction:  61.2628173828125 \n",
      "\n",
      "Iteration 11191, Loss: 36.5318717956543, L1: 10.526241302490234, L3: 26.005630493164062\n",
      "Current prediction:  61.26385498046875 \n",
      "\n",
      "Iteration 11192, Loss: 37.14073181152344, L1: 10.520562171936035, L3: 26.620168685913086\n",
      "Current prediction:  61.2679328918457 \n",
      "\n",
      "Iteration 11193, Loss: 36.3347053527832, L1: 10.524873733520508, L3: 25.809831619262695\n",
      "Current prediction:  61.27217102050781 \n",
      "\n",
      "Iteration 11194, Loss: 37.3872184753418, L1: 10.513087272644043, L3: 26.87413215637207\n",
      "Current prediction:  61.27240753173828 \n",
      "\n",
      "Iteration 11195, Loss: 35.822261810302734, L1: 10.519194602966309, L3: 25.303068161010742\n",
      "Current prediction:  61.274173736572266 \n",
      "\n",
      "Iteration 11196, Loss: 36.125099182128906, L1: 10.512947082519531, L3: 25.612154006958008\n",
      "Current prediction:  61.27509689331055 \n",
      "\n",
      "Iteration 11197, Loss: 36.628761291503906, L1: 10.522798538208008, L3: 26.10596466064453\n",
      "Current prediction:  61.27587127685547 \n",
      "\n",
      "Iteration 11198, Loss: 35.914695739746094, L1: 10.515376091003418, L3: 25.39931869506836\n",
      "Current prediction:  61.2765007019043 \n",
      "\n",
      "Iteration 11199, Loss: 37.46583557128906, L1: 10.51439094543457, L3: 26.951444625854492\n",
      "Current prediction:  61.2765007019043 \n",
      "\n",
      "Iteration 11200, Loss: 37.044281005859375, L1: 10.513370513916016, L3: 26.530912399291992\n",
      "Current prediction:  61.27819061279297 \n",
      "\n",
      "Iteration 11201, Loss: 37.10573196411133, L1: 10.5235013961792, L3: 26.582231521606445\n",
      "Current prediction:  61.27547073364258 \n",
      "\n",
      "Iteration 11202, Loss: 36.22054672241211, L1: 10.515973091125488, L3: 25.704574584960938\n",
      "Current prediction:  61.27294921875 \n",
      "\n",
      "Iteration 11203, Loss: 35.948326110839844, L1: 10.517909049987793, L3: 25.430416107177734\n",
      "Current prediction:  61.273048400878906 \n",
      "\n",
      "Iteration 11204, Loss: 36.226924896240234, L1: 10.524128913879395, L3: 25.702796936035156\n",
      "Current prediction:  61.273921966552734 \n",
      "\n",
      "Iteration 11205, Loss: 36.6971435546875, L1: 10.510943412780762, L3: 26.186201095581055\n",
      "Current prediction:  61.27473449707031 \n",
      "\n",
      "Iteration 11206, Loss: 36.595489501953125, L1: 10.50760555267334, L3: 26.0878849029541\n",
      "Current prediction:  61.27582931518555 \n",
      "\n",
      "Iteration 11207, Loss: 35.805442810058594, L1: 10.507680892944336, L3: 25.297760009765625\n",
      "Current prediction:  61.27738952636719 \n",
      "\n",
      "Iteration 11208, Loss: 37.9610481262207, L1: 10.505192756652832, L3: 27.455856323242188\n",
      "Current prediction:  61.277549743652344 \n",
      "\n",
      "Iteration 11209, Loss: 36.17999267578125, L1: 10.529199600219727, L3: 25.650793075561523\n",
      "Current prediction:  61.27739334106445 \n",
      "\n",
      "Iteration 11210, Loss: 35.93069076538086, L1: 10.515131950378418, L3: 25.415559768676758\n",
      "Current prediction:  61.28120422363281 \n",
      "\n",
      "Iteration 11211, Loss: 37.12005615234375, L1: 10.5037202835083, L3: 26.616334915161133\n",
      "Current prediction:  61.28492736816406 \n",
      "\n",
      "Iteration 11212, Loss: 36.31255340576172, L1: 10.52167797088623, L3: 25.790874481201172\n",
      "Current prediction:  61.28549575805664 \n",
      "\n",
      "Iteration 11213, Loss: 36.496429443359375, L1: 10.50358772277832, L3: 25.992843627929688\n",
      "Current prediction:  61.286048889160156 \n",
      "\n",
      "Iteration 11214, Loss: 36.707855224609375, L1: 10.515881538391113, L3: 26.191972732543945\n",
      "Current prediction:  61.282997131347656 \n",
      "\n",
      "Iteration 11215, Loss: 36.01663589477539, L1: 10.508801460266113, L3: 25.50783348083496\n",
      "Current prediction:  61.28070068359375 \n",
      "\n",
      "Iteration 11216, Loss: 36.475067138671875, L1: 10.513142585754395, L3: 25.961925506591797\n",
      "Current prediction:  61.27482604980469 \n",
      "\n",
      "Iteration 11217, Loss: 36.77245330810547, L1: 10.520214080810547, L3: 26.252241134643555\n",
      "Current prediction:  61.270225524902344 \n",
      "\n",
      "Iteration 11218, Loss: 35.699363708496094, L1: 10.518915176391602, L3: 25.18044662475586\n",
      "Current prediction:  61.264549255371094 \n",
      "\n",
      "Iteration 11219, Loss: 37.78546142578125, L1: 10.536413192749023, L3: 27.24905014038086\n",
      "Current prediction:  61.25676727294922 \n",
      "\n",
      "Iteration 11220, Loss: 36.99540710449219, L1: 10.529886245727539, L3: 26.46552276611328\n",
      "Current prediction:  61.24855041503906 \n",
      "\n",
      "Iteration 11221, Loss: 36.29620361328125, L1: 10.529990196228027, L3: 25.766212463378906\n",
      "Current prediction:  61.24375915527344 \n",
      "\n",
      "Iteration 11222, Loss: 36.53013610839844, L1: 10.55307388305664, L3: 25.977060317993164\n",
      "Current prediction:  61.2402458190918 \n",
      "\n",
      "Iteration 11223, Loss: 37.59123992919922, L1: 10.548503875732422, L3: 27.042734146118164\n",
      "Current prediction:  61.23476791381836 \n",
      "\n",
      "Iteration 11224, Loss: 36.75544357299805, L1: 10.558581352233887, L3: 26.196863174438477\n",
      "Current prediction:  61.23155212402344 \n",
      "\n",
      "Iteration 11225, Loss: 36.88856506347656, L1: 10.560578346252441, L3: 26.327987670898438\n",
      "Current prediction:  61.230995178222656 \n",
      "\n",
      "Iteration 11226, Loss: 36.89341354370117, L1: 10.558411598205566, L3: 26.335002899169922\n",
      "Current prediction:  61.23069763183594 \n",
      "\n",
      "Iteration 11227, Loss: 36.391746520996094, L1: 10.549570083618164, L3: 25.842178344726562\n",
      "Current prediction:  61.23162841796875 \n",
      "\n",
      "Iteration 11228, Loss: 36.38136291503906, L1: 10.552863121032715, L3: 25.828500747680664\n",
      "Current prediction:  61.23907470703125 \n",
      "\n",
      "Iteration 11229, Loss: 36.038177490234375, L1: 10.545442581176758, L3: 25.49273681640625\n",
      "Current prediction:  61.246463775634766 \n",
      "\n",
      "Iteration 11230, Loss: 36.129150390625, L1: 10.540764808654785, L3: 25.5883846282959\n",
      "Current prediction:  61.25265121459961 \n",
      "\n",
      "Iteration 11231, Loss: 35.99430465698242, L1: 10.541159629821777, L3: 25.45314598083496\n",
      "Current prediction:  61.26026916503906 \n",
      "\n",
      "Iteration 11232, Loss: 36.92705535888672, L1: 10.528637886047363, L3: 26.39841651916504\n",
      "Current prediction:  61.26887130737305 \n",
      "\n",
      "Iteration 11233, Loss: 36.7886848449707, L1: 10.58705997467041, L3: 26.20162582397461\n",
      "Current prediction:  61.27519607543945 \n",
      "\n",
      "Iteration 11234, Loss: 35.362586975097656, L1: 10.532275199890137, L3: 24.830312728881836\n",
      "Current prediction:  61.27935791015625 \n",
      "\n",
      "Iteration 11235, Loss: 36.609500885009766, L1: 10.52202033996582, L3: 26.087480545043945\n",
      "Current prediction:  61.2825927734375 \n",
      "\n",
      "Iteration 11236, Loss: 36.57376480102539, L1: 10.512388229370117, L3: 26.061376571655273\n",
      "Current prediction:  61.2862663269043 \n",
      "\n",
      "Iteration 11237, Loss: 36.497764587402344, L1: 10.500310897827148, L3: 25.997453689575195\n",
      "Current prediction:  61.286407470703125 \n",
      "\n",
      "Iteration 11238, Loss: 36.702842712402344, L1: 10.507153511047363, L3: 26.195688247680664\n",
      "Current prediction:  61.28519058227539 \n",
      "\n",
      "Iteration 11239, Loss: 37.010765075683594, L1: 10.506985664367676, L3: 26.5037784576416\n",
      "Current prediction:  61.28290939331055 \n",
      "\n",
      "Iteration 11240, Loss: 37.013771057128906, L1: 10.510346412658691, L3: 26.50342559814453\n",
      "Current prediction:  61.27853775024414 \n",
      "\n",
      "Iteration 11241, Loss: 36.29836654663086, L1: 10.516534805297852, L3: 25.781831741333008\n",
      "Current prediction:  61.273765563964844 \n",
      "\n",
      "Iteration 11242, Loss: 36.920101165771484, L1: 10.518418312072754, L3: 26.401683807373047\n",
      "Current prediction:  61.27333068847656 \n",
      "\n",
      "Iteration 11243, Loss: 36.869956970214844, L1: 10.513410568237305, L3: 26.356544494628906\n",
      "Current prediction:  61.27001190185547 \n",
      "\n",
      "Iteration 11244, Loss: 37.201168060302734, L1: 10.525346755981445, L3: 26.67582130432129\n",
      "Current prediction:  61.26908493041992 \n",
      "\n",
      "Iteration 11245, Loss: 37.19526672363281, L1: 10.511043548583984, L3: 26.68422508239746\n",
      "Current prediction:  61.26240158081055 \n",
      "\n",
      "Iteration 11246, Loss: 36.73394775390625, L1: 10.524343490600586, L3: 26.20960235595703\n",
      "Current prediction:  61.256431579589844 \n",
      "\n",
      "Iteration 11247, Loss: 37.29983901977539, L1: 10.538350105285645, L3: 26.761489868164062\n",
      "Current prediction:  61.24724578857422 \n",
      "\n",
      "Iteration 11248, Loss: 37.19386672973633, L1: 10.542360305786133, L3: 26.651506423950195\n",
      "Current prediction:  61.241180419921875 \n",
      "\n",
      "Iteration 11249, Loss: 36.629676818847656, L1: 10.54306411743164, L3: 26.08661460876465\n",
      "Current prediction:  61.23894119262695 \n",
      "\n",
      "Iteration 11250, Loss: 36.84480285644531, L1: 10.556601524353027, L3: 26.2882022857666\n",
      "Current prediction:  61.23894119262695 \n",
      "\n",
      "Iteration 11251, Loss: 35.6322021484375, L1: 10.544923782348633, L3: 25.087278366088867\n",
      "Current prediction:  61.242431640625 \n",
      "\n",
      "Iteration 11252, Loss: 35.9246711730957, L1: 10.543601036071777, L3: 25.38106918334961\n",
      "Current prediction:  61.24407958984375 \n",
      "\n",
      "Iteration 11253, Loss: 35.9163818359375, L1: 10.539264678955078, L3: 25.377119064331055\n",
      "Current prediction:  61.24940872192383 \n",
      "\n",
      "Iteration 11254, Loss: 35.48059844970703, L1: 10.538662910461426, L3: 24.94193458557129\n",
      "Current prediction:  61.25395965576172 \n",
      "\n",
      "Iteration 11255, Loss: 37.185020446777344, L1: 10.533605575561523, L3: 26.651412963867188\n",
      "Current prediction:  61.25576400756836 \n",
      "\n",
      "Iteration 11256, Loss: 36.196678161621094, L1: 10.521498680114746, L3: 25.675180435180664\n",
      "Current prediction:  61.260169982910156 \n",
      "\n",
      "Iteration 11257, Loss: 37.909828186035156, L1: 10.53477668762207, L3: 27.375049591064453\n",
      "Current prediction:  61.258235931396484 \n",
      "\n",
      "Iteration 11258, Loss: 36.80954360961914, L1: 10.524813652038574, L3: 26.284730911254883\n",
      "Current prediction:  61.254432678222656 \n",
      "\n",
      "Iteration 11259, Loss: 36.38386154174805, L1: 10.523137092590332, L3: 25.86072540283203\n",
      "Current prediction:  61.25421142578125 \n",
      "\n",
      "Iteration 11260, Loss: 36.71044158935547, L1: 10.53553581237793, L3: 26.17490577697754\n",
      "Current prediction:  61.25349426269531 \n",
      "\n",
      "Iteration 11261, Loss: 36.20616149902344, L1: 10.532008171081543, L3: 25.67415428161621\n",
      "Current prediction:  61.253395080566406 \n",
      "\n",
      "Iteration 11262, Loss: 36.44709014892578, L1: 10.534026145935059, L3: 25.913063049316406\n",
      "Current prediction:  61.255279541015625 \n",
      "\n",
      "Iteration 11263, Loss: 37.048072814941406, L1: 10.544342994689941, L3: 26.50373077392578\n",
      "Current prediction:  61.25592041015625 \n",
      "\n",
      "Iteration 11264, Loss: 36.07594299316406, L1: 10.520984649658203, L3: 25.554956436157227\n",
      "Current prediction:  61.25910949707031 \n",
      "\n",
      "Iteration 11265, Loss: 37.003509521484375, L1: 10.540392875671387, L3: 26.463115692138672\n",
      "Current prediction:  61.260169982910156 \n",
      "\n",
      "Iteration 11266, Loss: 38.17975997924805, L1: 10.53744125366211, L3: 27.642318725585938\n",
      "Current prediction:  61.256103515625 \n",
      "\n",
      "Iteration 11267, Loss: 36.879573822021484, L1: 10.533885955810547, L3: 26.345687866210938\n",
      "Current prediction:  61.25220489501953 \n",
      "\n",
      "Iteration 11268, Loss: 37.067440032958984, L1: 10.54304027557373, L3: 26.52440071105957\n",
      "Current prediction:  61.24669647216797 \n",
      "\n",
      "Iteration 11269, Loss: 35.970767974853516, L1: 10.547293663024902, L3: 25.423473358154297\n",
      "Current prediction:  61.23936080932617 \n",
      "\n",
      "Iteration 11270, Loss: 35.793067932128906, L1: 10.553191184997559, L3: 25.23987579345703\n",
      "Current prediction:  61.2324104309082 \n",
      "\n",
      "Iteration 11271, Loss: 38.04445266723633, L1: 10.549979209899902, L3: 27.494474411010742\n",
      "Current prediction:  61.22539138793945 \n",
      "\n",
      "Iteration 11272, Loss: 35.86546325683594, L1: 10.563039779663086, L3: 25.302425384521484\n",
      "Current prediction:  61.2214469909668 \n",
      "\n",
      "Iteration 11273, Loss: 36.47221374511719, L1: 10.561260223388672, L3: 25.910953521728516\n",
      "Current prediction:  61.22064971923828 \n",
      "\n",
      "Iteration 11274, Loss: 37.522796630859375, L1: 10.55705738067627, L3: 26.965740203857422\n",
      "Current prediction:  61.22112274169922 \n",
      "\n",
      "Iteration 11275, Loss: 36.07792282104492, L1: 10.562389373779297, L3: 25.515533447265625\n",
      "Current prediction:  61.222984313964844 \n",
      "\n",
      "Iteration 11276, Loss: 36.7579460144043, L1: 10.56966495513916, L3: 26.188282012939453\n",
      "Current prediction:  61.223541259765625 \n",
      "\n",
      "Iteration 11277, Loss: 35.70341491699219, L1: 10.561098098754883, L3: 25.142316818237305\n",
      "Current prediction:  61.22420883178711 \n",
      "\n",
      "Iteration 11278, Loss: 36.49693298339844, L1: 10.565760612487793, L3: 25.931171417236328\n",
      "Current prediction:  61.22240447998047 \n",
      "\n",
      "Iteration 11279, Loss: 36.7994384765625, L1: 10.565732955932617, L3: 26.23370361328125\n",
      "Current prediction:  61.223636627197266 \n",
      "\n",
      "Iteration 11280, Loss: 36.79091262817383, L1: 10.568387031555176, L3: 26.222524642944336\n",
      "Current prediction:  61.228172302246094 \n",
      "\n",
      "Iteration 11281, Loss: 36.133480072021484, L1: 10.562740325927734, L3: 25.57073974609375\n",
      "Current prediction:  61.23439025878906 \n",
      "\n",
      "Iteration 11282, Loss: 35.9581413269043, L1: 10.541505813598633, L3: 25.416635513305664\n",
      "Current prediction:  61.24172592163086 \n",
      "\n",
      "Iteration 11283, Loss: 36.26898956298828, L1: 10.542197227478027, L3: 25.72679328918457\n",
      "Current prediction:  61.24820327758789 \n",
      "\n",
      "Iteration 11284, Loss: 36.243255615234375, L1: 10.546985626220703, L3: 25.69626808166504\n",
      "Current prediction:  61.25578689575195 \n",
      "\n",
      "Iteration 11285, Loss: 36.68803405761719, L1: 10.530698776245117, L3: 26.15733528137207\n",
      "Current prediction:  61.26033401489258 \n",
      "\n",
      "Iteration 11286, Loss: 36.50115203857422, L1: 10.540581703186035, L3: 25.960569381713867\n",
      "Current prediction:  61.262489318847656 \n",
      "\n",
      "Iteration 11287, Loss: 37.09375, L1: 10.524954795837402, L3: 26.56879425048828\n",
      "Current prediction:  61.26363754272461 \n",
      "\n",
      "Iteration 11288, Loss: 35.662696838378906, L1: 10.511895179748535, L3: 25.150802612304688\n",
      "Current prediction:  61.26805877685547 \n",
      "\n",
      "Iteration 11289, Loss: 36.58760452270508, L1: 10.52925968170166, L3: 26.0583438873291\n",
      "Current prediction:  61.27044677734375 \n",
      "\n",
      "Iteration 11290, Loss: 35.305938720703125, L1: 10.521327018737793, L3: 24.78461265563965\n",
      "Current prediction:  61.2730827331543 \n",
      "\n",
      "Iteration 11291, Loss: 36.53892517089844, L1: 10.523979187011719, L3: 26.014944076538086\n",
      "Current prediction:  61.27399826049805 \n",
      "\n",
      "Iteration 11292, Loss: 35.95965576171875, L1: 10.517322540283203, L3: 25.442331314086914\n",
      "Current prediction:  61.27534484863281 \n",
      "\n",
      "Iteration 11293, Loss: 36.471405029296875, L1: 10.518318176269531, L3: 25.953086853027344\n",
      "Current prediction:  61.277244567871094 \n",
      "\n",
      "Iteration 11294, Loss: 36.38859176635742, L1: 10.517278671264648, L3: 25.871313095092773\n",
      "Current prediction:  61.27962112426758 \n",
      "\n",
      "Iteration 11295, Loss: 35.770198822021484, L1: 10.508843421936035, L3: 25.261356353759766\n",
      "Current prediction:  61.281898498535156 \n",
      "\n",
      "Iteration 11296, Loss: 36.62736892700195, L1: 10.518078804016113, L3: 26.109291076660156\n",
      "Current prediction:  61.28022384643555 \n",
      "\n",
      "Iteration 11297, Loss: 35.821998596191406, L1: 10.516823768615723, L3: 25.30517578125\n",
      "Current prediction:  61.280948638916016 \n",
      "\n",
      "Iteration 11298, Loss: 36.4624137878418, L1: 10.510364532470703, L3: 25.952049255371094\n",
      "Current prediction:  61.2838249206543 \n",
      "\n",
      "Iteration 11299, Loss: 35.57110595703125, L1: 10.505599021911621, L3: 25.065505981445312\n",
      "Current prediction:  61.289676666259766 \n",
      "\n",
      "Iteration 11300, Loss: 36.970943450927734, L1: 10.50229263305664, L3: 26.468650817871094\n",
      "Current prediction:  61.29344940185547 \n",
      "\n",
      "Iteration 11301, Loss: 35.94721221923828, L1: 10.499246597290039, L3: 25.44796371459961\n",
      "Current prediction:  61.297576904296875 \n",
      "\n",
      "Iteration 11302, Loss: 35.25873565673828, L1: 10.499401092529297, L3: 24.75933265686035\n",
      "Current prediction:  61.303070068359375 \n",
      "\n",
      "Iteration 11303, Loss: 37.12416076660156, L1: 10.523548126220703, L3: 26.60061264038086\n",
      "Current prediction:  61.304500579833984 \n",
      "\n",
      "Iteration 11304, Loss: 36.69976043701172, L1: 10.504168510437012, L3: 26.19559097290039\n",
      "Current prediction:  61.304256439208984 \n",
      "\n",
      "Iteration 11305, Loss: 36.65413284301758, L1: 10.494231224060059, L3: 26.159900665283203\n",
      "Current prediction:  61.3018684387207 \n",
      "\n",
      "Iteration 11306, Loss: 36.760372161865234, L1: 10.495633125305176, L3: 26.264738082885742\n",
      "Current prediction:  61.29899978637695 \n",
      "\n",
      "Iteration 11307, Loss: 36.19099044799805, L1: 10.496663093566895, L3: 25.69432830810547\n",
      "Current prediction:  61.2963752746582 \n",
      "\n",
      "Iteration 11308, Loss: 36.069854736328125, L1: 10.501825332641602, L3: 25.568031311035156\n",
      "Current prediction:  61.293922424316406 \n",
      "\n",
      "Iteration 11309, Loss: 36.86920166015625, L1: 10.498418807983398, L3: 26.370784759521484\n",
      "Current prediction:  61.286376953125 \n",
      "\n",
      "Iteration 11310, Loss: 36.81621170043945, L1: 10.512429237365723, L3: 26.303781509399414\n",
      "Current prediction:  61.278099060058594 \n",
      "\n",
      "Iteration 11311, Loss: 36.81978988647461, L1: 10.521474838256836, L3: 26.298315048217773\n",
      "Current prediction:  61.26945495605469 \n",
      "\n",
      "Iteration 11312, Loss: 36.46253204345703, L1: 10.530794143676758, L3: 25.931739807128906\n",
      "Current prediction:  61.26473617553711 \n",
      "\n",
      "Iteration 11313, Loss: 35.74654769897461, L1: 10.527533531188965, L3: 25.21901512145996\n",
      "Current prediction:  61.2589111328125 \n",
      "\n",
      "Iteration 11314, Loss: 36.54304122924805, L1: 10.537803649902344, L3: 26.005237579345703\n",
      "Current prediction:  61.2542610168457 \n",
      "\n",
      "Iteration 11315, Loss: 37.24714660644531, L1: 10.541735649108887, L3: 26.705411911010742\n",
      "Current prediction:  61.250240325927734 \n",
      "\n",
      "Iteration 11316, Loss: 36.118141174316406, L1: 10.53715705871582, L3: 25.580984115600586\n",
      "Current prediction:  61.2457275390625 \n",
      "\n",
      "Iteration 11317, Loss: 36.974815368652344, L1: 10.546263694763184, L3: 26.428552627563477\n",
      "Current prediction:  61.24448776245117 \n",
      "\n",
      "Iteration 11318, Loss: 37.16031265258789, L1: 10.547207832336426, L3: 26.61310577392578\n",
      "Current prediction:  61.24432373046875 \n",
      "\n",
      "Iteration 11319, Loss: 36.08257293701172, L1: 10.540791511535645, L3: 25.541780471801758\n",
      "Current prediction:  61.245689392089844 \n",
      "\n",
      "Iteration 11320, Loss: 36.204341888427734, L1: 10.549203872680664, L3: 25.65513801574707\n",
      "Current prediction:  61.24674987792969 \n",
      "\n",
      "Iteration 11321, Loss: 36.72857666015625, L1: 10.550763130187988, L3: 26.177812576293945\n",
      "Current prediction:  61.24470138549805 \n",
      "\n",
      "Iteration 11322, Loss: 37.24344253540039, L1: 10.535197257995605, L3: 26.70824432373047\n",
      "Current prediction:  61.243404388427734 \n",
      "\n",
      "Iteration 11323, Loss: 36.66008758544922, L1: 10.54684829711914, L3: 26.11324119567871\n",
      "Current prediction:  61.239990234375 \n",
      "\n",
      "Iteration 11324, Loss: 36.25143051147461, L1: 10.542716026306152, L3: 25.708715438842773\n",
      "Current prediction:  61.23865509033203 \n",
      "\n",
      "Iteration 11325, Loss: 36.1721076965332, L1: 10.554039001464844, L3: 25.61806869506836\n",
      "Current prediction:  61.2391242980957 \n",
      "\n",
      "Iteration 11326, Loss: 35.822906494140625, L1: 10.54978084564209, L3: 25.27312469482422\n",
      "Current prediction:  61.24155044555664 \n",
      "\n",
      "Iteration 11327, Loss: 36.02781677246094, L1: 10.548644065856934, L3: 25.479171752929688\n",
      "Current prediction:  61.245262145996094 \n",
      "\n",
      "Iteration 11328, Loss: 38.2650032043457, L1: 10.553315162658691, L3: 27.711687088012695\n",
      "Current prediction:  61.24885940551758 \n",
      "\n",
      "Iteration 11329, Loss: 36.638572692871094, L1: 10.541657447814941, L3: 26.096914291381836\n",
      "Current prediction:  61.25326919555664 \n",
      "\n",
      "Iteration 11330, Loss: 37.055782318115234, L1: 10.5435152053833, L3: 26.51226806640625\n",
      "Current prediction:  61.25790786743164 \n",
      "\n",
      "Iteration 11331, Loss: 36.40868377685547, L1: 10.542366027832031, L3: 25.866315841674805\n",
      "Current prediction:  61.26318359375 \n",
      "\n",
      "Iteration 11332, Loss: 36.82236862182617, L1: 10.527873039245605, L3: 26.294496536254883\n",
      "Current prediction:  61.26887512207031 \n",
      "\n",
      "Iteration 11333, Loss: 36.522499084472656, L1: 10.528718948364258, L3: 25.99378204345703\n",
      "Current prediction:  61.26981735229492 \n",
      "\n",
      "Iteration 11334, Loss: 36.799495697021484, L1: 10.525128364562988, L3: 26.27436637878418\n",
      "Current prediction:  61.271663665771484 \n",
      "\n",
      "Iteration 11335, Loss: 36.90410614013672, L1: 10.530170440673828, L3: 26.373937606811523\n",
      "Current prediction:  61.26927947998047 \n",
      "\n",
      "Iteration 11336, Loss: 36.4896240234375, L1: 10.518387794494629, L3: 25.971235275268555\n",
      "Current prediction:  61.26264190673828 \n",
      "\n",
      "Iteration 11337, Loss: 36.48012924194336, L1: 10.532175064086914, L3: 25.947954177856445\n",
      "Current prediction:  61.257198333740234 \n",
      "\n",
      "Iteration 11338, Loss: 35.966896057128906, L1: 10.536381721496582, L3: 25.430513381958008\n",
      "Current prediction:  61.25443649291992 \n",
      "\n",
      "Iteration 11339, Loss: 37.12553405761719, L1: 10.548229217529297, L3: 26.577306747436523\n",
      "Current prediction:  61.24901580810547 \n",
      "\n",
      "Iteration 11340, Loss: 36.92970275878906, L1: 10.54494571685791, L3: 26.384756088256836\n",
      "Current prediction:  61.24364471435547 \n",
      "\n",
      "Iteration 11341, Loss: 36.5184211730957, L1: 10.550250053405762, L3: 25.968172073364258\n",
      "Current prediction:  61.23740005493164 \n",
      "\n",
      "Iteration 11342, Loss: 37.720088958740234, L1: 10.547022819519043, L3: 27.173065185546875\n",
      "Current prediction:  61.23469543457031 \n",
      "\n",
      "Iteration 11343, Loss: 37.24709701538086, L1: 10.55138111114502, L3: 26.695714950561523\n",
      "Current prediction:  61.234249114990234 \n",
      "\n",
      "Iteration 11344, Loss: 37.69302749633789, L1: 10.561188697814941, L3: 27.131837844848633\n",
      "Current prediction:  61.231807708740234 \n",
      "\n",
      "Iteration 11345, Loss: 36.72489929199219, L1: 10.554494857788086, L3: 26.170406341552734\n",
      "Current prediction:  61.23148727416992 \n",
      "\n",
      "Iteration 11346, Loss: 35.43605041503906, L1: 10.5618257522583, L3: 24.874223709106445\n",
      "Current prediction:  61.23012924194336 \n",
      "\n",
      "Iteration 11347, Loss: 36.65796661376953, L1: 10.556236267089844, L3: 26.101728439331055\n",
      "Current prediction:  61.226985931396484 \n",
      "\n",
      "Iteration 11348, Loss: 36.15268325805664, L1: 10.55797004699707, L3: 25.59471321105957\n",
      "Current prediction:  61.22383499145508 \n",
      "\n",
      "Iteration 11349, Loss: 36.65705108642578, L1: 10.563570022583008, L3: 26.093481063842773\n",
      "Current prediction:  61.21852493286133 \n",
      "\n",
      "Iteration 11350, Loss: 35.96363067626953, L1: 10.564935684204102, L3: 25.39869499206543\n",
      "Current prediction:  61.21669006347656 \n",
      "\n",
      "Iteration 11351, Loss: 37.09440994262695, L1: 10.56940746307373, L3: 26.52500343322754\n",
      "Current prediction:  61.21694564819336 \n",
      "\n",
      "Iteration 11352, Loss: 36.52586364746094, L1: 10.569565773010254, L3: 25.956296920776367\n",
      "Current prediction:  61.21826934814453 \n",
      "\n",
      "Iteration 11353, Loss: 36.299034118652344, L1: 10.569536209106445, L3: 25.72949981689453\n",
      "Current prediction:  61.223270416259766 \n",
      "\n",
      "Iteration 11354, Loss: 35.651702880859375, L1: 10.57536792755127, L3: 25.07633399963379\n",
      "Current prediction:  61.229366302490234 \n",
      "\n",
      "Iteration 11355, Loss: 37.05416488647461, L1: 10.564789772033691, L3: 26.4893741607666\n",
      "Current prediction:  61.23594665527344 \n",
      "\n",
      "Iteration 11356, Loss: 37.49348068237305, L1: 10.560898780822754, L3: 26.932580947875977\n",
      "Current prediction:  61.24513244628906 \n",
      "\n",
      "Iteration 11357, Loss: 36.25313186645508, L1: 10.548903465270996, L3: 25.7042293548584\n",
      "Current prediction:  61.25992202758789 \n",
      "\n",
      "Iteration 11358, Loss: 35.49298858642578, L1: 10.536468505859375, L3: 24.956518173217773\n",
      "Current prediction:  61.27669143676758 \n",
      "\n",
      "Iteration 11359, Loss: 37.7236328125, L1: 10.520220756530762, L3: 27.203413009643555\n",
      "Current prediction:  61.28868103027344 \n",
      "\n",
      "Iteration 11360, Loss: 36.796024322509766, L1: 10.508028984069824, L3: 26.287996292114258\n",
      "Current prediction:  61.29806900024414 \n",
      "\n",
      "Iteration 11361, Loss: 36.41560363769531, L1: 10.508800506591797, L3: 25.906801223754883\n",
      "Current prediction:  61.30550003051758 \n",
      "\n",
      "Iteration 11362, Loss: 37.20831298828125, L1: 10.500032424926758, L3: 26.708280563354492\n",
      "Current prediction:  61.310813903808594 \n",
      "\n",
      "Iteration 11363, Loss: 36.257957458496094, L1: 10.499513626098633, L3: 25.75844383239746\n",
      "Current prediction:  61.316776275634766 \n",
      "\n",
      "Iteration 11364, Loss: 36.25761413574219, L1: 10.486577987670898, L3: 25.77103614807129\n",
      "Current prediction:  61.32783889770508 \n",
      "\n",
      "Iteration 11365, Loss: 36.01088333129883, L1: 10.471569061279297, L3: 25.53931427001953\n",
      "Current prediction:  61.333900451660156 \n",
      "\n",
      "Iteration 11366, Loss: 36.33756637573242, L1: 10.476481437683105, L3: 25.861083984375\n",
      "Current prediction:  61.334232330322266 \n",
      "\n",
      "Iteration 11367, Loss: 36.43345642089844, L1: 10.479485511779785, L3: 25.953969955444336\n",
      "Current prediction:  61.33419418334961 \n",
      "\n",
      "Iteration 11368, Loss: 35.87947082519531, L1: 10.478445053100586, L3: 25.401025772094727\n",
      "Current prediction:  61.33028030395508 \n",
      "\n",
      "Iteration 11369, Loss: 36.01145553588867, L1: 10.479941368103027, L3: 25.53151512145996\n",
      "Current prediction:  61.32646179199219 \n",
      "\n",
      "Iteration 11370, Loss: 36.64984130859375, L1: 10.485819816589355, L3: 26.16402244567871\n",
      "Current prediction:  61.321266174316406 \n",
      "\n",
      "Iteration 11371, Loss: 36.26188659667969, L1: 10.488935470581055, L3: 25.77294921875\n",
      "Current prediction:  61.313358306884766 \n",
      "\n",
      "Iteration 11372, Loss: 35.79779052734375, L1: 10.49443531036377, L3: 25.303354263305664\n",
      "Current prediction:  61.30570983886719 \n",
      "\n",
      "Iteration 11373, Loss: 36.47374725341797, L1: 10.497455596923828, L3: 25.976289749145508\n",
      "Current prediction:  61.29922103881836 \n",
      "\n",
      "Iteration 11374, Loss: 36.85797882080078, L1: 10.500460624694824, L3: 26.357519149780273\n",
      "Current prediction:  61.292579650878906 \n",
      "\n",
      "Iteration 11375, Loss: 36.47732925415039, L1: 10.508753776550293, L3: 25.96857452392578\n",
      "Current prediction:  61.28591537475586 \n",
      "\n",
      "Iteration 11376, Loss: 36.63840103149414, L1: 10.515713691711426, L3: 26.12268829345703\n",
      "Current prediction:  61.27714538574219 \n",
      "\n",
      "Iteration 11377, Loss: 37.134342193603516, L1: 10.51949405670166, L3: 26.61484718322754\n",
      "Current prediction:  61.2675666809082 \n",
      "\n",
      "Iteration 11378, Loss: 37.98345947265625, L1: 10.525924682617188, L3: 27.457536697387695\n",
      "Current prediction:  61.256187438964844 \n",
      "\n",
      "Iteration 11379, Loss: 36.28416061401367, L1: 10.54041576385498, L3: 25.743743896484375\n",
      "Current prediction:  61.24193572998047 \n",
      "\n",
      "Iteration 11380, Loss: 36.95143127441406, L1: 10.543739318847656, L3: 26.407690048217773\n",
      "Current prediction:  61.23082733154297 \n",
      "\n",
      "Iteration 11381, Loss: 37.38683319091797, L1: 10.5658597946167, L3: 26.820972442626953\n",
      "Current prediction:  61.226741790771484 \n",
      "\n",
      "Iteration 11382, Loss: 36.90671157836914, L1: 10.559103012084961, L3: 26.34760856628418\n",
      "Current prediction:  61.22355270385742 \n",
      "\n",
      "Iteration 11383, Loss: 36.811275482177734, L1: 10.564208030700684, L3: 26.247068405151367\n",
      "Current prediction:  61.22415542602539 \n",
      "\n",
      "Iteration 11384, Loss: 35.994659423828125, L1: 10.564242362976074, L3: 25.430418014526367\n",
      "Current prediction:  61.22636413574219 \n",
      "\n",
      "Iteration 11385, Loss: 36.48565673828125, L1: 10.567924499511719, L3: 25.917734146118164\n",
      "Current prediction:  61.22879409790039 \n",
      "\n",
      "Iteration 11386, Loss: 36.110328674316406, L1: 10.564785957336426, L3: 25.545541763305664\n",
      "Current prediction:  61.23352813720703 \n",
      "\n",
      "Iteration 11387, Loss: 36.6407470703125, L1: 10.559913635253906, L3: 26.08083152770996\n",
      "Current prediction:  61.24062728881836 \n",
      "\n",
      "Iteration 11388, Loss: 36.440101623535156, L1: 10.555727005004883, L3: 25.88437271118164\n",
      "Current prediction:  61.24750518798828 \n",
      "\n",
      "Iteration 11389, Loss: 35.141204833984375, L1: 10.544715881347656, L3: 24.59649085998535\n",
      "Current prediction:  61.25554656982422 \n",
      "\n",
      "Iteration 11390, Loss: 36.57134246826172, L1: 10.526416778564453, L3: 26.044925689697266\n",
      "Current prediction:  61.2652702331543 \n",
      "\n",
      "Iteration 11391, Loss: 36.15184783935547, L1: 10.525456428527832, L3: 25.62639045715332\n",
      "Current prediction:  61.271888732910156 \n",
      "\n",
      "Iteration 11392, Loss: 36.136260986328125, L1: 10.518454551696777, L3: 25.61780548095703\n",
      "Current prediction:  61.277488708496094 \n",
      "\n",
      "Iteration 11393, Loss: 36.94414520263672, L1: 10.511672973632812, L3: 26.432472229003906\n",
      "Current prediction:  61.280216217041016 \n",
      "\n",
      "Iteration 11394, Loss: 37.591426849365234, L1: 10.518223762512207, L3: 27.073204040527344\n",
      "Current prediction:  61.28056335449219 \n",
      "\n",
      "Iteration 11395, Loss: 36.532188415527344, L1: 10.509443283081055, L3: 26.02274513244629\n",
      "Current prediction:  61.28109359741211 \n",
      "\n",
      "Iteration 11396, Loss: 36.5361328125, L1: 10.517208099365234, L3: 26.018922805786133\n",
      "Current prediction:  61.27815628051758 \n",
      "\n",
      "Iteration 11397, Loss: 35.49036407470703, L1: 10.515616416931152, L3: 24.974748611450195\n",
      "Current prediction:  61.27717590332031 \n",
      "\n",
      "Iteration 11398, Loss: 35.75529479980469, L1: 10.516538619995117, L3: 25.23875617980957\n",
      "Current prediction:  61.27922439575195 \n",
      "\n",
      "Iteration 11399, Loss: 36.13519287109375, L1: 10.519805908203125, L3: 25.615388870239258\n",
      "Current prediction:  61.281524658203125 \n",
      "\n",
      "Iteration 11400, Loss: 35.90167236328125, L1: 10.50731372833252, L3: 25.394357681274414\n",
      "Current prediction:  61.28691101074219 \n",
      "\n",
      "Iteration 11401, Loss: 37.328765869140625, L1: 10.508295059204102, L3: 26.82046890258789\n",
      "Current prediction:  61.28703689575195 \n",
      "\n",
      "Iteration 11402, Loss: 36.350765228271484, L1: 10.502902030944824, L3: 25.847862243652344\n",
      "Current prediction:  61.284759521484375 \n",
      "\n",
      "Iteration 11403, Loss: 37.09871292114258, L1: 10.503875732421875, L3: 26.594837188720703\n",
      "Current prediction:  61.280433654785156 \n",
      "\n",
      "Iteration 11404, Loss: 37.1256103515625, L1: 10.516159057617188, L3: 26.609453201293945\n",
      "Current prediction:  61.274444580078125 \n",
      "\n",
      "Iteration 11405, Loss: 36.13995361328125, L1: 10.516752243041992, L3: 25.62320327758789\n",
      "Current prediction:  61.26930618286133 \n",
      "\n",
      "Iteration 11406, Loss: 36.182411193847656, L1: 10.525304794311523, L3: 25.6571044921875\n",
      "Current prediction:  61.26359558105469 \n",
      "\n",
      "Iteration 11407, Loss: 36.76826095581055, L1: 10.541276931762695, L3: 26.22698402404785\n",
      "Current prediction:  61.25641632080078 \n",
      "\n",
      "Iteration 11408, Loss: 36.91105651855469, L1: 10.528949737548828, L3: 26.38210678100586\n",
      "Current prediction:  61.25043487548828 \n",
      "\n",
      "Iteration 11409, Loss: 36.44557571411133, L1: 10.537635803222656, L3: 25.907939910888672\n",
      "Current prediction:  61.24453353881836 \n",
      "\n",
      "Iteration 11410, Loss: 37.15862274169922, L1: 10.549461364746094, L3: 26.609159469604492\n",
      "Current prediction:  61.24141311645508 \n",
      "\n",
      "Iteration 11411, Loss: 36.98531723022461, L1: 10.550680160522461, L3: 26.43463706970215\n",
      "Current prediction:  61.23640823364258 \n",
      "\n",
      "Iteration 11412, Loss: 36.561092376708984, L1: 10.549933433532715, L3: 26.011159896850586\n",
      "Current prediction:  61.232452392578125 \n",
      "\n",
      "Iteration 11413, Loss: 36.577388763427734, L1: 10.558030128479004, L3: 26.019357681274414\n",
      "Current prediction:  61.232200622558594 \n",
      "\n",
      "Iteration 11414, Loss: 36.29724884033203, L1: 10.5649995803833, L3: 25.732248306274414\n",
      "Current prediction:  61.234134674072266 \n",
      "\n",
      "Iteration 11415, Loss: 36.81806564331055, L1: 10.559366226196289, L3: 26.258699417114258\n",
      "Current prediction:  61.23678970336914 \n",
      "\n",
      "Iteration 11416, Loss: 36.8118896484375, L1: 10.550910949707031, L3: 26.26097869873047\n",
      "Current prediction:  61.24241638183594 \n",
      "\n",
      "Iteration 11417, Loss: 36.71204376220703, L1: 10.542672157287598, L3: 26.169370651245117\n",
      "Current prediction:  61.2494010925293 \n",
      "\n",
      "Iteration 11418, Loss: 37.2862663269043, L1: 10.536532402038574, L3: 26.74973487854004\n",
      "Current prediction:  61.252044677734375 \n",
      "\n",
      "Iteration 11419, Loss: 35.91922378540039, L1: 10.525850296020508, L3: 25.393373489379883\n",
      "Current prediction:  61.25334930419922 \n",
      "\n",
      "Iteration 11420, Loss: 36.686424255371094, L1: 10.538110733032227, L3: 26.148311614990234\n",
      "Current prediction:  61.256134033203125 \n",
      "\n",
      "Iteration 11421, Loss: 37.1111946105957, L1: 10.536947250366211, L3: 26.574247360229492\n",
      "Current prediction:  61.259464263916016 \n",
      "\n",
      "Iteration 11422, Loss: 36.622501373291016, L1: 10.528996467590332, L3: 26.093505859375\n",
      "Current prediction:  61.261051177978516 \n",
      "\n",
      "Iteration 11423, Loss: 35.295135498046875, L1: 10.519746780395508, L3: 24.775386810302734\n",
      "Current prediction:  61.26065444946289 \n",
      "\n",
      "Iteration 11424, Loss: 37.19245147705078, L1: 10.525753021240234, L3: 26.66670036315918\n",
      "Current prediction:  61.26427459716797 \n",
      "\n",
      "Iteration 11425, Loss: 36.17986297607422, L1: 10.526796340942383, L3: 25.653064727783203\n",
      "Current prediction:  61.268211364746094 \n",
      "\n",
      "Iteration 11426, Loss: 36.46575164794922, L1: 10.518993377685547, L3: 25.94675636291504\n",
      "Current prediction:  61.27023696899414 \n",
      "\n",
      "Iteration 11427, Loss: 37.01872634887695, L1: 10.517441749572754, L3: 26.501283645629883\n",
      "Current prediction:  61.26997756958008 \n",
      "\n",
      "Iteration 11428, Loss: 36.69963073730469, L1: 10.51786994934082, L3: 26.1817626953125\n",
      "Current prediction:  61.268157958984375 \n",
      "\n",
      "Iteration 11429, Loss: 35.98154830932617, L1: 10.524746894836426, L3: 25.456802368164062\n",
      "Current prediction:  61.26716613769531 \n",
      "\n",
      "Iteration 11430, Loss: 35.69379425048828, L1: 10.516297340393066, L3: 25.17749786376953\n",
      "Current prediction:  61.26964569091797 \n",
      "\n",
      "Iteration 11431, Loss: 36.12747573852539, L1: 10.514849662780762, L3: 25.612627029418945\n",
      "Current prediction:  61.27089309692383 \n",
      "\n",
      "Iteration 11432, Loss: 36.54077911376953, L1: 10.513227462768555, L3: 26.027549743652344\n",
      "Current prediction:  61.26679229736328 \n",
      "\n",
      "Iteration 11433, Loss: 36.9930419921875, L1: 10.529281616210938, L3: 26.46375846862793\n",
      "Current prediction:  61.261802673339844 \n",
      "\n",
      "Iteration 11434, Loss: 37.75819778442383, L1: 10.522128105163574, L3: 27.236068725585938\n",
      "Current prediction:  61.25648498535156 \n",
      "\n",
      "Iteration 11435, Loss: 36.7646484375, L1: 10.519218444824219, L3: 26.24542808532715\n",
      "Current prediction:  61.25299072265625 \n",
      "\n",
      "Iteration 11436, Loss: 37.536231994628906, L1: 10.531137466430664, L3: 27.005094528198242\n",
      "Current prediction:  61.247032165527344 \n",
      "\n",
      "Iteration 11437, Loss: 36.17171859741211, L1: 10.53496265411377, L3: 25.636756896972656\n",
      "Current prediction:  61.2435188293457 \n",
      "\n",
      "Iteration 11438, Loss: 36.79593276977539, L1: 10.5359468460083, L3: 26.259984970092773\n",
      "Current prediction:  61.23731231689453 \n",
      "\n",
      "Iteration 11439, Loss: 36.85906982421875, L1: 10.551594734191895, L3: 26.30747413635254\n",
      "Current prediction:  61.230716705322266 \n",
      "\n",
      "Iteration 11440, Loss: 36.624698638916016, L1: 10.556036949157715, L3: 26.068660736083984\n",
      "Current prediction:  61.2248420715332 \n",
      "\n",
      "Iteration 11441, Loss: 36.415077209472656, L1: 10.548078536987305, L3: 25.86699867248535\n",
      "Current prediction:  61.21926498413086 \n",
      "\n",
      "Iteration 11442, Loss: 35.665321350097656, L1: 10.55936050415039, L3: 25.105960845947266\n",
      "Current prediction:  61.21475601196289 \n",
      "\n",
      "Iteration 11443, Loss: 36.94596862792969, L1: 10.572112083435059, L3: 26.373855590820312\n",
      "Current prediction:  61.211463928222656 \n",
      "\n",
      "Iteration 11444, Loss: 36.026573181152344, L1: 10.5732421875, L3: 25.453330993652344\n",
      "Current prediction:  61.20899200439453 \n",
      "\n",
      "Iteration 11445, Loss: 36.806400299072266, L1: 10.569148063659668, L3: 26.237253189086914\n",
      "Current prediction:  61.20861053466797 \n",
      "\n",
      "Iteration 11446, Loss: 37.00307083129883, L1: 10.567657470703125, L3: 26.435413360595703\n",
      "Current prediction:  61.20791244506836 \n",
      "\n",
      "Iteration 11447, Loss: 36.800968170166016, L1: 10.57283878326416, L3: 26.228130340576172\n",
      "Current prediction:  61.209896087646484 \n",
      "\n",
      "Iteration 11448, Loss: 35.95110321044922, L1: 10.574302673339844, L3: 25.376802444458008\n",
      "Current prediction:  61.21197509765625 \n",
      "\n",
      "Iteration 11449, Loss: 34.85152816772461, L1: 10.561800956726074, L3: 24.28972625732422\n",
      "Current prediction:  61.21501922607422 \n",
      "\n",
      "Iteration 11450, Loss: 36.29317092895508, L1: 10.564943313598633, L3: 25.728227615356445\n",
      "Current prediction:  61.220462799072266 \n",
      "\n",
      "Iteration 11451, Loss: 36.64106369018555, L1: 10.562631607055664, L3: 26.078432083129883\n",
      "Current prediction:  61.22487258911133 \n",
      "\n",
      "Iteration 11452, Loss: 36.43024444580078, L1: 10.557497024536133, L3: 25.87274742126465\n",
      "Current prediction:  61.22913360595703 \n",
      "\n",
      "Iteration 11453, Loss: 36.42887878417969, L1: 10.550651550292969, L3: 25.87822723388672\n",
      "Current prediction:  61.23130798339844 \n",
      "\n",
      "Iteration 11454, Loss: 36.663352966308594, L1: 10.56289291381836, L3: 26.1004581451416\n",
      "Current prediction:  61.2350959777832 \n",
      "\n",
      "Iteration 11455, Loss: 37.08828353881836, L1: 10.554306030273438, L3: 26.533977508544922\n",
      "Current prediction:  61.239524841308594 \n",
      "\n",
      "Iteration 11456, Loss: 36.18764877319336, L1: 10.548396110534668, L3: 25.639251708984375\n",
      "Current prediction:  61.241947174072266 \n",
      "\n",
      "Iteration 11457, Loss: 35.985721588134766, L1: 10.547228813171387, L3: 25.438491821289062\n",
      "Current prediction:  61.244300842285156 \n",
      "\n",
      "Iteration 11458, Loss: 35.65327072143555, L1: 10.544461250305176, L3: 25.108808517456055\n",
      "Current prediction:  61.249202728271484 \n",
      "\n",
      "Iteration 11459, Loss: 36.42435073852539, L1: 10.540385246276855, L3: 25.88396644592285\n",
      "Current prediction:  61.255165100097656 \n",
      "\n",
      "Iteration 11460, Loss: 36.524227142333984, L1: 10.530082702636719, L3: 25.994144439697266\n",
      "Current prediction:  61.261417388916016 \n",
      "\n",
      "Iteration 11461, Loss: 36.418731689453125, L1: 10.52307415008545, L3: 25.895658493041992\n",
      "Current prediction:  61.2657356262207 \n",
      "\n",
      "Iteration 11462, Loss: 36.06450653076172, L1: 10.518150329589844, L3: 25.546356201171875\n",
      "Current prediction:  61.267818450927734 \n",
      "\n",
      "Iteration 11463, Loss: 36.855648040771484, L1: 10.518471717834473, L3: 26.337177276611328\n",
      "Current prediction:  61.27303695678711 \n",
      "\n",
      "Iteration 11464, Loss: 35.77058410644531, L1: 10.520894050598145, L3: 25.24968910217285\n",
      "Current prediction:  61.27573776245117 \n",
      "\n",
      "Iteration 11465, Loss: 37.10386657714844, L1: 10.516275405883789, L3: 26.58759307861328\n",
      "Current prediction:  61.275245666503906 \n",
      "\n",
      "Iteration 11466, Loss: 36.771724700927734, L1: 10.512200355529785, L3: 26.259525299072266\n",
      "Current prediction:  61.275142669677734 \n",
      "\n",
      "Iteration 11467, Loss: 36.1740837097168, L1: 10.527613639831543, L3: 25.646469116210938\n",
      "Current prediction:  61.27617645263672 \n",
      "\n",
      "Iteration 11468, Loss: 36.92777633666992, L1: 10.504202842712402, L3: 26.423572540283203\n",
      "Current prediction:  61.276329040527344 \n",
      "\n",
      "Iteration 11469, Loss: 37.00177001953125, L1: 10.517884254455566, L3: 26.483884811401367\n",
      "Current prediction:  61.275291442871094 \n",
      "\n",
      "Iteration 11470, Loss: 37.71968078613281, L1: 10.51500129699707, L3: 27.20467758178711\n",
      "Current prediction:  61.27311325073242 \n",
      "\n",
      "Iteration 11471, Loss: 36.74674987792969, L1: 10.501344680786133, L3: 26.245407104492188\n",
      "Current prediction:  61.270782470703125 \n",
      "\n",
      "Iteration 11472, Loss: 36.799747467041016, L1: 10.514244079589844, L3: 26.285503387451172\n",
      "Current prediction:  61.265132904052734 \n",
      "\n",
      "Iteration 11473, Loss: 36.41819381713867, L1: 10.531418800354004, L3: 25.88677406311035\n",
      "Current prediction:  61.2549934387207 \n",
      "\n",
      "Iteration 11474, Loss: 35.93117904663086, L1: 10.539400100708008, L3: 25.39177894592285\n",
      "Current prediction:  61.246158599853516 \n",
      "\n",
      "Iteration 11475, Loss: 36.70541763305664, L1: 10.531719207763672, L3: 26.17369842529297\n",
      "Current prediction:  61.23603820800781 \n",
      "\n",
      "Iteration 11476, Loss: 36.9109001159668, L1: 10.545347213745117, L3: 26.36555290222168\n",
      "Current prediction:  61.229671478271484 \n",
      "\n",
      "Iteration 11477, Loss: 37.385047912597656, L1: 10.539466857910156, L3: 26.8455810546875\n",
      "Current prediction:  61.22373580932617 \n",
      "\n",
      "Iteration 11478, Loss: 36.51786804199219, L1: 10.558692932128906, L3: 25.95917510986328\n",
      "Current prediction:  61.218196868896484 \n",
      "\n",
      "Iteration 11479, Loss: 38.40327453613281, L1: 10.556548118591309, L3: 27.846725463867188\n",
      "Current prediction:  61.20953369140625 \n",
      "\n",
      "Iteration 11480, Loss: 36.40876388549805, L1: 10.572797775268555, L3: 25.835966110229492\n",
      "Current prediction:  61.20121765136719 \n",
      "\n",
      "Iteration 11481, Loss: 36.7340087890625, L1: 10.5805025100708, L3: 26.153505325317383\n",
      "Current prediction:  61.1977424621582 \n",
      "\n",
      "Iteration 11482, Loss: 35.95814895629883, L1: 10.580754280090332, L3: 25.37739372253418\n",
      "Current prediction:  61.197139739990234 \n",
      "\n",
      "Iteration 11483, Loss: 36.86957550048828, L1: 10.58294677734375, L3: 26.286630630493164\n",
      "Current prediction:  61.199764251708984 \n",
      "\n",
      "Iteration 11484, Loss: 36.726219177246094, L1: 10.572074890136719, L3: 26.154146194458008\n",
      "Current prediction:  61.205631256103516 \n",
      "\n",
      "Iteration 11485, Loss: 36.64278793334961, L1: 10.579262733459473, L3: 26.06352424621582\n",
      "Current prediction:  61.209861755371094 \n",
      "\n",
      "Iteration 11486, Loss: 37.161197662353516, L1: 10.567709922790527, L3: 26.593486785888672\n",
      "Current prediction:  61.21283721923828 \n",
      "\n",
      "Iteration 11487, Loss: 35.25482177734375, L1: 10.578995704650879, L3: 24.675827026367188\n",
      "Current prediction:  61.22056579589844 \n",
      "\n",
      "Iteration 11488, Loss: 36.188743591308594, L1: 10.561235427856445, L3: 25.62750816345215\n",
      "Current prediction:  61.232017517089844 \n",
      "\n",
      "Iteration 11489, Loss: 36.37876892089844, L1: 10.558425903320312, L3: 25.820344924926758\n",
      "Current prediction:  61.24226760864258 \n",
      "\n",
      "Iteration 11490, Loss: 36.55472946166992, L1: 10.542080879211426, L3: 26.01264762878418\n",
      "Current prediction:  61.25033187866211 \n",
      "\n",
      "Iteration 11491, Loss: 36.1368408203125, L1: 10.528261184692383, L3: 25.608579635620117\n",
      "Current prediction:  61.259925842285156 \n",
      "\n",
      "Iteration 11492, Loss: 36.62143325805664, L1: 10.529927253723145, L3: 26.091506958007812\n",
      "Current prediction:  61.2631950378418 \n",
      "\n",
      "Iteration 11493, Loss: 36.89027404785156, L1: 10.511857986450195, L3: 26.378416061401367\n",
      "Current prediction:  61.26506805419922 \n",
      "\n",
      "Iteration 11494, Loss: 37.44854736328125, L1: 10.523642539978027, L3: 26.924903869628906\n",
      "Current prediction:  61.26294708251953 \n",
      "\n",
      "Iteration 11495, Loss: 36.0068244934082, L1: 10.52427864074707, L3: 25.482545852661133\n",
      "Current prediction:  61.258785247802734 \n",
      "\n",
      "Iteration 11496, Loss: 36.564937591552734, L1: 10.52214241027832, L3: 26.042795181274414\n",
      "Current prediction:  61.253334045410156 \n",
      "\n",
      "Iteration 11497, Loss: 36.19752502441406, L1: 10.535301208496094, L3: 25.6622257232666\n",
      "Current prediction:  61.249053955078125 \n",
      "\n",
      "Iteration 11498, Loss: 36.177799224853516, L1: 10.54250431060791, L3: 25.635295867919922\n",
      "Current prediction:  61.24396514892578 \n",
      "\n",
      "Iteration 11499, Loss: 36.980873107910156, L1: 10.539695739746094, L3: 26.441177368164062\n",
      "Current prediction:  61.24041748046875 \n",
      "\n",
      "Iteration 11500, Loss: 37.12770462036133, L1: 10.551579475402832, L3: 26.57612419128418\n",
      "Current prediction:  61.2415885925293 \n",
      "\n",
      "Iteration 11501, Loss: 37.09376525878906, L1: 10.535379409790039, L3: 26.558387756347656\n",
      "Current prediction:  61.2455940246582 \n",
      "\n",
      "Iteration 11502, Loss: 36.118831634521484, L1: 10.5354585647583, L3: 25.583372116088867\n",
      "Current prediction:  61.250823974609375 \n",
      "\n",
      "Iteration 11503, Loss: 36.575950622558594, L1: 10.539710998535156, L3: 26.036239624023438\n",
      "Current prediction:  61.25811767578125 \n",
      "\n",
      "Iteration 11504, Loss: 36.857139587402344, L1: 10.540292739868164, L3: 26.31684684753418\n",
      "Current prediction:  61.26374816894531 \n",
      "\n",
      "Iteration 11505, Loss: 37.261497497558594, L1: 10.519729614257812, L3: 26.74176597595215\n",
      "Current prediction:  61.27033615112305 \n",
      "\n",
      "Iteration 11506, Loss: 36.652347564697266, L1: 10.508601188659668, L3: 26.143747329711914\n",
      "Current prediction:  61.27571487426758 \n",
      "\n",
      "Iteration 11507, Loss: 37.611568450927734, L1: 10.509718894958496, L3: 27.101850509643555\n",
      "Current prediction:  61.280303955078125 \n",
      "\n",
      "Iteration 11508, Loss: 36.91830825805664, L1: 10.50844955444336, L3: 26.40985870361328\n",
      "Current prediction:  61.28507614135742 \n",
      "\n",
      "Iteration 11509, Loss: 36.613162994384766, L1: 10.513290405273438, L3: 26.099872589111328\n",
      "Current prediction:  61.28931427001953 \n",
      "\n",
      "Iteration 11510, Loss: 37.104148864746094, L1: 10.50243091583252, L3: 26.60171890258789\n",
      "Current prediction:  61.28927993774414 \n",
      "\n",
      "Iteration 11511, Loss: 35.788475036621094, L1: 10.502840042114258, L3: 25.285634994506836\n",
      "Current prediction:  61.2867317199707 \n",
      "\n",
      "Iteration 11512, Loss: 37.97690963745117, L1: 10.484129905700684, L3: 27.492780685424805\n",
      "Current prediction:  61.27802276611328 \n",
      "\n",
      "Iteration 11513, Loss: 36.342098236083984, L1: 10.506719589233398, L3: 25.835378646850586\n",
      "Current prediction:  61.26933288574219 \n",
      "\n",
      "Iteration 11514, Loss: 35.97074890136719, L1: 10.50682544708252, L3: 25.463924407958984\n",
      "Current prediction:  61.26165008544922 \n",
      "\n",
      "Iteration 11515, Loss: 36.40732192993164, L1: 10.521012306213379, L3: 25.886308670043945\n",
      "Current prediction:  61.2531623840332 \n",
      "\n",
      "Iteration 11516, Loss: 36.43309020996094, L1: 10.529593467712402, L3: 25.90349769592285\n",
      "Current prediction:  61.24762725830078 \n",
      "\n",
      "Iteration 11517, Loss: 37.20222473144531, L1: 10.531787872314453, L3: 26.67043685913086\n",
      "Current prediction:  61.242610931396484 \n",
      "\n",
      "Iteration 11518, Loss: 36.39842987060547, L1: 10.54835033416748, L3: 25.850080490112305\n",
      "Current prediction:  61.23821258544922 \n",
      "\n",
      "Iteration 11519, Loss: 35.73926544189453, L1: 10.556325912475586, L3: 25.182937622070312\n",
      "Current prediction:  61.234500885009766 \n",
      "\n",
      "Iteration 11520, Loss: 36.26050567626953, L1: 10.546895980834961, L3: 25.713611602783203\n",
      "Current prediction:  61.232975006103516 \n",
      "\n",
      "Iteration 11521, Loss: 36.82931900024414, L1: 10.550575256347656, L3: 26.278743743896484\n",
      "Current prediction:  61.22982406616211 \n",
      "\n",
      "Iteration 11522, Loss: 37.438514709472656, L1: 10.557199478149414, L3: 26.881315231323242\n",
      "Current prediction:  61.2278938293457 \n",
      "\n",
      "Iteration 11523, Loss: 36.50758361816406, L1: 10.562967300415039, L3: 25.944618225097656\n",
      "Current prediction:  61.22781753540039 \n",
      "\n",
      "Iteration 11524, Loss: 36.45237350463867, L1: 10.545668601989746, L3: 25.90670394897461\n",
      "Current prediction:  61.22795104980469 \n",
      "\n",
      "Iteration 11525, Loss: 36.79715347290039, L1: 10.531929969787598, L3: 26.265222549438477\n",
      "Current prediction:  61.22784423828125 \n",
      "\n",
      "Iteration 11526, Loss: 36.37400817871094, L1: 10.553475379943848, L3: 25.820533752441406\n",
      "Current prediction:  61.23589324951172 \n",
      "\n",
      "Iteration 11527, Loss: 36.70132064819336, L1: 10.545592308044434, L3: 26.15572738647461\n",
      "Current prediction:  61.24506759643555 \n",
      "\n",
      "Iteration 11528, Loss: 37.80853271484375, L1: 10.538558006286621, L3: 27.269975662231445\n",
      "Current prediction:  61.25188446044922 \n",
      "\n",
      "Iteration 11529, Loss: 36.713523864746094, L1: 10.534611701965332, L3: 26.178913116455078\n",
      "Current prediction:  61.257781982421875 \n",
      "\n",
      "Iteration 11530, Loss: 36.17698669433594, L1: 10.532381057739258, L3: 25.64460563659668\n",
      "Current prediction:  61.26435089111328 \n",
      "\n",
      "Iteration 11531, Loss: 36.30683898925781, L1: 10.52411937713623, L3: 25.7827205657959\n",
      "Current prediction:  61.2717399597168 \n",
      "\n",
      "Iteration 11532, Loss: 36.2984619140625, L1: 10.519186019897461, L3: 25.779277801513672\n",
      "Current prediction:  61.27689743041992 \n",
      "\n",
      "Iteration 11533, Loss: 36.73435974121094, L1: 10.519083023071289, L3: 26.21527862548828\n",
      "Current prediction:  61.27792739868164 \n",
      "\n",
      "Iteration 11534, Loss: 35.410179138183594, L1: 10.515364646911621, L3: 24.894813537597656\n",
      "Current prediction:  61.27574157714844 \n",
      "\n",
      "Iteration 11535, Loss: 36.28178787231445, L1: 10.516079902648926, L3: 25.76570701599121\n",
      "Current prediction:  61.27360153198242 \n",
      "\n",
      "Iteration 11536, Loss: 35.8089714050293, L1: 10.505545616149902, L3: 25.303424835205078\n",
      "Current prediction:  61.27119827270508 \n",
      "\n",
      "Iteration 11537, Loss: 37.4278450012207, L1: 10.52854061126709, L3: 26.899303436279297\n",
      "Current prediction:  61.26644515991211 \n",
      "\n",
      "Iteration 11538, Loss: 36.849891662597656, L1: 10.520822525024414, L3: 26.329069137573242\n",
      "Current prediction:  61.259681701660156 \n",
      "\n",
      "Iteration 11539, Loss: 37.03080749511719, L1: 10.520050048828125, L3: 26.510759353637695\n",
      "Current prediction:  61.25321960449219 \n",
      "\n",
      "Iteration 11540, Loss: 37.87139892578125, L1: 10.542929649353027, L3: 27.328468322753906\n",
      "Current prediction:  61.24972152709961 \n",
      "\n",
      "Iteration 11541, Loss: 37.105533599853516, L1: 10.529065132141113, L3: 26.57646942138672\n",
      "Current prediction:  61.242923736572266 \n",
      "\n",
      "Iteration 11542, Loss: 36.62049102783203, L1: 10.540637969970703, L3: 26.079853057861328\n",
      "Current prediction:  61.23997116088867 \n",
      "\n",
      "Iteration 11543, Loss: 36.893821716308594, L1: 10.543845176696777, L3: 26.349977493286133\n",
      "Current prediction:  61.239234924316406 \n",
      "\n",
      "Iteration 11544, Loss: 36.59016799926758, L1: 10.551586151123047, L3: 26.03858184814453\n",
      "Current prediction:  61.236331939697266 \n",
      "\n",
      "Iteration 11545, Loss: 36.683204650878906, L1: 10.552273750305176, L3: 26.130931854248047\n",
      "Current prediction:  61.2317008972168 \n",
      "\n",
      "Iteration 11546, Loss: 36.4050407409668, L1: 10.552443504333496, L3: 25.852598190307617\n",
      "Current prediction:  61.22652053833008 \n",
      "\n",
      "Iteration 11547, Loss: 36.526893615722656, L1: 10.554181098937988, L3: 25.97271156311035\n",
      "Current prediction:  61.22279357910156 \n",
      "\n",
      "Iteration 11548, Loss: 36.4580192565918, L1: 10.549077987670898, L3: 25.9089412689209\n",
      "Current prediction:  61.222023010253906 \n",
      "\n",
      "Iteration 11549, Loss: 36.06483840942383, L1: 10.562363624572754, L3: 25.502473831176758\n",
      "Current prediction:  61.22450256347656 \n",
      "\n",
      "Iteration 11550, Loss: 36.29481506347656, L1: 10.551137924194336, L3: 25.74367904663086\n",
      "Current prediction:  61.228023529052734 \n",
      "\n",
      "Iteration 11551, Loss: 36.22475051879883, L1: 10.561921119689941, L3: 25.662830352783203\n",
      "Current prediction:  61.23265838623047 \n",
      "\n",
      "Iteration 11552, Loss: 35.52016830444336, L1: 10.551651954650879, L3: 24.968517303466797\n",
      "Current prediction:  61.23949432373047 \n",
      "\n",
      "Iteration 11553, Loss: 36.77111053466797, L1: 10.54592227935791, L3: 26.225187301635742\n",
      "Current prediction:  61.24962615966797 \n",
      "\n",
      "Iteration 11554, Loss: 37.993614196777344, L1: 10.532644271850586, L3: 27.460968017578125\n",
      "Current prediction:  61.25501251220703 \n",
      "\n",
      "Iteration 11555, Loss: 36.79646301269531, L1: 10.532509803771973, L3: 26.263952255249023\n",
      "Current prediction:  61.26076126098633 \n",
      "\n",
      "Iteration 11556, Loss: 36.897926330566406, L1: 10.529069900512695, L3: 26.36885643005371\n",
      "Current prediction:  61.2679557800293 \n",
      "\n",
      "Iteration 11557, Loss: 36.63548278808594, L1: 10.52226448059082, L3: 26.113216400146484\n",
      "Current prediction:  61.278079986572266 \n",
      "\n",
      "Iteration 11558, Loss: 37.09246063232422, L1: 10.518482208251953, L3: 26.5739803314209\n",
      "Current prediction:  61.28847122192383 \n",
      "\n",
      "Iteration 11559, Loss: 35.900238037109375, L1: 10.4982271194458, L3: 25.40201187133789\n",
      "Current prediction:  61.29689407348633 \n",
      "\n",
      "Iteration 11560, Loss: 38.240291595458984, L1: 10.507550239562988, L3: 27.73274040222168\n",
      "Current prediction:  61.29777145385742 \n",
      "\n",
      "Iteration 11561, Loss: 34.91627502441406, L1: 10.502633094787598, L3: 24.41364288330078\n",
      "Current prediction:  61.301361083984375 \n",
      "\n",
      "Iteration 11562, Loss: 35.72475814819336, L1: 10.497159957885742, L3: 25.227598190307617\n",
      "Current prediction:  61.30337905883789 \n",
      "\n",
      "Iteration 11563, Loss: 36.370697021484375, L1: 10.497061729431152, L3: 25.87363624572754\n",
      "Current prediction:  61.308349609375 \n",
      "\n",
      "Iteration 11564, Loss: 36.785465240478516, L1: 10.493265151977539, L3: 26.292200088500977\n",
      "Current prediction:  61.31224060058594 \n",
      "\n",
      "Iteration 11565, Loss: 37.02346420288086, L1: 10.482248306274414, L3: 26.541215896606445\n",
      "Current prediction:  61.311912536621094 \n",
      "\n",
      "Iteration 11566, Loss: 36.211997985839844, L1: 10.482414245605469, L3: 25.729583740234375\n",
      "Current prediction:  61.312618255615234 \n",
      "\n",
      "Iteration 11567, Loss: 37.17829895019531, L1: 10.480371475219727, L3: 26.697927474975586\n",
      "Current prediction:  61.30876922607422 \n",
      "\n",
      "Iteration 11568, Loss: 36.09225845336914, L1: 10.490364074707031, L3: 25.60189437866211\n",
      "Current prediction:  61.30643844604492 \n",
      "\n",
      "Iteration 11569, Loss: 36.80009460449219, L1: 10.476184844970703, L3: 26.323911666870117\n",
      "Current prediction:  61.30445098876953 \n",
      "\n",
      "Iteration 11570, Loss: 36.251190185546875, L1: 10.485331535339355, L3: 25.765859603881836\n",
      "Current prediction:  61.30433654785156 \n",
      "\n",
      "Iteration 11571, Loss: 36.43307876586914, L1: 10.496296882629395, L3: 25.93678092956543\n",
      "Current prediction:  61.302425384521484 \n",
      "\n",
      "Iteration 11572, Loss: 35.87297058105469, L1: 10.493575096130371, L3: 25.37939453125\n",
      "Current prediction:  61.304710388183594 \n",
      "\n",
      "Iteration 11573, Loss: 37.23975372314453, L1: 10.492164611816406, L3: 26.747587203979492\n",
      "Current prediction:  61.306007385253906 \n",
      "\n",
      "Iteration 11574, Loss: 36.420372009277344, L1: 10.486196517944336, L3: 25.93417739868164\n",
      "Current prediction:  61.30677795410156 \n",
      "\n",
      "Iteration 11575, Loss: 36.08155059814453, L1: 10.495115280151367, L3: 25.586437225341797\n",
      "Current prediction:  61.307804107666016 \n",
      "\n",
      "Iteration 11576, Loss: 36.182395935058594, L1: 10.501386642456055, L3: 25.681011199951172\n",
      "Current prediction:  61.30636978149414 \n",
      "\n",
      "Iteration 11577, Loss: 36.55796813964844, L1: 10.493253707885742, L3: 26.064716339111328\n",
      "Current prediction:  61.30602264404297 \n",
      "\n",
      "Iteration 11578, Loss: 36.46913528442383, L1: 10.504254341125488, L3: 25.964879989624023\n",
      "Current prediction:  61.30470275878906 \n",
      "\n",
      "Iteration 11579, Loss: 36.979827880859375, L1: 10.495750427246094, L3: 26.48407745361328\n",
      "Current prediction:  61.296878814697266 \n",
      "\n",
      "Iteration 11580, Loss: 36.08244705200195, L1: 10.48654842376709, L3: 25.595897674560547\n",
      "Current prediction:  61.288578033447266 \n",
      "\n",
      "Iteration 11581, Loss: 36.98281478881836, L1: 10.498717308044434, L3: 26.48409652709961\n",
      "Current prediction:  61.27836227416992 \n",
      "\n",
      "Iteration 11582, Loss: 35.855796813964844, L1: 10.515207290649414, L3: 25.340587615966797\n",
      "Current prediction:  61.26570129394531 \n",
      "\n",
      "Iteration 11583, Loss: 36.321571350097656, L1: 10.530085563659668, L3: 25.791486740112305\n",
      "Current prediction:  61.252227783203125 \n",
      "\n",
      "Iteration 11584, Loss: 37.23884963989258, L1: 10.531563758850098, L3: 26.707286834716797\n",
      "Current prediction:  61.242698669433594 \n",
      "\n",
      "Iteration 11585, Loss: 37.12754821777344, L1: 10.543562889099121, L3: 26.583984375\n",
      "Current prediction:  61.23954772949219 \n",
      "\n",
      "Iteration 11586, Loss: 36.32831573486328, L1: 10.546493530273438, L3: 25.781822204589844\n",
      "Current prediction:  61.236263275146484 \n",
      "\n",
      "Iteration 11587, Loss: 36.70750427246094, L1: 10.554771423339844, L3: 26.152732849121094\n",
      "Current prediction:  61.23661804199219 \n",
      "\n",
      "Iteration 11588, Loss: 36.32367706298828, L1: 10.554279327392578, L3: 25.769397735595703\n",
      "Current prediction:  61.23832702636719 \n",
      "\n",
      "Iteration 11589, Loss: 36.30318069458008, L1: 10.552641868591309, L3: 25.750539779663086\n",
      "Current prediction:  61.24132537841797 \n",
      "\n",
      "Iteration 11590, Loss: 36.18550491333008, L1: 10.544705390930176, L3: 25.64080047607422\n",
      "Current prediction:  61.24517822265625 \n",
      "\n",
      "Iteration 11591, Loss: 37.168251037597656, L1: 10.547327995300293, L3: 26.620922088623047\n",
      "Current prediction:  61.247066497802734 \n",
      "\n",
      "Iteration 11592, Loss: 37.27873229980469, L1: 10.548446655273438, L3: 26.730287551879883\n",
      "Current prediction:  61.24909973144531 \n",
      "\n",
      "Iteration 11593, Loss: 36.79193878173828, L1: 10.54898738861084, L3: 26.242952346801758\n",
      "Current prediction:  61.250282287597656 \n",
      "\n",
      "Iteration 11594, Loss: 37.463775634765625, L1: 10.531561851501465, L3: 26.932212829589844\n",
      "Current prediction:  61.24811935424805 \n",
      "\n",
      "Iteration 11595, Loss: 36.68632125854492, L1: 10.53661060333252, L3: 26.14971160888672\n",
      "Current prediction:  61.24342346191406 \n",
      "\n",
      "Iteration 11596, Loss: 36.71967697143555, L1: 10.545524597167969, L3: 26.174152374267578\n",
      "Current prediction:  61.23798370361328 \n",
      "\n",
      "Iteration 11597, Loss: 36.961944580078125, L1: 10.547369003295898, L3: 26.41457748413086\n",
      "Current prediction:  61.23685836791992 \n",
      "\n",
      "Iteration 11598, Loss: 37.33576965332031, L1: 10.553412437438965, L3: 26.78235626220703\n",
      "Current prediction:  61.23408889770508 \n",
      "\n",
      "Iteration 11599, Loss: 36.488548278808594, L1: 10.543896675109863, L3: 25.944652557373047\n",
      "Current prediction:  61.23482131958008 \n",
      "\n",
      "Iteration 11600, Loss: 37.39992904663086, L1: 10.550241470336914, L3: 26.849687576293945\n",
      "Current prediction:  61.236907958984375 \n",
      "\n",
      "Iteration 11601, Loss: 36.007503509521484, L1: 10.553593635559082, L3: 25.45391082763672\n",
      "Current prediction:  61.239810943603516 \n",
      "\n",
      "Iteration 11602, Loss: 36.26728057861328, L1: 10.542840957641602, L3: 25.72443962097168\n",
      "Current prediction:  61.241416931152344 \n",
      "\n",
      "Iteration 11603, Loss: 36.19253921508789, L1: 10.544516563415527, L3: 25.648021697998047\n",
      "Current prediction:  61.2452507019043 \n",
      "\n",
      "Iteration 11604, Loss: 37.20606994628906, L1: 10.541899681091309, L3: 26.66417121887207\n",
      "Current prediction:  61.24757766723633 \n",
      "\n",
      "Iteration 11605, Loss: 36.698707580566406, L1: 10.538533210754395, L3: 26.160173416137695\n",
      "Current prediction:  61.244842529296875 \n",
      "\n",
      "Iteration 11606, Loss: 38.09518051147461, L1: 10.54345417022705, L3: 27.551727294921875\n",
      "Current prediction:  61.23951721191406 \n",
      "\n",
      "Iteration 11607, Loss: 37.499053955078125, L1: 10.54403305053711, L3: 26.955018997192383\n",
      "Current prediction:  61.232730865478516 \n",
      "\n",
      "Iteration 11608, Loss: 36.016822814941406, L1: 10.553315162658691, L3: 25.4635066986084\n",
      "Current prediction:  61.2303352355957 \n",
      "\n",
      "Iteration 11609, Loss: 36.337005615234375, L1: 10.550570487976074, L3: 25.786434173583984\n",
      "Current prediction:  61.22764587402344 \n",
      "\n",
      "Iteration 11610, Loss: 36.529666900634766, L1: 10.5475435256958, L3: 25.98212242126465\n",
      "Current prediction:  61.22663116455078 \n",
      "\n",
      "Iteration 11611, Loss: 36.89867401123047, L1: 10.557158470153809, L3: 26.341516494750977\n",
      "Current prediction:  61.226905822753906 \n",
      "\n",
      "Iteration 11612, Loss: 36.80958938598633, L1: 10.55015754699707, L3: 26.259431838989258\n",
      "Current prediction:  61.22789001464844 \n",
      "\n",
      "Iteration 11613, Loss: 36.435523986816406, L1: 10.544715881347656, L3: 25.890810012817383\n",
      "Current prediction:  61.2279052734375 \n",
      "\n",
      "Iteration 11614, Loss: 37.00497055053711, L1: 10.547673225402832, L3: 26.45729637145996\n",
      "Current prediction:  61.22824478149414 \n",
      "\n",
      "Iteration 11615, Loss: 37.824913024902344, L1: 10.556018829345703, L3: 27.26889419555664\n",
      "Current prediction:  61.225303649902344 \n",
      "\n",
      "Iteration 11616, Loss: 36.64278030395508, L1: 10.55422306060791, L3: 26.088558197021484\n",
      "Current prediction:  61.22215270996094 \n",
      "\n",
      "Iteration 11617, Loss: 36.903263092041016, L1: 10.560982704162598, L3: 26.342281341552734\n",
      "Current prediction:  61.218997955322266 \n",
      "\n",
      "Iteration 11618, Loss: 36.695919036865234, L1: 10.564908981323242, L3: 26.131010055541992\n",
      "Current prediction:  61.22116470336914 \n",
      "\n",
      "Iteration 11619, Loss: 35.951873779296875, L1: 10.553874969482422, L3: 25.398000717163086\n",
      "Current prediction:  61.230472564697266 \n",
      "\n",
      "Iteration 11620, Loss: 36.29682159423828, L1: 10.546706199645996, L3: 25.7501163482666\n",
      "Current prediction:  61.23956298828125 \n",
      "\n",
      "Iteration 11621, Loss: 35.49973678588867, L1: 10.554841995239258, L3: 24.944894790649414\n",
      "Current prediction:  61.24936294555664 \n",
      "\n",
      "Iteration 11622, Loss: 36.4813232421875, L1: 10.521197319030762, L3: 25.960126876831055\n",
      "Current prediction:  61.25701904296875 \n",
      "\n",
      "Iteration 11623, Loss: 36.64094543457031, L1: 10.527654647827148, L3: 26.113290786743164\n",
      "Current prediction:  61.26081085205078 \n",
      "\n",
      "Iteration 11624, Loss: 35.63617706298828, L1: 10.528156280517578, L3: 25.108020782470703\n",
      "Current prediction:  61.26736831665039 \n",
      "\n",
      "Iteration 11625, Loss: 36.66069412231445, L1: 10.51341438293457, L3: 26.147279739379883\n",
      "Current prediction:  61.273216247558594 \n",
      "\n",
      "Iteration 11626, Loss: 37.1384391784668, L1: 10.507989883422852, L3: 26.630449295043945\n",
      "Current prediction:  61.27935791015625 \n",
      "\n",
      "Iteration 11627, Loss: 35.56214904785156, L1: 10.498506546020508, L3: 25.063640594482422\n",
      "Current prediction:  61.28385925292969 \n",
      "\n",
      "Iteration 11628, Loss: 36.91140365600586, L1: 10.50312614440918, L3: 26.40827751159668\n",
      "Current prediction:  61.2846794128418 \n",
      "\n",
      "Iteration 11629, Loss: 35.66712951660156, L1: 10.506671905517578, L3: 25.16045570373535\n",
      "Current prediction:  61.28471374511719 \n",
      "\n",
      "Iteration 11630, Loss: 36.1708984375, L1: 10.509811401367188, L3: 25.66108512878418\n",
      "Current prediction:  61.28056335449219 \n",
      "\n",
      "Iteration 11631, Loss: 36.59967803955078, L1: 10.505645751953125, L3: 26.094030380249023\n",
      "Current prediction:  61.27482604980469 \n",
      "\n",
      "Iteration 11632, Loss: 37.67876052856445, L1: 10.512663841247559, L3: 27.16609764099121\n",
      "Current prediction:  61.269222259521484 \n",
      "\n",
      "Iteration 11633, Loss: 37.006019592285156, L1: 10.523300170898438, L3: 26.48272132873535\n",
      "Current prediction:  61.2619743347168 \n",
      "\n",
      "Iteration 11634, Loss: 36.441810607910156, L1: 10.530556678771973, L3: 25.9112548828125\n",
      "Current prediction:  61.254234313964844 \n",
      "\n",
      "Iteration 11635, Loss: 35.65986633300781, L1: 10.526312828063965, L3: 25.13355255126953\n",
      "Current prediction:  61.24320602416992 \n",
      "\n",
      "Iteration 11636, Loss: 37.35002136230469, L1: 10.553895950317383, L3: 26.796125411987305\n",
      "Current prediction:  61.230499267578125 \n",
      "\n",
      "Iteration 11637, Loss: 37.054931640625, L1: 10.552230834960938, L3: 26.502702713012695\n",
      "Current prediction:  61.21653747558594 \n",
      "\n",
      "Iteration 11638, Loss: 36.64239501953125, L1: 10.567543029785156, L3: 26.074853897094727\n",
      "Current prediction:  61.21138381958008 \n",
      "\n",
      "Iteration 11639, Loss: 36.23200607299805, L1: 10.570773124694824, L3: 25.66123390197754\n",
      "Current prediction:  61.20820999145508 \n",
      "\n",
      "Iteration 11640, Loss: 37.109222412109375, L1: 10.581320762634277, L3: 26.52790069580078\n",
      "Current prediction:  61.201988220214844 \n",
      "\n",
      "Iteration 11641, Loss: 36.48958969116211, L1: 10.558798789978027, L3: 25.930789947509766\n",
      "Current prediction:  61.197872161865234 \n",
      "\n",
      "Iteration 11642, Loss: 36.46918487548828, L1: 10.565160751342773, L3: 25.90402603149414\n",
      "Current prediction:  61.19279098510742 \n",
      "\n",
      "Iteration 11643, Loss: 36.76350784301758, L1: 10.583930015563965, L3: 26.179576873779297\n",
      "Current prediction:  61.190372467041016 \n",
      "\n",
      "Iteration 11644, Loss: 36.24797058105469, L1: 10.583039283752441, L3: 25.66493034362793\n",
      "Current prediction:  61.18832778930664 \n",
      "\n",
      "Iteration 11645, Loss: 36.72183609008789, L1: 10.566676139831543, L3: 26.155160903930664\n",
      "Current prediction:  61.18608474731445 \n",
      "\n",
      "Iteration 11646, Loss: 36.649147033691406, L1: 10.583333969116211, L3: 26.065814971923828\n",
      "Current prediction:  61.18512725830078 \n",
      "\n",
      "Iteration 11647, Loss: 36.771568298339844, L1: 10.589672088623047, L3: 26.181896209716797\n",
      "Current prediction:  61.1884651184082 \n",
      "\n",
      "Iteration 11648, Loss: 37.228145599365234, L1: 10.58954906463623, L3: 26.63859748840332\n",
      "Current prediction:  61.189918518066406 \n",
      "\n",
      "Iteration 11649, Loss: 37.14429473876953, L1: 10.591161727905273, L3: 26.55313491821289\n",
      "Current prediction:  61.191707611083984 \n",
      "\n",
      "Iteration 11650, Loss: 36.4097785949707, L1: 10.569591522216797, L3: 25.840187072753906\n",
      "Current prediction:  61.196224212646484 \n",
      "\n",
      "Iteration 11651, Loss: 37.0784797668457, L1: 10.571651458740234, L3: 26.50682830810547\n",
      "Current prediction:  61.20441818237305 \n",
      "\n",
      "Iteration 11652, Loss: 35.88059997558594, L1: 10.580338478088379, L3: 25.300260543823242\n",
      "Current prediction:  61.21708297729492 \n",
      "\n",
      "Iteration 11653, Loss: 37.00874710083008, L1: 10.552495002746582, L3: 26.456253051757812\n",
      "Current prediction:  61.23310089111328 \n",
      "\n",
      "Iteration 11654, Loss: 37.27757263183594, L1: 10.547624588012695, L3: 26.729949951171875\n",
      "Current prediction:  61.24909591674805 \n",
      "\n",
      "Iteration 11655, Loss: 36.66520690917969, L1: 10.547256469726562, L3: 26.117948532104492\n",
      "Current prediction:  61.26149368286133 \n",
      "\n",
      "Iteration 11656, Loss: 36.54671859741211, L1: 10.52423095703125, L3: 26.02248764038086\n",
      "Current prediction:  61.27593231201172 \n",
      "\n",
      "Iteration 11657, Loss: 36.39923095703125, L1: 10.516342163085938, L3: 25.882890701293945\n",
      "Current prediction:  61.2913703918457 \n",
      "\n",
      "Iteration 11658, Loss: 36.90684509277344, L1: 10.491508483886719, L3: 26.41533660888672\n",
      "Current prediction:  61.30655288696289 \n",
      "\n",
      "Iteration 11659, Loss: 36.38240432739258, L1: 10.485733985900879, L3: 25.896671295166016\n",
      "Current prediction:  61.3175163269043 \n",
      "\n",
      "Iteration 11660, Loss: 36.798404693603516, L1: 10.465479850769043, L3: 26.33292579650879\n",
      "Current prediction:  61.32328796386719 \n",
      "\n",
      "Iteration 11661, Loss: 36.19089126586914, L1: 10.475078582763672, L3: 25.71581268310547\n",
      "Current prediction:  61.32579803466797 \n",
      "\n",
      "Iteration 11662, Loss: 36.68852233886719, L1: 10.477876663208008, L3: 26.210647583007812\n",
      "Current prediction:  61.31879806518555 \n",
      "\n",
      "Iteration 11663, Loss: 35.23236083984375, L1: 10.475069999694824, L3: 24.75728988647461\n",
      "Current prediction:  61.311519622802734 \n",
      "\n",
      "Iteration 11664, Loss: 36.38554000854492, L1: 10.484458923339844, L3: 25.901081085205078\n",
      "Current prediction:  61.308624267578125 \n",
      "\n",
      "Iteration 11665, Loss: 36.91145706176758, L1: 10.487868309020996, L3: 26.4235897064209\n",
      "Current prediction:  61.303592681884766 \n",
      "\n",
      "Iteration 11666, Loss: 36.27919006347656, L1: 10.485457420349121, L3: 25.793733596801758\n",
      "Current prediction:  61.299495697021484 \n",
      "\n",
      "Iteration 11667, Loss: 36.508243560791016, L1: 10.502663612365723, L3: 26.00558090209961\n",
      "Current prediction:  61.29142379760742 \n",
      "\n",
      "Iteration 11668, Loss: 36.54648208618164, L1: 10.510543823242188, L3: 26.035938262939453\n",
      "Current prediction:  61.28395080566406 \n",
      "\n",
      "Iteration 11669, Loss: 36.71601486206055, L1: 10.504271507263184, L3: 26.211742401123047\n",
      "Current prediction:  61.27705383300781 \n",
      "\n",
      "Iteration 11670, Loss: 35.50102996826172, L1: 10.505953788757324, L3: 24.995075225830078\n",
      "Current prediction:  61.26967239379883 \n",
      "\n",
      "Iteration 11671, Loss: 36.568389892578125, L1: 10.52914047241211, L3: 26.039249420166016\n",
      "Current prediction:  61.263885498046875 \n",
      "\n",
      "Iteration 11672, Loss: 37.00757598876953, L1: 10.515304565429688, L3: 26.49226951599121\n",
      "Current prediction:  61.2576789855957 \n",
      "\n",
      "Iteration 11673, Loss: 36.42109298706055, L1: 10.537514686584473, L3: 25.88357925415039\n",
      "Current prediction:  61.25161361694336 \n",
      "\n",
      "Iteration 11674, Loss: 36.08949661254883, L1: 10.54334545135498, L3: 25.54615020751953\n",
      "Current prediction:  61.247802734375 \n",
      "\n",
      "Iteration 11675, Loss: 36.270294189453125, L1: 10.544233322143555, L3: 25.726062774658203\n",
      "Current prediction:  61.2469596862793 \n",
      "\n",
      "Iteration 11676, Loss: 37.27915954589844, L1: 10.543701171875, L3: 26.735456466674805\n",
      "Current prediction:  61.248023986816406 \n",
      "\n",
      "Iteration 11677, Loss: 36.358543395996094, L1: 10.53155517578125, L3: 25.826988220214844\n",
      "Current prediction:  61.248104095458984 \n",
      "\n",
      "Iteration 11678, Loss: 36.3680534362793, L1: 10.531647682189941, L3: 25.83640480041504\n",
      "Current prediction:  61.246177673339844 \n",
      "\n",
      "Iteration 11679, Loss: 36.080997467041016, L1: 10.548513412475586, L3: 25.53248405456543\n",
      "Current prediction:  61.248409271240234 \n",
      "\n",
      "Iteration 11680, Loss: 36.87934875488281, L1: 10.536787033081055, L3: 26.342561721801758\n",
      "Current prediction:  61.24851608276367 \n",
      "\n",
      "Iteration 11681, Loss: 36.0677375793457, L1: 10.526429176330566, L3: 25.541309356689453\n",
      "Current prediction:  61.247337341308594 \n",
      "\n",
      "Iteration 11682, Loss: 36.78593444824219, L1: 10.535303115844727, L3: 26.25063133239746\n",
      "Current prediction:  61.24625778198242 \n",
      "\n",
      "Iteration 11683, Loss: 37.26872253417969, L1: 10.54619026184082, L3: 26.7225341796875\n",
      "Current prediction:  61.24542999267578 \n",
      "\n",
      "Iteration 11684, Loss: 36.53028106689453, L1: 10.540430068969727, L3: 25.989849090576172\n",
      "Current prediction:  61.24585723876953 \n",
      "\n",
      "Iteration 11685, Loss: 36.523338317871094, L1: 10.539851188659668, L3: 25.983488082885742\n",
      "Current prediction:  61.25142288208008 \n",
      "\n",
      "Iteration 11686, Loss: 36.56959533691406, L1: 10.525657653808594, L3: 26.043935775756836\n",
      "Current prediction:  61.25597381591797 \n",
      "\n",
      "Iteration 11687, Loss: 36.002742767333984, L1: 10.533831596374512, L3: 25.468910217285156\n",
      "Current prediction:  61.259578704833984 \n",
      "\n",
      "Iteration 11688, Loss: 36.97148895263672, L1: 10.529622077941895, L3: 26.44186782836914\n",
      "Current prediction:  61.265403747558594 \n",
      "\n",
      "Iteration 11689, Loss: 35.966957092285156, L1: 10.523030281066895, L3: 25.443927764892578\n",
      "Current prediction:  61.26995086669922 \n",
      "\n",
      "Iteration 11690, Loss: 36.34062957763672, L1: 10.528319358825684, L3: 25.81230926513672\n",
      "Current prediction:  61.274658203125 \n",
      "\n",
      "Iteration 11691, Loss: 37.65979766845703, L1: 10.509114265441895, L3: 27.150684356689453\n",
      "Current prediction:  61.27510452270508 \n",
      "\n",
      "Iteration 11692, Loss: 36.997962951660156, L1: 10.506970405578613, L3: 26.49099349975586\n",
      "Current prediction:  61.274227142333984 \n",
      "\n",
      "Iteration 11693, Loss: 37.007102966308594, L1: 10.512823104858398, L3: 26.494281768798828\n",
      "Current prediction:  61.270973205566406 \n",
      "\n",
      "Iteration 11694, Loss: 37.52155303955078, L1: 10.516197204589844, L3: 27.005355834960938\n",
      "Current prediction:  61.26588439941406 \n",
      "\n",
      "Iteration 11695, Loss: 37.462833404541016, L1: 10.52573299407959, L3: 26.93709945678711\n",
      "Current prediction:  61.262821197509766 \n",
      "\n",
      "Iteration 11696, Loss: 36.51995849609375, L1: 10.520185470581055, L3: 25.999771118164062\n",
      "Current prediction:  61.260337829589844 \n",
      "\n",
      "Iteration 11697, Loss: 35.825965881347656, L1: 10.532245635986328, L3: 25.29372215270996\n",
      "Current prediction:  61.25941848754883 \n",
      "\n",
      "Iteration 11698, Loss: 36.809600830078125, L1: 10.53272819519043, L3: 26.276874542236328\n",
      "Current prediction:  61.257816314697266 \n",
      "\n",
      "Iteration 11699, Loss: 36.60344696044922, L1: 10.542217254638672, L3: 26.061229705810547\n",
      "Current prediction:  61.25510787963867 \n",
      "\n",
      "Iteration 11700, Loss: 36.65117645263672, L1: 10.534510612487793, L3: 26.11666488647461\n",
      "Current prediction:  61.253753662109375 \n",
      "\n",
      "Iteration 11701, Loss: 36.650482177734375, L1: 10.545440673828125, L3: 26.10504150390625\n",
      "Current prediction:  61.25327682495117 \n",
      "\n",
      "Iteration 11702, Loss: 36.62879943847656, L1: 10.543333053588867, L3: 26.085468292236328\n",
      "Current prediction:  61.25090789794922 \n",
      "\n",
      "Iteration 11703, Loss: 37.053009033203125, L1: 10.524917602539062, L3: 26.528093338012695\n",
      "Current prediction:  61.24330139160156 \n",
      "\n",
      "Iteration 11704, Loss: 37.54603576660156, L1: 10.552558898925781, L3: 26.993478775024414\n",
      "Current prediction:  61.23661422729492 \n",
      "\n",
      "Iteration 11705, Loss: 35.87614059448242, L1: 10.552594184875488, L3: 25.32354736328125\n",
      "Current prediction:  61.228515625 \n",
      "\n",
      "Iteration 11706, Loss: 36.07356643676758, L1: 10.553479194641113, L3: 25.52008819580078\n",
      "Current prediction:  61.22439956665039 \n",
      "\n",
      "Iteration 11707, Loss: 37.061279296875, L1: 10.549560546875, L3: 26.51171875\n",
      "Current prediction:  61.21932601928711 \n",
      "\n",
      "Iteration 11708, Loss: 35.971153259277344, L1: 10.566843032836914, L3: 25.404312133789062\n",
      "Current prediction:  61.216522216796875 \n",
      "\n",
      "Iteration 11709, Loss: 36.57705307006836, L1: 10.579416275024414, L3: 25.997636795043945\n",
      "Current prediction:  61.220375061035156 \n",
      "\n",
      "Iteration 11710, Loss: 36.26473617553711, L1: 10.573084831237793, L3: 25.691652297973633\n",
      "Current prediction:  61.22886657714844 \n",
      "\n",
      "Iteration 11711, Loss: 37.16046142578125, L1: 10.563437461853027, L3: 26.59702491760254\n",
      "Current prediction:  61.23622131347656 \n",
      "\n",
      "Iteration 11712, Loss: 37.29143142700195, L1: 10.550345420837402, L3: 26.741085052490234\n",
      "Current prediction:  61.24161148071289 \n",
      "\n",
      "Iteration 11713, Loss: 37.22966003417969, L1: 10.537410736083984, L3: 26.69224739074707\n",
      "Current prediction:  61.25014877319336 \n",
      "\n",
      "Iteration 11714, Loss: 36.5301399230957, L1: 10.542006492614746, L3: 25.98813247680664\n",
      "Current prediction:  61.25855255126953 \n",
      "\n",
      "Iteration 11715, Loss: 36.42991256713867, L1: 10.524762153625488, L3: 25.905149459838867\n",
      "Current prediction:  61.26310348510742 \n",
      "\n",
      "Iteration 11716, Loss: 36.71758270263672, L1: 10.528854370117188, L3: 26.18872833251953\n",
      "Current prediction:  61.26713180541992 \n",
      "\n",
      "Iteration 11717, Loss: 37.309722900390625, L1: 10.523473739624023, L3: 26.78624725341797\n",
      "Current prediction:  61.26918411254883 \n",
      "\n",
      "Iteration 11718, Loss: 37.59398651123047, L1: 10.527688026428223, L3: 27.066299438476562\n",
      "Current prediction:  61.270240783691406 \n",
      "\n",
      "Iteration 11719, Loss: 37.22390365600586, L1: 10.517857551574707, L3: 26.70604705810547\n",
      "Current prediction:  61.26597595214844 \n",
      "\n",
      "Iteration 11720, Loss: 35.65104293823242, L1: 10.524150848388672, L3: 25.12689208984375\n",
      "Current prediction:  61.26183319091797 \n",
      "\n",
      "Iteration 11721, Loss: 36.55233383178711, L1: 10.53165340423584, L3: 26.020679473876953\n",
      "Current prediction:  61.25768280029297 \n",
      "\n",
      "Iteration 11722, Loss: 36.99476623535156, L1: 10.548173904418945, L3: 26.44659423828125\n",
      "Current prediction:  61.25353240966797 \n",
      "\n",
      "Iteration 11723, Loss: 36.53757095336914, L1: 10.534442901611328, L3: 26.003128051757812\n",
      "Current prediction:  61.24713897705078 \n",
      "\n",
      "Iteration 11724, Loss: 36.165550231933594, L1: 10.541997909545898, L3: 25.623554229736328\n",
      "Current prediction:  61.23974609375 \n",
      "\n",
      "Iteration 11725, Loss: 36.332603454589844, L1: 10.544266700744629, L3: 25.78833770751953\n",
      "Current prediction:  61.23537063598633 \n",
      "\n",
      "Iteration 11726, Loss: 37.006412506103516, L1: 10.54655647277832, L3: 26.459856033325195\n",
      "Current prediction:  61.23027420043945 \n",
      "\n",
      "Iteration 11727, Loss: 36.98833465576172, L1: 10.545865058898926, L3: 26.442468643188477\n",
      "Current prediction:  61.22640609741211 \n",
      "\n",
      "Iteration 11728, Loss: 37.398189544677734, L1: 10.556655883789062, L3: 26.841533660888672\n",
      "Current prediction:  61.22052001953125 \n",
      "\n",
      "Iteration 11729, Loss: 37.3031120300293, L1: 10.558284759521484, L3: 26.744827270507812\n",
      "Current prediction:  61.21725082397461 \n",
      "\n",
      "Iteration 11730, Loss: 36.9605712890625, L1: 10.565213203430176, L3: 26.395357131958008\n",
      "Current prediction:  61.2161750793457 \n",
      "\n",
      "Iteration 11731, Loss: 35.68259048461914, L1: 10.567093849182129, L3: 25.115495681762695\n",
      "Current prediction:  61.217994689941406 \n",
      "\n",
      "Iteration 11732, Loss: 36.87945556640625, L1: 10.559242248535156, L3: 26.320215225219727\n",
      "Current prediction:  61.21820831298828 \n",
      "\n",
      "Iteration 11733, Loss: 36.645591735839844, L1: 10.563015937805176, L3: 26.08257484436035\n",
      "Current prediction:  61.22344207763672 \n",
      "\n",
      "Iteration 11734, Loss: 37.33308029174805, L1: 10.559114456176758, L3: 26.77396583557129\n",
      "Current prediction:  61.229190826416016 \n",
      "\n",
      "Iteration 11735, Loss: 36.9647216796875, L1: 10.560791969299316, L3: 26.403928756713867\n",
      "Current prediction:  61.23545837402344 \n",
      "\n",
      "Iteration 11736, Loss: 36.7362060546875, L1: 10.546642303466797, L3: 26.189565658569336\n",
      "Current prediction:  61.243125915527344 \n",
      "\n",
      "Iteration 11737, Loss: 35.49525451660156, L1: 10.536798477172852, L3: 24.958457946777344\n",
      "Current prediction:  61.253944396972656 \n",
      "\n",
      "Iteration 11738, Loss: 37.28248596191406, L1: 10.532426834106445, L3: 26.750059127807617\n",
      "Current prediction:  61.25967788696289 \n",
      "\n",
      "Iteration 11739, Loss: 37.01449203491211, L1: 10.540509223937988, L3: 26.473983764648438\n",
      "Current prediction:  61.265113830566406 \n",
      "\n",
      "Iteration 11740, Loss: 36.69574737548828, L1: 10.515201568603516, L3: 26.1805477142334\n",
      "Current prediction:  61.26627731323242 \n",
      "\n",
      "Iteration 11741, Loss: 36.96057891845703, L1: 10.516575813293457, L3: 26.44400405883789\n",
      "Current prediction:  61.269283294677734 \n",
      "\n",
      "Iteration 11742, Loss: 37.12367630004883, L1: 10.521966934204102, L3: 26.601709365844727\n",
      "Current prediction:  61.27409362792969 \n",
      "\n",
      "Iteration 11743, Loss: 36.403175354003906, L1: 10.516767501831055, L3: 25.88640785217285\n",
      "Current prediction:  61.28058624267578 \n",
      "\n",
      "Iteration 11744, Loss: 35.739681243896484, L1: 10.514002799987793, L3: 25.225677490234375\n",
      "Current prediction:  61.28631591796875 \n",
      "\n",
      "Iteration 11745, Loss: 36.224918365478516, L1: 10.499189376831055, L3: 25.72572898864746\n",
      "Current prediction:  61.28728485107422 \n",
      "\n",
      "Iteration 11746, Loss: 37.154930114746094, L1: 10.500387191772461, L3: 26.654542922973633\n",
      "Current prediction:  61.28545379638672 \n",
      "\n",
      "Iteration 11747, Loss: 36.88262939453125, L1: 10.508834838867188, L3: 26.373794555664062\n",
      "Current prediction:  61.282081604003906 \n",
      "\n",
      "Iteration 11748, Loss: 37.1339111328125, L1: 10.507912635803223, L3: 26.62599754333496\n",
      "Current prediction:  61.28042984008789 \n",
      "\n",
      "Iteration 11749, Loss: 37.09709548950195, L1: 10.503374099731445, L3: 26.593721389770508\n",
      "Current prediction:  61.275474548339844 \n",
      "\n",
      "Iteration 11750, Loss: 36.79161834716797, L1: 10.516199111938477, L3: 26.275419235229492\n",
      "Current prediction:  61.275089263916016 \n",
      "\n",
      "Iteration 11751, Loss: 36.234031677246094, L1: 10.511645317077637, L3: 25.722387313842773\n",
      "Current prediction:  61.27409362792969 \n",
      "\n",
      "Iteration 11752, Loss: 36.84281921386719, L1: 10.51235580444336, L3: 26.330463409423828\n",
      "Current prediction:  61.27104949951172 \n",
      "\n",
      "Iteration 11753, Loss: 37.62353515625, L1: 10.510375022888184, L3: 27.1131591796875\n",
      "Current prediction:  61.265010833740234 \n",
      "\n",
      "Iteration 11754, Loss: 36.91965866088867, L1: 10.523701667785645, L3: 26.39595603942871\n",
      "Current prediction:  61.259944915771484 \n",
      "\n",
      "Iteration 11755, Loss: 37.127986907958984, L1: 10.530077934265137, L3: 26.597909927368164\n",
      "Current prediction:  61.25434494018555 \n",
      "\n",
      "Iteration 11756, Loss: 36.88767623901367, L1: 10.53382682800293, L3: 26.353849411010742\n",
      "Current prediction:  61.249141693115234 \n",
      "\n",
      "Iteration 11757, Loss: 36.991455078125, L1: 10.541502952575684, L3: 26.449953079223633\n",
      "Current prediction:  61.24223327636719 \n",
      "\n",
      "Iteration 11758, Loss: 36.0271110534668, L1: 10.527724266052246, L3: 25.499385833740234\n",
      "Current prediction:  61.23947525024414 \n",
      "\n",
      "Iteration 11759, Loss: 36.35239028930664, L1: 10.545647621154785, L3: 25.80674171447754\n",
      "Current prediction:  61.235328674316406 \n",
      "\n",
      "Iteration 11760, Loss: 36.42210388183594, L1: 10.548624992370605, L3: 25.873477935791016\n",
      "Current prediction:  61.23377990722656 \n",
      "\n",
      "Iteration 11761, Loss: 36.29744338989258, L1: 10.545771598815918, L3: 25.751670837402344\n",
      "Current prediction:  61.23533248901367 \n",
      "\n",
      "Iteration 11762, Loss: 36.80085754394531, L1: 10.5463285446167, L3: 26.254528045654297\n",
      "Current prediction:  61.24090576171875 \n",
      "\n",
      "Iteration 11763, Loss: 36.837310791015625, L1: 10.543184280395508, L3: 26.294124603271484\n",
      "Current prediction:  61.24756622314453 \n",
      "\n",
      "Iteration 11764, Loss: 36.27232360839844, L1: 10.527656555175781, L3: 25.744665145874023\n",
      "Current prediction:  61.254302978515625 \n",
      "\n",
      "Iteration 11765, Loss: 36.5855827331543, L1: 10.522802352905273, L3: 26.062780380249023\n",
      "Current prediction:  61.26002883911133 \n",
      "\n",
      "Iteration 11766, Loss: 36.29741668701172, L1: 10.508087158203125, L3: 25.78932762145996\n",
      "Current prediction:  61.26563262939453 \n",
      "\n",
      "Iteration 11767, Loss: 36.59061813354492, L1: 10.517708778381348, L3: 26.07291030883789\n",
      "Current prediction:  61.27175521850586 \n",
      "\n",
      "Iteration 11768, Loss: 36.827850341796875, L1: 10.504812240600586, L3: 26.32303810119629\n",
      "Current prediction:  61.27669906616211 \n",
      "\n",
      "Iteration 11769, Loss: 35.3055534362793, L1: 10.49743938446045, L3: 24.808115005493164\n",
      "Current prediction:  61.280357360839844 \n",
      "\n",
      "Iteration 11770, Loss: 36.465057373046875, L1: 10.500638961791992, L3: 25.964418411254883\n",
      "Current prediction:  61.2840576171875 \n",
      "\n",
      "Iteration 11771, Loss: 36.62189865112305, L1: 10.504583358764648, L3: 26.1173152923584\n",
      "Current prediction:  61.28662872314453 \n",
      "\n",
      "Iteration 11772, Loss: 37.00474166870117, L1: 10.508013725280762, L3: 26.496728897094727\n",
      "Current prediction:  61.282958984375 \n",
      "\n",
      "Iteration 11773, Loss: 36.24010467529297, L1: 10.504226684570312, L3: 25.735876083374023\n",
      "Current prediction:  61.28474426269531 \n",
      "\n",
      "Iteration 11774, Loss: 36.34436798095703, L1: 10.505690574645996, L3: 25.83867835998535\n",
      "Current prediction:  61.28763198852539 \n",
      "\n",
      "Iteration 11775, Loss: 35.69953536987305, L1: 10.498461723327637, L3: 25.201074600219727\n",
      "Current prediction:  61.2893180847168 \n",
      "\n",
      "Iteration 11776, Loss: 36.412986755371094, L1: 10.494719505310059, L3: 25.91826629638672\n",
      "Current prediction:  61.28837585449219 \n",
      "\n",
      "Iteration 11777, Loss: 36.087181091308594, L1: 10.497734069824219, L3: 25.589447021484375\n",
      "Current prediction:  61.288082122802734 \n",
      "\n",
      "Iteration 11778, Loss: 36.949363708496094, L1: 10.503375053405762, L3: 26.445987701416016\n",
      "Current prediction:  61.287322998046875 \n",
      "\n",
      "Iteration 11779, Loss: 36.90788269042969, L1: 10.493481636047363, L3: 26.414400100708008\n",
      "Current prediction:  61.28123092651367 \n",
      "\n",
      "Iteration 11780, Loss: 35.85889434814453, L1: 10.505878448486328, L3: 25.353017807006836\n",
      "Current prediction:  61.27744674682617 \n",
      "\n",
      "Iteration 11781, Loss: 36.69140625, L1: 10.517114639282227, L3: 26.17428970336914\n",
      "Current prediction:  61.268760681152344 \n",
      "\n",
      "Iteration 11782, Loss: 36.97283935546875, L1: 10.513050079345703, L3: 26.45979118347168\n",
      "Current prediction:  61.25902557373047 \n",
      "\n",
      "Iteration 11783, Loss: 36.9476318359375, L1: 10.53958511352539, L3: 26.40804672241211\n",
      "Current prediction:  61.24885940551758 \n",
      "\n",
      "Iteration 11784, Loss: 36.102840423583984, L1: 10.543397903442383, L3: 25.5594425201416\n",
      "Current prediction:  61.239990234375 \n",
      "\n",
      "Iteration 11785, Loss: 37.9793701171875, L1: 10.544878959655762, L3: 27.434490203857422\n",
      "Current prediction:  61.23066711425781 \n",
      "\n",
      "Iteration 11786, Loss: 36.812042236328125, L1: 10.563056945800781, L3: 26.248985290527344\n",
      "Current prediction:  61.21928405761719 \n",
      "\n",
      "Iteration 11787, Loss: 36.46247100830078, L1: 10.541546821594238, L3: 25.92092514038086\n",
      "Current prediction:  61.20528030395508 \n",
      "\n",
      "Iteration 11788, Loss: 35.945308685302734, L1: 10.568887710571289, L3: 25.376420974731445\n",
      "Current prediction:  61.193843841552734 \n",
      "\n",
      "Iteration 11789, Loss: 36.897254943847656, L1: 10.584864616394043, L3: 26.31239128112793\n",
      "Current prediction:  61.18278121948242 \n",
      "\n",
      "Iteration 11790, Loss: 36.317161560058594, L1: 10.587477684020996, L3: 25.72968292236328\n",
      "Current prediction:  61.17173385620117 \n",
      "\n",
      "Iteration 11791, Loss: 35.856971740722656, L1: 10.58895206451416, L3: 25.268020629882812\n",
      "Current prediction:  61.167362213134766 \n",
      "\n",
      "Iteration 11792, Loss: 36.865474700927734, L1: 10.58600902557373, L3: 26.279464721679688\n",
      "Current prediction:  61.16254425048828 \n",
      "\n",
      "Iteration 11793, Loss: 36.49034118652344, L1: 10.606582641601562, L3: 25.883760452270508\n",
      "Current prediction:  61.15753173828125 \n",
      "\n",
      "Iteration 11794, Loss: 36.56583786010742, L1: 10.618019104003906, L3: 25.947818756103516\n",
      "Current prediction:  61.157737731933594 \n",
      "\n",
      "Iteration 11795, Loss: 36.37427520751953, L1: 10.603141784667969, L3: 25.771133422851562\n",
      "Current prediction:  61.161861419677734 \n",
      "\n",
      "Iteration 11796, Loss: 35.78950881958008, L1: 10.609088897705078, L3: 25.180419921875\n",
      "Current prediction:  61.16994857788086 \n",
      "\n",
      "Iteration 11797, Loss: 36.160884857177734, L1: 10.609078407287598, L3: 25.55180549621582\n",
      "Current prediction:  61.18155288696289 \n",
      "\n",
      "Iteration 11798, Loss: 36.51186752319336, L1: 10.590729713439941, L3: 25.9211368560791\n",
      "Current prediction:  61.198814392089844 \n",
      "\n",
      "Iteration 11799, Loss: 37.342628479003906, L1: 10.579694747924805, L3: 26.76293182373047\n",
      "Current prediction:  61.21658706665039 \n",
      "\n",
      "Iteration 11800, Loss: 36.806640625, L1: 10.558233261108398, L3: 26.248409271240234\n",
      "Current prediction:  61.23468017578125 \n",
      "\n",
      "Iteration 11801, Loss: 37.23822021484375, L1: 10.539508819580078, L3: 26.69870948791504\n",
      "Current prediction:  61.253536224365234 \n",
      "\n",
      "Iteration 11802, Loss: 36.469825744628906, L1: 10.528812408447266, L3: 25.941011428833008\n",
      "Current prediction:  61.26744842529297 \n",
      "\n",
      "Iteration 11803, Loss: 36.34716033935547, L1: 10.519926071166992, L3: 25.827234268188477\n",
      "Current prediction:  61.28532409667969 \n",
      "\n",
      "Iteration 11804, Loss: 36.135658264160156, L1: 10.4927339553833, L3: 25.64292335510254\n",
      "Current prediction:  61.29937744140625 \n",
      "\n",
      "Iteration 11805, Loss: 36.33587646484375, L1: 10.502019882202148, L3: 25.83385467529297\n",
      "Current prediction:  61.3079948425293 \n",
      "\n",
      "Iteration 11806, Loss: 36.96613311767578, L1: 10.48573112487793, L3: 26.48040008544922\n",
      "Current prediction:  61.31505584716797 \n",
      "\n",
      "Iteration 11807, Loss: 37.75772476196289, L1: 10.468476295471191, L3: 27.289247512817383\n",
      "Current prediction:  61.31870651245117 \n",
      "\n",
      "Iteration 11808, Loss: 36.70792770385742, L1: 10.463728904724121, L3: 26.244199752807617\n",
      "Current prediction:  61.3168830871582 \n",
      "\n",
      "Iteration 11809, Loss: 35.892051696777344, L1: 10.468144416809082, L3: 25.423906326293945\n",
      "Current prediction:  61.31416702270508 \n",
      "\n",
      "Iteration 11810, Loss: 35.39532470703125, L1: 10.467674255371094, L3: 24.92765235900879\n",
      "Current prediction:  61.310543060302734 \n",
      "\n",
      "Iteration 11811, Loss: 36.68706512451172, L1: 10.471057891845703, L3: 26.21600914001465\n",
      "Current prediction:  61.310386657714844 \n",
      "\n",
      "Iteration 11812, Loss: 37.429420471191406, L1: 10.46774673461914, L3: 26.9616756439209\n",
      "Current prediction:  61.30399703979492 \n",
      "\n",
      "Iteration 11813, Loss: 36.45338439941406, L1: 10.47408390045166, L3: 25.97930145263672\n",
      "Current prediction:  61.29608917236328 \n",
      "\n",
      "Iteration 11814, Loss: 37.01033401489258, L1: 10.490659713745117, L3: 26.51967430114746\n",
      "Current prediction:  61.2839241027832 \n",
      "\n",
      "Iteration 11815, Loss: 36.0726432800293, L1: 10.491031646728516, L3: 25.58161163330078\n",
      "Current prediction:  61.2703742980957 \n",
      "\n",
      "Iteration 11816, Loss: 36.13250732421875, L1: 10.514961242675781, L3: 25.617544174194336\n",
      "Current prediction:  61.25844955444336 \n",
      "\n",
      "Iteration 11817, Loss: 36.50539779663086, L1: 10.522805213928223, L3: 25.982593536376953\n",
      "Current prediction:  61.24557113647461 \n",
      "\n",
      "Iteration 11818, Loss: 36.1084098815918, L1: 10.524584770202637, L3: 25.583824157714844\n",
      "Current prediction:  61.237300872802734 \n",
      "\n",
      "Iteration 11819, Loss: 37.14230728149414, L1: 10.533577919006348, L3: 26.60873031616211\n",
      "Current prediction:  61.23054885864258 \n",
      "\n",
      "Iteration 11820, Loss: 37.89628982543945, L1: 10.545855522155762, L3: 27.350435256958008\n",
      "Current prediction:  61.2210807800293 \n",
      "\n",
      "Iteration 11821, Loss: 35.82299041748047, L1: 10.563929557800293, L3: 25.259061813354492\n",
      "Current prediction:  61.217227935791016 \n",
      "\n",
      "Iteration 11822, Loss: 35.74295425415039, L1: 10.561911582946777, L3: 25.181041717529297\n",
      "Current prediction:  61.21905517578125 \n",
      "\n",
      "Iteration 11823, Loss: 36.62698745727539, L1: 10.5507230758667, L3: 26.076263427734375\n",
      "Current prediction:  61.21885681152344 \n",
      "\n",
      "Iteration 11824, Loss: 37.51630401611328, L1: 10.55516242980957, L3: 26.961139678955078\n",
      "Current prediction:  61.22142791748047 \n",
      "\n",
      "Iteration 11825, Loss: 37.39752960205078, L1: 10.540547370910645, L3: 26.85698127746582\n",
      "Current prediction:  61.22731018066406 \n",
      "\n",
      "Iteration 11826, Loss: 36.913421630859375, L1: 10.566515922546387, L3: 26.346906661987305\n",
      "Current prediction:  61.234310150146484 \n",
      "\n",
      "Iteration 11827, Loss: 36.28223419189453, L1: 10.551287651062012, L3: 25.730947494506836\n",
      "Current prediction:  61.23823547363281 \n",
      "\n",
      "Iteration 11828, Loss: 35.97038269042969, L1: 10.550952911376953, L3: 25.419429779052734\n",
      "Current prediction:  61.239349365234375 \n",
      "\n",
      "Iteration 11829, Loss: 36.528282165527344, L1: 10.544865608215332, L3: 25.983417510986328\n",
      "Current prediction:  61.24170684814453 \n",
      "\n",
      "Iteration 11830, Loss: 36.302452087402344, L1: 10.541791915893555, L3: 25.76066017150879\n",
      "Current prediction:  61.24753189086914 \n",
      "\n",
      "Iteration 11831, Loss: 36.85333251953125, L1: 10.548073768615723, L3: 26.305259704589844\n",
      "Current prediction:  61.25442123413086 \n",
      "\n",
      "Iteration 11832, Loss: 36.2641487121582, L1: 10.524823188781738, L3: 25.73932456970215\n",
      "Current prediction:  61.2548828125 \n",
      "\n",
      "Iteration 11833, Loss: 37.64920425415039, L1: 10.520197868347168, L3: 27.129005432128906\n",
      "Current prediction:  61.250892639160156 \n",
      "\n",
      "Iteration 11834, Loss: 36.009422302246094, L1: 10.510576248168945, L3: 25.498844146728516\n",
      "Current prediction:  61.24534606933594 \n",
      "\n",
      "Iteration 11835, Loss: 36.41114044189453, L1: 10.546579360961914, L3: 25.864559173583984\n",
      "Current prediction:  61.24338150024414 \n",
      "\n",
      "Iteration 11836, Loss: 37.04875946044922, L1: 10.533042907714844, L3: 26.515718460083008\n",
      "Current prediction:  61.24113082885742 \n",
      "\n",
      "Iteration 11837, Loss: 36.46870422363281, L1: 10.550468444824219, L3: 25.918235778808594\n",
      "Current prediction:  61.23823928833008 \n",
      "\n",
      "Iteration 11838, Loss: 36.437870025634766, L1: 10.533525466918945, L3: 25.90434455871582\n",
      "Current prediction:  61.23838806152344 \n",
      "\n",
      "Iteration 11839, Loss: 37.05128860473633, L1: 10.534623146057129, L3: 26.516666412353516\n",
      "Current prediction:  61.23949432373047 \n",
      "\n",
      "Iteration 11840, Loss: 37.00090408325195, L1: 10.539070129394531, L3: 26.461833953857422\n",
      "Current prediction:  61.23922348022461 \n",
      "\n",
      "Iteration 11841, Loss: 36.55355453491211, L1: 10.53088092803955, L3: 26.022672653198242\n",
      "Current prediction:  61.23770523071289 \n",
      "\n",
      "Iteration 11842, Loss: 36.50321960449219, L1: 10.556294441223145, L3: 25.946924209594727\n",
      "Current prediction:  61.23549270629883 \n",
      "\n",
      "Iteration 11843, Loss: 36.933502197265625, L1: 10.524862289428711, L3: 26.408639907836914\n",
      "Current prediction:  61.234161376953125 \n",
      "\n",
      "Iteration 11844, Loss: 35.974483489990234, L1: 10.524721145629883, L3: 25.44976234436035\n",
      "Current prediction:  61.23566436767578 \n",
      "\n",
      "Iteration 11845, Loss: 34.85505294799805, L1: 10.549427032470703, L3: 24.305625915527344\n",
      "Current prediction:  61.240081787109375 \n",
      "\n",
      "Iteration 11846, Loss: 36.20806884765625, L1: 10.541369438171387, L3: 25.666698455810547\n",
      "Current prediction:  61.24603271484375 \n",
      "\n",
      "Iteration 11847, Loss: 36.61528778076172, L1: 10.527795791625977, L3: 26.087493896484375\n",
      "Current prediction:  61.25188446044922 \n",
      "\n",
      "Iteration 11848, Loss: 36.71138000488281, L1: 10.549826622009277, L3: 26.16155242919922\n",
      "Current prediction:  61.25809097290039 \n",
      "\n",
      "Iteration 11849, Loss: 35.975894927978516, L1: 10.567706108093262, L3: 25.408187866210938\n",
      "Current prediction:  61.262298583984375 \n",
      "\n",
      "Iteration 11850, Loss: 34.950469970703125, L1: 10.515045166015625, L3: 24.435426712036133\n",
      "Current prediction:  61.26841735839844 \n",
      "\n",
      "Iteration 11851, Loss: 36.93922805786133, L1: 10.513178825378418, L3: 26.426050186157227\n",
      "Current prediction:  61.271759033203125 \n",
      "\n",
      "Iteration 11852, Loss: 36.47815704345703, L1: 10.508739471435547, L3: 25.969419479370117\n",
      "Current prediction:  61.27015686035156 \n",
      "\n",
      "Iteration 11853, Loss: 36.33753967285156, L1: 10.503397941589355, L3: 25.83414077758789\n",
      "Current prediction:  61.267295837402344 \n",
      "\n",
      "Iteration 11854, Loss: 36.34652328491211, L1: 10.516758918762207, L3: 25.829763412475586\n",
      "Current prediction:  61.26339340209961 \n",
      "\n",
      "Iteration 11855, Loss: 37.08894348144531, L1: 10.521862030029297, L3: 26.567081451416016\n",
      "Current prediction:  61.2601432800293 \n",
      "\n",
      "Iteration 11856, Loss: 35.90556716918945, L1: 10.51879596710205, L3: 25.38677215576172\n",
      "Current prediction:  61.259132385253906 \n",
      "\n",
      "Iteration 11857, Loss: 35.929664611816406, L1: 10.52235221862793, L3: 25.407310485839844\n",
      "Current prediction:  61.2633171081543 \n",
      "\n",
      "Iteration 11858, Loss: 36.122344970703125, L1: 10.51697826385498, L3: 25.605365753173828\n",
      "Current prediction:  61.266448974609375 \n",
      "\n",
      "Iteration 11859, Loss: 37.039485931396484, L1: 10.514470100402832, L3: 26.525014877319336\n",
      "Current prediction:  61.265445709228516 \n",
      "\n",
      "Iteration 11860, Loss: 36.4046630859375, L1: 10.51546573638916, L3: 25.889196395874023\n",
      "Current prediction:  61.2646369934082 \n",
      "\n",
      "Iteration 11861, Loss: 37.58602523803711, L1: 10.509132385253906, L3: 27.076892852783203\n",
      "Current prediction:  61.26271438598633 \n",
      "\n",
      "Iteration 11862, Loss: 36.36859130859375, L1: 10.523948669433594, L3: 25.844642639160156\n",
      "Current prediction:  61.2594108581543 \n",
      "\n",
      "Iteration 11863, Loss: 36.819393157958984, L1: 10.513331413269043, L3: 26.306062698364258\n",
      "Current prediction:  61.25935363769531 \n",
      "\n",
      "Iteration 11864, Loss: 36.325157165527344, L1: 10.505038261413574, L3: 25.820117950439453\n",
      "Current prediction:  61.25464630126953 \n",
      "\n",
      "Iteration 11865, Loss: 35.57468032836914, L1: 10.519364356994629, L3: 25.055316925048828\n",
      "Current prediction:  61.247745513916016 \n",
      "\n",
      "Iteration 11866, Loss: 37.25560760498047, L1: 10.538800239562988, L3: 26.716806411743164\n",
      "Current prediction:  61.24265670776367 \n",
      "\n",
      "Iteration 11867, Loss: 36.49565505981445, L1: 10.531113624572754, L3: 25.964540481567383\n",
      "Current prediction:  61.23735427856445 \n",
      "\n",
      "Iteration 11868, Loss: 36.91015625, L1: 10.527546882629395, L3: 26.382610321044922\n",
      "Current prediction:  61.23125076293945 \n",
      "\n",
      "Iteration 11869, Loss: 35.996849060058594, L1: 10.534098625183105, L3: 25.462749481201172\n",
      "Current prediction:  61.22669982910156 \n",
      "\n",
      "Iteration 11870, Loss: 35.43109893798828, L1: 10.534346580505371, L3: 24.896753311157227\n",
      "Current prediction:  61.22077941894531 \n",
      "\n",
      "Iteration 11871, Loss: 37.1768798828125, L1: 10.548709869384766, L3: 26.6281681060791\n",
      "Current prediction:  61.21598434448242 \n",
      "\n",
      "Iteration 11872, Loss: 36.01813507080078, L1: 10.56761360168457, L3: 25.450519561767578\n",
      "Current prediction:  61.21493911743164 \n",
      "\n",
      "Iteration 11873, Loss: 37.2382926940918, L1: 10.542724609375, L3: 26.695568084716797\n",
      "Current prediction:  61.21434783935547 \n",
      "\n",
      "Iteration 11874, Loss: 37.55744552612305, L1: 10.565433502197266, L3: 26.99201202392578\n",
      "Current prediction:  61.21341323852539 \n",
      "\n",
      "Iteration 11875, Loss: 36.83210754394531, L1: 10.574832916259766, L3: 26.257274627685547\n",
      "Current prediction:  61.21237564086914 \n",
      "\n",
      "Iteration 11876, Loss: 37.220420837402344, L1: 10.557853698730469, L3: 26.662569046020508\n",
      "Current prediction:  61.21391296386719 \n",
      "\n",
      "Iteration 11877, Loss: 36.468719482421875, L1: 10.546093940734863, L3: 25.922624588012695\n",
      "Current prediction:  61.22030258178711 \n",
      "\n",
      "Iteration 11878, Loss: 37.301273345947266, L1: 10.543728828430176, L3: 26.757543563842773\n",
      "Current prediction:  61.226356506347656 \n",
      "\n",
      "Iteration 11879, Loss: 36.667503356933594, L1: 10.56517505645752, L3: 26.10232925415039\n",
      "Current prediction:  61.23297882080078 \n",
      "\n",
      "Iteration 11880, Loss: 36.49248504638672, L1: 10.541617393493652, L3: 25.95086669921875\n",
      "Current prediction:  61.232177734375 \n",
      "\n",
      "Iteration 11881, Loss: 37.110469818115234, L1: 10.540602684020996, L3: 26.569868087768555\n",
      "Current prediction:  61.2311897277832 \n",
      "\n",
      "Iteration 11882, Loss: 36.05052185058594, L1: 10.564248085021973, L3: 25.48627471923828\n",
      "Current prediction:  61.22772216796875 \n",
      "\n",
      "Iteration 11883, Loss: 36.47993087768555, L1: 10.53283405303955, L3: 25.94709587097168\n",
      "Current prediction:  61.22631072998047 \n",
      "\n",
      "Iteration 11884, Loss: 35.60679244995117, L1: 10.539396286010742, L3: 25.06739616394043\n",
      "Current prediction:  61.22706985473633 \n",
      "\n",
      "Iteration 11885, Loss: 36.870426177978516, L1: 10.537848472595215, L3: 26.332578659057617\n",
      "Current prediction:  61.22681427001953 \n",
      "\n",
      "Iteration 11886, Loss: 36.096736907958984, L1: 10.545313835144043, L3: 25.551424026489258\n",
      "Current prediction:  61.227783203125 \n",
      "\n",
      "Iteration 11887, Loss: 35.930938720703125, L1: 10.54031753540039, L3: 25.390623092651367\n",
      "Current prediction:  61.22749710083008 \n",
      "\n",
      "Iteration 11888, Loss: 36.766563415527344, L1: 10.529964447021484, L3: 26.236597061157227\n",
      "Current prediction:  61.22520446777344 \n",
      "\n",
      "Iteration 11889, Loss: 36.926448822021484, L1: 10.547650337219238, L3: 26.378799438476562\n",
      "Current prediction:  61.2237663269043 \n",
      "\n",
      "Iteration 11890, Loss: 36.45191955566406, L1: 10.536237716674805, L3: 25.915681838989258\n",
      "Current prediction:  61.22351837158203 \n",
      "\n",
      "Iteration 11891, Loss: 37.05717468261719, L1: 10.551952362060547, L3: 26.505224227905273\n",
      "Current prediction:  61.22148132324219 \n",
      "\n",
      "Iteration 11892, Loss: 36.56787872314453, L1: 10.553688049316406, L3: 26.014188766479492\n",
      "Current prediction:  61.2210578918457 \n",
      "\n",
      "Iteration 11893, Loss: 36.63128662109375, L1: 10.529744148254395, L3: 26.101543426513672\n",
      "Current prediction:  61.21771240234375 \n",
      "\n",
      "Iteration 11894, Loss: 37.52260971069336, L1: 10.53773021697998, L3: 26.984878540039062\n",
      "Current prediction:  61.21464157104492 \n",
      "\n",
      "Iteration 11895, Loss: 36.048492431640625, L1: 10.546431541442871, L3: 25.502059936523438\n",
      "Current prediction:  61.2133674621582 \n",
      "\n",
      "Iteration 11896, Loss: 37.07111358642578, L1: 10.554007530212402, L3: 26.517107009887695\n",
      "Current prediction:  61.214210510253906 \n",
      "\n",
      "Iteration 11897, Loss: 36.53007125854492, L1: 10.544781684875488, L3: 25.98529052734375\n",
      "Current prediction:  61.21736526489258 \n",
      "\n",
      "Iteration 11898, Loss: 36.068016052246094, L1: 10.542680740356445, L3: 25.52533531188965\n",
      "Current prediction:  61.22306442260742 \n",
      "\n",
      "Iteration 11899, Loss: 36.655189514160156, L1: 10.543254852294922, L3: 26.111934661865234\n",
      "Current prediction:  61.22919464111328 \n",
      "\n",
      "Iteration 11900, Loss: 38.5357666015625, L1: 11.932064056396484, L3: 26.603700637817383\n",
      "Current prediction:  61.23117446899414 \n",
      "\n",
      "Iteration 11901, Loss: 37.3330078125, L1: 12.66943073272705, L3: 24.663578033447266\n",
      "Current prediction:  61.22898864746094 \n",
      "\n",
      "Iteration 11902, Loss: 38.25320816040039, L1: 12.043060302734375, L3: 26.210147857666016\n",
      "Current prediction:  61.23112869262695 \n",
      "\n",
      "Iteration 11903, Loss: 38.19220733642578, L1: 11.698013305664062, L3: 26.49419403076172\n",
      "Current prediction:  61.23979568481445 \n",
      "\n",
      "Iteration 11904, Loss: 37.00253677368164, L1: 11.229818344116211, L3: 25.77271842956543\n",
      "Current prediction:  61.260719299316406 \n",
      "\n",
      "Iteration 11905, Loss: 37.81256866455078, L1: 11.082523345947266, L3: 26.73004722595215\n",
      "Current prediction:  60.63304901123047 \n",
      "\n",
      "Iteration 11906, Loss: 38.27058029174805, L1: 11.300732612609863, L3: 26.969846725463867\n",
      "Current prediction:  59.547821044921875 \n",
      "\n",
      "Iteration 11907, Loss: 37.86478042602539, L1: 11.47974681854248, L3: 26.385034561157227\n",
      "Current prediction:  59.63579559326172 \n",
      "\n",
      "Iteration 11908, Loss: 39.1110725402832, L1: 11.706478118896484, L3: 27.40459442138672\n",
      "Current prediction:  59.74456024169922 \n",
      "\n",
      "Iteration 11909, Loss: 39.22602844238281, L1: 12.046638488769531, L3: 27.17938804626465\n",
      "Current prediction:  59.87532424926758 \n",
      "\n",
      "Iteration 11910, Loss: 40.351348876953125, L1: 12.198003768920898, L3: 28.153343200683594\n",
      "Current prediction:  60.020233154296875 \n",
      "\n",
      "Iteration 11911, Loss: 38.66550827026367, L1: 12.531923294067383, L3: 26.13358497619629\n",
      "Current prediction:  60.175445556640625 \n",
      "\n",
      "Iteration 11912, Loss: 38.75886154174805, L1: 12.426924705505371, L3: 26.331937789916992\n",
      "Current prediction:  60.33735275268555 \n",
      "\n",
      "Iteration 11913, Loss: 39.206119537353516, L1: 12.39293384552002, L3: 26.813186645507812\n",
      "Current prediction:  60.50264358520508 \n",
      "\n",
      "Iteration 11914, Loss: 37.39588165283203, L1: 12.139973640441895, L3: 25.25590705871582\n",
      "Current prediction:  60.67156219482422 \n",
      "\n",
      "Iteration 11915, Loss: 37.9627799987793, L1: 12.494154930114746, L3: 25.468624114990234\n",
      "Current prediction:  60.83766555786133 \n",
      "\n",
      "Iteration 11916, Loss: 38.16522216796875, L1: 11.743917465209961, L3: 26.421302795410156\n",
      "Current prediction:  60.99641799926758 \n",
      "\n",
      "Iteration 11917, Loss: 37.2158088684082, L1: 11.550261497497559, L3: 25.665546417236328\n",
      "Current prediction:  61.143096923828125 \n",
      "\n",
      "Iteration 11918, Loss: 36.2120361328125, L1: 11.193504333496094, L3: 25.018531799316406\n",
      "Current prediction:  61.27753448486328 \n",
      "\n",
      "Iteration 11919, Loss: 35.86499786376953, L1: 10.510933876037598, L3: 25.35406494140625\n",
      "Current prediction:  61.4010009765625 \n",
      "\n",
      "Iteration 11920, Loss: 37.49934768676758, L1: 10.5281400680542, L3: 26.971208572387695\n",
      "Current prediction:  61.50555419921875 \n",
      "\n",
      "Iteration 11921, Loss: 37.352970123291016, L1: 10.317169189453125, L3: 27.03580093383789\n",
      "Current prediction:  61.59060287475586 \n",
      "\n",
      "Iteration 11922, Loss: 37.33140563964844, L1: 10.236364364624023, L3: 27.095043182373047\n",
      "Current prediction:  61.6572380065918 \n",
      "\n",
      "Iteration 11923, Loss: 36.695404052734375, L1: 10.194490432739258, L3: 26.500911712646484\n",
      "Current prediction:  61.70311737060547 \n",
      "\n",
      "Iteration 11924, Loss: 36.13972473144531, L1: 10.157439231872559, L3: 25.982284545898438\n",
      "Current prediction:  61.72983932495117 \n",
      "\n",
      "Iteration 11925, Loss: 36.337982177734375, L1: 10.114076614379883, L3: 26.223905563354492\n",
      "Current prediction:  61.738433837890625 \n",
      "\n",
      "Iteration 11926, Loss: 36.5855712890625, L1: 10.101530075073242, L3: 26.484041213989258\n",
      "Current prediction:  61.73359298706055 \n",
      "\n",
      "Iteration 11927, Loss: 37.396018981933594, L1: 10.105386734008789, L3: 27.290630340576172\n",
      "Current prediction:  61.71485900878906 \n",
      "\n",
      "Iteration 11928, Loss: 37.70707321166992, L1: 10.152365684509277, L3: 27.554706573486328\n",
      "Current prediction:  61.686344146728516 \n",
      "\n",
      "Iteration 11929, Loss: 36.85962677001953, L1: 10.172323226928711, L3: 26.687305450439453\n",
      "Current prediction:  61.65018081665039 \n",
      "\n",
      "Iteration 11930, Loss: 35.91487503051758, L1: 10.177000999450684, L3: 25.737873077392578\n",
      "Current prediction:  61.607337951660156 \n",
      "\n",
      "Iteration 11931, Loss: 36.890724182128906, L1: 10.230695724487305, L3: 26.660030364990234\n",
      "Current prediction:  61.557579040527344 \n",
      "\n",
      "Iteration 11932, Loss: 36.17100143432617, L1: 10.279619216918945, L3: 25.891382217407227\n",
      "Current prediction:  61.50809097290039 \n",
      "\n",
      "Iteration 11933, Loss: 35.966766357421875, L1: 10.289806365966797, L3: 25.67696189880371\n",
      "Current prediction:  61.460472106933594 \n",
      "\n",
      "Iteration 11934, Loss: 36.76472091674805, L1: 10.321062088012695, L3: 26.44365882873535\n",
      "Current prediction:  61.4146842956543 \n",
      "\n",
      "Iteration 11935, Loss: 36.66306686401367, L1: 10.41848087310791, L3: 26.244586944580078\n",
      "Current prediction:  61.366912841796875 \n",
      "\n",
      "Iteration 11936, Loss: 36.97532653808594, L1: 10.439396858215332, L3: 26.535930633544922\n",
      "Current prediction:  61.32038879394531 \n",
      "\n",
      "Iteration 11937, Loss: 37.27302551269531, L1: 10.455571174621582, L3: 26.817453384399414\n",
      "Current prediction:  61.273658752441406 \n",
      "\n",
      "Iteration 11938, Loss: 36.2891845703125, L1: 10.505744934082031, L3: 25.7834415435791\n",
      "Current prediction:  61.2318115234375 \n",
      "\n",
      "Iteration 11939, Loss: 36.903160095214844, L1: 10.547091484069824, L3: 26.356069564819336\n",
      "Current prediction:  61.19504928588867 \n",
      "\n",
      "Iteration 11940, Loss: 36.758544921875, L1: 10.585840225219727, L3: 26.172704696655273\n",
      "Current prediction:  61.161834716796875 \n",
      "\n",
      "Iteration 11941, Loss: 37.72106170654297, L1: 10.58044147491455, L3: 27.140621185302734\n",
      "Current prediction:  61.13422775268555 \n",
      "\n",
      "Iteration 11942, Loss: 36.77946472167969, L1: 10.6240873336792, L3: 26.155378341674805\n",
      "Current prediction:  61.11629104614258 \n",
      "\n",
      "Iteration 11943, Loss: 35.92898178100586, L1: 10.630960464477539, L3: 25.29802131652832\n",
      "Current prediction:  61.1046142578125 \n",
      "\n",
      "Iteration 11944, Loss: 36.75358963012695, L1: 10.651559829711914, L3: 26.10202980041504\n",
      "Current prediction:  61.096431732177734 \n",
      "\n",
      "Iteration 11945, Loss: 36.7987060546875, L1: 10.646442413330078, L3: 26.152265548706055\n",
      "Current prediction:  61.09122848510742 \n",
      "\n",
      "Iteration 11946, Loss: 36.86426544189453, L1: 10.661872863769531, L3: 26.202394485473633\n",
      "Current prediction:  61.094322204589844 \n",
      "\n",
      "Iteration 11947, Loss: 35.70413589477539, L1: 10.633209228515625, L3: 25.070926666259766\n",
      "Current prediction:  61.10388946533203 \n",
      "\n",
      "Iteration 11948, Loss: 37.55827331542969, L1: 10.652609825134277, L3: 26.905662536621094\n",
      "Current prediction:  61.11396408081055 \n",
      "\n",
      "Iteration 11949, Loss: 36.346153259277344, L1: 10.654741287231445, L3: 25.691410064697266\n",
      "Current prediction:  61.12875747680664 \n",
      "\n",
      "Iteration 11950, Loss: 36.101768493652344, L1: 10.617597579956055, L3: 25.48417091369629\n",
      "Current prediction:  61.14186096191406 \n",
      "\n",
      "Iteration 11951, Loss: 36.677284240722656, L1: 10.59701919555664, L3: 26.08026695251465\n",
      "Current prediction:  61.15477752685547 \n",
      "\n",
      "Iteration 11952, Loss: 36.97050476074219, L1: 10.608112335205078, L3: 26.362390518188477\n",
      "Current prediction:  61.16707229614258 \n",
      "\n",
      "Iteration 11953, Loss: 37.101287841796875, L1: 10.606512069702148, L3: 26.49477767944336\n",
      "Current prediction:  61.18052673339844 \n",
      "\n",
      "Iteration 11954, Loss: 36.65424346923828, L1: 10.56485652923584, L3: 26.089387893676758\n",
      "Current prediction:  61.19451904296875 \n",
      "\n",
      "Iteration 11955, Loss: 36.22035217285156, L1: 10.565133094787598, L3: 25.65522003173828\n",
      "Current prediction:  61.20608139038086 \n",
      "\n",
      "Iteration 11956, Loss: 36.66426086425781, L1: 10.562559127807617, L3: 26.101703643798828\n",
      "Current prediction:  61.21818542480469 \n",
      "\n",
      "Iteration 11957, Loss: 35.6817741394043, L1: 10.540657043457031, L3: 25.141117095947266\n",
      "Current prediction:  61.22933578491211 \n",
      "\n",
      "Iteration 11958, Loss: 36.16823959350586, L1: 10.536208152770996, L3: 25.632030487060547\n",
      "Current prediction:  61.2392692565918 \n",
      "\n",
      "Iteration 11959, Loss: 36.568748474121094, L1: 10.539764404296875, L3: 26.028982162475586\n",
      "Current prediction:  61.248695373535156 \n",
      "\n",
      "Iteration 11960, Loss: 37.18412780761719, L1: 10.525228500366211, L3: 26.658899307250977\n",
      "Current prediction:  61.25477981567383 \n",
      "\n",
      "Iteration 11961, Loss: 36.23943328857422, L1: 10.513324737548828, L3: 25.72610855102539\n",
      "Current prediction:  61.26247024536133 \n",
      "\n",
      "Iteration 11962, Loss: 36.482826232910156, L1: 10.522408485412598, L3: 25.960418701171875\n",
      "Current prediction:  61.269371032714844 \n",
      "\n",
      "Iteration 11963, Loss: 36.31328201293945, L1: 10.500306129455566, L3: 25.81297492980957\n",
      "Current prediction:  61.27528762817383 \n",
      "\n",
      "Iteration 11964, Loss: 37.237144470214844, L1: 10.4959716796875, L3: 26.741174697875977\n",
      "Current prediction:  61.27737045288086 \n",
      "\n",
      "Iteration 11965, Loss: 36.03472137451172, L1: 10.496038436889648, L3: 25.538684844970703\n",
      "Current prediction:  61.28115463256836 \n",
      "\n",
      "Iteration 11966, Loss: 36.61931610107422, L1: 10.498693466186523, L3: 26.120624542236328\n",
      "Current prediction:  61.284217834472656 \n",
      "\n",
      "Iteration 11967, Loss: 37.92736053466797, L1: 10.475372314453125, L3: 27.451990127563477\n",
      "Current prediction:  61.281822204589844 \n",
      "\n",
      "Iteration 11968, Loss: 36.880794525146484, L1: 10.478119850158691, L3: 26.40267562866211\n",
      "Current prediction:  61.27963638305664 \n",
      "\n",
      "Iteration 11969, Loss: 36.59458541870117, L1: 10.474068641662598, L3: 26.12051773071289\n",
      "Current prediction:  61.27699661254883 \n",
      "\n",
      "Iteration 11970, Loss: 34.90738296508789, L1: 10.50096607208252, L3: 24.406417846679688\n",
      "Current prediction:  61.275428771972656 \n",
      "\n",
      "Iteration 11971, Loss: 35.93562316894531, L1: 10.504383087158203, L3: 25.43124008178711\n",
      "Current prediction:  61.27445983886719 \n",
      "\n",
      "Iteration 11972, Loss: 36.861019134521484, L1: 10.513419151306152, L3: 26.34760093688965\n",
      "Current prediction:  61.27400588989258 \n",
      "\n",
      "Iteration 11973, Loss: 37.85944366455078, L1: 10.516844749450684, L3: 27.342599868774414\n",
      "Current prediction:  61.27336883544922 \n",
      "\n",
      "Iteration 11974, Loss: 36.36882019042969, L1: 10.502145767211914, L3: 25.86667251586914\n",
      "Current prediction:  61.269126892089844 \n",
      "\n",
      "Iteration 11975, Loss: 35.89923095703125, L1: 10.505364418029785, L3: 25.39386749267578\n",
      "Current prediction:  61.263671875 \n",
      "\n",
      "Iteration 11976, Loss: 36.96271514892578, L1: 10.503923416137695, L3: 26.45879364013672\n",
      "Current prediction:  61.2591667175293 \n",
      "\n",
      "Iteration 11977, Loss: 36.068023681640625, L1: 10.514555931091309, L3: 25.553468704223633\n",
      "Current prediction:  61.25234603881836 \n",
      "\n",
      "Iteration 11978, Loss: 36.7829704284668, L1: 10.523331642150879, L3: 26.2596378326416\n",
      "Current prediction:  61.24536895751953 \n",
      "\n",
      "Iteration 11979, Loss: 36.75615310668945, L1: 10.52164077758789, L3: 26.234512329101562\n",
      "Current prediction:  61.237152099609375 \n",
      "\n",
      "Iteration 11980, Loss: 36.60828399658203, L1: 10.53093433380127, L3: 26.077350616455078\n",
      "Current prediction:  61.229759216308594 \n",
      "\n",
      "Iteration 11981, Loss: 36.55477523803711, L1: 10.527904510498047, L3: 26.026870727539062\n",
      "Current prediction:  61.222389221191406 \n",
      "\n",
      "Iteration 11982, Loss: 36.17013931274414, L1: 10.558821678161621, L3: 25.611318588256836\n",
      "Current prediction:  61.217872619628906 \n",
      "\n",
      "Iteration 11983, Loss: 36.64690017700195, L1: 10.549357414245605, L3: 26.097543716430664\n",
      "Current prediction:  61.21549606323242 \n",
      "\n",
      "Iteration 11984, Loss: 37.127445220947266, L1: 10.54883098602295, L3: 26.578615188598633\n",
      "Current prediction:  61.209510803222656 \n",
      "\n",
      "Iteration 11985, Loss: 37.06365966796875, L1: 10.558262825012207, L3: 26.50539779663086\n",
      "Current prediction:  61.202571868896484 \n",
      "\n",
      "Iteration 11986, Loss: 35.75983428955078, L1: 10.569856643676758, L3: 25.189977645874023\n",
      "Current prediction:  61.1981086730957 \n",
      "\n",
      "Iteration 11987, Loss: 36.30673599243164, L1: 10.562581062316895, L3: 25.74415397644043\n",
      "Current prediction:  61.19806671142578 \n",
      "\n",
      "Iteration 11988, Loss: 37.235084533691406, L1: 10.567776679992676, L3: 26.667308807373047\n",
      "Current prediction:  61.200462341308594 \n",
      "\n",
      "Iteration 11989, Loss: 37.34087371826172, L1: 10.566229820251465, L3: 26.774642944335938\n",
      "Current prediction:  61.203643798828125 \n",
      "\n",
      "Iteration 11990, Loss: 37.21097946166992, L1: 10.57677936553955, L3: 26.634199142456055\n",
      "Current prediction:  61.20370864868164 \n",
      "\n",
      "Iteration 11991, Loss: 36.056209564208984, L1: 10.56047534942627, L3: 25.49573516845703\n",
      "Current prediction:  61.20263671875 \n",
      "\n",
      "Iteration 11992, Loss: 36.14848709106445, L1: 10.577272415161133, L3: 25.57121467590332\n",
      "Current prediction:  61.206993103027344 \n",
      "\n",
      "Iteration 11993, Loss: 38.0653076171875, L1: 10.569265365600586, L3: 27.49604034423828\n",
      "Current prediction:  61.21080780029297 \n",
      "\n",
      "Iteration 11994, Loss: 36.9259147644043, L1: 10.565702438354492, L3: 26.360212326049805\n",
      "Current prediction:  61.21468734741211 \n",
      "\n",
      "Iteration 11995, Loss: 37.240814208984375, L1: 10.555376052856445, L3: 26.685440063476562\n",
      "Current prediction:  61.21796417236328 \n",
      "\n",
      "Iteration 11996, Loss: 37.013145446777344, L1: 10.559773445129395, L3: 26.453371047973633\n",
      "Current prediction:  61.22163009643555 \n",
      "\n",
      "Iteration 11997, Loss: 36.005165100097656, L1: 10.560946464538574, L3: 25.444217681884766\n",
      "Current prediction:  61.225311279296875 \n",
      "\n",
      "Iteration 11998, Loss: 36.009952545166016, L1: 10.558266639709473, L3: 25.45168685913086\n",
      "Current prediction:  61.2307243347168 \n",
      "\n",
      "â†³ LR reduced to 2.5e-04 at iteration 12000 \n",
      "\n",
      "Iteration 11999, Loss: 36.406349182128906, L1: 10.545187950134277, L3: 25.861160278320312\n",
      "Current prediction:  61.235504150390625 \n",
      "\n",
      "Iteration 12000, Loss: 35.943756103515625, L1: 10.528019905090332, L3: 25.415735244750977\n",
      "Current prediction:  61.24168014526367 \n",
      "\n",
      "Iteration 12001, Loss: 36.49943542480469, L1: 10.5303955078125, L3: 25.969039916992188\n",
      "Current prediction:  61.24575424194336 \n",
      "\n",
      "Iteration 12002, Loss: 37.26896286010742, L1: 10.534566879272461, L3: 26.73439598083496\n",
      "Current prediction:  61.24869918823242 \n",
      "\n",
      "Iteration 12003, Loss: 36.67592239379883, L1: 10.515440940856934, L3: 26.160480499267578\n",
      "Current prediction:  61.24907302856445 \n",
      "\n",
      "Iteration 12004, Loss: 37.90824890136719, L1: 10.530296325683594, L3: 27.377952575683594\n",
      "Current prediction:  61.2495002746582 \n",
      "\n",
      "Iteration 12005, Loss: 36.50236511230469, L1: 10.516067504882812, L3: 25.986299514770508\n",
      "Current prediction:  61.249427795410156 \n",
      "\n",
      "Iteration 12006, Loss: 36.592323303222656, L1: 10.522022247314453, L3: 26.070302963256836\n",
      "Current prediction:  61.25053787231445 \n",
      "\n",
      "Iteration 12007, Loss: 36.42342758178711, L1: 10.520343780517578, L3: 25.90308380126953\n",
      "Current prediction:  61.25180435180664 \n",
      "\n",
      "Iteration 12008, Loss: 36.91532897949219, L1: 10.527400016784668, L3: 26.387928009033203\n",
      "Current prediction:  61.2519645690918 \n",
      "\n",
      "Iteration 12009, Loss: 37.23442840576172, L1: 10.517487525939941, L3: 26.71693992614746\n",
      "Current prediction:  61.251869201660156 \n",
      "\n",
      "Iteration 12010, Loss: 36.553123474121094, L1: 10.531203269958496, L3: 26.02191925048828\n",
      "Current prediction:  61.251033782958984 \n",
      "\n",
      "Iteration 12011, Loss: 37.19048309326172, L1: 10.520575523376465, L3: 26.669906616210938\n",
      "Current prediction:  61.250999450683594 \n",
      "\n",
      "Iteration 12012, Loss: 35.95476150512695, L1: 10.524117469787598, L3: 25.43064308166504\n",
      "Current prediction:  61.25389862060547 \n",
      "\n",
      "Iteration 12013, Loss: 36.24679183959961, L1: 10.529945373535156, L3: 25.716846466064453\n",
      "Current prediction:  61.255455017089844 \n",
      "\n",
      "Iteration 12014, Loss: 36.4030647277832, L1: 10.516341209411621, L3: 25.886722564697266\n",
      "Current prediction:  61.25547790527344 \n",
      "\n",
      "Iteration 12015, Loss: 36.574623107910156, L1: 10.522990226745605, L3: 26.051633834838867\n",
      "Current prediction:  61.25864028930664 \n",
      "\n",
      "Iteration 12016, Loss: 37.035919189453125, L1: 10.50814437866211, L3: 26.527774810791016\n",
      "Current prediction:  61.257320404052734 \n",
      "\n",
      "Iteration 12017, Loss: 35.57966995239258, L1: 10.532626152038574, L3: 25.04704475402832\n",
      "Current prediction:  61.25415802001953 \n",
      "\n",
      "Iteration 12018, Loss: 36.677711486816406, L1: 10.506582260131836, L3: 26.171131134033203\n",
      "Current prediction:  61.24991226196289 \n",
      "\n",
      "Iteration 12019, Loss: 36.08115768432617, L1: 10.528672218322754, L3: 25.552486419677734\n",
      "Current prediction:  61.24339294433594 \n",
      "\n",
      "Iteration 12020, Loss: 36.640892028808594, L1: 10.525979995727539, L3: 26.114912033081055\n",
      "Current prediction:  61.23708724975586 \n",
      "\n",
      "Iteration 12021, Loss: 36.69377899169922, L1: 10.530611991882324, L3: 26.16316795349121\n",
      "Current prediction:  61.23363494873047 \n",
      "\n",
      "Iteration 12022, Loss: 37.45820236206055, L1: 10.54984188079834, L3: 26.90835952758789\n",
      "Current prediction:  61.231109619140625 \n",
      "\n",
      "Iteration 12023, Loss: 36.922908782958984, L1: 10.534415245056152, L3: 26.38849449157715\n",
      "Current prediction:  61.22356414794922 \n",
      "\n",
      "Iteration 12024, Loss: 37.67425537109375, L1: 10.544042587280273, L3: 27.130212783813477\n",
      "Current prediction:  61.21478271484375 \n",
      "\n",
      "Iteration 12025, Loss: 36.0550422668457, L1: 10.563093185424805, L3: 25.4919490814209\n",
      "Current prediction:  61.21269607543945 \n",
      "\n",
      "Iteration 12026, Loss: 36.880393981933594, L1: 10.561912536621094, L3: 26.3184814453125\n",
      "Current prediction:  61.20962142944336 \n",
      "\n",
      "Iteration 12027, Loss: 37.55466079711914, L1: 10.557459831237793, L3: 26.99720001220703\n",
      "Current prediction:  61.21162414550781 \n",
      "\n",
      "Iteration 12028, Loss: 37.54899215698242, L1: 10.560489654541016, L3: 26.988502502441406\n",
      "Current prediction:  61.21358108520508 \n",
      "\n",
      "Iteration 12029, Loss: 36.783966064453125, L1: 10.559066772460938, L3: 26.224899291992188\n",
      "Current prediction:  61.21650695800781 \n",
      "\n",
      "Iteration 12030, Loss: 36.783203125, L1: 10.550649642944336, L3: 26.232555389404297\n",
      "Current prediction:  61.22148513793945 \n",
      "\n",
      "Iteration 12031, Loss: 36.9556999206543, L1: 10.547922134399414, L3: 26.407777786254883\n",
      "Current prediction:  61.22504425048828 \n",
      "\n",
      "Iteration 12032, Loss: 36.34291076660156, L1: 10.547527313232422, L3: 25.795381546020508\n",
      "Current prediction:  61.22705078125 \n",
      "\n",
      "Iteration 12033, Loss: 35.77265930175781, L1: 10.541954040527344, L3: 25.2307071685791\n",
      "Current prediction:  61.232120513916016 \n",
      "\n",
      "Iteration 12034, Loss: 37.05326461791992, L1: 10.544600486755371, L3: 26.508663177490234\n",
      "Current prediction:  61.238224029541016 \n",
      "\n",
      "Iteration 12035, Loss: 36.043701171875, L1: 10.522781372070312, L3: 25.520919799804688\n",
      "Current prediction:  61.24306869506836 \n",
      "\n",
      "Iteration 12036, Loss: 37.918296813964844, L1: 10.535954475402832, L3: 27.382341384887695\n",
      "Current prediction:  61.24587631225586 \n",
      "\n",
      "Iteration 12037, Loss: 37.135616302490234, L1: 10.534122467041016, L3: 26.60149383544922\n",
      "Current prediction:  61.24461364746094 \n",
      "\n",
      "Iteration 12038, Loss: 35.4865837097168, L1: 10.53327465057373, L3: 24.95330810546875\n",
      "Current prediction:  61.245601654052734 \n",
      "\n",
      "Iteration 12039, Loss: 37.84583282470703, L1: 10.52584171295166, L3: 27.319990158081055\n",
      "Current prediction:  61.24441909790039 \n",
      "\n",
      "Iteration 12040, Loss: 35.731483459472656, L1: 10.535240173339844, L3: 25.196243286132812\n",
      "Current prediction:  61.240806579589844 \n",
      "\n",
      "Iteration 12041, Loss: 37.16822052001953, L1: 10.556178092956543, L3: 26.612043380737305\n",
      "Current prediction:  61.238521575927734 \n",
      "\n",
      "Iteration 12042, Loss: 34.97157287597656, L1: 10.547295570373535, L3: 24.424278259277344\n",
      "Current prediction:  61.23972702026367 \n",
      "\n",
      "Iteration 12043, Loss: 36.51283645629883, L1: 10.537196159362793, L3: 25.97564125061035\n",
      "Current prediction:  61.24082946777344 \n",
      "\n",
      "Iteration 12044, Loss: 36.881065368652344, L1: 10.53577995300293, L3: 26.34528350830078\n",
      "Current prediction:  61.24015426635742 \n",
      "\n",
      "Iteration 12045, Loss: 36.75600051879883, L1: 10.526201248168945, L3: 26.229799270629883\n",
      "Current prediction:  61.239906311035156 \n",
      "\n",
      "Iteration 12046, Loss: 37.24541473388672, L1: 10.539795875549316, L3: 26.705617904663086\n",
      "Current prediction:  61.23775100708008 \n",
      "\n",
      "Iteration 12047, Loss: 35.541805267333984, L1: 10.541876792907715, L3: 24.999927520751953\n",
      "Current prediction:  61.23696517944336 \n",
      "\n",
      "Iteration 12048, Loss: 36.37746047973633, L1: 10.547408103942871, L3: 25.830053329467773\n",
      "Current prediction:  61.237823486328125 \n",
      "\n",
      "Iteration 12049, Loss: 36.59880065917969, L1: 10.534412384033203, L3: 26.064388275146484\n",
      "Current prediction:  61.23841857910156 \n",
      "\n",
      "Iteration 12050, Loss: 36.30049133300781, L1: 10.54378890991211, L3: 25.756702423095703\n",
      "Current prediction:  61.23796081542969 \n",
      "\n",
      "Iteration 12051, Loss: 36.62034606933594, L1: 10.540809631347656, L3: 26.07953643798828\n",
      "Current prediction:  61.23690414428711 \n",
      "\n",
      "Iteration 12052, Loss: 36.07273864746094, L1: 10.54202938079834, L3: 25.530710220336914\n",
      "Current prediction:  61.2387580871582 \n",
      "\n",
      "Iteration 12053, Loss: 37.06324005126953, L1: 10.542425155639648, L3: 26.52081298828125\n",
      "Current prediction:  61.238643646240234 \n",
      "\n",
      "Iteration 12054, Loss: 36.333412170410156, L1: 10.541729927062988, L3: 25.791683197021484\n",
      "Current prediction:  61.24196243286133 \n",
      "\n",
      "Iteration 12055, Loss: 36.379722595214844, L1: 10.538384437561035, L3: 25.841337203979492\n",
      "Current prediction:  61.24300765991211 \n",
      "\n",
      "Iteration 12056, Loss: 36.72606658935547, L1: 10.537211418151855, L3: 26.188854217529297\n",
      "Current prediction:  61.245849609375 \n",
      "\n",
      "Iteration 12057, Loss: 36.14117431640625, L1: 10.54619312286377, L3: 25.594982147216797\n",
      "Current prediction:  61.24773025512695 \n",
      "\n",
      "Iteration 12058, Loss: 35.843997955322266, L1: 10.529755592346191, L3: 25.314241409301758\n",
      "Current prediction:  61.252559661865234 \n",
      "\n",
      "Iteration 12059, Loss: 36.439796447753906, L1: 10.536614418029785, L3: 25.903182983398438\n",
      "Current prediction:  61.260040283203125 \n",
      "\n",
      "Iteration 12060, Loss: 36.37342834472656, L1: 10.516958236694336, L3: 25.856468200683594\n",
      "Current prediction:  61.26797103881836 \n",
      "\n",
      "Iteration 12061, Loss: 37.20539855957031, L1: 10.520873069763184, L3: 26.684524536132812\n",
      "Current prediction:  61.274314880371094 \n",
      "\n",
      "Iteration 12062, Loss: 36.24146270751953, L1: 10.508565902709961, L3: 25.732898712158203\n",
      "Current prediction:  61.281795501708984 \n",
      "\n",
      "Iteration 12063, Loss: 36.00404739379883, L1: 10.499894142150879, L3: 25.504154205322266\n",
      "Current prediction:  61.287132263183594 \n",
      "\n",
      "Iteration 12064, Loss: 36.428104400634766, L1: 10.49280834197998, L3: 25.9352970123291\n",
      "Current prediction:  61.2899169921875 \n",
      "\n",
      "Iteration 12065, Loss: 36.830238342285156, L1: 10.487545013427734, L3: 26.342693328857422\n",
      "Current prediction:  61.294715881347656 \n",
      "\n",
      "Iteration 12066, Loss: 36.156898498535156, L1: 10.493631362915039, L3: 25.66326904296875\n",
      "Current prediction:  61.2988166809082 \n",
      "\n",
      "Iteration 12067, Loss: 36.256649017333984, L1: 10.496333122253418, L3: 25.760316848754883\n",
      "Current prediction:  61.301055908203125 \n",
      "\n",
      "Iteration 12068, Loss: 36.583038330078125, L1: 10.490028381347656, L3: 26.09300994873047\n",
      "Current prediction:  61.3005485534668 \n",
      "\n",
      "Iteration 12069, Loss: 36.14488983154297, L1: 10.489660263061523, L3: 25.655231475830078\n",
      "Current prediction:  61.30073165893555 \n",
      "\n",
      "Iteration 12070, Loss: 36.48856735229492, L1: 10.497411727905273, L3: 25.99115562438965\n",
      "Current prediction:  61.29807662963867 \n",
      "\n",
      "Iteration 12071, Loss: 36.68706512451172, L1: 10.4933500289917, L3: 26.193716049194336\n",
      "Current prediction:  61.298736572265625 \n",
      "\n",
      "Iteration 12072, Loss: 36.4345703125, L1: 10.49600601196289, L3: 25.938566207885742\n",
      "Current prediction:  61.29960250854492 \n",
      "\n",
      "Iteration 12073, Loss: 36.2985725402832, L1: 10.481122970581055, L3: 25.81744956970215\n",
      "Current prediction:  61.30064392089844 \n",
      "\n",
      "Iteration 12074, Loss: 36.48802185058594, L1: 10.49380111694336, L3: 25.994218826293945\n",
      "Current prediction:  61.30302047729492 \n",
      "\n",
      "Iteration 12075, Loss: 35.38045120239258, L1: 10.482428550720215, L3: 24.89802360534668\n",
      "Current prediction:  61.305477142333984 \n",
      "\n",
      "Iteration 12076, Loss: 36.47528076171875, L1: 10.483420372009277, L3: 25.991859436035156\n",
      "Current prediction:  61.3092041015625 \n",
      "\n",
      "Iteration 12077, Loss: 37.23747634887695, L1: 10.484248161315918, L3: 26.75322723388672\n",
      "Current prediction:  61.310935974121094 \n",
      "\n",
      "Iteration 12078, Loss: 36.8537483215332, L1: 10.47754955291748, L3: 26.376197814941406\n",
      "Current prediction:  61.309410095214844 \n",
      "\n",
      "Iteration 12079, Loss: 35.842987060546875, L1: 10.480339050292969, L3: 25.36264991760254\n",
      "Current prediction:  61.30961608886719 \n",
      "\n",
      "Iteration 12080, Loss: 36.642635345458984, L1: 10.479960441589355, L3: 26.162673950195312\n",
      "Current prediction:  61.309879302978516 \n",
      "\n",
      "Iteration 12081, Loss: 36.38787078857422, L1: 10.485343933105469, L3: 25.902528762817383\n",
      "Current prediction:  61.307884216308594 \n",
      "\n",
      "Iteration 12082, Loss: 36.66108322143555, L1: 10.477527618408203, L3: 26.183555603027344\n",
      "Current prediction:  61.30309295654297 \n",
      "\n",
      "Iteration 12083, Loss: 36.198814392089844, L1: 10.48257827758789, L3: 25.716238021850586\n",
      "Current prediction:  61.29874038696289 \n",
      "\n",
      "Iteration 12084, Loss: 35.953269958496094, L1: 10.49844741821289, L3: 25.454824447631836\n",
      "Current prediction:  61.2937126159668 \n",
      "\n",
      "Iteration 12085, Loss: 36.94523239135742, L1: 10.499629974365234, L3: 26.445602416992188\n",
      "Current prediction:  61.28627395629883 \n",
      "\n",
      "Iteration 12086, Loss: 36.02350616455078, L1: 10.510589599609375, L3: 25.512916564941406\n",
      "Current prediction:  61.2765998840332 \n",
      "\n",
      "Iteration 12087, Loss: 36.4984016418457, L1: 10.515386581420898, L3: 25.983015060424805\n",
      "Current prediction:  61.26636505126953 \n",
      "\n",
      "Iteration 12088, Loss: 37.05014419555664, L1: 10.517784118652344, L3: 26.532360076904297\n",
      "Current prediction:  61.25473403930664 \n",
      "\n",
      "Iteration 12089, Loss: 37.327632904052734, L1: 10.544927597045898, L3: 26.782705307006836\n",
      "Current prediction:  61.242958068847656 \n",
      "\n",
      "Iteration 12090, Loss: 37.471656799316406, L1: 10.541543960571289, L3: 26.93011474609375\n",
      "Current prediction:  61.23136520385742 \n",
      "\n",
      "Iteration 12091, Loss: 36.288761138916016, L1: 10.549806594848633, L3: 25.738954544067383\n",
      "Current prediction:  61.22319030761719 \n",
      "\n",
      "Iteration 12092, Loss: 36.58930206298828, L1: 10.554571151733398, L3: 26.034732818603516\n",
      "Current prediction:  61.2161979675293 \n",
      "\n",
      "Iteration 12093, Loss: 35.243160247802734, L1: 10.566450119018555, L3: 24.67671012878418\n",
      "Current prediction:  61.21397399902344 \n",
      "\n",
      "Iteration 12094, Loss: 36.44055938720703, L1: 10.564888000488281, L3: 25.875669479370117\n",
      "Current prediction:  61.21538543701172 \n",
      "\n",
      "Iteration 12095, Loss: 37.45697021484375, L1: 10.568493843078613, L3: 26.88847541809082\n",
      "Current prediction:  61.21630859375 \n",
      "\n",
      "Iteration 12096, Loss: 37.0577392578125, L1: 10.569640159606934, L3: 26.488100051879883\n",
      "Current prediction:  61.21805953979492 \n",
      "\n",
      "Iteration 12097, Loss: 36.636634826660156, L1: 10.557239532470703, L3: 26.07939338684082\n",
      "Current prediction:  61.2203254699707 \n",
      "\n",
      "Iteration 12098, Loss: 36.49031066894531, L1: 10.550127029418945, L3: 25.940181732177734\n",
      "Current prediction:  61.22315216064453 \n",
      "\n",
      "Iteration 12099, Loss: 36.93695068359375, L1: 10.562826156616211, L3: 26.37412452697754\n",
      "Current prediction:  61.22614669799805 \n",
      "\n",
      "Iteration 12100, Loss: 37.04519271850586, L1: 10.558703422546387, L3: 26.486488342285156\n",
      "Current prediction:  61.23168182373047 \n",
      "\n",
      "Iteration 12101, Loss: 36.94613265991211, L1: 10.556575775146484, L3: 26.389556884765625\n",
      "Current prediction:  61.23935317993164 \n",
      "\n",
      "Iteration 12102, Loss: 36.526145935058594, L1: 10.543169021606445, L3: 25.98297691345215\n",
      "Current prediction:  61.25004577636719 \n",
      "\n",
      "Iteration 12103, Loss: 36.479408264160156, L1: 10.532258987426758, L3: 25.9471492767334\n",
      "Current prediction:  61.257137298583984 \n",
      "\n",
      "Iteration 12104, Loss: 36.156333923339844, L1: 10.527423858642578, L3: 25.6289119720459\n",
      "Current prediction:  61.261207580566406 \n",
      "\n",
      "Iteration 12105, Loss: 36.87603759765625, L1: 10.526907920837402, L3: 26.349130630493164\n",
      "Current prediction:  61.2659797668457 \n",
      "\n",
      "Iteration 12106, Loss: 37.11800765991211, L1: 10.523911476135254, L3: 26.594097137451172\n",
      "Current prediction:  61.27042007446289 \n",
      "\n",
      "Iteration 12107, Loss: 37.030189514160156, L1: 10.514991760253906, L3: 26.515199661254883\n",
      "Current prediction:  61.274322509765625 \n",
      "\n",
      "Iteration 12108, Loss: 35.679039001464844, L1: 10.509024620056152, L3: 25.170013427734375\n",
      "Current prediction:  61.27769470214844 \n",
      "\n",
      "Iteration 12109, Loss: 37.36997985839844, L1: 10.507303237915039, L3: 26.86267852783203\n",
      "Current prediction:  61.27824401855469 \n",
      "\n",
      "Iteration 12110, Loss: 36.38040542602539, L1: 10.503863334655762, L3: 25.876543045043945\n",
      "Current prediction:  61.277427673339844 \n",
      "\n",
      "Iteration 12111, Loss: 36.367610931396484, L1: 10.510725975036621, L3: 25.856884002685547\n",
      "Current prediction:  61.277164459228516 \n",
      "\n",
      "Iteration 12112, Loss: 36.565696716308594, L1: 10.518973350524902, L3: 26.046722412109375\n",
      "Current prediction:  61.27647399902344 \n",
      "\n",
      "Iteration 12113, Loss: 36.60726547241211, L1: 10.510679244995117, L3: 26.096586227416992\n",
      "Current prediction:  61.276695251464844 \n",
      "\n",
      "Iteration 12114, Loss: 35.781524658203125, L1: 10.512992858886719, L3: 25.26853370666504\n",
      "Current prediction:  61.27772521972656 \n",
      "\n",
      "Iteration 12115, Loss: 36.729862213134766, L1: 10.514181137084961, L3: 26.215681076049805\n",
      "Current prediction:  61.2775764465332 \n",
      "\n",
      "Iteration 12116, Loss: 36.980552673339844, L1: 10.516865730285645, L3: 26.463687896728516\n",
      "Current prediction:  61.27006912231445 \n",
      "\n",
      "Iteration 12117, Loss: 36.1895751953125, L1: 10.627178192138672, L3: 25.562397003173828\n",
      "Current prediction:  61.260414123535156 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DGM.DGMNet(layer_width, n_layers, input_dim=input_dim)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=5000, gamma=0.5) \n",
    "iteration_counter = 0\n",
    "\n",
    "t_0 = torch.tensor([[0.0]], dtype=torch.float32)         # t = 0 (shape: [1, 1])\n",
    "x_100 = torch.ones((1, d), dtype=torch.float32) * 100    # x = (100, ..., 100) (shape: [1, d])\n",
    "\n",
    "for i in range(sampling_stages):\n",
    "    # Sample time-space pairs\n",
    "    t_interior, S_interior, t_terminal, S_terminal = sampler(nSim_interior, nSim_terminal)\n",
    "\n",
    "    iteration_counter += 1\n",
    "    \n",
    "    # Perform the training steps\n",
    "    for _ in range(steps_per_sample):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute the loss\n",
    "        L1, L3 = loss(model, t_interior, S_interior, t_terminal, S_terminal)\n",
    "        total_loss = L1 + L3\n",
    "        \n",
    "        # Backpropagate\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step()\n",
    "    if iteration_counter % 1000 == 0:\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"â†³ LR reduced to {current_lr:.1e} at iteration {iteration_counter}\", '\\n')\n",
    "        \n",
    "    \n",
    "    print(f\"Iteration {i}, Loss: {total_loss.item()}, L1: {L1.item()}, L3: {L3.item()}\")\n",
    "    print(\"Current prediction: \", model(t_0, x_100).item(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a744f965-d384-4e43-9f03-60ed34b7fee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a03d834-25ee-404e-b905-c2be33cb821e",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c5bcd37-fa1f-46c4-89ff-599c58c4cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model value at t=0, x=(100, ..., 100): 66.86580657958984\n"
     ]
    }
   ],
   "source": [
    "t_0 = torch.tensor([[0.0]], dtype=torch.float32)         # t = 0 (shape: [1, 1])\n",
    "x_100 = torch.ones((1, d), dtype=torch.float32) * 100    # x = (100, ..., 100) (shape: [1, d])\n",
    "\n",
    "model_value = model(t_0, x_100)\n",
    "print(f\"Model value at t=0, x=(100, ..., 100): {model_value.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737fe183-a263-4957-9737-a50259e5cf86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
